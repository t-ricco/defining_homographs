{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Kojak\n",
    "\n",
    "** The problem  **\n",
    "\n",
    "We will attempt to identify amboguously defined words - words that are homographs (spelled the same, but with multiple meanings) and determine the exact meaning of the word from a context window.\n",
    "\n",
    "Here we attempt to do this in a few stages\n",
    "1. train a word embedding on some training corpus using skip-gram (Here we use 1000 sholarly research papers) \n",
    "2. identify common homographs and extract the various context windows\n",
    "3. interpret the context windows as vectors in the embedding space and appy a clustering algorith (DBSCAN). Each cluster is interpreted as a distinct definition of the homograph. Each cluster then is representative vector.\n",
    "4. apply to a test corpus - match context of given homograph to most similar group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare stopwords, preprocess the data from source file\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop += ['?','!',':',';','[',']','[]','“' ]\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“', '?', '!', '’', 'et', 'al', 'al.']\n",
    "stop = set(stop)\n",
    "\n",
    "class MyPapers(object):\n",
    "    # a memory-friendly way to load a large corpora\n",
    "     def __init__(self, dirname):\n",
    "            self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "        with open(self.dirname) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "        # iterate through all file names in our directory\n",
    "        for paper in data:\n",
    "            try:\n",
    "                line = [word for word in paper['full_text'].lower().split() if word not in stop]\n",
    "                line = [re.sub(r'[?\\.,!:;\\(\\)“\\[\\]]',' ',l) for l in line]\n",
    "                yield line\n",
    "            except:\n",
    "                print(\"Empty document found\")\n",
    "                continue\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Context Window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare what word we are searchig for\n",
    "target = u'extract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate iterable on the data\n",
    "\n",
    "#papers is an iterable of scholarly papers, tokenized for prcessing\n",
    "papers = MyPapers('data/train_data.json') \n",
    "\n",
    "# target_corpus will be a list of ony those papers containing the target word\n",
    "target_corpus = []\n",
    "\n",
    "for paper in papers:\n",
    "    if target in paper:\n",
    "        target_corpus.append(paper)\n",
    "        \n",
    "len(target_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arguments: target word and a starting number\n",
    "# Prints all context windows for the target word for 10 papers in \n",
    "\n",
    "def print_10_contexts(target, paper_start, target_corpus):\n",
    "    \n",
    "    paper_count = 0\n",
    "    print(target.upper())\n",
    "    while paper_count < 10:\n",
    "    #for paper_num in range(paper_start,paper_start + 10 ):\n",
    "        try:\n",
    "            paper = target_corpus[paper_start + paper_count]\n",
    "            paper_count += 1\n",
    "            windows = generate_windows([paper],6)\n",
    "            count = 1\n",
    "        except:\n",
    "            break\n",
    "        print('\\nPAPER {}'.format(paper_count))\n",
    "        for w in windows:\n",
    "            if w[0] == target:\n",
    "                print('{}: {}'.format(count, make_sentence(w[1])))\n",
    "                count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes a list of strings (words_list)\n",
    "# Returns single string of all words in words_list seperated by a white space.\n",
    "\n",
    "def make_sentence(words_list):\n",
    "    return ''.join([word + ' ' for word in words_list]).encode('utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#paper_start = 50\n",
    "\n",
    "#print_10_contexts(target, paper_start, target_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize the model\n",
    "\n",
    "model = gensim.models.word2vec.Word2Vec(sentences = papers, size=300, window=6, min_count=1, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2848"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"data/journal.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200481"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** contexts to vectors **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# The function takes as arguments a list of tokenized documents and a window size\n",
    "# and returns each word in the document along with its window context as a tuple\n",
    "\n",
    "def generate_word_counts(documents, window_size = 6):\n",
    "    maxlen = window_size*2\n",
    "    counts = Counter()\n",
    "    \n",
    "    for document in documents:\n",
    "        L = len(document)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(document):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            context_words = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    #in_words.append([word])\n",
    "                    context_words.append(document[i])\n",
    "            x = word\n",
    "            y = context_words\n",
    "            counts[x] += 1\n",
    "            for _ in y:\n",
    "                counts[_] += 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = generate_word_counts(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes list of word tokens as arguments\n",
    "# Returns a list of vectors whose components are the arithmetic mean of the \n",
    "# corresponding component of all of the input vectors\n",
    "\n",
    "def get_vectors(word_list):\n",
    "    vecs = []\n",
    "    for word in word_list:\n",
    "        vecs.append(vectors[word])        \n",
    "    return vecs\n",
    "\n",
    "# Takes list of vectors as arguments\n",
    "# Returns a single vector whose components are the arithmetic mean of the \n",
    "# corresponding component of all of the input vectors\n",
    "\n",
    "def vector_average(vector_list):\n",
    "    A = np.array(vector_list)\n",
    "    dim = A.shape[0]\n",
    "    ones = np.ones(dim)\n",
    "    return ones.dot(A)/dim\n",
    "\n",
    "# Takes list of tokenized documents, target word and window size as arguments\n",
    "# Returns list of vectors where each vector represents the context window \n",
    "# of the target word in the word embedding space\n",
    "\n",
    "def context2vectors(documents,target,window_size = 6):\n",
    "\n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        if target in document:\n",
    "            windows = generate_windows([document],window_size)\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append(vector_average2(get_vectors(w[1])))\n",
    "                    \n",
    "    return context_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes list of vectors as arguments\n",
    "# Returns a single vector whose components are the arithmetic mean of the \n",
    "# corresponding component of all of the input vectors weighted by Inverse Document Frecuency\n",
    "\n",
    "def vector_average2(words): #, word_counts, vectors):\n",
    "    total = sum(list(word_counts.values()))\n",
    "    words = [x for x in words if x in list(vectors.vocab.keys())]\n",
    "    vector_list = list(map((lambda x: vectors[x]*np.log((1 + total)/(1 + word_counts[x]))),words))\n",
    "    vector_sum = reduce((lambda x,y: np.add(x,y)),vector_list)\n",
    "    weighted_average = (1.0/len(words))*vector_sum\n",
    "    \n",
    "    return weighted_average\n",
    "\n",
    "# Takes list of tokenized documents, target word and window size as arguments\n",
    "# Returns list of vectors where each vector represents the context window \n",
    "# of the target word in the word embedding space\n",
    "\n",
    "def context2vectors2(documents,target,window_size = 6):\n",
    "\n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        if target in document:\n",
    "            windows = generate_windows([document],window_size)\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append(vector_average2(w[1]))\n",
    "                    \n",
    "    return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40521717e+00,  -1.19343948e+00,  -1.42371440e+00,\n",
       "         8.04274201e-01,  -9.90902126e-01,  -6.63835406e-01,\n",
       "        -1.86185598e-01,  -8.15678716e-01,  -1.22230339e+00,\n",
       "         4.86684978e-01,   9.18192387e-01,   6.61811233e-02,\n",
       "         2.46386677e-01,   1.81828976e-01,   6.29535437e-01,\n",
       "         1.30542898e+00,  -1.30601048e+00,  -1.55078506e+00,\n",
       "         2.45777607e+00,  -5.20702481e-01,   8.27073097e-01,\n",
       "         2.75134623e-01,   2.69613647e+00,   6.78881824e-01,\n",
       "         7.89619327e-01,   3.08323646e+00,  -9.20088410e-01,\n",
       "         1.98221743e-01,  -1.96112704e+00,   6.03531599e-01,\n",
       "        -1.56243110e+00,  -2.66677094e+00,  -1.92627811e+00,\n",
       "        -1.79501092e+00,   3.01403999e+00,   4.84251618e-01,\n",
       "        -3.92093658e+00,   3.61459541e+00,  -3.49280381e+00,\n",
       "         2.02580500e+00,  -1.61408472e+00,   3.01098049e-01,\n",
       "         1.93309903e-01,  -7.72316515e-01,   6.16106629e-01,\n",
       "         1.62125945e-01,  -1.94880807e+00,   1.47990203e+00,\n",
       "         1.53829885e+00,  -9.12607789e-01,  -3.08967471e-01,\n",
       "        -3.43533218e-01,  -2.84629011e+00,   1.32865357e+00,\n",
       "        -4.85321045e+00,   1.40155184e+00,  -1.05326593e-01,\n",
       "        -8.06596041e-01,  -8.97936940e-01,   5.07516384e-01,\n",
       "         1.71619034e+00,   1.83397627e+00,   2.54606676e+00,\n",
       "         1.02870703e-01,  -1.02903509e+00,   7.79870629e-01,\n",
       "        -1.74467623e-01,   1.53261161e+00,  -2.15309334e+00,\n",
       "         1.23335707e+00,   7.39260256e-01,  -1.75054312e-01,\n",
       "        -6.85392737e-01,  -5.85691184e-02,   5.07075429e-01,\n",
       "        -1.14728570e-01,   2.38327932e+00,   1.98349905e+00,\n",
       "         1.09440458e+00,   2.09805846e+00,   1.58274853e+00,\n",
       "         1.60762000e+00,  -2.39218354e-01,   1.97529173e+00,\n",
       "         4.46968526e-03,   3.00477505e-01,  -6.11027181e-01,\n",
       "        -2.38717318e+00,   2.53786683e+00,   8.75768185e-01,\n",
       "         1.70577332e-01,   1.48427105e+00,   3.46307725e-01,\n",
       "         1.56473994e+00,   5.05850792e-01,  -1.52659774e+00,\n",
       "         1.97920859e-01,   2.08800030e+00,   2.99549967e-01,\n",
       "        -9.33882236e-01,  -1.70152271e+00,  -1.01225591e+00,\n",
       "         1.04484081e-01,   1.29320669e+00,   1.35242796e+00,\n",
       "         2.32215357e+00,   4.28075981e+00,   1.08576334e+00,\n",
       "         4.61942863e+00,  -3.12235385e-01,  -2.04968214e+00,\n",
       "        -1.65247583e+00,  -2.36740351e-01,   1.28947175e+00,\n",
       "         4.06018108e-01,  -1.19224191e+00,   2.02824640e+00,\n",
       "        -8.38305652e-01,  -3.81310153e+00,   9.65660274e-01,\n",
       "         1.15380991e+00,  -2.07793283e+00,  -1.23224258e+00,\n",
       "         1.91958237e+00,  -3.17681253e-01,   6.08084977e-01,\n",
       "        -3.18432093e+00,  -3.43001318e+00,  -5.47696233e-01,\n",
       "        -1.25829506e+00,  -4.27865893e-01,   1.10562694e+00,\n",
       "        -3.19369674e-01,  -1.82599592e+00,   1.31281435e+00,\n",
       "         1.95273668e-01,  -2.21802145e-02,   3.89880121e-01,\n",
       "         6.67613208e-01,  -8.20278376e-03,   4.62352216e-01,\n",
       "        -2.02983841e-01,  -2.76978064e+00,  -1.76934123e+00,\n",
       "         1.90012634e-01,  -1.35795343e+00,   2.16638461e-01,\n",
       "         7.40623236e-01,   1.38839555e+00,   2.63534486e-01,\n",
       "        -6.39559507e-01,   1.40392292e+00,   1.75514078e+00,\n",
       "         2.78233814e+00,  -7.93927073e-01,   9.68581438e-02,\n",
       "         9.92053375e-02,   9.90607977e-01,   4.47173655e-01,\n",
       "        -3.48706245e+00,   3.61531764e-01,   4.60504442e-02,\n",
       "        -1.84597576e+00,  -9.90184963e-01,   9.01246250e-01,\n",
       "         1.66302764e+00,  -1.30554914e+00,   1.06400180e+00,\n",
       "         5.01574636e-01,   9.05260503e-01,  -7.33513653e-01,\n",
       "         6.11078702e-02,   4.58748609e-01,   6.08885884e-01,\n",
       "        -6.10314608e-02,  -4.89637673e-01,  -8.16632509e-02,\n",
       "         3.50106120e-01,   1.09663308e+00,   1.04738176e-01,\n",
       "         2.15053856e-01,   2.05454850e+00,  -1.66553050e-01,\n",
       "         6.23640895e-01,   1.89695507e-01,  -1.93002272e+00,\n",
       "         1.10625148e+00,   1.02935755e+00,   2.11000860e-01,\n",
       "         2.63893461e+00,   1.91162753e+00,  -2.30932474e+00,\n",
       "        -4.68825042e-01,   1.54254460e+00,  -5.35965443e-01,\n",
       "         2.78230238e+00,  -1.26258206e+00,  -1.65126812e+00,\n",
       "        -5.72417617e-01,  -1.56312871e+00,   1.04092383e+00,\n",
       "        -3.22136736e+00,   4.69015300e-01,   2.09293675e+00,\n",
       "         3.18404555e-01,  -1.86226511e+00,  -1.04419875e+00,\n",
       "        -7.23292351e-01,   5.28595686e-01,   9.99473214e-01,\n",
       "        -2.94911802e-01,   1.35873705e-01,  -7.70229340e-01,\n",
       "         9.31704521e-01,   1.02040541e+00,   1.31389022e+00,\n",
       "         1.07054234e+00,   2.58385390e-01,  -1.13431132e+00,\n",
       "         9.23058927e-01,   1.34186161e+00,   9.64316189e-01,\n",
       "        -8.63817751e-01,  -1.32281637e+00,   1.48577023e+00,\n",
       "        -1.94162577e-01,   1.47436678e-01,  -7.09674954e-01,\n",
       "        -1.03927612e+00,   7.91020989e-02,  -5.74224591e-01,\n",
       "         2.16130495e+00,  -1.68416965e+00,   2.02714252e+00,\n",
       "        -2.71331787e-01,  -3.30108404e-01,   4.33687568e-02,\n",
       "        -5.51554799e-01,  -1.83521104e+00,  -8.72744501e-01,\n",
       "         4.82871383e-01,   1.47292542e+00,  -8.45734596e-01,\n",
       "        -1.78171277e+00,   4.49767828e-01,  -2.08060551e+00,\n",
       "        -2.58702826e+00,   2.31138158e+00,   6.30816698e-01,\n",
       "        -5.80716968e-01,  -9.94340599e-01,   3.42910111e-01,\n",
       "        -1.30700827e+00,   2.83019209e+00,   5.51970184e-01,\n",
       "        -9.96070206e-01,   2.07503891e+00,  -2.59367734e-01,\n",
       "        -3.58450502e-01,  -3.52294326e-01,  -3.56443882e+00,\n",
       "        -7.76604652e-01,  -7.88037896e-01,   1.35549152e+00,\n",
       "        -2.25440502e+00,   5.80226004e-01,   1.61631808e-01,\n",
       "        -3.04543555e-01,   6.39893055e-01,  -6.08820021e-01,\n",
       "        -3.46814394e-02,  -1.61612141e+00,   1.70712304e+00,\n",
       "        -2.60388702e-01,   9.64795589e-01,   1.19696510e+00,\n",
       "        -8.93074870e-01,   4.96240795e-01,  -1.25498688e+00,\n",
       "        -1.92536712e+00,  -6.77573740e-01,  -3.32091272e-01,\n",
       "        -1.17024446e+00,   1.03508449e+00,  -1.15740013e+00,\n",
       "         1.36213458e+00,   1.07157484e-01,   2.46620715e-01,\n",
       "        -5.48323393e-01,  -2.85670549e-01,   1.95557976e+00,\n",
       "         1.97771740e+00,  -2.41271883e-01,  -2.20275259e+00,\n",
       "        -7.31096148e-01,  -1.49518836e+00,  -5.68752170e-01,\n",
       "        -5.47270060e-01,  -9.47444320e-01,   2.04926705e+00], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toys = ['reduce', 'exhibit', 'uhuhuhuh']\n",
    "vector_average2(toys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.dictionary.Dictionary(papers)\n",
    "text = [dictionary.doc2bow(c) for c in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = u'extract'\n",
    "#papers = MyPapers('abstract_scraper/full.json')\n",
    "context_vectors = context2vectors2(papers, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Clustering with DBSAN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_cluster_context(cluster_number, documents, target, labels, window_size = 6):\n",
    "    `\n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        text = document\n",
    "        if target in text:\n",
    "            #print(target)\n",
    "            windows = generate_windows([text],window_size)\n",
    "            #print windows[:2]\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append((w[1]))\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == cluster_number:\n",
    "            print(context_vectors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps = .085, metric = 'cosine', algorithm = 'brute', min_samples = 5)\n",
    "dbscan.fit(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = dbscan.labels_\n",
    "n_clusters = len(set(labels)) # - (1 if -1 in labels else 0)\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1,  0, -1, -1, -1,  0,  0, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1,  0,  0,  3,  0,  0,  0, -1, -1, -1, -1, -1,  0,  0,  0,  0,  0,\n",
       "        0, -1,  1, -1, -1,  1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1,  0,\n",
       "        0,  0,  0, -1, -1, -1,  0, -1, -1, -1,  1,  1,  1, -1, -1,  1, -1,\n",
       "        0,  0, -1,  2, -1, -1,  2, -1,  0,  0,  0, -1, -1,  0,  0, -1,  0,\n",
       "        0,  0,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  0, -1,\n",
       "       -1,  0,  0, -1, -1, -1, -1,  0,  0,  0,  0,  0, -1, -1, -1, -1,  2,\n",
       "       -1,  1,  1,  1,  1,  0,  0,  3, -1,  0, -1, -1,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0, -1,  0,  0,  2, -1, -1, -1,  0,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0,  0, -1,  0,  0,  0,  0,  0,  3,  0, -1, -1, -1, -1, -1, -1,\n",
       "        0,  0,  0,  0, -1,  2, -1,  1,  0,  0, -1,  0,  0,  0, -1, -1, -1,\n",
       "        1,  1, -1,  1,  0, -1,  0, -1, -1, -1, -1,  0,  0, -1, -1,  1,  1,\n",
       "        1,  1,  1,  1,  1, -1,  1,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1, -1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1, -1,  1,  1, -1,  1, -1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1, -1,  1, -1,  1,  1, -1,  1, -1,  1,  1, -1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  0,  0,  0,  0,  0, -1,\n",
       "       -1,  1,  1,  0,  3,  0, -1, -1,  3,  3,  0, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1, -1,  0,  0, -1,  0,  0, -1, -1,  0,  1, -1,  0, -1, -1,\n",
       "        0, -1, -1,  0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_cluster_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-56d27447a86d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#papers = MyPapers('abstract_scraper/full.json')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint_cluster_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpapers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'print_cluster_context' is not defined"
     ]
    }
   ],
   "source": [
    "#papers = MyPapers('abstract_scraper/full.json')\n",
    "\n",
    "print_cluster_context(0,papers, target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='cosine', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='complete',\n",
       "            memory=Memory(cachedir=None), n_clusters=4,\n",
       "            pooling_func=<function mean at 0x7fa1180f8488>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agglomorative clustering\n",
    "\n",
    "ag = AgglomerativeClustering(n_clusters = 4, affinity = 'cosine', linkage = 'complete')\n",
    "ag.fit(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "labels = ag.labels_\n",
    "n_clusters = len(set(labels)) # - (1 if -1 in labels else 0)\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dimension Reduction **\n",
    "\n",
    "Since vectors are dimension 100+, DBSCAN is ineffective. Other clustering algorithms are successful at seperating into a predetermined number of clusters which correspond to different definitions. However, If a word has more or less than these number of actual definitions, then this is counter-fproductive. \n",
    "\n",
    "In order for DBSCAN to me more effective, we will apply Singular Value Decomposition in order to reduce dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=3)\n",
    "X = svd.fit_transform(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.09653699e+00   9.86560049e-01  -4.98928487e-02]\n",
      " [  1.94450880e+00  -6.11871523e-01   3.64613031e-01]\n",
      " [  2.03454784e+00  -1.19563971e-01   3.25418990e-01]\n",
      " [  2.09710585e+00  -1.30777934e-01  -6.34665916e-02]\n",
      " [  2.02457269e+00   3.07932152e-01   7.13693917e-02]\n",
      " [  1.91288530e+00   3.58860554e-01   1.25679875e-01]\n",
      " [  2.12255555e+00   4.89122976e-01  -5.58709171e-02]\n",
      " [  1.87757986e+00   9.92744570e-01   7.74797185e-01]\n",
      " [  2.07143207e+00  -4.44112490e-01   6.42279786e-02]\n",
      " [  1.86279062e+00  -4.43689195e-01   1.54736463e-01]\n",
      " [  1.92619547e+00  -1.95084935e-01   3.89550796e-01]\n",
      " [  2.12316574e+00  -4.92781616e-01   3.06446612e-01]\n",
      " [  2.25374134e+00  -4.70839000e-01   2.59099326e-01]\n",
      " [  1.99044402e+00  -4.46640690e-01   3.91463825e-01]\n",
      " [  1.97654672e+00  -2.35431915e-01   1.46650506e-01]\n",
      " [  1.69957163e+00  -4.33071575e-01   5.19589092e-01]\n",
      " [  2.07675030e+00  -4.04436838e-01  -7.57310571e-02]\n",
      " [  1.97515014e+00  -4.23584322e-01   2.21571590e-01]\n",
      " [  1.92706746e+00  -5.85987933e-01   2.19178048e-01]\n",
      " [  1.89641972e+00  -4.51138423e-01   1.61572131e-01]\n",
      " [  1.95675962e+00  -2.58089669e-02   2.70956236e-02]\n",
      " [  1.95405503e+00  -5.47306481e-01   1.86030140e-01]\n",
      " [  1.95524075e+00  -5.67056264e-01   2.70487881e-01]\n",
      " [  1.96528665e+00  -1.52765929e-01   2.02224289e-02]\n",
      " [  1.99954107e+00  -1.22034720e-01   2.80509983e-02]\n",
      " [  1.93176094e+00  -4.75367204e-01   3.32918758e-01]\n",
      " [  2.15394162e+00  -5.26937682e-01   2.82502367e-01]\n",
      " [  2.13676850e+00  -4.73233227e-01   3.33141940e-03]\n",
      " [  2.19929907e+00  -3.00240348e-01  -1.43153858e-01]\n",
      " [  1.68154763e+00  -3.72184737e-01   1.12544602e-01]\n",
      " [  1.99210544e+00  -8.65967881e-02  -1.94235702e-01]\n",
      " [  1.80241786e+00  -5.20580509e-01   9.01772197e-02]\n",
      " [  1.82886393e+00  -1.00589296e-01   1.01976131e-02]\n",
      " [  2.17761874e+00   1.92297586e-01  -3.02764915e-01]\n",
      " [  1.93774430e+00   4.94390658e-01   1.91469785e-01]\n",
      " [  1.57284052e+00   5.28086173e-01   2.68750940e-01]\n",
      " [  1.91443235e+00   9.94961876e-01   4.84263428e-01]\n",
      " [  2.08123235e+00   7.71631213e-01   2.41076809e-01]\n",
      " [  2.09854927e+00   5.83399597e-01   3.10620334e-01]\n",
      " [  1.99876190e+00   8.41039885e-01   2.50209907e-01]\n",
      " [  1.55778344e+00   2.75038607e-01   2.02795589e-01]\n",
      " [  1.90742092e+00  -4.41876337e-02   3.40472413e-01]\n",
      " [  2.02475493e+00  -5.50194665e-01   1.24164938e-01]\n",
      " [  1.93546439e+00  -4.49080783e-01   2.16843155e-01]\n",
      " [  2.09409637e+00  -7.49795140e-01   2.61065282e-01]\n",
      " [  2.16015424e+00  -7.72237228e-01   8.32795673e-02]\n",
      " [  2.19573643e+00  -5.87188077e-01   3.85074349e-02]\n",
      " [  2.08927386e+00  -2.82391064e-01   4.63907132e-02]\n",
      " [  2.16998242e+00  -5.41492814e-01   1.24418005e-01]\n",
      " [  1.84803868e+00  -2.87921307e-01   2.71389696e-01]\n",
      " [  2.04007593e+00  -6.48486322e-01  -6.46616097e-02]\n",
      " [  2.20810637e+00  -5.24041737e-01   1.39459948e-01]\n",
      " [  1.98666749e+00   3.14205441e-02   4.27576774e-02]\n",
      " [  2.23890913e+00   5.05985655e-01  -2.98091389e-02]\n",
      " [  1.99242637e+00   4.41300638e-01  -1.04601869e-01]\n",
      " [  1.93393560e+00   3.65599885e-01   3.82565042e-01]\n",
      " [  1.93531772e+00   9.00322597e-01   3.82944905e-01]\n",
      " [  1.93531772e+00   9.00322597e-01   3.82944905e-01]\n",
      " [  2.06558888e+00   8.64714073e-01   2.71118761e-01]\n",
      " [  1.87839526e+00   6.02719998e-01   2.73524329e-01]\n",
      " [  1.91354471e+00   4.43206884e-01   7.45499733e-01]\n",
      " [  2.19279468e+00  -4.46200855e-02  -3.74937220e-01]\n",
      " [  2.26968528e+00  -8.69218620e-02  -4.10888663e-01]\n",
      " [  2.19690749e+00  -1.97972808e-01  -4.54240956e-01]\n",
      " [  2.20489695e+00  -1.36396636e-01  -2.62884395e-01]\n",
      " [  2.20318504e+00   1.74096940e-01  -2.08537541e-01]\n",
      " [  2.32005142e+00   1.93271867e-01  -1.21909369e-01]\n",
      " [  1.62100490e+00   3.22182239e-01  -1.22505607e-01]\n",
      " [  2.09477070e+00   1.91932299e-01  -1.18094939e-01]\n",
      " [  2.20850085e+00   3.11012836e-01  -2.96628160e-01]\n",
      " [  1.94838395e+00   8.84013611e-01   3.01526499e-01]\n",
      " [  1.84496347e+00  -1.82339337e-01   4.22932653e-02]\n",
      " [  1.90141784e+00  -3.16937567e-01   4.70321471e-02]\n",
      " [  1.83728189e+00  -1.67351269e-01  -1.77318478e-02]\n",
      " [  1.82096563e+00  -6.62287040e-02  -2.72729423e-03]\n",
      " [  1.72542403e+00  -3.48533142e-01   1.79432452e-01]\n",
      " [  1.79977096e+00   5.60500220e-01   5.28723770e-02]\n",
      " [  1.72416363e+00   3.44233190e-01   6.34686517e-02]\n",
      " [  1.91443581e+00   7.18812158e-01   2.03870084e-01]\n",
      " [  2.29375380e+00  -4.55220568e-02  -4.26793554e-01]\n",
      " [  2.21676557e+00   2.31193076e-01  -3.80411599e-01]\n",
      " [  2.13299075e+00   3.46448452e-01  -5.00317261e-01]\n",
      " [  2.29224677e+00   4.11902292e-01  -5.13295275e-01]\n",
      " [  2.24175369e+00   7.23735444e-04  -5.22128352e-01]\n",
      " [  2.19482681e+00  -3.12792463e-02  -5.97902079e-01]\n",
      " [  2.20711091e+00  -1.48030459e-02  -5.81756138e-01]\n",
      " [  2.22782083e+00  -2.14771648e-01  -3.64237848e-01]\n",
      " [  2.14341969e+00  -1.37358641e-01  -3.62185967e-01]\n",
      " [  2.23780421e+00  -6.97735145e-02  -3.52848134e-01]\n",
      " [  2.25895860e+00  -2.02795725e-01  -3.10308758e-01]\n",
      " [  2.20488445e+00  -2.50582209e-01  -4.31872170e-01]\n",
      " [  2.01799388e+00  -3.06149640e-01  -4.11323625e-01]\n",
      " [  2.24529780e+00  -1.80238738e-01  -4.47568719e-01]\n",
      " [  2.19055478e+00  -9.12975419e-02  -1.38641233e-01]\n",
      " [  2.20504732e+00   8.64737854e-03  -2.31360989e-01]\n",
      " [  2.18795310e+00  -2.50063392e-04  -3.41870147e-01]\n",
      " [  2.06903600e+00   5.50145288e-02  -2.16478983e-01]\n",
      " [  2.37014908e+00  -4.89266686e-01  -3.42604138e-01]\n",
      " [  2.06456083e+00  -1.65243542e-01   1.74638778e-02]\n",
      " [  1.96484406e+00   5.44410938e-02   5.70644053e-02]\n",
      " [  1.92442553e+00   6.41113547e-01  -1.32265004e-02]\n",
      " [  1.95627766e+00   7.26942984e-01  -1.12221983e-01]\n",
      " [  1.85413935e+00   7.37050105e-01   1.79871525e-01]\n",
      " [  2.01565514e+00   8.65234620e-01  -1.88276296e-02]\n",
      " [  1.89051824e+00   6.47362241e-01   4.62313833e-02]\n",
      " [  1.99961980e+00   7.67024307e-01  -5.53495841e-02]\n",
      " [  2.14707360e+00   7.26216769e-01  -1.37388593e-01]\n",
      " [  1.90540278e+00   4.80821690e-01  -2.59048379e-01]\n",
      " [  1.74095563e+00   6.12177926e-01   7.01266498e-02]\n",
      " [  1.73341630e+00   6.50925588e-01   1.55605778e-02]\n",
      " [  2.16320137e+00   7.16806069e-01   2.24511104e-02]\n",
      " [  1.86959088e+00   5.86258519e-01   3.88325016e-01]\n",
      " [  2.12136034e+00   8.48305586e-01   2.71724787e-01]\n",
      " [  2.25824054e+00   7.01068214e-01   1.77735221e-01]\n",
      " [  2.00341780e+00   9.50545739e-01  -1.51159118e-01]\n",
      " [  2.10271358e+00  -4.68425022e-01   5.28282302e-01]\n",
      " [  1.82551170e+00  -3.58439714e-01   2.61113736e-01]\n",
      " [  1.73040027e+00  -2.87493773e-01   2.69692908e-01]\n",
      " [  1.88321499e+00  -4.56536329e-01   6.63489170e-01]\n",
      " [  2.17938148e+00  -3.86566683e-01   2.48305081e-01]\n",
      " [  1.82336976e+00  -2.54266663e-01  -3.88880605e-02]\n",
      " [  1.79505705e+00  -5.79746400e-02   6.46335804e-02]\n",
      " [  1.83386568e+00  -4.99443597e-01   3.95395326e-01]\n",
      " [  2.23127978e+00   2.57249440e-01  -2.59666889e-01]\n",
      " [  1.78275912e+00  -7.43808439e-01  -2.29396323e-01]\n",
      " [  1.75810208e+00  -9.73303625e-02   2.51580260e-02]\n",
      " [  2.01228900e+00  -3.02583125e-01   1.74787722e-02]\n",
      " [  2.12529338e+00  -7.01858638e-01  -3.47275156e-01]\n",
      " [  1.93035230e+00  -6.28157866e-01  -7.25841396e-02]\n",
      " [  1.94871082e+00  -5.32285180e-01  -6.60920896e-02]\n",
      " [  2.00098754e+00  -4.11776236e-01  -1.95915302e-01]\n",
      " [  2.19240315e+00  -6.47947954e-01  -9.71717493e-02]\n",
      " [  2.05771736e+00  -6.54561951e-01  -6.30345221e-02]\n",
      " [  2.20950770e+00  -3.56019778e-01  -1.14755205e-01]\n",
      " [  2.12112697e+00  -4.60218148e-01   8.85845061e-02]\n",
      " [  2.24099716e+00  -3.56862762e-01   8.89085242e-04]\n",
      " [  1.95840252e+00  -4.20778346e-01  -2.45350365e-01]\n",
      " [  2.12037741e+00  -5.14927600e-01   2.03257302e-01]\n",
      " [  2.25767323e+00  -5.07378372e-01  -1.57557922e-01]\n",
      " [  2.20479285e+00  -4.85302889e-01   1.85655767e-02]\n",
      " [  2.10972311e+00  -1.08869430e-01   3.49255295e-02]\n",
      " [  2.28819728e+00   4.90684413e-02  -1.18968220e-01]\n",
      " [  2.05625721e+00  -1.48113412e-01   1.75169000e-02]\n",
      " [  2.29093789e+00   5.37668969e-01  -5.52690587e-02]\n",
      " [  1.92592615e+00  -4.15210759e-01   1.93158298e-01]\n",
      " [  2.22048994e+00  -3.11977348e-01   4.37282408e-02]\n",
      " [  2.02052351e+00  -2.57973963e-01  -3.54401530e-03]\n",
      " [  2.28185205e+00  -8.72297766e-02  -4.28718545e-01]\n",
      " [  2.03184568e+00  -1.42519431e-02   5.51947126e-01]\n",
      " [  1.97964542e+00   3.26506005e-01  -3.06274154e-01]\n",
      " [  2.04172079e+00  -3.81126052e-01  -1.74628527e-01]\n",
      " [  2.34031070e+00   3.04616386e-01  -2.95022048e-01]\n",
      " [  1.95976748e+00  -3.84862410e-01  -3.74684088e-02]\n",
      " [  2.11220308e+00  -3.40947965e-01  -8.52293808e-02]\n",
      " [  1.94983592e+00  -3.54925874e-01   2.19408589e-02]\n",
      " [  2.26297739e+00  -3.12811524e-01  -2.43587593e-01]\n",
      " [  1.96941864e+00  -2.29096191e-01  -3.41292048e-02]\n",
      " [  1.82562609e+00  -2.39220569e-01   1.22055148e-02]\n",
      " [  1.87169545e+00  -4.25475214e-01   4.58867266e-02]\n",
      " [  1.72671065e+00   3.98080078e-02   6.97932459e-03]\n",
      " [  1.77612272e+00   5.74980486e-01   2.21418265e-03]\n",
      " [  1.93153016e+00   3.48652289e-01  -2.61451533e-01]\n",
      " [  2.14457921e+00   8.64410564e-02  -2.45881993e-01]\n",
      " [  2.27965263e+00  -1.92432208e-01  -1.98262270e-01]\n",
      " [  1.68917438e+00   6.30569289e-01   1.30325791e-01]\n",
      " [  1.73562357e+00   7.27463954e-01  -2.93842615e-02]\n",
      " [  2.17185693e+00  -2.31370402e-01   1.83987544e-01]\n",
      " [  2.06751235e+00  -2.11100176e-01   1.12585495e-01]\n",
      " [  2.17636161e+00  -2.12329899e-01   4.29716116e-02]\n",
      " [  2.19276122e+00  -4.43620047e-01   2.18603478e-01]\n",
      " [  1.95759611e+00  -4.09931213e-01   1.96196021e-01]\n",
      " [  1.96529496e+00  -1.63768432e-01   2.07676434e-01]\n",
      " [  2.13633039e+00   1.09029122e+00   3.76446319e-01]\n",
      " [  2.13105796e+00   1.01927451e+00  -4.33970156e-01]\n",
      " [  1.92862275e+00   5.86893179e-01  -5.40626432e-01]\n",
      " [  1.79393172e+00  -2.56346547e-01   5.28567117e-01]\n",
      " [  2.01381553e+00   1.11716014e+00  -2.81917162e-01]\n",
      " [  2.06052034e+00   4.28935338e-01  -4.19211699e-01]\n",
      " [  2.13417834e+00  -1.11842622e-01  -5.65153375e-02]\n",
      " [  1.94319033e+00   5.95502085e-01  -1.29107140e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='auto', eps=0.2, leaf_size=30, metric='euclidean',\n",
       "    min_samples=5, n_jobs=1, p=None)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps = .2, metric = 'euclidean', algorithm = 'auto')\n",
    "dbscan.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "labels = dbscan.labels_\n",
    "n_clusters = len(set(labels)) # - (1 if -1 in labels else 0)\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0,  0,  0, -1, -1,  1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        1, -1,  1,  1, -1,  1, -1,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0, -1,  1, -1,  1,  1,  1,  1, -1,  0,  0,  0,  0,  0,  0, -1,\n",
       "        0,  0,  1,  0,  0,  0,  0,  0,  1, -1,  1,  0,  0,  0, -1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  0,  0,  0, -1,\n",
       "        0,  0,  0,  0,  0, -1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0, -1,  1,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  0,  0,\n",
       "        0,  0, -1, -1, -1, -1, -1, -1,  0,  1])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attach', 'onto', 'surface', 'due', 'gravity ', 'electrostatic', 'mechanical', 'effects', ' wind', 'water', 'droplets  ', 'deposition ']\n",
      "['first', 'report', '[] ', 'increase', 'force', 'explained', 'accumulation', 'borders', 'microscopic', 'air', 'gap ', 'dielectrics']\n",
      "['speeds ', 'model', 'needs', 'modifications', 'account', 'virtual', 'decrease', 'due', 'relative', 'motions the', 'voltages', 'used']\n",
      "['broad', 'peak', 'centered', '~3', 'ev ', 'corresponding', 'transfer', 'absorption ', 'could', 'extract', 'band', 'gap']\n",
      "['[nbo]', 'octahedral', 'structures', 'favor', 'possible', 'delocalization', 'carriers', '[] ', 'secondly ', 'conduction', 'bands', 'consisting']\n",
      "['potential', 'h+/h', 'promote', 'separation', 'transfer', 'photo-induced', 'carriers', 'result', 'high', 'photocatalytic', 'activities', '[] ']\n",
      "['470', 'nm', 'linbo ', 'means', 'linbo', 'longer', 'carrier', 'lifetime', 'improved', 'efficiency', 'interfacial', 'charge']\n",
      "['charge', 'carrier', 'lifetime', 'improved', 'efficiency', 'interfacial', 'transfer ', 'attributed', 'layered', 'structure', 'reduced', 'symmetry']\n",
      "['suppress', 'agglomeration', 'probably', 'due', 'surface', 'activity', 'compensating', 'large', 'ls¯', 'anions', 'localize', 'around']\n",
      "['shells', 'around', 'ps', 'cores ', 'case ', 'number', 'compensating', 'ls', 'anions', 'around', 'inside', 'positively']\n",
      "['photoanode', '[] ', 'nattestad', 'al ', 'reported', 'decrease', 'recombination', 'nio', 'photocathode', 'optimization', 'donor–acceptor', 'dyes']\n",
      "['incoming', 'light', 'mostly', 'absorbed', 'si ', 'light-induced', 'carriers', 'separated', 'built-in', 'electric', 'field ', 'order']\n",
      "['spin-coating ', 'eis', 'measurements', 'stated', 'clearly', 'results', 'recombination', 'loss', 'hybrid', 'solar', 'cells', 'methanol']\n",
      "['<', '1 3', 'tetrahedral', 'positions', 'corresponding', 'mn', 'state ', 'appearance', 'octahedral-coordinated', 'manganese', 'ions', 'charge']\n",
      "['charge', 'state ', 'appearance', 'octahedral-coordinated', 'manganese', 'ions', 'state', 'detected', 'values', 'within', '0 8–1 2', 'range ']\n",
      "['luminescent', 'applications ', 'example ', 'occurrence', '→', 'eu', 'transfer', 'transition', 'eu-activated', 'reos', 'significantly', 'extends']\n",
      "['zn', 'taken', 'stoichiometric', 'ratio ', 'cu', 'concentration', 'consisted', '~1 5', 'wt % ', 'using', 'sem', 'data ']\n",
      "['cucl', 'directly', 'synthesis', 'process ', 'concentration', 'cu', 'consisted', '~1 5', 'wt % ', 'zns cu', 'obtained', 'temperatures']\n",
      "['several', 'microns', ' fig ', 'b  thus ', 'chosen', 'composition', 'synthesis', 'regimes', 'enabled', 'simultaneously', 'obtain ', 'is ']\n",
      "['practically', 'absent ', 'necessary', 'note', 'copper', 'concentration', 'consisted', '~1 5', 'wt % ', 'thus ', 'testify', 'mentioned']\n",
      "['space-charge', 'region', ' scr ', 'zns ', 'particles', 'region', 'carrier', 'depletion ', 'due', 'fact', 'particle', 'size']\n",
      "['valence', 'band', '[] ', 'consequently ', 'concentration', 'main', 'carriers', 'particle', 'size', '2', 'turns', 'low ']\n",
      "['zns cu', 'obtained', 'shs', 'method', 'adding', 'nacl', 'flux', 'shown ', 'introduction', 'nacl', 'enables', 'increase']\n",
      "['selection', 'synthesis', 'regimes ', 'flux', 'material', 'quantity', 'enables', 'using', 'shs', 'method', 'realize', 'selective']\n",
      "['processes', 'exciton', 'generation ', 'radiative', 'recombination ', 'photogenerated', 'transfer', 'poly 3-hexylthiophene ', ' p3ht ', 'nanoscale', 'aggregates', 'crystallites']\n",
      "['printed', 'circuit', 'board', 'unpolarized ', 'discuss', 'connection', 'potential ', 'electric', 'charge ', 'electric', 'potential', 'unpolarized']\n",
      "['mw ', 'frequency', 'range', '1 96', '1 98', 'ghz', '5', 'v', 'super-capacitor ', 'maximum', 'rf', 'dc']\n",
      "['gudan', 'al ', 'also', 'designed', 'wph', 'system', 'nimh', 'battery', 'using', 'either', 'ambient', 'wi-fi']\n",
      "['materials', 'important ', 'meanwhile', 'classical', 'description', 'motion', 'carriers', 'longer', 'valid', 'situation ', 'reason', 'research']\n",
      "['time', 'picoseconds', 'recovery', 'time', 'limited', 'free', 'carriers', ' fcc ', 'life', 'time', '500', 'ps ']\n",
      "['negligible', '1550', 'nm', 'wavelength', 'probe  as', 'increase', 'always', 'associated', 'power', 'dissipation ', 'phase', 'modulator']\n",
      "['heating', 'electron', 'gas ', 'multiphoton', 'photoionization ', 'space', 'formation ', 'responsible', 'observed', 'phenomena the', 'electronic', 'sources']\n",
      "['volume', 'charge', 'manifests', 'saturation-like', 'dependence', 'emitted', 'laser', 'pulse', 'energy', 'applied', 'voltage', '100']\n",
      "['however', 'applied', 'voltage', '100', 'v ', 'emitted', 'nearly', 'linear', 'function', 'pulse', 'energy ', 'dynamics']\n",
      "['electron', 'emission', 'femtosecond', 'excitation', 'involves', 'complicated', 'redistribution ', 'particular ', 'repulsion', 'cloud', 'already', 'emitted']\n",
      "['laser', 'pulse', 'energy', 'one', 'control', 'emitted', '0 03', 'nc', ' it', 'corresponds', 'emission', 'charge']\n",
      "['charge', '0 03', 'nc', ' it', 'corresponds', 'emission', 'density', '0 24', 'nc/cm ', '1 6', 'nc', ' 13']\n",
      "['nanocarbon', 'films ', 'experiment ', 'demonstrate', 'density', 'electric', 'collected', 'anode', 'vacuum', 'diode', 'ncf', 'cathode']\n",
      "['bias', 'voltage', '500', 'v ', 'dependence', 'emitted', 'applied', 'voltage', 'evidences', 'accumulation', 'spatial', 'charge ']\n",
      "['charge ', 'vehicle', 'cost', 'price', 'discounted', 'varying', 'using', 'genetic', 'algorithm ', 'fegad', 'al ', 'used']\n",
      "['design', ' cad ', 'tools', 'determination', 'mobility', 'curves', 'carriers ', 'obtained', 'experimental', 'data', 'directly', 'solving']\n",
      "['plan', 'paper', 'follows ', 'section', 'transport', 'equation', 'carriers', 'graphene', 'presented', 'along', 'derivation', 'mobility']\n",
      "['applied', 'getting', 'deterministic', 'solutions', 'boltzmann', 'equation', 'transport', 'semiconductors ', 'several', 'works', 'based', 'weighted']\n",
      "['output', 'ports ', 'respectively ', 'component ', 'whereas', 'algorithm', 'processing', 'transformation', 'incoming', 'data', 'stream ', 'thus ']\n",
      "['potential', 'efficient', 'execution ', 'example ', 'file', 'server', 'locating', 'streaming', 'file', 'contents', 'trees ', 'streams']\n",
      "['come', 'high', 'licensing', 'cost ', 'per', 'cpu', 'scheme ', 'makes', 'scaling', 'multiple', 'machines', 'expensive']\n",
      "['assembled', 'produce', 'paper', 'released', 'open ', 'free', 'along', 'publication ', 'another', 'form', 'contribution', 'future']\n",
      "['conducted', 'one', 'authors', 'research', 'paper ', 'one', 'collecting', 'data ', 'maintaining', 'unique', 'flow', 'order']\n",
      "[' photovoltaic', 'device ', 'operates', 'by ', ' 1 ', 'generation', 'carriers', ' electrons', 'holes ', 'following', 'absorption', 'photons ']\n",
      "['following', 'absorption', 'photons ', ' 2 ', 'separation', 'photo-generated', 'carriers', 'via', 'charge', 'selective', 'contact s  ', ' 3 ']\n",
      "[' 2 ', 'separation', 'photo-generated', 'charge', 'carriers', 'via', 'selective', 'contact s  ', ' 3 ', 'collection', 'photo-generated', 'charge']\n",
      "['charge', 'selective', 'contact s  ', ' 3 ', 'collection', 'photo-generated', 'carriers', 'external', 'circuit', 'resulting', 'electricity in', 'particular']\n",
      "['  ', 'porosity', 'present', 'adsorbent', 'earmarks', 'basically', 'adsorption ', 'pore', 'size', 'confined', 'copolymer', 'template']\n",
      "['electric', 'force', 'strength', 'exerted', 'test', 'point', 'different', 'charge', 'distributions ', 'sample', 'undergraduate', 'engineering']\n",
      "['strength', 'exerted', 'test', 'point', 'charge', 'different', 'distributions ', 'sample', 'undergraduate', 'engineering', 'students ', 'although']\n",
      "['law', 'symmetry', 'coulomb’s', 'law', 'electric', 'point', 'situations ', ' b ', 'students’', 'inability', 'identify', 'new']\n",
      "['controlled', 'user ', 'is ', 'user', 'control', 'position', ' i e  ', 'test', 'charge ', 'charge', 'point', 'used']\n",
      "['control', 'position', 'charge', ' i e  ', 'test', 'charge ', 'point', 'used', 'navigate', 'scene ', 'assume', 'test']\n",
      "['point', 'used', 'navigate', 'scene ', 'assume', 'test', 'moved', 'user', 'electrical', 'field', 'formed', 'given']\n",
      "['user', 'electrical', 'field', 'formed', 'given', 'electric', 'distribution', 'vicinity ', 'force', 'exerted', 'positive', 'test']\n",
      "['distribution', 'vicinity ', 'force', 'exerted', 'positive', 'test', 'electrical', 'field', 'calculated ', 'scaled ', 'sent', 'handle']\n",
      "['hand ', 'user', 'experiences', 'variations', 'force', 'test', 'moving', 'around', 'electrical', 'field', 'produced', 'charge']\n",
      "['charge', 'moving', 'around', 'electrical', 'field', 'produced', 'distribution ', 'haptic', 'experience', 'coupled', 'real-time', 'visual']\n",
      "['experience', 'coupled', 'real-time', 'visual', 'animation', 'test', 'manipulated ', 'electric', 'charge', 'distribution', 'resulting', 'electric']\n",
      "['visual', 'animation', 'test', 'charge', 'manipulated ', 'electric', 'distribution', 'resulting', 'electric', 'field', ' field', 'lines  ']\n",
      "['experience', 'strength', 'direction', 'force', 'exerted', 'test', 'electric', 'field', 'change', 'according', 'motion', 'around']\n",
      "['number', 'students', 'demonstrated', 'higher', 'understanding', 'point', 'compared', 'line', 'charge', 'ring', 'charge', 'simulations ']\n",
      "['higher', 'understanding', 'point', 'charge', 'compared', 'line', 'ring', 'charge', 'simulations ', 'specifically ', 'students', 'able']\n",
      "['point', 'charge', 'compared', 'line', 'charge', 'ring', 'simulations ', 'specifically ', 'students', 'able', 'clearly', 'explain']\n",
      "['clearly', 'explain', 'force', 'changed', 'moved', 'test', 'center', 'ring', 'inside', 'circumference ', 'away', 'outside']\n",
      "['charge ', 'linear', 'decrease', 'infinitely', 'long', 'line', 'quadratic', 'decrease', 'point', 'charge', ' neri', 'al  ']\n",
      "['long', 'line', 'charge', 'quadratic', 'decrease', 'point', ' neri', 'al  ', '  ', 'also ', 'different', 'tactile']\n",
      "['[ ', '] ', 'surface', 'hydroxyl', 'groups', 'increase ', 'recombination', 'reduction', 'immobilized', 'systems this', 'study', 'attempted']\n",
      "['ni', '[–]  ', 'types', 'defects', 'inducing', 'layer', 'deficit ', 'example ', 'layer', 'charge', '0 86–1 22', '1 58']\n",
      "['inducing', 'layer', 'charge', 'deficit ', 'example ', 'layer', '0 86–1 22', '1 58', 'valence', 'unit', ' v u  ', 'per']\n",
      "['octahedral', 'sheets ', 'mn-oxides', 'configuration', 'derive', 'negative', 'reactivity', 'presence', 'vacancy', 'sites', '[] ', 'differences']\n",
      "['metal', 'oxyhydroxide', 'surfaces', 'predicted', 'using', 'shared', 'value', ' scv  ', 'positive', 'oxyanion', 'charge', 'divided']\n",
      "['shared', 'charge', 'value', ' scv  ', 'positive', 'oxyanion', 'divided', 'number', 'bonded', 'atoms ', 'lower', 'scv ']\n",
      "['causes', 'pollution', 'cr ', 'sulfate', 'chromate', 'equivalent', ' 2−  ', 'crystal', 'structure', 'similar', 'ionic', 'radii']\n",
      "['derived', 'experience ', 'verified', 'means', 'inspection', 'personnel', 'checking', 'status', 'affected', 'network', 'elements ', 'either']\n",
      "['indicated', 'dotted', 'line', 'fig ', '   ', 'assuming', 'density', '≈10', ' right', 'scale', 'number', 'decades  ']\n",
      "['curve', 'message', 'show', 'density', 'ruled', 'recombination', 'transfer', 'early', 'afterglow ', ' ho ', 'cluster', 'ion']\n",
      "['spectrometer', 'allows', 'determination', 'mass-to-charge', 'ratio', 'absolute', 'mass', 'single', 'nanoparticles ', 'nine-stage', 'linear', 'accelerator/decelerator']\n",
      "['spectrometric', 'measurements', 'single', 'charged', 'nanoparticles', 'use', 'detection', 'mass', 'spectrometry', ' cdms ', 'techniques', ' 1  ']\n",
      "[' cdms ', 'techniques', ' 1  ', 'cdms', 'determines', 'absolute', 'particle', 'magnitude', 'image', 'charge', 'induced', 'pickup']\n",
      "['determines', 'absolute', 'charge', 'particle', 'magnitude', 'image', 'induced', 'pickup', 'electrode', 'charged', 'particle', 'passes']\n",
      "['electrode', 'charged', 'particle', 'passes', 'through ', 'image', 'waveform', 'also', 'yields', 'particle', 'time-of-flight', ' tof ']\n",
      "['acceleration', 'ion', 'ensembles', 'technique', 'hindered', 'space', 'limitations', 'since', 'effect', 'space', 'charge', 'progressively']\n",
      "['space', 'charge', 'limitations', 'since', 'effect', 'space', 'progressively', 'reduced', 'particles', 'accelerate ', 'deceleration', 'ion']\n",
      "['particles', '90', 'based', 'kinetic', 'energy', 'per', 'next', 'chamber', 'contains', 'net ', 'net', 'linear']\n",
      "['net', 'linear', 'electrostatic', 'trap', 'configured', 'image', 'mass', 'spectrometer ', 'first', 'described', 'zajfman', 'co-workers']\n",
      "['particle', 'time ', 'measuring', 'mass-to-charge', 'ratio', 'absolute', 'particle ', 'particles', 'trapped', 'net', 'efficiency', '~70%']\n",
      "['weak', 'magnetic', 'fields ', 'perturbations', 'due', 'space', 'imperfections', 'trap', 'geometry ', 'well', 'collisions', 'background']\n",
      "['involving', 'large', 'number', 'charged', 'particles ', 'space', 'effects', 'trap', '[] ', 'inhomogeneity', 'external', 'magnetic']\n",
      "['density', '10', 'trapped', 'situation', 'enhances', 'space', 'effects', 'individual', 'electron’s', 'degrees', 'freedom', '[ ']\n",
      "['energy', 'consumption', 'characteristics ', 'is ', 'daily', 'battery', 'discharge', 'pattern', 'behavior ', 'prediction', 'algorithm', 'used']\n",
      "['connection', 'microcontroller', 'port ', 'capable', 'measuring', 'battery', 'level ', 'voltage ', 'current ', 'temperature ', 'according', 'datasheet ']\n",
      "['along', 'protein', 'sequence', 'relevant', 'protein', 'function/interaction ', 'transfer', 'protein', 'backbone', 'considered', 'produce', 'electromagnetic']\n",
      "['produce', 'electromagnetic', 'radiation', 'specific', 'frequency', 'depending', 'velocity ', 'ten', 'groups', 'proteins', 'relevant', 'interactions']\n",
      "['could', 'represented', 'three', 'rrm', 'frequencies ', 'depending', 'transfer', 'mechanism', ' velocity ', 'along', 'protein ', 'different']\n",
      "['frequencies', 'estimated ', 'assuming', 'several', 'plausible', 'scenarios', 'movement', 'mechanisms ', 'based', 'obtained', 'results ', 'propose']\n",
      "['type', 'imaging', 'system', 'bioluminescence ', 'high', 'sensitivity', 'coupled', 'device', ' ccd ', 'coupled', 'focusing', 'optics']\n",
      "['normally', 'centralized', 'network ', 'node', 'group', 'nodes', 'collecting', 'sensed', 'data ', 'gateway', 'could', 'possible']\n",
      "['region', 'spacetime', ' the', 'ergosphere ', 'inside', 'iacs ', 'totally', 'transverse', 'momentum', 'flux', ' the', 'momentum']\n",
      "['affect', 'simulated', 'magnetospheres', 'attain', 'substantial', 'goldreich-julian', 'density', 'associated', 'rotating', 'magnetic', 'field ', 'order']\n",
      "['support', 'outgoing', 'relativistic', 'jet ', 'rotate', 'goldreich-julian', 'density ', 'alfven', 'critical', 'surface', 'inflow', ' iacs ']\n",
      "['evolution', 'magnetic', 'field', 'rotation', 'rate', 'induced', 'density ', 'one', 'must', 'able', 'simulate', 'role']\n",
      "['wave', 'communication', 'out-flowing', 'wind ', 'thus ', 'iacs', 'horizon', ' a', 'one-way', 'membrane ', 'global', 'flow']\n",
      "['since', 'alfven', 'wave', 'wave', 'carries', 'physical', ' punsly', '  ', 'iacs', 'causally', 'excises', 'active']\n",
      "['chip', 'dielectrophoretic', 'cages', 'isolate', 'single', 'cells', '[ ', '] ', 'cellcelector', 'uses', 'robotic', 'arm']\n",
      "['slow', 'relentless ', 'true', 'downscaling', 'architecture’s', 'positive', 'driving', 'force', 'change there', 'myriad', 'material', 'hi-tech']\n",
      "['electrons', 'mapped', 'participants’', 'actual', 'movements', 'changes', 'virtual', 'hand ', 'strong', 'example', 'gestural', 'congruency']\n",
      "['move', 'finger', 'large', 'surface', 'demonstrate', 'free', 'might', 'move', 'electric', 'field ', 'type', 'assessment']\n",
      "['larger', 'object ', 'smaller', 'particles', 'affected ', 'np', 'also', 'modified ', 'quantity', 'identity', 'proteins', 'corona']\n",
      "['identity', 'proteins', 'corona', 'affected', 'np', 'size', 'also', 'functionalization ', 'case ', 'composition', 'corona', 'exact']\n",
      "['provide', 'partial', 'control', 'of ', 'example ', 'surface', 'interaction', 'serum', 'proteins ', 'gives', 'full', 'range']\n",
      "['from ', 'shape', 'size ', 'surface', 'coating ', 'net', 'np ', 'parameters', 'influence', 'cellular', 'uptake', 'biological']\n",
      "['  the', 'applied', 'coating', 'also', 'allows', 'controlling', 'surface', 'nps ', 'addition', 'interactions', 'opsonin', 'proteins ']\n",
      "['interactions', 'opsonin', 'proteins ', 'already', 'mentioned ', 'surface', 'plays', 'role', 'stability', 'aunps', ' for', 'example ']\n",
      "['pathways', 'otherwise', 'captured', 'temperature', 'raised', 'additional', 'provided ', 'manner ', 'find', 'coo', 'polycrystalline', 'films']\n"
     ]
    }
   ],
   "source": [
    "#papers = MyPapers('abstract_scraper/full.json')\n",
    "\n",
    "print_cluster_context(0,papers, target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
