{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Kojak\n",
    "\n",
    "** The problem  **\n",
    "\n",
    "We will attempt to identify amboguously defined words - words that are homographs (spelled the same, but with multiple meanings) and determine the exact meaning of the word from a context window.\n",
    "\n",
    "Here we attempt to do this in a few stages\n",
    "1. train a word embedding on some training corpus using skip-gram (Here we use 1000 sholarly research papers) \n",
    "2. identify common homographs and extract the various context windows\n",
    "3. interpret the context windows as vectors in the embedding space and appy a clustering algorith (DBSCAN). Each cluster is interpreted as a distinct definition of the homograph. Each cluster then is representative vector.\n",
    "4. apply to a test corpus - match context of given homograph to most similar group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare stopwords, preprocess the data from source file\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop+=['?','!','.',',',':',';','[',']','[]','“' ]\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“', '?', '!', '’', 'et', 'al']\n",
    "stop = set(stop)\n",
    "\n",
    "class MyPapers(object):\n",
    "    # a memory-friendly way to load a large corpora\n",
    "     def __init__(self, dirname):\n",
    "            self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "        with open(self.dirname) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "        # iterate through all file names in our directory\n",
    "        for paper in data:\n",
    "            try:\n",
    "                line = [word for word in paper['full_text'].lower().split() if word not in stop]\n",
    "                line = [re.sub(r'[?\\.,!:;\\(\\)“]',' ',l) for l in line]\n",
    "                yield line\n",
    "            except:\n",
    "                print(\"Empty document found\")\n",
    "                continue\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Context Window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare what word we are searchig for\n",
    "target = u'state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate iterable on the data\n",
    "\n",
    "#papers is an iterable of scholarly papers, tokenized for prcessing\n",
    "papers = MyPapers('abstract_scraper/full.json') \n",
    "\n",
    "# target_corpus will be a list of ony those papers containing the target word\n",
    "target_corpus = []\n",
    "\n",
    "for paper in papers:\n",
    "    if target in paper:\n",
    "        target_corpus.append(paper)\n",
    "        \n",
    "len(target_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function takes as arguments a list of tokenized documents and a window size\n",
    "# and returns each word in the document along with its window context as a tuple\n",
    "\n",
    "def generate_windows(documents, window_size):\n",
    "    maxlen = window_size*2\n",
    "    \n",
    "    for document in documents:\n",
    "        L = len(document)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(document):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            context_words = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    #in_words.append([word])\n",
    "                    context_words.append(document[i])\n",
    "            x = word\n",
    "            y = context_words\n",
    "\n",
    "            #x = np.array(in_words,dtype=np.int32)\n",
    "            #y = np_utils.to_categorical(context_words, V)\n",
    "            yield(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes a list of strings (words_list)\n",
    "# Returns single string of all words in words_list seperated by a white space.\n",
    "\n",
    "def make_sentence(words_list):\n",
    "    return ''.join([word + ' ' for word in words_list]).encode('utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arguments: target word and a starting number\n",
    "# Prints all context windows for the target word for 10 papers in \n",
    "\n",
    "def print_10_contexts(target, paper_start, target_corpus):\n",
    "    \n",
    "    paper_count = 0\n",
    "    print(target.upper())\n",
    "    while paper_count < 10:\n",
    "    #for paper_num in range(paper_start,paper_start + 10 ):\n",
    "        try:\n",
    "            paper = target_corpus[paper_start + paper_count]\n",
    "            paper_count += 1\n",
    "            windows = generate_windows([paper],6)\n",
    "            count = 1\n",
    "        except:\n",
    "            break\n",
    "        print('\\nPAPER {}'.format(paper_count))\n",
    "        for w in windows:\n",
    "            if w[0] == target:\n",
    "                print('{}: {}'.format(count, make_sentence(w[1])))\n",
    "                count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper_start = 50\n",
    "\n",
    "print_10_contexts(target, paper_start, target_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize the model\n",
    "\n",
    "model = gensim.models.word2vec.Word2Vec(sentences = papers, size=100, window=6, min_count=1, workers=2,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "705"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** contexts to vectors **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes list of word tokens as arguments\n",
    "# Returns a list of vectors whose components are the arithmetic mean of the \n",
    "# corresponding component of all of the input vectors\n",
    "\n",
    "def get_vectors(word_list):\n",
    "    vecs = []\n",
    "    for word in word_list:\n",
    "        vecs.append(vectors[word])        \n",
    "    return vecs\n",
    "\n",
    "# Takes list of vectors as arguments\n",
    "# Returns a single vector whose components are the arithmetic mean of the \n",
    "# corresponding component of all of the input vectors\n",
    "\n",
    "def vector_average(vector_list):\n",
    "    A = np.array(vector_list)\n",
    "    dim = A.shape[0]\n",
    "    ones = np.ones(dim)\n",
    "    return ones.dot(A)/dim\n",
    "\n",
    "# Takes list of tokenized documents, target word and window size as arguments\n",
    "# Returns list of vectors where each vector represents the context window \n",
    "# of the target word in the word embedding space\n",
    "\n",
    "def context2vectors(documents,target,window_size = 6):\n",
    "\n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        if target in document:\n",
    "            windows = generate_windows([document],window_size)\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append(vector_average(get_vectors(w[1])))\n",
    "                    \n",
    "    return context_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.dictionary.Dictionary(papers)\n",
    "text = [dictionary.doc2bow(c) for c in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = u'charge'\n",
    "#papers = MyPapers('abstract_scraper/full.json')\n",
    "context_vectors = context2vectors(papers, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Clustering with DBSAN **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='brute', eps=0.05, leaf_size=30, metric='cosine',\n",
       "    min_samples=5, n_jobs=1, p=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps = .05, metric = 'cosine', algorithm = 'brute')\n",
    "dbscan.fit(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "labels = dbscan.labels_\n",
    "n_clusters = len(set(labels)) # - (1 if -1 in labels else 0)\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0, -1,  0,  0, -1,  0,  0, -1,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='cosine', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='complete',\n",
       "            memory=Memory(cachedir=None), n_clusters=4,\n",
       "            pooling_func=<function mean at 0x1045d05f0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag = AgglomerativeClustering(n_clusters = 4, affinity = 'cosine', linkage = 'complete')\n",
    "ag.fit(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "labels = ag.labels_\n",
    "n_clusters = len(set(labels)) # - (1 if -1 in labels else 0)\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_cluster_context(cluster_number, documents, target, labels, window_size = 6):\n",
    "    \n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        text = document\n",
    "        if target in text:\n",
    "            #print(target)\n",
    "            windows = generate_windows([text],window_size)\n",
    "            #print windows[:2]\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append((w[1]))\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == cluster_number:\n",
    "            print(context_vectors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'provide', u'partial', u'control', u'of ', u'example ', u'surface', u'interaction', u'serum', u'proteins ', u'gives', u'full', u'range']\n",
      "[u'weights', u'assigned', u'variables', u' characteristics  ', u'since', u'dea', u'task ', u'revealing', u'optimal', u'multipliers', u'set', u'device']\n",
      "[u'study ', u'latter', u'executed', u'space', u'interlocution', u'authors', u'participate', u'solving', u'problems ', u'special', u'knowledge ', u'proposing']\n",
      "[u'around', u'world ', u'minimum', u'often', u'available', u'without', u'relevant', u'statistical', u'authorities', u'non-commercial', u'use ', u'code']\n"
     ]
    }
   ],
   "source": [
    "#papers = MyPapers('abstract_scraper/full.json')\n",
    "\n",
    "print_cluster_context(3,papers, target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dimension Reduction **\n",
    "\n",
    "Since vectors are dimension 100+, DBSCAN is ineffective. Other clustering algorithms are successful at seperating into a predetermined number of clusters which correspond to different definitions. However, If a word has more or less than these number of actual definitions, then this is counter-fproductive. \n",
    "\n",
    "In order for DBSCAN to me more effective, we will apply Singular Value Decomposition in order to reduce dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=3)\n",
    "X = svd.fit_transform(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.73029258 -0.25748841  0.04560457]\n",
      " [ 1.76554675 -0.27716833 -0.07840145]\n",
      " [ 1.80722314  0.25192535  0.03140236]\n",
      " [ 1.85302673 -0.12986292 -0.31118489]\n",
      " [ 1.82413751 -0.54283378  0.084833  ]\n",
      " [ 1.74573225 -0.05066371 -0.01500504]\n",
      " [ 1.78398019 -0.00504734  0.01977244]\n",
      " [ 1.78961389  0.86771412  0.21581959]\n",
      " [ 1.74517046  0.00291113 -0.41058351]\n",
      " [ 1.8590407   0.30771191 -0.40301317]\n",
      " [ 1.88834485  0.70226794  0.0373219 ]\n",
      " [ 1.76278322 -0.32581087  0.00832723]\n",
      " [ 1.77024706 -0.21489309 -0.15950789]\n",
      " [ 1.45304812 -0.01347501  0.74768944]\n",
      " [ 1.51778093  0.0569271  -0.08929314]\n",
      " [ 1.23342456  0.04562962  0.15050466]\n",
      " [ 1.46784686 -0.07163069  0.19147038]\n",
      " [ 1.68096168 -0.17033584  0.03774441]\n",
      " [ 1.65657643  0.01218753 -0.16097386]\n",
      " [ 1.80956014  0.1359716  -0.04900305]\n",
      " [ 1.57167187 -0.43375923  0.21901102]\n",
      " [ 1.75091924 -0.00550093  0.12701703]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
       "    min_samples=5, n_jobs=1, p=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps = .5, metric = 'euclidean', algorithm = 'auto')\n",
    "dbscan.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "labels = dbscan.labels_\n",
    "n_clusters = len(set(labels)) # - (1 if -1 in labels else 0)\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
