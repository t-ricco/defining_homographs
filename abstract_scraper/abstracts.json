[
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-016-0007-7", "title": "A rapid and intelligent designing technique for patient-specific and 3D-printed orthopedic cast", "authors": ["Hui Lin", "Lin Shi", "Defeng Wang"], "publication": "3D Printing in Medicine", "publication_date": "22 September 2016", "abstract": "Two point four out of 100 people suffer from one or more fractures in the course of average lifetimes. Traditional casts are featured as cumbersome structures that result in high risk of cutaneous complications. Clinical demands for developing a hygienic cast have gotten more and more attention. 3D printing technique is rapidly growing in the fabrication of custom-made rehabilitation tools. The objective of this study is to develop a rapid and intelligent modeling technique for developing patient-specific and hygienic orthopedic casts produced by 3D printing technologies.", "full_text": "Bone fracture occurs in general population resulting from mechanical impact or bone diseases. Plaster or fiberglass cast have been employing for the treatment of most fracture patients. Traditional orthopedic casts or orthoses are produced by body-based contacting model []. The bottom mold for a cast is generated from surface shapes of injury limbs and filled up with plaster. Thermoplastic material, PE (Polyethylene) and CPP (copolymer polypropylene), are laid on a mold and removed after cooling down for the formation of an orthopedic cast [\u2013]. Fracture patients wear plaster splints after surgery followed by a cast for further recovery. Those casts develop several skin diseases and potential bone and joint injuries due to heavy structure and poor ventilation. Moreover, patients suffer mechanical pressures during the mold manufacture and multi-reproduction of physical molds are unfeasible [].3D printing technology is a rapid growing manufacture technique for producing a complex physical model in term of 3D a digitizing model [\u2013]. It recently has been extensively applying on surgical practices and medical training [, \u2013]. The rapid manufacture of the physical model from medical images provides technical means with minimal invasion for medical planning and treatment [, , \u2013]. Custom-made rehabilitation tools produced from 3D printing technique have been reported in the new development of orthopedic cast [, ]. Conventional custom-fit casts generate the surface geometry from subject\u2019s injured limbs. Mavroidis et al. developed a novel engineering process using rapid prototyping technique for creating patient-specific ankle-foot orthosis []. The process basically included, 3D scanning injury ankle and leg, designing in a Computer Aided Design (CAD) software and exporting STereoLithography (STL) file, and manufacture using a 3D-printing technique [, ]. The engineering method is also employed in the design and manufacture for wrist orthosis [, ]. The upper limb was scanned and 3D digitized. The polygonal data, STL file, was generated and input into a CAD software for designing a desired model. The designing model was output for rapid prototyping.Some novel concepts for potential substitutes for plaster cast and manufactured by 3D printing technology are reported [, , ]. Jake Evill proposed a new design for orthoses named as Cortex, a custom-fitted web structure []. The mesh-like structure forms an artistic surface pattern with treatment consideration by changing the webby density, more solid at the region of fracture. Another idea proposed by Deniz Karasahin developed a similar model as Cortex but mounted an ultrasound device for promoting the therapeutic process. Those new designs are fabricated by 3D printing technique using environment-friendly material [, ]. Cast geometries are generated from 3D scan provide patient-specific models that offer wearing comfort and fashionable appearance. The mesh-like structure definitely presents excellent ventilation. However, the weak strength of the structure is present for supporting the injury limbs. Mechanical impact with low intensity may easily break the webby beam. In addition, the webby shape is most likely to develop crack and fatigue failure due to the slender connecting bar. Those fancy designs are still in technical assessment without any clinical application and approval.A hybrid model for custom-fit wrist orthosis that combined the webby frame with shell cover to enhance the structural strength as well as keep ventilation [, ]. The design process basically included modeling inner frame and outer cover via a CAD system. This new model improved the stiffness and prevented the structure from breaking. However, an experienced CAD engineer was involved in creating appropriate engineering structure. This new design is a concept model without any clinical application.Despite the technical advantage and economic potential, 3D-printing technologies have not become the primary mean in the fabrication of orthopedic cast []. The significant technical expertise is required for designing a cast and high cost and time for both design and fabrication are present. In order to perform CAD process, the scanned data of subject\u2019s limbs must be converted into specific CAD file with modification of the geometry. An experienced CAD engineer is required for creating the model and converting the model CAD file to STL file for 3D-printing [, ]. The entire design process is manual and time cost. As reported in the literature, it took over three hours to design a wrist orthosis [].Clinical demands for developing a cast with good ventilation, light weight, and automatic design process and few requirements of expertise, have been getting more and more attention. The medical application of 3D printing is increasing for its rapid manufacture and cost-effective benefits. The growing technologies on 3D printing make it possible on the fabrication of complex geometric model presenting in orthopedic casts and significantly reduce the manufacturing time and cost [, ]. The objective of this study is to develop a rapid designing technique for creating patient-specific and hygienic orthopedic cast. 3D-printed technologies are used in the fabrication of the proposed design.An intelligent designing system that integrates all above algorithms is developed by using Visualization Toolkit (VTK Kitware). Figure\u00a0 shows a cast model for an arm orthosis. The sequence of steps of designing are outlined as follows: creating a raw cast, computing centerline, building fine cast, modeling flare, developing ventilation pattern, designing opening gap, creating the solid model. The proposed modeling algorithms are developed as an intelligent tool to assist orthopedist with few specific technical training. It takes about 20\u00a0min to perform a cast design for an orthopedic technician with few computer designing skills. In comparison, an experienced CAD (computer-aided design) designer spends more than 2.5\u00a0h to design a 3D-printed orthosis frame using a commercial software []. There is rare literature exploring designing orthopedic cast fabricated by 3D printing technologies. The rapid modeling technique developed in this study addresses on image-based designing technique to create a patient-specific cast. The specific techniques, such as computational geometry reference and surface pattern creation, facilitate the design process based on patients\u2019 image data. Commercial CAD software are not user-friendly in those designs due to the complexity of geometry. Rich experiences in CAD skills and long designing time are required for an orthopaedist who uses a commercial CAD software.A cast geometry is built from a patient\u2019s data makes an orthosis custom-made, which creates the best fit geometry for a limb as shown in Fig.\u00a0. The patient-specific model would develop benefits on patient comfort and minimize the distortion after healing. A loose cast can lead the deformity of the injured limb during the healing process since the necessary correction forces are not appropriately applied. Furthermore, an ill-advisedly cast would create poor overlying skin which results in local pressure sores.Ventilation is one of the most important concerns for a fracture patient. Ventilation pattern is designed as holed surface on the cast that improves the hygienic outcome during fracture healing and generates the lightweight structure (Fig.\u00a0). The specific structure avoids sweat-trapping under the cast where cutaneous complications are induced. Holed crust allows air circulation for reducing the risk of irritation and infection with higher likelihood in a damp environment where the skin is exposed. Another physical-related side benefits of the structure are lightweight and fashionable appearance. A cumbersome feature is labeled as traditional cast with a long history in the application. The novel design would improve patients\u2019 experiences via a wearable cast with only minimal disturbance to their daily life.However, the 3D scanning is a time-consuming process and a particular position of the injured limb is required for scanning in order to acquire completed data. Patients with acute fracture would be difficult to perform the required scanning. In the future study, we would design a frame to position the injured limb appropriately and facilitate scanning. In addition, the 3D printing is still a slow process as per current technologies. For example, it takes around ten hours to print the orthosis as shown in Fig.\u00a0. Presently, fabrication of orthopedic cast using 3D printing is not an economical but affordable approach in compared to traditional methods.This study develops a rapid designing technique to create a patient-specific orthopedic cast fabricated by 3D printing technology. The newly developed cast has ventilated feature that reduce the risk of cutaneous complications occurring in traditional alternatives. In addition to ventilation feature, significantly lightweight structure is present in the cast physical model. Engineering analysis using FEA technique validate the high strength of the structure. It is a wearing-friendly and adjustable cast to create the best fit for a patient\u2019s injured limb during the entire treatment process. The design time is short and few technical experiences are required."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-017-0011-6", "title": "Fabrication approaches for the creation of physical models from microscopy data", "authors": ["Benjamin L. Cox", "Nathan Schumacher", "John Konieczny", "Issac Reifschneider", "Thomas R. Mackie", "Marisa S. Otegui", "Kevin W. Eliceiri"], "publication": "3D Printing in Medicine", "publication_date": "14 February 2017", "abstract": "Three-dimensional (3D) printing has become a useful method of fabrication for many clinical applications. It is also a technique that is becoming increasingly accessible, as the price of the necessary tools and supplies decline. One emerging, and unreported, application for 3D printing is to aid in the visualization of 3D imaging data by creating physical models of select structures of interest.", "full_text": "Although 3D printing is now widely used for medical applications [\u2013], there are lesser-known biomedical applications, including the manufacturing of complex parts for research, such as microfluidic components, and for education [, ]. Aiding both research and education, 3D printing can be used to create physical models from biomedical imaging data. Physical 3D models are useful as they allow researchers to hold and feel a structure that they might otherwise only be able to see on a computer screen [, ]. These models can prove especially useful in fields where concepts are often difficult to spatially comprehend from a two-dimension (2D) image. For these reasons, these models are also extremely effective teaching tools to help students understand complex 3D biological phenomena, such as membrane architecture and dynamics.Most 3D printing of biomedical images to date has utilized clinical imaging methods such as computed tomography (CT), positron emission tomography (PET), magnetic resonance imaging (MRI) and 3D ultrasound [, , , ]. Here we describe 3D printing from optical imaging methods, such as confocal microscopy and multiphoton microscopy. The creation of the models, discussed here, highlight similarities that can be applied to create models from many microscopy techniques including electron microscopy and other optical modalities.There are an increasing number of free and open source software packages that are capable of creating a computer model from 3D image data as well as affordable and fairly high resolution 3D printers that are on the market or slated to be released soon. In addition, there are more choices in printing materials, depending on the needs for resolution, cost and durability. These developments have combined to make this process more accessible and practical than it has ever been []. Presented here are three physical models derived from different optical imaging modalities that were created using different fabrication techniques. These image models were chosen for their biological relevance and ability to represent different image modalities and fabrication challenges. Alternate options for fabrication and file creation and the implications of the increased accessibility of this process are also discussed.Three physical models were generated from three distinct sets of 3D imaging data. These include a multiphoton microscopy model of a  embryo [], a confocal microscopy model of the distal tip cell of  and an electron tomography model of a prevacuolar compartment in a maize endosperm cell [].The examples of models presented here all are vast improvements to 2D pictures when trying to convey complex biological concepts. However, none of these models are perfect. 3D printing is a continually improving technology. As more individual techniques to 3D print objects are developed and as existing techniques are refined, the quality of models produced will continue to improve.There are specific improvements to existing 3D printing techniques that will benefit the generation of physical models like the ones presented here. A printer capable of printing different colors within a clear plastic material would make a model of the prevacuolar compartment more accurate. Also, the capability to produce models of objects in free space would make some of these models more useful. This is especially true in the case of the prevacuolar compartment and of the distal tip cell. Finally, as the feature resolution and strength of available materials advance, the overall quality of the models will improve.The future of 3D printing and microcopy has a lot of potential and room for growth. Despite the success of the 3D printed models shown here, the technology, both in model creation and in model fabrication, still can struggle to match the spatial and temporal resolution of a dynamic 3D dataset. In this presented work we chose to work mostly with relatively straightforward 3D datasets. But many investigators want to show multiple models of a process changing over time for research and educational purposes. It can be costly to make the multiple models needed to capture an entire process and difficult to represent fine or floating details in intricate processes. 3D datasets that do not have clear spatial boundaries can also be difficult to reproduce faithfully.The 3D electron tomography model of the prevacuolar compartment presented in this paper nicely illustrates these issues. In this case, the data on screen can sometimes better show the features of interest as you can see the components more clearly as you rotate virtually, while some of that perspective is lost in the printed model. This is especially true when examining processes that happen over time, such as protein trafficking in this case, where the components are more in free space and can get left out of a printed model. And if included, they would only represent one snapshot of a process that is dynamic. As more 3D modeling, 3D printing and materials options become available, these types of models will improve.Another challenge is workflows and the accessibility of these tools. The process used here required commercial tools that are available in most 3D printing facilities but not necessarily user friendly or available to the benchtop biologist. It would be ideal if image processing programs, such as the popular image processing program ImageJ/FIJI [], had better support for creating 3D model files like STL files directly. If the same programs that microscopy professionals use to process their data could also be used directly to ready their data for 3D printing, there would be more widespread use of these modeling techniques, as the cost and difficulties in workflow would be reduced.The major uses of 3D printing of image models are for research purposes and for informal and formal science education. It is clear that a researcher would likely value having a physical representation of their dataset and that such a model might aid in discussion and informal science outreach. Such models, even if they do not lead directly to new insights, might facilitate others to become interested or be powerful demonstration aids for illustrating biological concepts. However, the path is less clear towards using such models for formal science education. A pedagogical study would need to be done to see if such printed models have utility in the classroom to teach fundamental concepts.There have been informal classroom studies showing the utilities of using real research data in the classroom [, ]. Additionally, there have been published studies showing the advantages of 3D molecular models in the classroom []. However, to our knowledge, there have been no studies showing if 3D printed models are an effective aid in formal education. For example, in a developmental biology class studying embryonic development, would 3D printed models of developing embryos directly improve comprehension of core concepts?Despite the described challenges in technology, adoption and education use, 3D printing of biological data does have great promise. These types of models offer a tangible method to visualize complex data. These examples and the process outlined here can serve as a guide to future studies of 3D printing of biological imaging data and will hopefully help further its application.3D printing technologies are being used for medical applications, but largely with medical imaging data from CT, MRI or PET. However, another biomedical application of 3D printing is the fabrication of models derived from optical imaging data. Models of microscopy datasets can aid both researchers and students in understanding complex biological problems. Three such models, each derived from a different imaging modality and fabricated with a different printing technology, are presented along with the details of their creation. A general workflow for the creation of other models of this type is also included. The emergence of free and open-source software tools, the reduction in price of 3D printing hardware and the creation of new 3D printing materials are all factors that continually increase the accessibility of these technologies. As that accessibility continues to increase, the models and methods outlined here can serve as a general guide to researchers and educators who want to make similar models."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-015-0003-3", "title": "The residual STL volume as a metric to evaluate accuracy and reproducibility of anatomic models for 3D printing: application in the validation of 3D-printable models of maxillofacial bone from reduced radiation dose CT images", "authors": ["Tianrun Cai", "Frank J. Rybicki", "Andreas A. Giannopoulos", "Kurt Schultz", "Kanako K. Kumamaru", "Peter Liacouras", "Shadpour Demehri", "Kirstin M. Shu Small", "Dimitris Mitsouras"], "publication": "3D Printing in Medicine", "publication_date": "27 November 2015", "abstract": "The effects of reduced radiation dose CT for the generation of maxillofacial bone STL models for 3D printing is currently unknown. Images of two full-face transplantation patients scanned with non-contrast 320-detector row CT were reconstructed at fractions of the acquisition radiation dose using noise simulation software and both filtered back-projection (FBP) and Adaptive Iterative Dose Reduction 3D (AIDR3D). The maxillofacial bone STL model segmented with thresholding from AIDR3D images at 100 % dose was considered the reference. For all other dose/reconstruction method combinations, a \u201cresidual STL volume\u201d was calculated as the topologic subtraction of the STL model derived from that dataset from the reference and correlated to radiation dose.", "full_text": "Medical 3D printing is currently undergoing a rapid transition from niche applications to more routine utilization, particularly in reconstructive surgeries as well as cardiovascular and neuro-interventions []. This increased utilization is secondary to lower costs and greater awareness that 3D printing can enhance patient care [\u2013]. Translating a new technology from the research to clinical domain however first requires standardization and validation.A key validation component involves metrics amenable for universal application of accuracy and reproducibility testing across equipment platforms and acquisition protocols. One example is that of quality assurance phantoms used to establish the accuracy of images generated by an imaging system based on standardized measurements, such as the American College of Radiology Computed Tomography phantom []. Similarly, for 3D printing, one strategy may include carefully developed phantoms that, once printed, can undergo standardized tests and measurements to establish the accuracy of the 3D printing hardware and software pipeline. This strategy does not readily translate to the relative quality assessment measures needed to assess individual clinical scans obtained with varying clinical protocols and that depict different tissues and pathologies.The signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR) are metrics of the diagnostic capacity of an individual scan used to this end. They take into account both the acquisition protocol (e.g., echo and repetition time in MRI; tube current and potential in CT), as well as the underlying tissue properties (e.g., T and T properties; X-ray attenuation). These metrics strongly contribute to the determination of an individual study\u2019s diagnostic accuracy. The present study is based on the author\u2019s experience that a similar approach is necessary to assess the expected accuracy of 3D printed models generated from individual clinical data sets. This was underscored by a recent study that reported a >1\u00a0mm variation in anatomical properties of a skull standard tessellation language (STL) model generated by three independent, specialist institutes from a single DICOM CT dataset [].3D printing of bone from CT images currently accounts for the majority of 3D printing applications in medicine, starting with maxillofacial and neurosurgical applications [, ]. We consider 3D printed models as the best method to select locations of appropriate and optimal osteosynthesis for full-face transplantation []. 3D models are also used to reduce surgical time after trauma and improve outcomes in patients with complex bone and joint injuries such as acetabulum and posterior wall pelvic fractures [\u2013], and to test the effectiveness of novel surgical tools for total shoulder arthroplasty []. For pelvic bone tumors, 3D printing has been used to generate patient-specific bone cutting instrumentation to enhance accuracy and potentially improve resection []. In spine intervention, 3D printed models of bone have assisted in the management of structural, traumatic and neoplastic diseases by helping to confirm pedicle screw placement [\u2013], and in congenital and acquired pediatric orthopedic disorders they have aided in better anatomical understanding of the lesion [, ]. Going forward, clinical applications of 3D-printed models will require a confidence metric that assesses the relationship between the imaging parameters used to acquire the source DICOM images and the resulting STL model derived from them.The purpose of this work was to develop such a metric, namely the \u201cresidual STL volume\u201d and use its topology for the assessment of maxillofacial bone models derived from CT images at multiple simulated radiation doses. CT data was acquired using a standard clinical protocol and simulated noise was added in the raw data space (sinogram) using a previously validated approach []. The degradation of image quality among those images with simulated lower radiation dose was carefully assessed to determine the impact on the novel 3D printing accuracy metric.3D printing requires the isolation and modeling of the volume occupied by individual tissues in DICOM images. If 3D printing is implemented in patient care, radiologists will be obligated to interpret those volumes as well as determine and ensure their accuracy. Thus, 3D printing-specific quality metrics will be required to assess 3D medical models, both as a function of the underlying imaging acquisition and pathology involved, as well as of the interpreting radiologist. While phantoms can be used to test the accuracy of 3D printing hardware, to date there are no measures of quality and benchmarks for the accuracy and reproducibility of STL models generated from DICOM images for 3D printing.This is important to help establish the requirements of medical images, in terms of e.g., image SNR, tissue CNR, and image resolution to enable accurate 3D printing. No studies have addressed these requirements to date either in the bone or any other tissue. Guidance regarding such choices is largely based on the experience of the operator []. An important application of the residual volume is thus that information extracted from mathematical measures of its shape can be used to guide the selection of imaging protocol and required SNR and CNR to achieve accurate 3D printing. For example, decreasing SNR due to reduced radiation exposure resulted in a \u201cshell\u201d of small additional thickness to the bone STL models studied here. This indicates that the effects of the increased noise for bone 3D printing translate to benign (<0.1\u00a0mm in width) uncertainty at the transitions between bone and tissue, rather than bulk anatomic errors.Similar study designs to that used here can, for example, assess inter-scan variability of 3D printing by applying the residual volume to assess models derived by repeated acquisition of images from a single patient. Similarly the residual volume can be used to assess intra-scan variability with different imaging parameters, e.g., help establish how different slice thicknesses affect anatomic features of interest in the printed model. Such studies will begin to address open issues regarding quality and standardization for 3D printing from DICOM data. Furthermore, strategies based on the residual volume can potentially help evaluate the effect of different segmentation methods or segmentation parameters. Choi et al. used thresholding segmentation cutoffs of 700 HU for the cranio-maxillary complex and 800 HU for the mandible [] while Salmi et al. used a 500 HU cutoff for the entire skull and mandible [], although both used CT datasets acquired at 120 kVp. The residual volume can be used to compare models derived from threshold ranges other than the widely-accepted software-default HU range for bone used here. Of note, the topology of the residual volume can be readily interrogated at pre-specified locations of particular clinical interest, such as along the axis of the mandibular body length used in orthognathic and reconstructive surgeries [] or at the anatomic limits of a tumor [], a region where manual segmentation, including decisions best suited for the radiologist, is important for diagnosis and surgical planning.Just as the initial volume rendering of bony structures portrayed on a 2D monitor [] inspired the field of 3D visualization and ushered the development of the \u201c3D lab\u201d in radiology, \u201c3D printing\u201d of radiology images has roots in skeletal radiology, including a more than 20\u00a0year history of in cranio-maxillofacial reconstruction [, ]. Our data also concentrates on cranio-maxillofacial bone. CT bone segmentation is relatively straightforward because of its high attenuation and signal. Its \u201chard\u201d separation form adjacent tissues aids 3D printing-specific STL refinements such as smoothing that are often employed to convert medical DICOM images into reasonable STL files []. Deviations from anatomy and pathology that can alter the anatomy portrayed in the 3D printed model can arise not only from such computer-aided design refinements, but from numerous factors including the quality of the source images, image post-processing techniques including segmentation technique, and STL generation algorithms that determine the set of triangles that will be used to represent the surface encompassing the segmentation. A large source of potential error also lies in the additive manufacturing process itself and its limitations, including the particular 3D printing modality and the selection of printing materials [].The residual volume is designed as a metric that enables either the ensemble or individual assessment of each acquisition, post-processing, and STL generation factor involved in 3D printing, so that individual models or practices to generating them can be assessed for e.g., reproducibility and accuracy before they are implemented to clinical practice. This expands the scope of prior accuracy studies, for example those that have concentrated on comparing a final printed model with the posthumous anatomy used to provide source DICOM images [, ], or those that have compared physical measurements to those made with 3D post-processing software in the source images [, , ].This work applied the residual volume to assess radiation dose reduction [] for 3D printing. Automated CT tube current and tube potential selection combined with iterative reconstruction methods optimize the tradeoffs between radiation exposure and SNR [], and have led to customized patient-centric imaging strategies. The residual volume demonstrates that a large reduction in radiation dose, using just 20\u00a0% of that used for surgical planning, has a very minor effect in the generation of 3D printable models of maxillofacial bone. The average difference of less than 0.1\u00a0mm was smaller than the spatial resolution of the CT hardware and of the typical dimensional error (0.5\u20130.9\u00a0mm) of the 3D printing process itself [, , ]. Our data thus mathematically confirms that current low-dose CT protocols meet the technical needs for 3D printing of bone.While we propose that the residual volume can be translated to all applications of 3D printing from DICOM images, we expect that results for soft tissue models will vary in comparison with those from bone because of the different SNR and CNR properties. Our preliminary work suggests that reduced arterial contrast opacification in contrast-enhanced CT angiography for example leads to large portions of vessels such as the aorta being excluded from semi-automated segmentation, leading to much larger residual volumes that we report for bone. More work is needed to assess the use of the residual volume in soft tissues and the contrast-enhanced blood pool and to similarly establish the impact of varying the image acquisition and segmentation parameters.This study introduces the residual volume, a simple topological difference metric of STL models generated from a tissue depicted in DICOM images. The residual volume is designed to enable the ensemble or individual assessment of each acquisition, post-processing, and STL generation factor involved in 3D printing, so that individual models or practices to generating them can be assessed for e.g., reproducibility and accuracy before they are implemented to clinical practice. The topology of the STL residual volume has two key properties; first, it appears to follow image SNR in an anticipated manner \u2013 higher SNR leads to a smaller residual volume, and second, the residual volume can be used to both visualize and mathematically measure the difference between STL models derived from DICOM image datasets depicting the anatomy. Information extracted from mathematical measures of the shape of the residual volume can be used to guide the selection of imaging protocol in terms of required resolution, SNR and CNR toward ensuring accurate 3D printing. Finally, application of the residual volume in musculoskeletal 3D printing supports that lower dose images of bone can be accurately 3D printed. A reduction of the tube current by up to 80\u00a0% in conjunction with iterative reconstruction results in bone STL models that have a small difference in bone volume of less than 4\u00a0%, concentrated in a shell surrounding it that has less than 0.1\u00a0mm thickness."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-016-0006-8", "title": "3D Printed replica of articular fractures for surgical planning and patient consent: a two years multi-centric experience", "authors": ["Nicola Bizzotto", "Ivan Tami", "Attilio Santucci", "Roberto Adani", "Paolo Poggi", "Denis Romani", "Guilherme Carpeggiani", "Filippo Ferraro", "Sandro Festa", "Bruno Magnan"], "publication": "3D Printing in Medicine", "publication_date": "1 September 2016", "abstract": "CT scanning with 3D reconstructed images are currently used to study articular fractures in orthopedic and trauma surgery. A 3D-Printer creates solid objects, starting from a 3D Computer representation.", "full_text": "3D printing is s a relatively low cost technology that uses a 3D computer representation (graphics or 3D virtual objects) to create solid replicas that can then be used for healthcare applications; 3D printing models of healthy or fractured bones are used in facial and neurosurgery to select locations of appropriate and optimal osteosynthesis, to study the appropriate fracture\u2019s pattern and to reduce surgical time and improve outcomes in patients [].In orthopedic and trauma surgery actually, X-rays and Computed tomography (CT) with MPR (MultiPlanar Reformations) and 3D Volume Rendering are used to understand the dislocation of fragments, the amount of displacement and the joint involvement of articular fractures.Very recently, with rapid distribution of commercial 3D printers within the hospital setting, orthopedic surgeons started to use 3D printed replica of pelvic fractures, acetabulum fractures [], clavicle [] and various articular fractures (like wrist [], elbow, tibial plate\u2026) to improve understanding of fracture by means of tactile and visual experience [, ]. Other pathologic conditions like spine disorders, dysplasia of hips or bone tumors are bone tumors are 3d-printed for surgical planning []. However, there is a paucity of publications that focus on a collection of patients. The purpose of this paper is to present our two-year multi-centric experience of 3D printed models of articular fractures in orthopedic and trauma surgery and hand surgery.This study included six hospitals with subspecialized surgical services for trauma and/or hand procedures. The study period was January 2014 to December 2015, during which 102 patients (age range 20\u201378, 45 male and 57 female)) were enrolled. Written informed consent was obtained for each patient (for this research project).The patients presented with the following fractures: distal radius (\u2009=\u200931), tibial plateau (\u2009=\u200919), radial head (\u2009=\u20099), calcaneus(\u2009=\u200915), astragalus (\u2009=\u20095), ankle (\u2009=\u200911), humeral head (\u2009=\u20098) and glenoid (\u2009=\u20094). A prerequisite for recruitment was that the patient is eligible for surgery (and hence has a clinical need for a medical model) because of fracture displacement, dislocation of the fragments, and/or instability.All 102 patients underwent CT scan: Hitachi Presto (Hitachi Medical Corporation, Japan), Siemens Somatom (Siemens, Germany), GE Optima CT660 (GE Medical System, USA), Philips iCT256 (Philips, NL) situated in hospitals of our enrolled centers. Data was reconstructed at 0.625\u00a0mm increments with 0.625\u00a0mm reconstructions. Reconstructed DICOM images were uploaded into a OsiriX Dicom Viewer. Multiplanar Reformatting (MPR) and 3D Volume rendering of the fracture were obtained for diagnosis and assessing the anatomy for 3D printing. Working on the 3D-Volume Rendering Reconstruction, the fractured bone was isolated with digital scissor tool.The models were exported in .obj and sent to a printer service near the hospitals. A post processing step (with a 3D Rendering Software) was sometimes necessary to create artificial bridges to connect serious displaced fragments to maintain the \u201canatomy\u201d of the fracture. A ProJet 660 Color printer (3D Systems, Rock Hill, SC) was used to 3D print the models with gypsum-dust material. In our experience we support this material (instead of acrylonitrile butadiene styrene material, ABS) because it very realistically replicates bone.The 1:1 models were printed in 4\u20138\u00a0h, depending of the anatomical regions. (4\u00a0h for a distal radius fracture and 8\u00a0h for a tibial plateau). The costs ranged from roughly $10USD for a model of the distal radius to $75USD for a tibial plateau.With the models in hands, the surgeons evaluated details as joint fragmentation, yielding and dislocation of the articular surface in a realistic way; these were presented to young residents and to medical-school-students to improve awareness of a trauma and fractures.Generally we noticed that 3d printed models reproducing extra-articular/non-displaced fractures are not very advantageous compared to models of complexes/serious displaced fractures. Surgeons rated the use of models most beneficial for articular fractures with articular gaps or steps of 2\u00a0mm, or with a multi-fragmentary pattern (i.e., AO Classification type B and C for articular fractures); for simple and methaphyseal fractures (i.e., AO Classification type A) the models were not useful.All 3D printed models were used with 3D visualization tools to acquire the informed consent with the patients, showing and explaining his specific situation, the risk of specific fragment necrosis and to illustrate the surgical procedure. Patients reported an enthusiastic general appreciation about the use of this new technology in our hospitals. There was a substantial improvement in comprehension of the fracture before and after seeing the 3D printed models.Personalized medicine with 3d printing technology will be one of the most important fields of future medical research []. 3D printing is currently developing worldwide where physicians 3D print orthopedic disorders, tumors, and congenital pediatric problems. The use of 3d printed replica for facial surgery and neurosurgery is well known worldwide [].The application of 3D printing to plan articular-trauma surgery is not as common; this gap maybe is due to the difficulties in organizing all the steps of the workflow.The CT images must be acquired with thin collimation and the images should be reconstructed with less than one millimeter thickness. Otherwise the final 3D model may not have adequate spatial resolution.The conversion to .stl file must be performed immediately after CT scan and directly sent to a 3D Printer situated in hospital or in a service nearby. The conversion is done directly with the CT-workstation or commercial Softwares like OrisiX or Mimics [], with a physician trained on it.With this workflow, the model is available for surgeons and patients usually in 12\u00a0h.Professional 3D Printers can print in different materials. For medical models, physicians print in ABS, PLA with different color (white, transparent, red\u2026),or VisiJet (like gypsum) differentiating anatomical parts and pathologies.In our experience, we suggest the use of VisiJet material (colored or not) to reproduce bone fractures because the models are more realistic then with other material.Alternately, a bone fracture printed in white ABS or PLA is an acceptable model for surgeon and patients.There are differences between Software in conversion \u201cDICOM to stl:\u201d numerous factors including segmentation technique and STL generation algorithms could be a source of potential error and loosening of details in the final model [, ].In future a more standardized process (physician/radiologist technician training, software algorithm segmentation, quality of printers\u2026) must be applied to allow a safe use of these models in clinical practice worldwide.In this multi-centric experience we notice that the use is these models is well appreciated by surgeons and patients and we are currently discussing to introduce the use of the 3d printed replica as a mandatory step for the surgical informed consent. We do not suggest to print simple or diaphyseal fractures, were first there is no indication to investigate the fractures with a CT scan (according to general good orthopedic practice). Further studies and cost analysis must be performed around this topic to investigate the feasibility of the process.3D printing of articular fractures are innovative procedures and generate models to achieve a real preoperative tangible evaluation of the fractures and procedures and to improve patients compliance and care. This application is a small step in the future of the personalized medicine and in the quality improvement of the health system.The authors did not received grants or outside funding in support of their research or preparation of this manuscript. No benefits in any form have been received or will be received from a commercial party related directly or indirectly to the subject of this article."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0189-3", "title": "Influence of bio-fertilizer containing beneficial fungi and rhizospheric bacteria on health promoting compounds and antioxidant activity of ", "authors": ["Muhammad Khalid", "\u2020", "Danial Hassani", "\u2020", "Muhammad Bilal", "Fayaz Asad", "Danfeng Huang"], "publication": "Botanical Studies", "publication_date": "16 August 2017", "abstract": "This study evaluates the influences of bio fertilizers containing mycorrhizal fungi (", "full_text": "The consumption of fruits and vegetables could increase the human innate immunity against chronic diseases (Bagchi et al. ; Yochum et al. ). The phytoconstituents including polyphenols, quercetin and flavonoids are largely demonstrated as important antioxidants and exhibit profound radical scavenging capabilities (Bravo ; Chu et al. ; Duthie et al. ; Gil et al. ; Middleton and Kandaswami ).  L. is one of the most important and commonly consumed leafy vegetable. It is commercially known as spinach which is claimed to possess therapeutic properties and being a rich source of flavonoids as well as phenolic compounds besides its economical and ease of availability (Bunea et al. ; Ferreres et al. ; Metha and Belemkar ; Sultana and Anwar ). The pro-health properties of spinach are attributed to its low calorific value, and its large supply of vitamins, micro- and macronutrients and others phytochemicals, including polyphenols and fiber (Llorach et al. ). The quality of fresh vegetables could be assessed based on their nutritional value, growing conditions and usage of fertilizer. Despite the fact that the genetic modification and agronomic manipulation methods are widely used to improve the nutritional value of plants, the inadequate public acceptance and soil specificity of genetically modified food are still the challenges (Mart\u00ednez-Ballesta et al. ).Started about 60\u00a0years ago, several studies have revealed the potentiality of beneficial microbes in increasing the plants resistance to biotic and abiotic stresses through the up-production of secondary metabolites (Shen ). Beneficial bacteria or fungi inhabit various sites such as plants rhizosphere, while others colonize on rhizoplane or even intercellular spaces (McCully ). Former studies revealed that phosphate and potassium solubilizing bacteria decompose the phosphate and potassium from their sources and make them available to the plants, assisting essential mineral uptake. Plant growth promoting rhizobacteria (PGPR) thrives in the rhizosphere of plants. It is worth mentioning that a substantial number of bacterial and fungal species entertain a functional relationship and establish an integrated system with the plants. They enhance the plant growth either by assisting in essential nutrients acquisition (minerals, nitrogen and phosphorus), eliciting pertinent hormones or acting as bio-control agents to reduce the inhibitory effects of various pathogens (Yang et al. ). Some strains such as  and  have shown to possess properties of biological nitrogen fixation both in legume and non-legume, exerting a positive effect on overall physiology and development of the plants (Dobbelaere et al. ; Goldstein and Liu ). Former literature survey revealed 30% improvement in the yield of wheat by the  inoculation (Zablotowicz et al. ). Likewise, root/shoot length and dry weight has been significantly increased in tomato, lettuce and canola by inoculation of  and  (Glick et al. ; Hall et al. ).Arbuscular mycorrhizal fungi (AMF) are associated with majority of the plants, growing under natural conditions and its contribution for micronutrients uptake is well-documented in the previous reports. Moreover, these beneficial microbes protect the plants from oxidative stress by synthesizing antioxidant enzymes including, peroxidase, catalase, superoxide and non-enzymatic antioxidants such as glutathione, ascorbate and \u03b1-tocopherol; hence, providing an suitable way to replace the hazardous agricultural chemical and agro-ecosystems destabilizing fertilizers.The current study was appraised to evaluate the influence of beneficial bacteria ( and ) and fungi ( and ) on the antioxidant properties and physiological activities of  L. and to develop an alternative method for improving the quality of health promoting phytochemicals and anti-radical activity of spinach.In the light of above findings, it is concluded that the application of bio fertilizer containing beneficial microbes displayed a stimulating effect on the soil properties and health qualities of spinach. The rhizobacterial inoculation resulted in increase of root infection by arbuscular mycorrhizal fungi. Moreover, the total phenolic compounds, antioxidant activity and chlorophyll content were also significantly enhanced by bio fertilizer treatments. These outcomes suggested that exploration of microbes display a high potential for use in the improvement of nutritious properties of fresh vegetables which could be a potential alternative to conventional approaches. However the mechanism underlying this phenomenon is not yet fully understood and will be remained for further investigations."},
{"url": "https://cancer-nano.springeropen.com/articles/10.1186/s12645-017-0028-y", "title": "Platinum nanoparticles: an exquisite tool to overcome radioresistance", "authors": ["Sha Li", "Erika Porcel", "Hynd Remita", "Sergio Marco", "Matthieu R\u00e9fr\u00e9giers", "Murielle Dutertre", "Fabrice Confalonieri", "Sandrine Lacombe"], "publication": "Cancer Nanotechnology", "publication_date": "11 July 2017", "abstract": "Small metallic nanoparticles are proposed as potential nanodrugs to optimize the performances of radiotherapy. This strategy, based on the enrichment of tumours with nanoparticles to amplify radiation effects in the tumour, aims at increasing the cytopathic effect in tumours while healthy tissue is preserved, an important challenge in radiotherapy. Another major cause of radiotherapy failure is the radioresistance of certain cancers. Surprisingly, the use of nanoparticles to overcome radioresistance has not, to the best of our knowledge, been extensively investigated. The mechanisms of radioresistance have been extensively studied using ", "full_text": "Radiation therapies are used to treat many cancers. One of the major causes of radiotherapy failure and subsequent tumour relapse is the radioresistance of tumours to conventional treatments (Shu et al. ). The development of treatments to combat radioresistance is a major challenge. The understanding of mechanisms and pathways involved in radioresistance has motivated intensive studies on several model organisms, including  a bacterium that can resist radiation exposure over 1000-fold greater than mammalian cells (Slade and Radman ). It has been shown that this organism exhibits an extraordinary ability to reassemble its functional genome after exposure to massive doses of radiation, while the genome of other organisms remains irreversibly shattered (Blasius et al. ; Confalonieri and Sommer ). Several groups have demonstrated that  resistance to radiation is attributed to a combination of physiological tools (Blasius et al. ; Levin-Zaidman et al. ; Daly et al. ), e.g. its efficient DNA repair machinery, its effective protection against oxidation of DNA repair proteins, and also the condensation of its nucleoid that may prevent dispersion of genomic DNA fragments produced by irradiation (Confalonieri and Sommer ). The resistance of  to radiation effects makes it an ideal candidate to probe the capacity of potential drugs such as NPs to enhance radiation effects in radioresistant cells and to characterize how these compounds may counteract the radioresistance mechanisms, and thus be subsequently explored in eukaryotic models.For over a decade, nanomedicine has been proposed as a new strategy to improve radiotherapy treatments. Studies have been devoted to the development of tumour-targeting nanodrugs with the aim to improve the radiation effects in the tumour and diminish the exposure of healthy tissues to cytotoxic effects (Yhee et al. ; Kim et al. ; Escorcia et al. ; Hainfeld et al. , ; Le Duc et al. ; Al Zaki et al. ). High-Z nanoagents, such as metallic (gold, platinum) and oxide (hafnium, gadolinium) nanoparticles (NPs), have been proposed as potential nanodrugs to amplify radiation effects. _ENREF_7 (Hainfeld et al. ; Porcel et al. , ; Jang et al. ; Le Duc et al. ). In a pioneering study, Hainfeld et al. () demonstrated that 1.9-nm gold NPs increase the effect of 250 kVp X-rays in the treatment of tumour-bearing mice. More recently, it has been shown that multimodal gold NPs improve not only the effect of ionizing radiation but also the performance of magnetic resonance imaging diagnosis (Miladi et al. ). Other metallic compounds, such as platinum complexes and platinum NPs (PtNPs), have shown excellent properties to amplify radiation effects (Usami et al. ; Charest et al. ; Porcel et al. ). Numerous studies, performed with various eukaryotic cells, have demonstrated the efficacy of high-Z NPs to enhance cell death in mammalian cells (Usami et al. ; Charest et al. ). This effect has been attributed to nanoscopic local dose deposition (Butterworth et al. ; Sancey et al. ). A relation between molecular damage and cell death has been established in the case of gadolinium NPs (Porcel et al. ). Surprisingly, the capacity of NPs to combat radioresistance in organisms treated by ionizing radiation has not yet, to the best of our knowledge, been reported.Here, we report the effect of small PtNPs on . In this perspective, we performed a toxicity study of PtNPs. The localization of PtNPs in  was characterized using two advanced microscopy techniques, namely Synchrotron Radiation Deep-UV fluorescence microscopy (SR-DUV) and high-angle annular dark-field scanning transmission electron microscopy (HAADF-STEM), which allows imaging of native NPs in bacteria without the use of any marker. The content of NPs in  cells was quantified by inductive coupled plasma mass spectrometry (ICP-MS). Lastly, we investigated the impact of NPs on the response of  to gamma-ray radiation exposure. This study opens the possibility to use small high-Z NPs to combat radioresistance.Using SR-DUV microscopy and HAADF-STEM to detect label-free nanoparticles, we demonstrated that ultra-small platinum NPs enter  cells in spite of its thick cell wall and that these nanoparticles have a MIC value of 4.8\u00a0mg L. We also showed that PtNPs, at a concentration of\u00a0\u22484700 PtNPs per cell, do not have any major effects on bacterial growth under normal growth conditions. In spite of the high resistance of this organism to radiation, we found that this amount of PtNPs slightly but reproducibly increases cell death by 37% after exposure to gamma rays at a dose of 8\u00a0kGy. Our results also suggest that this amplification effect is due to the confined production of ROS in nano-volumes around nanoparticles, which favors the induction of complex damage in biomolecules. By simulation, we observed that this effect is likely able to impact the genome as well as the proteome of the bacteria. These early-stage nanoscale processes may affect the biomolecules of many other cell types including eukaryotic cells. Thus, this work opens up the possibility to use NPs to overcome the resistance of certain tumours to radiation, thus representing a potential major breakthrough in radiotherapy."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0801-7", "title": "Temporal relations in hormone-withdrawal migraines and impact on prevention- a diary-based pilot study in combined hormonal contraceptive users", "authors": ["Gabriele S. Merki-Feld", "Gina Epple", "Nina Caveng", "Bruno Imthurn", "Burkhardt Seifert", "Peter Sandor", "Andreas R. Gantenbein"], "publication": "The Journal of Headache and Pain", "publication_date": "25 August 2017", "abstract": "Menstrually related migraine (MRM) in the hormone-free interval (HFI) of combined hormonal contraceptives (CHC) are according to the ICHD definition also estrogen withdrawal migraines (EWH). MRMs are less responsive to acute medication. Therefore short-term prevention, initiated 1\u20132\u00a0days before onset of the anticipated bleeding and continued for 6\u00a0days, is recommended. Such a long prophylactic triptan use might increase the risk for medication overuse headache in women suffering in addition from non-menstrual migraines. In CHC users onset of hormone decline is predictable. It is however unknown, whether the EWHs are rather associated with onset of hormone withdrawal or onset of bleeding. Improved understanding of this relation might contribute to better define and shorten the time interval for prevention.", "full_text": "In women migraine prevalence peaks during reproductive years []. Menstruation is a significant risk factor for migraine with attacks most likely to occur between 2\u00a0days before the onset of menstruation and the first three days of bleeding. The pathophysiology of menstrual attacks involves estrogen withdrawal and potentially abnormal release of prostaglandins triggered by the end-cycle drop in estrogen levels [, ]. In addition to menstrually related migraine (MRM) the ICHD-classification defines estrogen withdrawal headaches (EWH) as migraines arising in women using exogenous estrogen daily for three or more weeks followed by an interruption in which migraine develops within five days after the last estrogen intake []. In general MRMs last longer, are more severe, more disabling and less responsive to acute treatment [\u2013]. Short-term prevention therefore is an important approach for treatment. Most authors recommend to start preventive therapy two days before the onset of bleeding []. However, as in the natural cycle onset of bleeding is highly variable the starting day for prophylactic medications is difficult to predict. Too early start results in unplanned and unforeseeable long use of these pain medications. The latter can result in medication overuse headaches, in those suffering in addition from non-menstrually related migraines. In combined hormonal contraceptive (CHC) users however, the day of estrogen withdrawal is clear, what might facilitate the identification of an optimal interval for short-term prevention. It has however never been investigated, if onset of EWH in CHC users is more related to the onset of bleeding or the onset of the hormone-free time. While in the natural cycle the association of bleeding and migraine has been defined as MRM (days \u22122 to day 3), the time interval with higher probability for EWH in relation to bleeding in CHC users has never been studied. In contrast to the smooth decline of estrogen levels, the hormone decline in CHC users processes more abrupt and from higher estrogen levels. This might affect onset and course of the following headache. Most studies related to MRM characteristics and treatment do not differentiate women with migraine in the natural cycle and from CHC users experiencing EWH [, , \u2013]. The mixture of different headache types in studies potentially impacts precision of conclusions. With the present study, we aimed to identify the optimal interval for use of preventive migraine agents in CHC users with EWH. For this purpose we identified in addition to migraine days in the pill-free interval, the relationship between migraine onset and day of last pill intake on one hand and the first day of bleeding on the other hand.Patient charts of all female migraineurs were searched to identify premenopausal CHC users, who experienced any migraine in the hormone-free interval (HFI) and visited our clinic for hormonal migraines (University Hospital Z\u00fcrich) from 2009 to 2015. All charts include headache diaries, which are conducted prospectively over a period of three cycles, before patients come in for their first visit and prior to any intervention. This procedure allows us to identify the type of migraine, especially MRM and EWH according to ICHD criteria. The in the retrospect analysed diaries include daily information on migraine/headache occurrence, pain intensity, use of acute medication, use of hormones and vaginal bleeding. For ethical reasons diaries were anonymized before evaluation and only the main investigator had access to the key and all data. To comply with the ICHD criteria we included only CHC users with EWH in at least two of three cycles. A further criterion of inclusion was use of the CHC in the typical regimen of 21/7\u00a0days. Charts were excluded if women used other types of hormonal therapy including progestin-only contraceptives, CHC regimen different from 21/7 and reported headaches in only one of three withdrawal periods. Furthermore charts were excluded if women had not yet used their CHC for at least 6\u00a0months.According to the ICHD definition migraine lasts from 4 to 72\u00a0h. As MRM migraines tend to last longer, we evaluated both, migraine days and migraine episodes (lasting >24\u00a0h). To identify onset of bleeding, migraine days, the first day of migraine and the start of migraine episodes we collected data during days 1\u20137 of the HFI. To explore the vulnerable time frame for migraine onset in relation to the withdrawal bleeding we choose the interval of days \u22123 to day 5, which allowed us to exclude that we might miss an association by just including two days before bleeding and the first three bleeding days. A similar procedure has been used earlier by MacGregor [].In the present study we analyzed the onset and course of single migraine days and migraine episodes during the HFI in CHC users. In the latter migraine could be associated with either, the sharp hormone decline immediately after stopping the pill or with the onset of withdrawal bleeding some days later. To optimize the duration of short-term prevention and avoid medication overuse headaches in women suffering in addition from non-menstrual related attacks this issue is of relevance. Analyses of headaches and bleeding over the seven hormone-free days revealed that bleeding mostly starts during days 3\u20135 and onset is in contrast to the natural cycle highly predictable. The first migraine attack occured on days 1\u20135 (each 12\u201320% of cycles), indicating that prevention has to start earlier than the expected withdawal bleeding (Fig. ). Migraine start at days 1\u20135 is in accordance with the 5-day period defined for estrogen-withdrawal headaches by the ICHD. Most migraine days were observed on days 3\u20137 (each in more than 33% of cycles, Fig. ). In relation to withdrawal bleeding most migraines occurred between day \u22121 and day 4. There was a twofold frequency of migraine attacks on bleeding days 1\u20134 in comparison to day \u22122 in our study. This differs from previous studies exploring MRM in the natural cycle, which identified peaks of attacks from bleeding days \u22122 to day 3 and is exactly one day later, in comparison to the ICHD definition [, ]. The abrupt estrogen decline in women stopping use of exogenous applied estrogens together with its effects on bleeding could explain this difference. Also preliminary this finding suggests that using the same time frame for both, MRM in the natural cycle and after use of artificial estrogens in CHC users might be too imprecise.MRMs have been found to be longer lasting, more severe and refractory to abortive medications [, ]. As the majority of our study participants suffered from both MRM and non-menstrually related migraine attacks we cannot exclude that some of the migraine attacks during the HFI are only randomly associated with hormone withdrawal. Separate analyses of longer lasting attacks might provide a better approach to identify truly hormone- withdrawal associated migraines. Episodes (attacks lasting >1\u00a0day) in our study were observed in 49% of 103\u00a0cycles with a mean duration of more than 3\u00a0days, in spite of use of rescue medication. The high rate of long lasting episodes of EWH in CHC users indicates that these headaches similar as the MRM in the natural cycle, are also long lasting and less responsiveness to rescue medication [, ]. Most of these episodes started on days 1\u20135 of the HFI, as well.To our knowledge this is the first study analyzing the course of migraine during the hormone-free interval in CHC users, which includes in accordance with the ICHD criteria only women with EWM in at least two of three cycles. One study, including women with MRM in the HFI reported highest pain scores at HFI day 4 in eleven CHC users, but was limited with regard to the low number of CHC users and the observation period of only one cycle []. Lieba-Samal et al. investigated prevalence of headache and migraine in CHC users in association with their withdrawal bleeding []. They found that CHC users had a twofold hazard ratio for migraines at bleeding days 1\u20133, but not at days \u22121/\u22122. Inclusion criteria comprised at least one day of migraine per month, without consideration of the HFI. We in addition to bleeding days 1\u20133 observed more migraine attacks at days -1and day 4. Our daily analyses of headache days over days \u22123 to day 5 is not comparable with the mentioned interval analyses and much more exact for the objective. Numerous studies demonstrated that MRM attacks last longer in comparison to non-menstrual migraine attacks [, ]. Characteristics of migraine attacks are highly variable not only among patients but also within the same patient []. For preventive therapy of EWH the majority of medications have been tested to be started two days before the expected onset of menstrual flow and to be continued for 6\u00a0days [, \u2013]. The low predictability of bleeding and hormone withdrawal in the natural cycle makes it difficult to apply to such a regimen and frequently results in treatment periods of 8 and more days []. In women with additional non-menstrual attacks this may cause an unacceptable high use of rescue medications. Our data demonstrate that first day of migraine and migraine episodes rarely occur later than day 6 of the HFI. Referring to our results it appears to be reasonable to initiate short-term prevention in CHC users at the last day of pill intake or the first day of the HFI and continue for 5\u00a0days. Even if withdrawal bleeding in CHC users is highly predictable, our data indicate that use of the HFI as reference point is the better choice.Starting prevention at the last day of pill intake would also be optimal to prevent most of the episodes, which are very disabling, long-lasting and difficult to treat.Strengths of the current study include the exact diagnosis of menstrually related migraine according to the ICHD-criteria, the accurate prospectively conducted daily diaries, the inclusion of only women with the typical 21/7 CHC regimen and the headache documentation over three cycles. Analyzing first days of migraine episodes increases the probability to identify the onset of true menstrually related attacks with low responsiveness to abortive medications. Analyses of the reproducibility of the first bleeding day over several cycles might contribute to optimize treatment in some women. A potential limitation is the evaluation of the diaries in the retrospect. We cann\u2019t exclude that inclusion of CHC containing different ethinylestradiol dosages (20\u00a0\u03bcg and 30\u00a0\u03bcg) might exert an impact on our results. Ethinylestradiol is much more potent than the naturally released estradiol. With both CHC dosages, estrogen withdrawal occurs from higher estrogen levels and is much more abrupt in comparison to the natural cycle. Therefore we do not think that the small differences in ethinylestradiol dosage are relevant in the context of this study. The number of more than 100 evaluated cycles is sufficient for a pilot study, but the results have to be confirmed in prospectively conducted studies including more cycles. Data from women attending a specialist migraine office cannot always be extrapolated to the general population.Even taking into account these limitations we consider the present data as useful to clinicians who treat CHC users with EWH and have to define the time frame for short-term prevention. In this context it has to be mentioned that treatment of migraine in CHC users should not only imply rescue medications or prophylactic agents, but also the consideration to change to a progestin- only method. These are not only safer but may reduce migraine frequency and intensity as well [\u2013]. Use of CHC in extended cycles regimen is another approach to improve withdrawal migraine but is not recommended, as risk of breast cancer with those pill regimen is unknown and risk for ischemic stroke could further increase [, , ]. For women with MRM a multidisciplinary approach combining neurological and gynaecological consultations might be of advantage [].In CHC users migraine mostly starts on bleeding days \u22121 to day 4, what differs from findings in the natural cycle. Referring to the HFI interval most migraines and migraine episodes start at days 1\u20135. Around 50% of the migraine attacks last more than 24\u00a0h. Short-term prevention could contribute to better management of these episodes and should be initiated at the last day of pill use or the first day of the HFI and be continued for 5\u00a0days."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-016-0009-5", "title": "Maintaining safety and efficacy for 3D printing in medicine", "authors": ["Andy Christensen", "Frank J. Rybicki"], "publication": "3D Printing in Medicine", "publication_date": "26 January 2017", "abstract": "The increased and accelerating utilization of 3D printing in medicine opens up questions regarding safety and efficacy in the use of medical models. The authors recognize an important shift towards point-of-care manufacturing for medical models in a hospital environment. This change, and the role of the radiologist as a central facilitator of these services, opens discussion about topics ranging from clinical uses to patient safety to regulatory implications.", "full_text": "3D printing from volumetric medical images has entered a phase of steep growth. Although this growth has provided an appearance that the medical modeling application is nascent, 3D printing for surgical planning and the use of 3D printing to develop tools to enhance medical procedures has a rich history in both private practice and academic medicine [\u2013]. Medical device manufacturers and individual industry leaders have worked with the U.S. Food and Drug Administration (FDA) to empower healthcare providers with innovative, personalized devices that are safe and effective, while academicians have devised and tested new interventions that would not be possible without 3D printing [\u2013]. For these procedures, the Center for Devices and Radiological Health (CDRH) at the Food and Drug Administration (FDA) has reviewed and cleared 3D printed medical devices for more than 10\u00a0years []. An important shift is happening towards point-of-care manufacturing for medical models in a hospital environment. This change, and the role of the radiologist as a central facilitator of these services, opens discussion about topics ranging from clinical uses to patient safety to regulatory implications.Three important regulatory milestones should be acknowledged. First, the October 2014 FDA Public Workshop [], \u201cAdditive Manufacturing of Medical Devices: An Interactive Discussion on the Technical Considerations of 3D Printing\u201d, held by an FDA Working Group that detailed best practices for 3D printing quality and safety as well as an assessment of medical devices, followed by discussion of topics ranging from bioprinting to pharmacoprinting to metals implant printing. Approximately 500 people attended this two-day workshop with another few hundred watching online from around the world. The second milestone is the FDA \u201cleapfrog\u201d Draft Technical Guidance Document, \u201cTechnical Considerations for Additive Manufactured Devices\u201d [], released for public comment on May 10, 2016. The third milestone was the publication \u201cAdditively Manufactured Medical Products \u2013 The FDA Perspective\u201d [], that further explored the topic, including 3D printing of patient-specific anatomy and the 3D printers used for such tasks.This document provides the authors\u2019 collective opinion, and frames the quality and safety challenges that we perceive have emerged from state of the art medical 3D printing. We engage the FDA and the overall community to achieve needed clarity and specifics regarding the potential benefits achievable through standardization. We are determined to foster a conversation that ensures safety and efficacy for a large number of medical models, and to group these models with respect to risk, which may correlate to the regulatory classification associated with specific applications. Increasing complexity comes from using digital data from patient anatomy to further plan surgery and to provide 3D printed instruments, templates, and models to facilitate surgical intervention.The number of medical 3D printing applications continues to grow, as does the number of users dedicated to improving patient care pathways. The authors recognize potential divergent pathways for patient safety and the efficacy of medical models, and they provide recommendations for dialogue on best methods to integrate 3D printing to medical practice. 3D printing applications are separated into three groups:  Anatomical models, Modified anatomical models, and Virtual surgical planning with templates. These groups are explicitly defined with recommendations on optimizing safety and efficacy for each. Professionals engaged in 3D printing must maintain a high level of training and competence, and we recommend the use of FDA cleared software and hardware when appropriate.E plurbus unum (translation \u201cout of many, one\u201d) generally refers to the \u201cmelting pot\u201d of many individuals, for example from different ancestries, religions, and races. 3D printing in medicine will emerge as a universal tool, a melting pot for the delivery of medical care that integrates medical information from volumetric imaging devices. The relatively small number of users will expand. More widespread enthusiasm will ultimately be fueled by reimbursement. During this transition, we are obligated to pay particular attention to quality medical modeling, and we will rely on the FDA to ensure safety and efficacy.Very close collaboration between academics, industry, and the FDA is paramount to ensure safety for those U.S. patients whose quality of life will be improved by 3D printing. This article is designed to foster that collaboration and to encourage dialogue. Similarly, professionals engaged in 3D printing must maintain a high level of training and competence, and we believe that they should use FDA cleared software and hardware when appropriate. Education and clinical practice that focus on alternate, non-cleared software and hardware should be discouraged. For models designed to identically emulate patient anatomy, all hardware used to generate the image and post-process the relevant anatomy should be cleared by the FDA, while those steps related to the fabrication alone should be done with care, but should ultimately be monitored by the medical team caring for the patient in the Practice of Medicine. Patients for whom anatomy is altered significantly require use of software/hardware for that intended use which is cleared as a medical device. 3D printing has emerged as revolutionary paradigm shift for helping surgeons and interventionalists perform procedures in a more informed way, but for these patients and keeping safety at the forefront, all steps should be considered under the auspices of the FDA."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-015-0030-6", "title": "Fusing passive RFID and BIM for increased accuracy in indoor localization", "authors": ["Aaron M. Costin", "Jochen Teizer"], "publication": "Visualization in Engineering", "publication_date": "22 December 2015", "abstract": "Finding the current location of a specific utility or oneself in an unfamiliar facility can be difficult and time consuming. The hypothesis tested in this paper is that using the information contained within Building Information Models (BIM) can increase the accuracy of indoor positioning algorithms using context-aware sensing technology. The presented work demonstrates how the integration of passive Radio Frequency Identification (RFID) tracking technology and Building Information Modeling (BIM) can assist indoor localization for potential applications in facilities management for proactive preventative maintenance.", "full_text": "The operation and maintenance (O&M) phase of a building deals with management and maintenance of assets in the building for smooth functioning. Technology and innovation has been shown to improve the functionality, sustainability and flexibility of facility components (Olatunji and Sher ). Additionally, building systems are becoming increasingly complex, causing challenges for the management and operation of the facility (Kean ). Maintenance is an on-going process that runs throughout the project life cycle and requires continuous navigation throughout the facility. Facility management (FM) relies heavily on the constant monitoring of numerous items located throughout the building to be proactive when it comes to preventative maintenance.The process of manual inspection proves to be time consuming as it relies distinctly on a worker searching throughout a facility for problems often without precise information relating to location. Manually inspecting or locating various building components causes workers to spend excessive amounts of time searching for the desired equipment or materials, rather than working efficiently on the tasks required for proper maintenance. It has been shown that locating equipment in facilities is the core maintenance activity that causes significant delay in maintenance (Lee and Akin ). Moreover, locating building components is critical for timely repair of the component and mitigation of the damage (Taneja et al. ).Through the use of context-aware, e.g., location, time and three-dimensional (3D) space based, automated systems, this process can be greatly improved to increase the performance of facilities management. The need for such technologies in an indoor setting is crucial since research has shown that 85\u00a0% of the total project cost is spent in operation and maintenance from the owner\u2019s perspective (Teicholz ).Real-time access to the locations of workers, materials and equipment has become a significant advancement to the management of construction processes. Location-aware computing offers significant potential of improving such manual processes and supporting important decision making tasks in the field (Teizer et al. ). There have been a variety of technologies (e.g., Ultra Wideband, Global Positioning System, laser scanner) utilized to produce visualizations of the locations of resources on a construction site (Khoury and Kamat , Pradhananga and Teizer , Vasenev et al. , Pradhananga and Teizer , Cheng et al. c, Cheng et al. ). However, there is a lack of real-time data visualization of such technologies within an indoor environment to determine a person\u2019s location inside the facility, as well as an aid for navigation (Teizer et al. , Cheng and Teizer , ).One solution is the integration of emerging wireless remote sensing data with Building Information Models (BIM) which allows for the real-time visualization of the locations of workers, materials and equipment. Integrated building technologies allow a convergence and integration of systems to play a greater role in overall building performance (Kean ). Unfortunately, little research has been conducted regarding the reliability and practical benefits of the integration. The development of a building model holds large potentials in saving time, reducing miscommunications and enabling internet-based building/FM integration and interoperability, but it has yet to evolve to a point where it incorporates the needs of FM (Teicholz ).A building information model can contain an abundance of information about that facility. The purpose is to utilize that information stored within the BIM model to increase the accuracy of localization algorithms. This paper describes what information can be utilized, such as the spatial information (corridors, size of components, coordinates, etc.), building objects contained in the model (walls, doors, HVAC, pipes, etc.) and attributes associated with each object (object type, object properties, linked documentation, etc.). Additionally, that 3D model can be used for real-time visualization, which has a variety of benefits.As a study has shown, more than half of all maintenance resources and activities of a facility are reactive maintenance, which is to wait until a utility breaks before it is serviced (Sullivan et al. ). The most cost effective maintenance is preventative, in which actions are performed on a time-based schedule with the aim of sustaining or extending a resource\u2019s useful life through controlling degradation to an acceptable level (Sullivan et al. ).Automated systems have the potential to eliminate manual tasks (e.g., safety and inspection documents, warranties) and provide the ability for more cost-effective preventative maintenance (e.g. real-time scheduling, maintenance histories) (Thomas et al. ). For instance, they can alert the facility manager when a building component is due for an inspection and provide the digital details of that component, such as the maintenance history including the location within the facility, which are commonly marked up on paper documents. Moreover, integrating such systems with context-aware information provides potential for improvement in working practices, particularly with respect to productivity and safety (Anumba and Aziz ).The scope of this research is the use of passive radio frequency identification (RFID) as the sensing technology and focuses on the operation and maintenance phase of the lifecycle of an office building. It is assumed that objects in the facility are already tagged with RFID tags. Ideally the objects would be tagged via supply chain management, but the developed approach will also extend to the additional tagging of objects already in place inside a facility. The objectives of this paper are (1) develop a framework that utilizes the integration of commercially-available RFID and a building information model; (2) evaluate the framework for real-time resource location tracking within an indoor environment; and (3) develop an algorithm for real-time localization and visualization in a BIM. The goal is to achieve a system accuracy within 3\u00a0m at 95\u00a0% precision, as defined by the study of Taneja et al. (). This means that real-time visualization display must be within 3\u00a0m of the actual location 95\u00a0% of the time. An accuracy of 3\u00a0m is small enough to guide personnel to the general location of components or equipment in a facility, which then the personnel can distinguish the desired item. Previous works of this research () have demonstrated the ability to achieve this accuracy in a controlled test environment, and the next step is to deploy on a larger scale. This paper only deals with passive RFID location tracking based on utilities located inside a facility. The position of the reader can be visualized in real-time in the BIM model, which will aid in the determination of current location within the facility, as well as facilitating navigationThe contribution of this paper is to demonstrate the importance of utilizing a BIM model to increase the accuracy of indoor localization. This paper presents the fusion of commercially available passive RFID technology, indoor localization techniques and a BIM model for effective real-time indoor location tracking and visualization, without relying on existing building or wireless infrastructure.Each manufacturer of sensing equipment (e.g. RFID tags, readers, antenna, etc.) gives a range of specifications. For example, a reader angle may say \u201c65\u00b0 plus or minus a degree.\u201d Although the range may not seem large, it still may lead to false results if improperly calculated. Therefore, it is proper to test the specifications of each newly acquired piece of equipment to verify it is in accordance with the manufacture\u2019s specification. Each unit varies slightly, and it is important to use what has been verified from the testing to produce proper results in the final system. This section provides the experiments and results used in this research.The origin from the model floor was used as the origin for the testing (see Fig.\u00a0). The cart was pushed clockwise around the corridor loop, stating at point (\u22120.88,-1.20). The cart was pushed north until it reached the center of the next corridor (\u22120.88, 13.52), rotated counterclockwise 90\u00b0 proceed west to the ne12xt corridor (12.44, 13.52). Again it rotated and proceeded south until point (12.44, \u22121.21), where it then rotated again and finished back at the original starting point. The trial was repeated twice more, and then repeated three more times going in the counterclockwise direction, for a total of 6 trials. The ground truth of the data was keep by keeping the time of each movement (e.g., start, stop, turn) and the compass data.In this study, we investigated the role that Building Information Modeling (BIM) can have in common indoor localization techniques. The hypothesis is that using the information contained within a building information model can increase the accuracy of indoor positioning algorithms. As discussed before, some techniques need to estimate landmarks and the surrounding area in order to be used in the position estimations algorithms. Thus, we argue that building information models, which are becoming more prevalent in the industry, can provide that information. For example, when the algorithm estimates a landmark, that estimation will have an error associated to it, and will affect the total error when it is used to locate the position. Therefore, knowing the position of the landmarks can drastically increase the location estimation, as was shown in the results.The approach used in this study included passive RFID technology. We demonstrated the feasibility of using RFID technology and the benefits it has, such as low maintenance and cost. Additionally, we are taking advantage of the tags that are already on various building objects from the supply chain management and installation of those objects, in which the known coordinates would be placed in the building information model. However, if building assets are not tagged with RFID, adding tags throughout the building and inputting the locations in the building information model requires potentially significant other resources, such as additional or other sensing and interface infrastructure, manual effort, and time.Our proposed framework and method does implement passive RFID technology, however, the algorithms can be adjusted slightly to incorporate other technologies, such as WIFI, cellular, Bluetooth, or cameras, to name a few. The novel portion of the algorithms is how to integrate both the BIM data and landmark estimations to produce accurate location estimations. Significantly, the results validate the hypothesis that BIM can increase indoor localization accuracy, and show the usefulness of using BIM for indoor localization in addition to real-time visualization.The integration of real-time location tracking data in a BIM provides many useful applications to optimize safety, security and productivity. Integrating passive RFID technology and a BIM model into a single application and deploying it for indoor settings could play a more significant role in advancing decision making in FM. The integration provides valuable context-aware information that can be helpful for preventative maintenance. The data fusion between passive RFID technology and a BIM model serves to be an invaluable accomplishment that can be utilized for future research and applications.The major contribution of this paper is the demonstration of how information stored within the BIM model can increase the accuracy of localization algorithms. This paper described what information can be utilized, and how that information can be used. Other contributions include the framework to use the BIM data for indoor localization, and the algorithm incorporating the data.This research makes use of multiple localization techniques. With the framework presented, users can choose any technique and integrate the BIM mode data into it. The paper demonstrates the importance that the BIM model can pay on such techniques to provide additional context-aware information to increase the accuracy. The system accuracy was within the 3\u00a0m accuracy goal, validating that BIM can improve the accuracy of indoor localization techniques.The results provided the feasibility of integrating passive RFID with the BIM for indoor settings. However, the characteristics of the passive RFID present challenges for pinpointing the true locations, such as misreads or lower RSSI than expected. Further refining is required to correct these challenges, including adjustments to the RSSI maps and the Friis forward-link equation. Adding a particle filter may help calculate the locations of the tags at a higher probability, resulting in a lower system error. More defined testing will be needed to implement additional measures. Field implementation needs to be done with large numbers of tags, and performance of the application should be tested in real world conditions. Eventually robot-assisted systems become available (Cicirelli et al. ). This research can also be extended to consider fire and rescue operations because it offers the potential for developing a tool for saving lives in the event of an emergency or providing right-time alerts (Teizer et al. , Marks and Teizer , Marks and Teizer , Costin et al. ). In addition to finding tagged utilities, the location of medical personnel or equipment in hospitals, police dogs trained to find explosives in a building, and evacuation routes can all be found using indoor localization. Since RFID tags do not rely on a power supply from the facility, they keep working in the event of a power outage. This may prove crucial since the rescue team can still use the tags to guide them through the facility (heat and fire resistance of tags still needs to be researched)."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0778-2", "title": "Anodal transcranial direct current stimulation over the left temporal pole restores normal visual evoked potential habituation in interictal migraineurs", "authors": ["Francesca Cortese", "Francesco Pierelli", "Ilaria Bove", "Cherubino Di Lorenzo", "Maurizio Evangelista", "Armando Perrotta", "Mariano Serrao", "Vincenzo Parisi", "Gianluca Coppola"], "publication": "The Journal of Headache and Pain", "publication_date": "19 July 2017", "abstract": "Neuroimaging data has implicated the temporal pole (TP) in migraine pathophysiology; the density and functional activity of the TP were reported to fluctuate in accordance with the migraine cycle. Yet, the exact link between TP morpho-functional abnormalities and migraine is unknown. Here, we examined whether non-invasive anodal transcranial direct current stimulation (tDCS) ameliorates abnormal interictal multimodal sensory processing in patients with migraine.", "full_text": "Migraine is a neurological disorder that is characterized by recurrent clinical attacks separated by variable-length headache-free intervals. Although the pathogenesis of migraine is far from completely understood, clinical neurophysiology and neuroimaging studies in recent decades have disclosed subtle functional and morphological abnormalities that manifest during the interictal phase and distinguish migraineurs from normal healthy subjects [\u2013]. Among the various subcortical and cortical areas implicated in migraine pathophysiology, emerging evidence highlights the temporal pole (TP) as a key neural substrate. In humans, the TP serves as a multimodal neural hub that receives and integrates various sensory modalities including olfactory, auditory, taste, and visual inputs. Moreover, the TP participates in the ventral visual stream (VVS) for visual information processing [\u2013]. During an olfactory task, interictal migraineurs exhibited significantly higher brain glucose metabolism in the left TP compared to control subjects []. Moreover, BOLD signal in the TP in response to noxious stimulation was reduced in interictal patients compared to patients who were actively experiencing a migraine [, ]. In resting-state MRI studies comparing interictal migraineurs to healthy control subjects, decreased grey matter density was observed in the left TP [] and the left TP exhibited decreased connectivity with components of the default-mode network []. Finally, the TP was implicated as an important area for differentiating patients with migraine from healthy control subjects in a cross-sectional brain MRI investigation []. Taken together, these findings suggest that the TP is both intricately related to the pathophysiology of migraine and sensitive to the cyclical recurrence of migraine attacks.Transcranial direct current stimulation (tDCS) is a non-invasive technique for neuromodulation in humans that affects cortical excitability in a polarity-specific manner [, ]. Anodal polarization increases the excitability of cortical areas below electrodes, whereas cathodal polarization typically decreases cortical excitability []. A number of tDCS studies in different pain disorders [, ] have demonstrated that tDCS is well-tolerated by patients []. Anodal tDCS proved effective over either the motor cortex or the dorsolateral prefrontal cortex when used as prophylactic strategy both in episodic [] and chronic [, ] migraine. Moreover, some studies reported that, in addition to the therapeutic effects, tDCS over the visual cortex also normalized interictal cortical hyperresponsivity in episodic migraine [].Nonetheless, to our knowledge, no study to date has targeted the TP for anodal tDCS in migraine, to enhance interictal temporal lobe activity and thereby interfere with an aspect of migraine pathophysiology. Thus, we examined whether anodal stimulation of the TP could restore normal function of the TP and thus physiological information processing in migraine. Moreover, given that the TP processes all kinds of sensorial information except for somatosensory information, we examined the habituation responses of evoked potentials to somatosensory stimuli (as a negative control) as well as visual stimuli.The present study mainly revealed that a single session of anodal tDCS over the left temporal pole restored normal visual but not somatosensory habituation in interictal migraineurs.Neurophysiological studies have shown that interictal migraineurs exhibit dysfunctional sensory information processing in the form of habituation deficits in response to various sensory inputs, including visual and somatosensory inputs []. Recent neuroimaging studies have revealed subtle microstructural alterations in the brains of patients with migraine in areas associated with the ictal-interictal cycle. Among these studies, some evidence highlights a pathophysiological role for the TP in migraine [\u2013].The TP region encompasses the most anterior segment of the temporal lobe and receives extensive inputs from visual regions of the thalamus [, ]. Additionally, the TP is highly interconnected with the amygdala, hippocampus, superior temporal gyrus, hypothalamus, occipitobasal cortex, prefrontal regions, and insula, suggesting its participation in autonomic regulation, memory, and emotional processing [, ]. The TP is considered a multisensory associative cortex because it is also connected to the main sensory systems of the temporal lobe, including the visual, auditory, olfactory, and gustative systems, but not the somatosensory system [, ]. Indeed, neuroimaging studies have demonstrated subregional activation of the TP in response to specific sensory stimuli, with the ventromedial aspect of the TP having a predominant role in higher order visual information processing [] as part of the VVS.Our finding that anodal (excitatory) stimulation of the left TP restored physiological visual information processing but not somatosensory processing in interictal migraineurs is largely consistent with the abovementioned roles of the left TP in high-level multimodal perceptual processing. A selective effect of tDCS over the TP on visual information processing is probably related to the role of the TP in the VVS and its lack of participation in somatosensory elaboration. Interestingly, another study observed similar normalization of abnormal interictal VEP habituation in response to the application of tDCS over the occipital cortex in migraineurs []. This can be explained either by a direct interconnection between the TP and occipital cortex along the VVS or an indirect effect of the tDCS on brain structures that positively modulate both cortices.The VVS is involved in visual recognition and in the assignment or retrieval of a given meaning for visual information []. After early activation of the occipital area, the complexity of representation of visual information increases as information flows to the anterior regions of the VVS, with the TP located at the end of the stream and sending backward facilitatory projections to the occipital cortex to optimize sensory processing (e.g., improve perception and learning) [, ]. Consistent with this evidence, we observed that the enhancement of TP activity with anodal tDCS improved VEP amplitude habituation, a basic form of learning [], without affecting initial baseline excitability (reflected by non-significant changes in 1st block VEP amplitudes). In habituation paradigms, early and late responses can behave differently as a result of regulation by different mechanisms; according to the dual-process theory, increasing responsiveness (sensitization) competes with decreasing responsiveness (habituation) to determine final behavioural outcomes. Facilitation occurs at the beginning of the stimulus session and accounts for an initial temporary increase in response amplitude, whereas habituation occurs throughout the recording session and accounts for delayed decreases in responsiveness []. Therefore, our results regarding the selective effect of anodal tDCS on delayed habituation in migraineurs appear to be in line with the putative mechanism of tDCS; that is, the ability of tDCS to affect the potentiation of long-term learning processes and synaptic plasticity underlying learning and memory []. Alternatively, it has been shown that anodal tDCS exerts modulatory effects on thalamo-cortical circuits by increasing functional coupling between the thalamus and cortex [, ]. These experimental observations are of particular interest in migraine because independent research groups have previously reported reduced functional [, ] and morphological [, ] thalamic integrity coupled with decreased intracortical inhibition during visual stimulation in migraineurs [, ]. We thus can hypothesize that an alternative mechanism of action for anodal tDCS in the present study is increased thalamo-cortical activity, which in turn increased delayed inhibitory mechanisms to restore normal VEP habituation.Irrespective of the mechanism, the observation that tDCS over the left TP is able to restore normal VEP habituation in interictal migraineurs leads to hypothesize that together with the visual, motor, and dorsolateral prefrontal cortices [, ], the TP could represent a novel target for tDCS as a prophylactic strategy for treating migraine [].This study had some limitations. For example, we only stimulated the left TP, such that we cannot know whether anodal tDCS of the right TP would have yielded similar results. Several studies have shown divergent functional roles of the left and right TP, where the right TP is more involved in elaborating socio-emotional implications of multisensory perceptual stimuli [] while the left TP is mostly implicated in perceptual decoding, semantic processing, and conceptualization []. Nonetheless, both the left and right TPs are joined via the interior white commissure to advance multimodal perceptual analysis [], such that the relevance of the right TP cannot be discounted. Furthermore, the positioning method we used is accurate, although not as accurate as neuronavigation-based techniques, which are unfortunately only available for neurosurgical procedures in our clinic. Another shortcoming of the present study is the lack of inclusion of a healthy control group undergoing the same stimulations, although this would not add anything to the results of the study because the healthy subjects usually already habituate normally at the baseline, i.e. we cannot normalize the already normal information processing.In conclusion, anodal but not sham tDCS selectively enhanced visual but not somatosensory habituation in interictal migraineurs probably by restoring normal inhibitory activity of the left TP. We propose that this effect can be explained by either a direct interference with short- and long-term synaptic plasticity mechanisms or an indirect potentiation of the thalamo-cortical circuit. Further studies are needed to determine whether TP stimulation also normalizes the habituation response to other sensory inputs, such as auditory and nociceptive inputs. Regardless of the underlying cellular and molecular mechanisms of our observed effect, we propose that the TP should be considered as a key site of involvement in the pathophysiology of migraine and as a potential therapeutic target. Clinical studies are needed to clarify whether repeated sessions of anodal tDCS improve TP function and connectivity in patients with migraine to ultimately reduce the number and severity of migraine attacks."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0782-6", "title": "Functional connectivity and cognitive impairment in migraine with and without aura", "authors": ["Viviana Lo Buono", "Lilla Bonanno", "Francesco Corallo", "Laura Rosa Pisani", "Riccardo Lo Presti", "Rosario Grugno", "Giuseppe Di Lorenzo", "Placido Bramanti", "Silvia Marino"], "publication": "The Journal of Headache and Pain", "publication_date": "20 July 2017", "abstract": "Several fMRI studies in migraine assessed resting state functional connectivity in different networks suggesting that this neurological condition was associated with brain functional alteration. The aim of present study was to explore the association between cognitive functions and cerebral functional connectivity, in default mode network, in migraine patients without and with aura, during interictal episodic attack.", "full_text": "Migraine is a common episodic neurological disorder with a complex physiopathology. It is characterized by typical unilateral, often severe, pain throbbing with associated features such as hypersensitivity to multiple stimuli, including visual (photophobia), auditory (phonophobia), and sensory (cutaneous allodynia) stimuli during migraine attacks []. Indeed, about one third of patients had experience of aura associated to visual, motor, or somatosensory symptoms during attacks [, ].Migraine is a very common and debilitating disease that causes significant limitations in daily life with effects on emotional-behavioral and relational aspects []. Neuropsychological studies suggests that migraine affect also cognitive functions during attacks and interictal periods [], even though it is unclear the association between cognitive dysfunctions and migraine. Migraineurs could present executive dysfunction which presumably reflects frontal lobe abnormalities [], or alteration in memory areas. However, while several authors reported significant lower performances in migraine patients, others did not confirm these findings. In other cases authorsdescribed the presence of cognitive deficit only after a long disease duration [, ].Several fMRI studies in migraine assessed resting state functional connectivity in various networks suggesting an association with cortical functional alteration []. In particular, some authors reported increased connectivity in specifics cerebral areas, such as right rostral anterior cingulate cortex, prefrontal cortex, orbitofrontal cortex and supplementary motor area []. This altered connectivity could indicate intrinsic pathophysiological changes in migraine, even if only a very few studies explored the different functional connectivity in migraine with (MA) and without aura (MO) [].The aim of present study was to explore the association between cognitive functions and cerebral functional connectivity (FC) between MO and MA, during interictal episodic attack.Twenty-eight migraine patients (14 without aura and 14 with aura) and 14 sex and age matched health controls (HC), were enrolled. Aura included temporary visual or sensory disturbances nausea, and sensitivity to light and sound. The patients were recruited from migraine ambulatory. The diagnosis of definite MA or MO was performed by two neurologist, specialist in headache disorders, blinded to MRI and neuropsychological findings, according to International Headache Society criteria [] (Headache Classification Committee of the International Headache, 2013).Control subjects were volunteers recruited from local communities, with no history of neurological diseases. They did not suffer from migraine or headache and were free from medication intake. The study protocol was approved by the Local Ethics Committee according to Declaration of Helsinki. All patients gave written consent to study. All information related to migraine was collected by interviews and examination of medical records. All patients had a clinic diagnosis for at least 10\u00a0years. We excluded patients with: 1) other types of headache; 2) vascular disease or trauma; 3) history of major psychiatric disorders; 4) presence of metabolic disorders; 5) other neurological conditions.A battery of standardized neuropsychological test to evaluate cognitive functions, was administered by two psychologists, blinded to patients/controls status, diagnosis and MRI findings. Processing speed was assessed using the Trail Making Test, Part A (TMT-A), []. Attentional set-shifting was measured using the Trail Making Test, Part B (TMT-B). Memory was assessed using the Rey Auditory Verbal Learning Test (RAVLT) []. Language was assessed with semantic and phonemic verbal fluency test []. Wisconsin Card Sorting test (WCST) was used for executive function and cognitive flexibility. Finally, Hamilton Rating Scale for depression (HAM-D) and Hamilton Rating Scale for anxiety (HAM-A) were used to asses anxiety and depressive symptoms [, ].All patient underwent to a MRI examination with a scanner operating at 3.0\u00a0T (Achieva, Philips Healthcare, Best, The Netherlands), by using a 32-channel SENSE head coil. MRI scans were performed in the interictal stage at least 3\u00a0days after migraine attack. For each subject, T1 [TR\u00a0=\u00a08\u00a0ms, TE\u00a0=\u00a04\u00a0ms, slice thickness/gap\u00a0=\u00a01/0\u00a0mm, number of slices\u00a0=\u00a0173, field of view 240\u00a0mm], T2-weighted [TR\u00a0=\u00a03.0\u00a0s, TE\u00a0=\u00a080\u00a0ms, slice thickness/gap\u00a0=\u00a03.0/0.3\u00a0mm, number of slices\u00a0=\u00a030, field of view 230\u00a0mm] were acquired. The scan parameters of the resting-state functional magnetic resonance imaging (fMRI) scan were as follows: TR\u00a0=\u00a03.0\u00a0s; TE\u00a0=\u00a035\u00a0msec; flip angle\u00a0=\u00a090\u00b0; and voxel size 1.9 \u00b7 1.9 \u00b7 4.0\u00a0mm, scan duration 10\u00a0min. During the resting-state scan, participants were instructed to lie still with their eyes closed and not to fall asleep.Neuropsychological testing and MRI scanning were performed on same day.Recently, several studies investigated the activity of resting state network in migraine and showed alterations in brain functional reorganization. Altered functional connectivity was found in cognitive cerebral networks, such as executive control network, default mode network, visual network. It seem to be associated to disease duration, gender, and migraine chronicity [\u2013]. The DMN is a cerebral network related to different regions with relatively greater activity during rest-state than during active conditions [, ]. It refers to an interconnected group of brain structures that are hypothesized to be part of a functional system. Although the exact functional role of DMN is not completely know, it is thought to be involved in several cognitive processes, such as memory, problem solving and planning [, ]. In DMN, there are heteromodal association areas, which have a high number of connections with brain regions involved in integration processes, including pain matrics. In chronic pain DMN is altered [], and this is possibly due to the increase of baseline activity of other cognitive, salience, or sensorimotor networks. Over time, chronic pain becomes an intrinsic brain activity occurring even in the absence of explicit brain input or output: thus, the alterations in patient\u2019s brain at \u201crest\u201d could be considered as a different or altered DMN organization []. In our study we identified specific alterations, during resting state examination, in cortical DMN if we compared MA, MO and HC. Our findings showed an increase of functional connectivity, in MA, in frontal and parietal lobes, in particular in angular, supramarginal gyrus, , postcentral gyrus and primary somatosensory cortex. Since pain is inherently salient it is rational to speculate that the intrinsic connectivity in this network may be changed in chronic pain patients, like migraine subjects. In addition, in MA patients, we found an altered connectivity in insular cortex. It is know that insula is involved in triggering of pain matrix network and in the subjective pain experience []. It is also implicated in cognitive, affective, and regulatory functions, including interoceptive awareness, emotional responses, empathic and attentional processes []. The insula seems to be a cortical hub, to process complex sensory and emotional aspects in the migraine condition [], through connections in frontal, temporal and parietal cortex, basal ganglia, thalamus and limbic structures. It is important to understand if functional connectivity abnormalities in this network could be correlated to minimal impairments in neuropsychological performances, such as processing speed, verbal memory, as reported in migraine in interictal attack period. In fact, although MA showed a cognitive performance lower than MO in executive functions, we did not find a significant impairment in two groups. In other word, in our patients, connectivity altered in DMN dwas not associate to neuropsychological variables and cognitive performances.Moreover, we found in MA a greater cortical hyperexcitability than MO: resting-state abnormal activity could play a key role in the pathogenesis knowledge of migraine attacks with aura []. In particular, alterations of the DMN functional connectivity in migraine may lead to changes in pain modulating network, which could be considered as a neuroimaging biomarkers for disease pathophysiology.The importance of various frequencies of BOLD fluctuations is not yet known, even if recently few studies started to explore this feature, especially in pain conditions. Brain dysfunction affecting intrinsic connectivity in migraine, possibly reflecting the impact of long lasting and constant pain on brain function.Although our study was limited to a small sample size, our results confirmed that brain functional connectivity in migraine patients showed an alteration of DMN connectivity, suggesting that pain has a widespread impact on brain function, since modify the complex brain networks and beyond pain perception. Although migraine is one of the most investigated neurologic disorders, specific neuroimaging biomarker for its pathophysiology has not been found.Altered intrinsic functional connectivity architecture was identified in migraine patients and our finding could provide a new perspective to understand the pathogenesis of MA and MO migraine, in order to find a more appropriate therapeutic management."},
{"url": "https://videoeducationjournal.springeropen.com/articles/10.1186/s40990-016-0003-2", "title": "Construction and importance of video based analyses teaching in physical education by use of window live movie maker", "authors": ["Ramananda Ningthoujam"], "publication": "Video Journal of Education and Pedagogy", "publication_date": "2 August 2016", "abstract": "Today, Video Based Analysis (VBA) is one of the teaching methods widely used in different fields that help in effective teaching and learning process.", "full_text": "\n                        \n                        Today\u2019s world is a world of technology revolution, wherein the desire to capitalize the new generation\u2019s appetite for multimedia presentations are increasing (Van Laarhoven & Myers ). In such a scenario adopting the audio- visual method of teaching would be the most apt as teaching in Physical Education (PE) is a huge challenge (McLean & Daniel ) as it involves learning by doing. The classroom in actuality is a field of activity that involves myriad forms of unique teaching methodology (Singh et al. , p. 149). Video Based Analysis (VBA) is one of the many teaching aids that would offer teachers, lecturers and curriculum developers to inculcate interest (Towers , Zelaznik, ) and attention (Ozkan, ) among their students. Videobased lectures can become a cost effective teaching method () in that they can be created once, and then saved in libraries to be used by a large number of academic educators (R.E.-.S.H. El-Sayed, S.E.-H.A.E.-R. El-Sayed ).Video analysis is a common tool that is used in modern sports to increase coaching performance (Dam ; Martin ) for individual and team competitions (Harvey and Gittins, ). The National Association for Sport and Physical Education (NASPE ) believes that technology can be an effective tool for supplementing instruction when used appropriately.Video instruction used in physical education is to provide students a view of their own performance or feedback as to what they have done (Silverman, ), or to provide instructors/coaches the proper steps to instruct others, not necessarily feedback on their own performance (Aaberg et al. ). Video-based coaching is an educational modality that targets intro-operative judgement, technique and teaching (Hu et al. ). Coaches and trainers use this method as a corrective method that will help in improving the performance of the athletes since they are only able to recall 30\u201450\u00a0% of key performance factors they had witnessed, even with special training in observation (Franks and Miller ; 9 (3), 285\u2014297). It serves as a feedback for the players which in turn help in motivating them to perform better.A. Lyons et al (), expressed greater perceived learning through the mediators of perceived usefulness of videos, class interactivity, and felt comfort in the class. The value of a video clip as a teaching tool lies in its potential to: 1. Tap simultaneously into core intelligences, 2. Engage both hemispheres of the brain, 3. Manipulate students\u2019 alpha and beta brain waves i.e. relax or make them alert (Berk, ). Besides, the use of Video analysis software helps in gait analysis (Borel et al. ), biomechanics research (Curran and Frossard ) and injury rehabilitation (DeLisa et al. ; Eastlack et al. ) . Hence, the purpose of this paper is to emphasize the importance of Video Based Analyses teaching and inculcate the method in the field of Physical Education and Sports.A page devoted to the specification of some defining operation is shown in : Table 2, as the area encompassed by the term may differ from the views of many readers.In this paper, it explains the purpose of, instructional manual for creation of, and usage instructions for video based analysis (VBA) by used of Window Live Movie Maker (WLMM), an emerging technique of video review attempting to encourage the use of video-based analysing teaching in PE. The secondary purpose of video based analysing teaching is to present a video methodology to deliver a more informed concept in a more interactive and engaging manner to understand the minute detailing of the physical movement of the athlete.The materials included one digital video camera with a tripod stand and a computer with software (window Live Movie Maker) allowing frame-by-frame playback of the video. Reflective marker is optional but it is very much needed to see the clear movements of the joints.A National Basketball player who has represented University was recruited as subject for constructing a video-based analysis teaching video.The first step is \u201cRecording\u201d of the skill. This was done with the Digital high speed camera (Cannon- Model EOS-7D) made in Japan. The video recorded was converted into AVI format using video converter (version 3.0.3) that allows synchronous audio-with-video playback.The last step is \u201cPresentation\u201d of video which can be used as a teaching tool, giving feedback, visual perception of the skill, etc. The flow-chart steps to prepare the video model are shown below:Open WLMM\u2009<\u2009Add Video clips (Uncut)\u2009<\u2009Video tools\u2009<\u2009Set Start & End Point\u2009<\u2009Snap shoot\u2009<\u2009saving in a new folder\u2009<\u2009Open new WLMM\u2009<\u2009Adding the snap pictures\u2009<\u2009Add Title, music & caption\u2009<\u2009Editing the title\u2009<\u2009Chose Auto Movie Themes\u2009<\u2009Presentation.The outcomes of the study was that a self made video model was constructed by used of WLMM, which can be used as teaching tools, feedback tool, visual perception of the skill, creating interest to the participants. The progressive change of the joints starting from the ready position till the release of the ball was analysed using the video model. These fast movements are not generally noticed by the naked eye. Therefore, it is important to use such video method, as initial learning stages involve understanding global aspects of the task (Ahissar and Hochstein, ). The learning of motor skill (Set Shoot) initially involve perception of an object, especially visual perception, the interest in perception accompany the development of the interest in skill acquisition (Singh et al. , p.148). Mayer in 1996 also supported that learning a motor skill initially involves visual perception of the skill to be performed which inculcate interest in mental imaginary accompany the development of the interest in skill acquisition (Towers ).Use of such video method in teaching may improve learning outcomes for students, even though it adds to cognitive load (Homer et al., ) and it create a sense of social presence (Gunawardena, ). Though players have different level of motor educability and different ranges of initial skill levels (Machar Reid ), which is the ability to learn motor skills easily and quickly (Mc. Cloy and Young ). Karkare () found that group with higher motor educability have shown more physical skill compared to low motor educability group. Syamsuddin () also studied the influence of teaching styles and motor educability on the learning outcomes of volleyball. He found that students who have a high motor educability, learning outcomes of volleyball through application of a practice style is better as compared to an inclusion style (Lecturer teaching).As the outcomes of the videos, the progressive changes of body position, joints movements, etc can be depicted easily that will make to know the set shoot skill know more clearly. In the L-phase, throwing hand is in the shape of alphabet \u201cL-shape\u201d (Additional files , ,  and )."},
{"url": "https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-015-0028-3", "title": "Extensive Four-Dimensional Chaos in a Mesoscopic Model of the Electroencephalogram", "authors": ["Mathew P. Dafilis", "Federico Frascoli", "Peter J. Cadusch", "David T. J. Liley"], "publication": "The Journal of Mathematical Neuroscience (JMN)", "publication_date": "12 August 2015", "abstract": "In a previous work (Dafilis et al. in Chaos 23(2):023111, 2013), evidence was presented for four-dimensional chaos in Liley\u2019s mesoscopic model of the electroencephalogram. The study was limited to one parameter set of the model equations.", "full_text": ""},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0777-3", "title": "Therapeutical approaches to paroxysmal hemicrania, hemicrania continua and short lasting unilateral neuralgiform headache attacks: a critical appraisal", "authors": ["Carlo Baraldi", "Lanfranco Pellesi", "Simona Guerzoni", "Maria Michela Cainazzo", "Luigi Alberto Pini"], "publication": "The Journal of Headache and Pain", "publication_date": "20 July 2017", "abstract": "Hemicrania continua (HC), paroxysmal hemicrania (PH) and short lasting neuralgiform headache attacks (SUNCT and SUNA) are rare syndromes with a difficult therapeutic approach. The aim of this review is to summarize all articles dealing with treatments for HC, PH, SUNCT and SUNA, comparing them in terms of effectiveness and safety.", "full_text": "Trigeminal autonomic cephalalgias (TACs) is a rare group of headaches characterized by unilateral attacks of severe throbbing pain, mainly localized in the orbital region, associated with unilateral cranial autonomic signs such as lacrimation, conjunctival injection, palpebral ptosis, rhinorrhoea, eyelid edema, facial sweating, facial redness and ear-fullness. The International Classification of Headache Disorders 3rd Edition beta version (ICHD-III-beta) recognizes 4 TACs: cluster headache (CH), hemicrania continua (HC), paroxysmal hemicrania (PH) and short-lasting unilateral neuralgiform headache attacks (SUNCT and SUNA) []. HC is characterized by a continuous background of moderate pain intensity and has only recently been classified as a TAC []; on the contrary, CH, PH, SUNCT and SUNA lack the history of background pain []. TACs rather than CH are uncommon and neglected syndromes: the annual prevalence of PH and short lasting unilateral neuralgiform headache attacks is about 0.5/1000 in the general population and is still unknown for HC [], this facilitate their misdiagnosis, which often delays the correct treatment []. Treatment delay, especially in chronic forms, dramatically decreases the patients\u2019 quality of life because pain is often severe, highly-disabling and can last, even if not continuously, for many hours during the day []. Only a few therapeutic tools are available for these conditions and this is firstly due to their infrequent diagnosis, which makes the conduction of well-prepared randomized clinical placebo-controlled trials (RCPCTs) almost impossible. The effectiveness and safety of the treatments are reported mainly in case-reports, case-series, letters to the editor and brief communications. This leads to a not-scheduled treatment for TACs and the absence of shared guidelines. Furthermore, there aren\u2019t studies clearly ranking treatments to manage TACs, nor one comparing them in terms of effectiveness and/or safety. The aim of this study is to rank all therapeutic options available in literature for HC, PH, SUNCT and SUNA treatment and to compare, when possible, their effectiveness and safety. Since there are already shared guide-lines and a large amount of reviews dealing with CH, this won\u2019t be discussed further.PH, HC, SUNCT and SUNA represent a hard challenge for clinicians who work in headache or pain fields. Moreover, their infrequence makes difficult to study the pathogenesis of these conditions, as well as design well-done RCPCT for new drugs. From the review of the available literature indomethacin emerges as the best treatment for HC and PH, while other drugs like celecoxib, topiramate and gabapentin may be useful. SUNCT and SUNA should be managed with intravenous steroids or lidocaine in the worst cases and for short periods of time, with a subsequent change for preventive treatment to lamotrigine or ONS.In conclusion, it should be highlighted that further studies are required to implement guidelines to treat the disease and to discover new effective and safe therapies for these conditions."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0776-4", "title": "Is topiramate effective for migraine prevention in patients less than 18\u00a0years of age? A meta-analysis of randomized controlled trials", "authors": ["Kai Le", "Dafan Yu", "Jiamin Wang", "Abdoulaye Idriss Ali", "Yijing Guo"], "publication": "The Journal of Headache and Pain", "publication_date": "18 July 2017", "abstract": "Mainly based on evidence of success in adults, various medications are commonly used to prevent pediatric migraines. Topiramate has been approved for migraine prevention in children as young as 12\u00a0years of age. In this meta-analysis, we aimed to assess the currently published data pertaining to the efficacy of topiramate for migraine prevention in patients less than 18\u00a0years of age.", "full_text": "Migraine is the most common cause of headache in pediatric neurology outpatient clinics, and it has been recognized as one of the most prevalent neurological disorder in children and adolescents worldwide, affecting 5\u201310% of the pediatric population in multiple areas of life. Because patients miss school and social activities, migraines can impair the development of friendships that are vital to social development and self-esteem and may destroy family harmony [, ]. The mean age of onset of migraine is 7.2\u00a0years in boys and 10.9\u00a0years in girls [], and the prevalence of migraine increases with age, as demonstrated by clinical studies. The diagnostic criteria for migraine headaches have developed over time; modern migraine classification includes frequency as a criterion, with episodic headaches occurring up to 14\u00a0days per month, and chronic migraine is defined as the persistence of headache without aura for at least 15\u00a0days per month and for at least 3 consecutive months without medication overuse (ICHD-II) []. Because of the diversity of symptoms, the diagnosis of migraines in children and adolescents needs to be refined even further. Due to the harm caused by migraines, reducing the number of migraine attacks to the greatest extent possible should be a priority.At present, a variety of prophylactic therapy options are available to reduce the frequency or severity of headaches []. Topiramate is an antiepileptic drug with positive efficacy and safety for older children and adults with epilepsy [], and it has been approved for migraine prevention in adults in Europe since 2003 and in the United States since 2004 []. The exact mechanism of topiramate in the treatment of migraine is unknown, although it may be associated with the influence of topiramate on pain transmission in the trigeminocervical complex and the third-order neurons in the ventroposteromedial thalamus []. Several case series and open-label trials [\u2013] have shown that topiramate served as a preventive treatment for pediatric migraines, while the research of Scott W [] indicated that there were no significant differences between topiramate and placebo in the prevention of pediatric migraine. Hence, in the present study, we performed a meta-analysis of randomized controlled trials (RCTs) to evaluate the efficacy of topiramate for the prevention of migraine in patients less than 18\u00a0years of age.This meta-analysis examined the efficacy of topiramate in comparison with placebo for the prevention of migraines in patients less than 18\u00a0years of age. The IHS guidelines for conducting clinical trials indicate that a clinically meaningful end point in a migraine prevention trial is usually defined by a reduction in the total number of headache attacks in a 28-day period or the proportion of patients with a greater than 50% relative reduction in headache frequency [].Topiramate is a first-line option for the treatment of migraines in adults, and in March 2014, the U.S. Food and Drug Administration (FDA) approved topiramate for migraine prevention in the population aged 12 to 17 []. Moreover, this is the first and only medication currently approved for use in migraine patients 12\u00a0years and over. Nevertheless, neither the primary outcome of proportion of patients with a greater than 50% reduction in headache frequency nor the secondary outcome of reduced mean headache days in a 28-day period showed topiramate as more efficacious than placebo in our meta-analysis of four RCTs. According to the definition [], topiramate showed no statistically significant benefit over placebo in reducing the number of headache days over the treatment period. In fact, the 50% response rate of the topiramate group in 2 trials [, ] was not statistically significant compared with the placebo group, and in another trial [] a similar result was presented for the 50\u00a0mg/day topiramate treatment group. The finding conflicts with the outcomes of previous case series and open-label trials. There are at least three possible explanations for this finding. (1) Children tend to have a high placebo response rate, with younger patients in clinical trials demonstrating a greater tendency to respond to placebo. This age-dependent placebo response has ranged from 30% to 70% in migraine studies in general [\u2013]. The outcome of our study shows that the average number of patients experiencing a\u00a0\u2265\u00a050% relative reduction in headache frequency in the placebo group is 50.74% (69/136), which is higher than the rates reported in previous studies of topiramate preventing adult migraine (0\u201334.2%) [\u2013]. Rothner et al. [] suggested explanations for the higher placebo response rate in clinical trials with children and adolescents, such as the fact that they could not take medication while at school; \u201cgood doctor\u201d effects; and the fact that if their symptoms relieved spontaneously, children and adolescents were more likely than adults to believe that they were receiving a drug that had a definite effect on headache. We speculate that this phenomenon is associated with at least the following factors: 1. Different psychological and neurobiological mechanisms exist in pediatric patients compared with adults. There are at least four psychological mechanisms associated with the placebo response: expectation, conditioning, therapeutic relationship and empowerment []; psychological mechanisms, especially the conditioning and expectation may guide people\u2019s behavior. The differential course of the maturation of different neurotransmitter systems may explain the differences. 2. The characteristics of migraine attacks are different []: migraine headaches in children and adolescents are often bilateral and may be of shorter duration than in adults. 3. Children/adolescents and adults have significantly different cognitive levels. The pain sensation is a highly subjective experience that is influenced by cognitive factors, and placebo analgesia is one of the most striking examples of cognitive regulation of pain [, ]. In addition, the lack of pediatric research and the shortage of experience in experimental design may lead to different outcomes. In short, the topic of the difference about placebo response between children/adolescents and adults deserves further discussion. (2) The minimum age at which topiramate was approved for treatment of migraine was 12\u00a0years old, but the minimum age of patients in the included trials was 8\u00a0years. It is often difficult to calculate the attacks of headache in younger children accurately, and the guardians generally interpret the attacks indirectly from the child\u2019s activity level []. (3) Our included patients included those with either episodic or chronic migraine [], which may influence the results of our meta- analysis.The second finding of our meta-analysis is that topiramate decreased PedMIDAS scores in migraine patients. PedMIDAS is often used to measure disability related to school absences and functioning, home functioning, and social absences and functioning []. This finding, which contradicts our first finding, may indicate that headache-related disability is alleviated by topiramate. However, mean PedMIDAS scores in both the topiramate group and the placebo group decreased between baseline and endpoint, and the fact that only two trials [, ] used this tool as a trial assessment may be the cause of the heterogeneity.As with all antiepileptic drugs, topiramate has many potential side effects or adverse events, some of which may be serious and life-threatening []. The rate of adverse events in patients treated with topiramate was higher than that with placebo in our included trials. It has been reported that metabolic acidosis, renal calculi and nervous system effects, such as fatigue or somnolence, paresthesia, dizziness and cognitive disorder or aphasia, occurred in adults and pediatric patients taking topiramate in previous trials. Other adverse events, such as changes in visual acuity, including visual field deficits, acute myopia and secondary closed angle glaucoma, have also been reported. In addition, topiramate (100\u00a0mg/day) was related to modest increases in psychomotor reaction times []. Another more serious problem is the potential for suicidal behavior and ideation that has been observed in people taking antiepileptic drugs, including topiramate []. Thus, while the pathomechanism of migraine is not completely understood, the choice of medication for personalized therapy tailored to each patient needs to be made cautiously [].Some limitations in our meta-analysis must be acknowledged. First, because our analysis was limited to articles in the English language literature, we may have omitted some evidence. The secondary limitation is related to the data that we acquired from the four included trials. Three of the trials reported the baseline and follow-up data [, , ], and one reported baseline and change data []. We combined the follow-up and change data according to the research of da Costa, B. R []. In addition, one trial compared more than 1 dose []; it is likely that dose-finding pharmacologic studies are underrepresented and that additional unpublished industry trials exist. These situations might have resulted in ecological bias. Third, our data had obvious heterogeneity, and none of the variables we abstracted explained this variation. Because we only included four trials and because only three measurements were used in our study, therefore, our findings should be interpreted with caution. The variability in the selection criteria for RCTs and sample size, along with the incomplete reporting of intervention intensity, may also be limitations.This is the first meta-analysis of topiramate for migraine prevention in patients less than 18\u00a0years of age. We found that topiramate did not achieve a more effective clinical trial endpoint than placebo in the prevention of migraine in patients less than 18\u00a0years of age, and topiramate was associated with more adverse events in the included patients. It is possible that a high placebo response rate can be beneficial for children and adolescents with migraine and that drugs used to prevent pediatric migraine might be reconsidered. Because there was a significant placebo response, more placebo-controlled trials in the younger migraine population less than 12\u00a0years of age are needed."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0775-5", "title": "Migraine and the risk of post-traumatic stress disorder among a cohort of pregnant women", "authors": ["Lauren E. Friedman", "Christina Aponte", "\u2020", "Rigoberto Perez Hernandez", "\u2020", "Juan Carlos Velez", "Bizu Gelaye", "Sixto E. S\u00e1nchez", "Michelle A. Williams", "B. Lee Peterlin"], "publication": "The Journal of Headache and Pain", "publication_date": "6 July 2017", "abstract": "Individually both migraine and post-traumatic stress disorder (PTSD) prevalence estimates are higher among women. However, there is limited data on the association of migraine and PTSD in women during pregnancy.", "full_text": "According to the National Health Interview Survey\u00a0in 2011, 26.1% of women 18\u201344\u00a0years of age reported migraines or severe headaches in the last 3\u00a0months []. Migraine is more prevalent among reproductive-aged women as compared to men [] and from early to middle adulthood as compared to younger or older individuals []. Migraine also often adversely affects the health of large populations []. Further, migraine in pregnancy is associated with an increased risk of perinatal complications including preeclampsia [, ], preterm delivery [], placental abruption [], hypertensive disorders [], as well as cardiovascular disease and stroke [\u2013].Maternal mood and anxiety disorders have been implicated as important risk factors for migraine [\u2013]. Migraine during pregnancy is associated with an increased risk of depression [, ] and suicidal ideation []. Additionally, both migraine and PTSD are more prevalent in reproductive-aged women as compared to men [, ]. Although increasing data supports an association between posttraumatic stress disorder (PTSD) and migraine in U.S. cohorts [, , ], no prior study has examined the risk of PTSD in pregnant women. Further there is little evidence for the association between migraine and PTSD in women from low income countries or the impact of depression on this association. To fill in these gaps in the literature, we examined the association between migraine and PTSD among a cohort of pregnant women in Lima, Peru.In our cross-sectional study of pregnant Peruvian women, migraine (whether any migraine [migraine and probable migraine], migraine alone, or probable migraine alone) was associated with increased odds of PTSD. After adjusting for confounders including antepartum depression, women who reported any migraine had a 1.97-fold increased odds of PTSD (95% CI: 1.64\u20132.37) compared to women with no history of migraine. In a multivariable adjusted model, women with probable migraine had a 1.61-fold increased odds of PTSD (95% CI: 1.30\u20131.99), and women with migraine had a 2.85-fold increased odds of PTSD (95% CI: 2.18\u20133.74), compared to women without migraine (Table ). In the presence of antepartum depression, women with probable migraine or migraine had increased odds of PTSD (probable migraine: OR\u00a0=\u00a01.59; 95% CI: 1.07\u20132.35; migraine: OR\u00a0=\u00a03.13; 95% CI: 1.91\u20135.11) compared to non-migraineurs (Table ).Previous studies have shown significant comorbidities between migraine and PTSD. However, to our knowledge, this study is the first to evaluate the association between migraine and PTSD in pregnant women. Our current findings are comparable with prior studies of adult men and non-pregnant women. In a small clinic-based study of headache patients (including migraine or tension type headache; \u00a0=\u00a080), prevalence of PTSD-like symptomatology was similar to a comparison group of patients with masticatory muscle pain []. However, Peterlin et al. (2008) in their study of migraineurs attending an outpatient headache center demonstrated that PTSD was more frequently reported among chronic migraineurs than episodic migraineurs (42.9% vs. 9.4%, \u00a0=\u00a00.0059) []. In a general population study in 2011, Peterlin et al. reported that those with episodic migraine had a 3- to 4- fold increased odds of PTSD as compared to those without headaches after adjusting for confounders (lifetime prevalence: OR = 3.07, 95%CI: 2.12\u20134.46; 12-month prevalence: OR = 4.34, 95%CI: 2.73\u20136.89) []. In a cross-sectional study in Turkey, migraine was associated with PTSD among university students (OR = 10.16, 95%CI: 3.16\u201332.71, \u00a0=\u00a00.001) []. A recent study by Smitherman and Kolivas similarly found that those with migraine were almost twice as likely to fulfill diagnostic criteria for PTSD than non-migraineurs (25.7% vs. 14.2%, \u00a0<\u00a00.0001). Further, compared to those without migraine, migraineurs reported more traumatic events (3.0 vs. 2.4, \u00a0<\u00a00.0001) []. Despite differences in geographic location, population characteristics, and sociodemographics, previous findings consistently show comorbidity between migraine and PTSD.Several potential biological and neurochemical mechanisms have been postulated for the association between migraine and PTSD. These include the biochemical markers serotonin, cortisol, and norepinephrine. Migraineurs have been shown to have imbalances of serotonin, a regulator of pain in the nervous system []. Serotonin levels decrease during a migraine attack, causing the trigeminal nerve to release neuropeptides and cause severe migraine pain []. PTSD has been previously associated with serotonin function [, ]. The hypothalamic\u2013pituitary\u2013adrenal axis and related cortisol levels have also been associated with migraine and PTSD [, \u2013]. Additionally, decreased levels of cortisol and elevated levels of pro-inflammatory cytokines (e.g. tumor necrosis factor-alpha, interleukin-6) in patients with PTSD have been suggested to be linked to migraine [, ]. Videlock et al. (2008) found that norepinephrine plasma levels are lower in those with PTSD when compared to individuals without PTSD []. Migraine patients also may have lower levels of plasma and platelet norepinephrine []. Mental health during pregnancy is of particular interest given the high burden of violence in this population [, ]. A previous study in the same cohort found 70% of participants had a history of childhood abuse and 36.7% had a history of intimate partner violence, and their abuse history was associated with an increased risk of migraine []. PTSD is prevalent during pregnancy and may increase postpartum if it is not identified []. Although a large percentage of the population suffers from migraines, particularly those of reproductive age, the mechanisms underlying the development of migraines and PTSD have yet to be fully understood [].Our study has several strengths, including a large sample size and a population with a high prevalence of migraine and PTSD. However, some limitations should also be considered. First, this cross-sectional study does not establish temporal relationships between migraine and PTSD. Second, the study was conducted among low-income pregnant women in Peru; thereby, warranting caution when generalizing our study to other pregnant women. Lastly, migraine and PTSD diagnoses were established using self-reported questionnaires. Thus, we cannot exclude the possibility that PTSD and migraine status were underreported in our study. Studies that systematically use screening and confirmatory diagnostic evaluations will greatly attenuate concerns about misclassification of PTSD and migraine diagnoses in epidemiological studies [, ].Individually, migraine [\u2013] and PTSD [, ] each carry a high individual, societal, and economic burden. Our study found an association between migraine and PTSD, even after adjusting for antepartum depression. Furthermore, our findings extend the body of literature on the increased risk of PTSD in those with migraine to include those with probable migraine and pregnant women. Taken together, these findings support the need for additional research on the association between migraine and PTSD, including in pregnant women, as well as the need for research evaluating potential treatment implications of this comorbidity."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0783-5", "title": "Assessment of gray and white matter structural alterations in migraineurs without aura", "authors": ["Jilei Zhang", "\u2020", "Yi-Lan Wu", "\u2020", "Jingjing Su", "Qian Yao", "Mengxing Wang", "Ge-Fei Li", "Rong Zhao", "Yan-Hui Shi", "Ying Zhao", "Qiting Zhang", "Haifeng Lu", "Shuai Xu", "Zhaoxia Qin", "Guo-Hong Cui", "Jianqi Li", "Jian-Ren Liu", "Xiaoxia Du"], "publication": "The Journal of Headache and Pain", "publication_date": "21 July 2017", "abstract": "Migraine constitute a disorder characterized by recurrent headaches, and have a high prevalence, a high socio-economic burden and severe effects on quality of life. Our previous fMRI study demonstrated that some brain regions are functional alterations in migraineurs. As the function of the human brain is related to its structure, we further investigated white and gray matter structural alterations in migraineurs.", "full_text": "Migraine constitute a disorder characterized by recurrent headaches of moderate to severe intensity, pulsating quality, and unilateral location, that are aggravated by routine physical activity and associated with nausea, photophobia, and/or phonophobia []. It has been demonstrated that migraine have a high prevalence and a high social-economic burden, and they severely affect quality of life. In recent years, neuroimaging technology has provided more convenient methods for better understanding the pathological mechanism of migraine and identifying abnormal brain regions associated with migraine.Several MRI studies have identified functional and structural changes between migraine patients and healthy controls, and have suggested that brain malfunctioning may be associated with migraine pathophysiology [\u2013]. In addition, repeated and long-term migraine attacks may induce functional and structural plastic changes that may underlie the progression of the disorder [, ]. Voxel-based morphometry (VBM) and surface-based morphometry (SBM) are advantageous for evaluating structural alterations (such as gray matter volume, cortical thickness and gyrification index [GI]) of the gray matter due to their ability to localize abnormal brain regions in patients without a priori hypothesis [\u2013]. Two previous meta-analyses aimed to locate concordant gray matter alterations in migraine patients and found concordant decreases in gray matter volume (GMV) in some brain regions involved in pain-related processes [, ]. The patient groups of previous VBM studies have been primarily migraineurs without aura, but other subtypes (such as migraineurs with aura, and patients with chronic migraine) were also included. Previous research has proposed that the different subtypes of migraine may present specific structural alterations [, ]. In addition, diffusion tensor imaging (DTI) has been extensively used to evaluate the microstructural changes in white matter based on the diffusion characteristics of water molecules in the brain []. Migraineurs exhibited several microstructural alterations in previous DTI studies [\u2013]. Conversely, Need et al. did not identify microstructural white matter alterations in chronic and episodic migraine patients []. The results of structural studies of migraine patients seem contradictory and inconsistent, and the patients groups are characterized sample heterogeneity. Thus, structural alterations of the white matter and gray matter in migraineurs without aura should be further investigated.Our previous studies detected dysfunction in various brain regions in migraineurs based on task functional magnetic resonance imaging (fMRI) and task-free fMRI [, , ]. We found activation in the visual cortex and anterior cerebellum lobe/culmen during presentation of negative emotion picture stimuli in migraineurs []. Furthermore, migraineurs without aura exhibited dysfunction in the default mode network and sensorimotor network during the task-free state [, ].It has been demonstrated that the function of the human brain is intimately related to its structure [, ]. Functional abnormalities in migraineurs may be caused by corresponding structural changes. Therefore, we used SBM, VBM and DTI analyses to detect structural alterations of the white matter and gray matter in 32 migraineurs without aura in current study. In addition, we hypothesized that migraine patients without aura may exhibit structural changes and that these changes may be consistent with the findings of previous fMRI studies and associated with the pathological mechanisms of migraine.The East China Normal University Committee on Human Research (Project No. HR2016/03022) and the Independent Ethics Committee of Shanghai Ninth People\u2019s Hospital (Project No. [2016]01), Shanghai Jiao Tong University School of Medicine, approved the current study. All migraineurs without aura and healthy controls provided written informed consent using forms approved by the committee.Compared with controls, migraineurs without aura exhibited significantly increased gray matter volume in the bilateral cerebellar culmen (lobule I-IV and lobule V) extending to the lingual gyrus, thalamus, fusiform and parahippocampal gyrus. The cerebellum has been recently proposed to be associated with cognitive, sensorimotor, pain and affective information processing [\u2013], and to be involved in pathophysiological mechanism of migraine []. Moulton et al. found that the cerebellar activation areas overlapped with both unpleasant picture viewing and heat pain in healthy subjects and suggested that the cerebellum may have specific areas associated with encoding of generalized aversive processing []. In our previous task-fMRI study, we found that migraineurs exhibited hyperactivation in the anterior cerebellum lobe/culmen and visual cortex while viewing negative minus neutral affective pictures compared with healthy controls, and we proposed that migraine patients may have hypersensitivities to negative affective stimuli or that there is less inhibition in the cerebellum of migraineurs []. In addition, Mehnert J et al. demonstrated that the cerebellum, including lobules V and I-IV, is active during trigeminal nociceptive stimulation, and that the activity of the cerebellum is modulated by the perceived intensity of pain []. Nociceptive and negative emotion picture stimuli are parts of aversive stimuli and can active cerebellar responses. Thus, we propose that the increased GMV in the anterior cerebellum lobe (lobules V and I-IV) is consistent with our previous task-fMRI study and may be involved in the pathology mechanism of migraine.Migraine patients exhibited increased cortical thickness in left inferior temporal gyrus and lateral occipital cortex and increased GI in the right lateral occipital cortex. It has been demonstrated that the lateral occipital-temporal cortex plays an important role in the multi-sensory integration of visual, auditory and tactile information [, ]. In line with our results, Messina et al. found that patients with migraine had an increased thickness of the left temporo-occipital incisure compared with control subjects []. The structural abnormalities of temporo-occipital cortex might be to explain the interictal deficits in visual motion processing described in migraineurs [\u2013].In our study, we found that migraineurs without aura exhibited decreased cortical thickness in the right insula, decreased GI in the left rostral middle frontal gyrus, and increased GI in the left postcentral gyrus and superior parietal lobule. In fact, it has been proposed that the postcentral gyrus, superior parietal lobule, insula and dorsolateral prefrontal cortex are involved in sensory discrimination of pain information [\u2013]. Furthermore, Mehnert et al. observed increases in cerebellar-cortical connectivity in some brain regions, including insula and lingual gyrus [], during trigeminal nociception stimuli. The postcentral gyrus and insula play important roles in the ascending trigemino-thalamo-cortical nociceptive pathway and have been implicated in the pathophysiology mechanism of migraine [, ]. Our previous study revealed that the bilateral postcentral gyrus is functional alterations in migraineurs without aura and that the postcentral gyrus has decreased functional connectivity with the contralateral insula, superior parietal lobule, prefrontal cortex and occipital cortex []. These findings suggest that structural alterations in the insula, postcentral gyrus, superior parietal lobule and rostral prefrontal cortex may disrupt the pathway used to discriminate sensory features of pain or the trigemino-thalamo-cortical pathway, and induce hypersensitivities to painful stimuli in migraineurs.DTI analysis did not reveal any significant differences in migraineurs without aura comparing with healthy controls. Previous DTI studies in migraine patients detected several alterations, although the results of these studies are contradictory and inconsistent [\u2013]. Liu et al. did not find that the migraineurs without aura exhibited significant microstructural alterations of white matter at a 1-year follow-up evaluation []. In line with our results, Need et al. demonstrated no microstructural white matter changes in episodic and chronic migraine patients based on Tract-based spatial statistics (TBSS) analysis [].In the current study, we evaluated the structural alterations of white matter and gray matter in migraine patients without aura using VBM, SBM and DTI analyses. Gray matter structural alterations in migraine patients were detected in the bilateral cerebellar culmen, lateral occipital-temporal cortex, right insula, left prefrontal cortex, left postcentral gyrus and superior parietal lobule. No significant changes in white matter regions were found in the DTI analyses. Our findings are consistent with previous fMRI studies, and we propose that the significant alterations in the gray matter, which are associated with sensory discrimination of pain, multi-sensory integration and nociceptive information processing, and may be involved in the pathological mechanism of migraine without aura."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0785-3", "title": "The comorbidity burden of patients with cluster headache: a population-based study", "authors": ["Shivang Joshi", "Paul Rizzoli", "Elizabeth Loder"], "publication": "The Journal of Headache and Pain", "publication_date": "24 July 2017", "abstract": "Evidence is limited regarding the comorbidity burden of patients with cluster headache (CH). We aimed to characterize comorbid conditions in a cohort of CH patients diagnosed by headache experts, using electronic health record information from the Partners Research Patient Data Registry (RPDR).", "full_text": "Cluster headache is a relatively uncommon primary headache disorder that is one of the trigeminal autonomic cephalgias. Its lifetime prevalence is estimated to be 124 per 100,000 persons with a one-year prevalence of 53 per million [, ]. Cluster headache is considered to be among the most severe forms of pain. Patients with cluster headache experience substantial decrements in quality of life and have many other health-related burdens [].The Partners Healthcare Institutional Review Board approved the study. We used the Partners Research Patient Data Registry (RPDR) to identify unique patients with CH. The RPDR is a relational database containing clinical and administrative information on millions of patients seen within the Partners Healthcare System []. Partners Health Care is a not-for-profit, integrated health care system in Boston, Massachusetts that provides care for roughly 50% of the population in the Boston metropolitan area. Partners Health Care includes community and specialty hospitals, a managed care organization, a physician network, community health centers, home care and other health related services. During the period of this study, Partners Healthcare used a proprietary electronic medical record system known as the Longitudinal Medical Record (LMR). Physicians could directly dictate or enter typed information into the medical record. Physicians recorded International Classification of Diseases, version 9 (ICD-9) diagnoses on a billing sheet by circling or checking a list of common diagnoses. Physicians could also hand-write an ICD-9 code on this sheet. This diagnostic information was entered into the electronic billing system by billing assistants and subsequently imported into RPDR.We searched for patients who had received a diagnosis of cluster headache during a 10\u00a0year period between 2002 and 2012 from one of nine headache specialists known to be practicing within the Partners system during this period. The search was limited to patients diagnosed by headache specialists because the accuracy of cluster headache diagnoses by non-specialists is known to be low [].We identified medical record numbers of unique patients seen between 1/1/2002 and 12/31/2012 who had received a diagnosis of cluster headache. Patients diagnosed with cluster headache were identified in two ways. First, we searched for patients for whom an International Classification of Diseases (ICD) code for cluster headache had ever been recorded. For example, the ICD-9 code for episodic cluster headache is 339.01. Second, we searched for charts in which relevant terms (\u201cEpisodic Cluster Headache\u201d, \u201cChronic Cluster Headache\u201d or \u201cCluster Headache\u201d) appeared in the free text problem list, as a diagnosis not associated with an ICD code, or elsewhere in the medical record. Our final list consisted of patients who met either of these criteria, with duplicate records removed. For reasons of privacy, we did not include records belonging to patients who were employees of Partners Healthcare. Charts that appeared to be inaccurately coded were also removed. For example, patients who had received a diagnostic code for CH but who were described in the notes as having migraine were eliminated based on the assumption that a data entry error had occurred.Two researchers independently examined each medical record and attempted to validate the diagnosis of cluster headache by locating information required to make a diagnosis of cluster headache. Information from each eligible patient\u2019s record indicating the physician considered him or her to have CH was considered the gold standard for the presence of CH. Patients were categorized as having \u201cDefinite cluster headache\u201d if the medical record contained sufficient information to make a diagnosis of cluster headache according to ICHD-2 criteria []. We did not distinguish between episodic and chronic forms of the disorder. Patients were categorized as having \u201cUnconfirmed cluster headache\u201d if information in the medical record indicated they met some criteria for cluster headache, but information was insufficient to make a Definite diagnosis according to ICHD criteria.Using structured RPDR query methods, we identified two age and sex matched controls without cluster headache for each patient in the group of Definite CH patients. RPDR allows researchers to define criteria for the selection of healthy controls, and to choose the number of controls for each case patient. Using an algorithm, patients matching these criteria, but without the disease in question, are randomly selected and their medical record numbers are returned to the researcher, who is then able to retrieve the full medical record.We also performed queries in RPDR to determine demographic characteristics and other information such as the average yearly number of emergency and outpatient visits for the entire CH group (Definite and Unconfirmed), the age and sex-matched control group, and the entire population of patients in RPDR, the latter to help put results for the CH population in context.Table  shows the demographic characteristics and comorbidity burden of patients with Definite CH compared to those with Unconfirmed CH and matched controls. The average age of the Definite CH group was similar to that of the Unconfirmed group (43.4 v 44\u00a0years), but the sex ratio was notably different. 60/75 (80%) of patients with Definite CH were men, compared to just 10/22 (45%) with Unconfirmed CH.The prevalence of eight of the 56 studied comorbidities was statistically significantly different in patients with Definite CH compared with age and sex-matched controls. Specifically, patients with Definite CH had a higher prevalence of diagnoses of cigarette smoking, deviated septum, dental/TMJ problems and depression than controls, and a lower prevalence of skin problems, diabetes, musculoskeletal complaints or \u201cother gastrointestinal\u201d problems.In this large population-based study, we identified a surprisingly small number of patients who met strict criteria for CH, but a larger number who were considered by expert clinicians to have a CH disorder. This suggests that the sensitivity of existing diagnostic criteria in clinical practice is low. However, in patients diagnosed by expert headache clinicians with CH over an 11\u00a0year period, we did identify a distinct pattern of selected comorbidities compared with age and sex-matched controls. The pattern of patient characteristics and comorbidities that emerged from our study is somewhat but not entirely consistent with that of the \u201cclassic\u201d CH patient depicted in the medical literature. Our study does not suggest that CH patients are more likely to have cardiovascular disease, or to use or abuse alcohol in comparison with non-CH controls. This is not necessarily inconsistent with prior studies, which often looked at alcohol consumption and use; even if CH patients are more likely to drink alcohol than non-CH sufferers, it is not clear they are more likely to have alcohol abuse. In our study CH patients were less likely to have diabetes than controls. The majority of patients diagnosed with cluster headache in this population were men and they were more likely to have received diagnoses of depression, and of deviated septum or dental/temporomandibular joint problems, perhaps reflecting previous inaccurate diagnoses of the cause of their unilateral head pain rather than true comorbidities.In fact it is difficult to distinguish between conditions that are comorbid with CH and those that represent misdiagnoses of the disorder or selection bias due to the high number of healthcare visits that CH patients have. This increases the likelihood that latent medical problems will be diagnosed during evaluation and testing, a phenomenon known as \u201cBerkson\u2019s bias\u201d []. CH patients are frequently diagnosed with sinus or dental problems, and many experience substantial delay in receiving a diagnosis [\u2013]. These things may in part explain the high frequency of medical visits in this population. Nonetheless, caution should be used in interpreting our results, since it is not possible to distinguish conditions that are genuinely comorbid with CH from those that reflect misdiagnoses or result from increased medical scrutiny of patients in frequent contact with the healthcare system. Several of the comorbidities we identified, particularly sinus or dental problems, are very likely to reflect misdiagnoses of CH rather than true comorbid conditions.Although patients with Definite CH were statistically significantly more likely to have a history of cigarette smoking than controls, this was not true for cardiovascular conditions or alcohol use disorders. The prevalence of diabetes was significantly lower in patients with CH than in matched controls. This raises the question of whether diabetes might protect against the development or expression of CH, something that could be explored in future studies. Notably, CH patients had a statistically significantly elevated prevalence of diagnoses of deviated septum and dental or temporomandibular problems, which might represent previous inaccurate diagnoses of their unilateral head pain.Our data are consistent with previous findings of a long delay between the appearance of CH symptoms and receipt of a correct diagnosis [, ]. Our findings also quantify the increased use of healthcare among patients with CH. A previous study from Denmark found that 43.5% of CH patients had consulted a general practitioner during the previous year, compared with 9.2% of the general public. 43.5% had consulted a specialist, compared with 3.3% of the general public [] In our study, CH patients had roughly 3 times the number of outpatient and 4 times the number of emergency department visits in an average year, indicating a high medical burden. It is possible that some of these visits represent attempts to obtain an accurate diagnosis or effective treatment for CH. This suggests that healthcare payers could realize important costs savings through improved recognition and treatment of CH.The majority of patients in the Unconfirmed CH group were female. This is in contrast to the predominance of males in the Definite CH group. In this subgroup a major reason patients did not meet criteria for Definite CH was duration of CH longer than the three-hour maximum allowed by diagnostic criteria. A longer duration of attacks in women compared with men who have CH was not noted in a previous study that examined sex-specific attack differences []. Thus, it is possible, even likely that many patients in the Unconfirmed group did not have cluster headache but instead had migraine with prominent autonomic symptoms. The sex distribution would support this hypothesis. On the other hand, the clinicians making the diagnosis of CH were experienced headache experts, who presumably considered this possibility but still felt that the headaches were more likely to be CH.It is interesting to note that the pattern of comorbidities and diagnostic delay was very similar in the Definite and Unconfirmed groups. These similarities support the view that these groups suffer from a single disorder. The ICHD criteria represent a compromise between the clinical need for sensitive criteria and the research need for specificity. It is thus not surprising that the sensitivity of the criteria is somewhat lower than ideal for clinical purposes. Clinicians should bear in mind that in clinical practice, strict adherence to ICHD criteria would result in missing patients who likely do have CH and would benefit from treatment. This is especially true for women with the disorder.Our findings provide high quality information in a representative sample of patients with expert-confirmed and carefully validated cluster headache diagnoses seen in a large academic medical system over the course of a decade. Strengths of this study include careful validation of CH diagnosis, an electronic medical record with semi-standardized data entry that allowed thorough characterization of the natural history and clinical course of patients, as well as high quality, complete information on comorbidities in our patient population and age and sex matched controls. Our study also has a number of limitations. We performed multiple comparisons, which increases the chance that some findings are false positives. Because of this our findings should be viewed as descriptive and hypothesis-generating. In most cases, comorbidities were diagnosed clinically rather than using structured diagnostic criteria. We used the second version of the ICHD criteria for this study rather than the current 3-beta version []. However, the only change to the diagnostic criteria in the latest version was the addition of a \u201csensation of ear fullness\u201d or \u201cforehead and facial flushing\u201d to the list of ipsilateral autonomic symptoms or signs. These symptoms were not mentioned in any of the clinical notes we reviewed, so these minor changes are unlikely to have any material effect on our findings or conclusions."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0781-7", "title": "OnabotulinumtoxinA injections in chronic migraine, targeted to sites of pericranial myofascial pain: an observational, open label, real-life cohort study", "authors": ["Dani\u00e8le Ranoux", "Gaelle Martin\u00e9", "Ga\u00eblle Espagne-Dubreuilh", "Marl\u00e8ne Amilhaud-Bordier", "Fran\u00e7ois Caire", "Laurent Magy"], "publication": "The Journal of Headache and Pain", "publication_date": "21 July 2017", "abstract": "OnabotulinumtoxinA has proven its efficacy in reducing the number of headache days in chronic migraine (CM) patients. The usual paradigm includes 31 pericranial injection sites with low dose (5\u00a0U) per site. The aim of this study is to present the results obtained using a simpler injection protocol of onabotulinumtoxinA, with injection sites targeted to pericranial myofascial sites of pain.", "full_text": "Chronic migraine (CM) is defined as headache occurring on 15 or more days per month for more than 3\u00a0months, which has the features of migraine headache on at least 8\u00a0days per month []. This disabling condition affects approximately 1\u20132% of the general population [] and has a much stronger impact on quality of life and employment than episodic migraine []. The reason why episodic migraine becomes chronic remains poorly understood. The most recent data highlight the role of decreased activity of the descending pain-modulating network, and of sensitization of central structures including the thalamus, periaqueductal grey matter, and spinal trigeminal ganglion []. Some risk factors for chronicization of migraine have been identified, including frequency of migraine attacks, obesity, excessive use of opioids and barbiturates, caffeine overuse, stressful life events, sleeps disorders and cutaneous allodynia []. Most patients with CM overuse medication, but it is unclear whether this fact is a cause or a consequence of chronicization of migraine, and in ICHD-3 (International Classification of Headache Disorders 3) beta version the diagnosis of CM can be made regardless of whether the patient overuses medication or not [].Treatment of CM is challenging, since triptans or ergot derivatives are inconsistently effective. It requires a multifaceted approach, including lifestyle modifications, management of triggering factors, education, support, and behavioral therapy []. Drug withdrawal is considered mandatory by most physicians who believe that acute medication overuse is the major cause of migraine chronicization. Some studies, however, have demonstrated that CM patients\u2019 condition could be improved without drug withdrawal []. Furthermore, the modalities of medication discontinuation are still a matter of debate []. Pharmacological treatment options are limited, relying on classical oral prophylactic drugs. With the exception of topiramate, however, these agents have not been specifically evaluated in patients with CM. Non-pharmacological options include invasive procedures such as occipital nerve stimulation, even if the first randomized studies did not confirm the promising preliminary data []. In 2010, two large, placebo-controlled trials, PREEMPT (Phase III Research Evaluating Migraine Prophylaxis Therapy) 1 and 2, demonstrated that OnabotulinumtoxinA (OnaA) (Botox\u00ae, Allergan Inc) significantly decreased the severity and frequency of CM headache [, ]. The design of these studies has been criticized [, ], stressing several methodological weaknesses such as the change of primary outcome measure between the two studies, and possibly inadequate blinding. Furthermore, the placebo effect was particularly strong in these trials. These results, however, led the Food and Drug Administration to approve Botox\u00ae use in CM, and the recent update of the American Academy of Neurology guidelines recommends (level A) the use of OnaA in CM to reduce the number of headache days [].The PREEMPT trials injection scheme relies on the observation that OnaA injections applied for hyperfunctional facial lines are able to alleviate migraine symptoms [], and consists in 31 injection sites throughout pericranial muscles. We postulated that, instead of injecting small doses in multiple sites, it could be more appropriate to inject higher dosage in a limited number of muscles known to be a source of myofascial pain in CM patients, such as the corrugator, temporalis and trapezius muscles [\u2013].We present here the clinical outcome of a cohort of 63 patients treated with this paradigm of injection.Sixty three consecutive patients were referred to our center for refractory CM from 2008 to 2015. All screened patients consented to receive OnaA injections and signed informed consent. They were 43 females and 14 males, aged 17 to 85\u00a0years (mean: 44.3).Cervical pain and muscle tenderness were particularly frequent at baseline, present in 33 responders, and was reduced by \u226550% in 31 of them (94%) after treatment.The optimal treatment regimen, determined during the adaptation period, included injections into the corrugator, temporalis and trapezius muscles bilaterally (total dose: 150\u00a0U) in the majority of patients (33/41, 80.5%). In five patients the treatment was administered into both trapezius muscles only (total dose: 80\u00a0U). The last three patients were injected into the corrugator and temporalis muscles only (total dose: 70\u00a0U). The trapezius muscle appeared to be a key target for OnaA injections in CM, since 38 patients out of 41 (92.7%) required injections into this muscle to improve.At baseline, 15 patients did not take any triptans because of contraindication, loss of efficacy, or because triptans had never been effective. In responders, among the 28 triptan consumers, rescue drug consumption dramatically decreased (mean: 81%). Most patients reported a much better efficacy of the triptans on residual migraine attacks compared to the pre-treatment period.Surprisingly, the onset of efficacy was abrupt in most patients, the maximum of benefit being reached in a few days, after a latency ranging from 5 to 30\u00a0days (mean\u00a0=\u00a014.8\u00a0days). The duration of action ranged from 3 to 4\u00a0months.The injections were well tolerated. The only significant adverse event we observed was local myalgia when the trapezius muscle was injected. This pain occurred 1 to 5\u00a0days after injection, and could last up to 15\u00a0days. It was qualified as severe in 4 patients (9.7%) and, interestingly, did not recur, or recurred very moderately, during the subsequent injection cycles. We observed no eyelid ptosis, perhaps due to the high concentration (1\u00a0ml/100 OnaA Units) used in our study to inject facial muscles. The patient satisfaction was particularly high, with a mean score of 8.6 (6.5\u201310) on a 0\u201310 numerical scale.Myofascial pain syndromes have been described in CM patients in various pericranial sites including the neck muscles, the supratrochlear and corrugator region, and the temporalis muscles [\u2013]. This preliminary study shows that OnaA injections targeted to those sites provide a high rate of response (65.1%, ITT) in CM. Furthermore, the magnitude of the response was large, with 70.7% of the responders exhibiting a\u00a0\u2265\u00a070% decrease in headache day frequency in at least two consecutive sessions of injections, and 22% being virtually headache-free. Finally, we observed a dramatic decrease (81%) in triptan consumption in responders.Most patients had not experienced an improvement of this magnitude in years, as attested by the high degree of patient satisfaction (mean 8.6/10).Our study, as well as the other recently published real-world experiments addressing the same issue [\u2013], has some limitations, including the small sample size and the absence of a placebo arm. Placebo effect is particularly high in headache conditions, especially when the treatment is administered by injection. In order to lower the impact of placebo effect on our results, we considered patients as responders only if OnaA treatment was efficacious in two consecutive sets of injections. In addition, some features in the response can hardly be explained only by the placebo effect, such as (i) the reproducibility of results at each injection session (up to 18 per patient); (ii) the fact that the response to treatment in single patients differed according to the muscles injected (for example, some patients did not respond to frontotemporal injections in the adaptation phase, but responded to trapezius muscles injection). Finally, the long latency to efficacy onset we observed (up to 30\u00a0days) is unusual for a placebo effect.Previous real-life studies provided conflicting results, with a\u00a0\u2265\u00a050% reduction in number of headache days achieved in a range as wide as 17.4 to 63% of patients [\u2013]. With a figure of 65.1%, our results are in the higher part of this range, which may be due to the technical choice we have made. Indeed, our plan of injection differed from the PREEMPT protocol they used. We injected only the corrugator muscle on the forehead, not the frontalis or the procerus muscles. The temporalis muscle was injected in one site rather than in four, and the only cervical muscle injected was the trapezius muscle, with a higher dose (40\u00a0U versus 15\u00a0U). This resulted in a simplified paradigm with a maximum of 10 injections sites. Since the pivotal PREEMPT studies, a variety of injection techniques have been proposed, with modification of doses or sites of injections. Negro et al. [] have demonstrated a dose-depending effect of OnaA in patients with CM and medication overuse headache, with a superior efficacy of OnaA 195\u00a0U compared to 155\u00a0U. Our results suggest that parameters others than the global injected dosage may be of relevance, such as the selection of a limited number of muscles using an individualized follow-the-pain approach, as well as an adequate dosage per muscle. Only a controlled, randomized study would be able to compare the efficacy of PREEMPT paradigm to such a tailored protocol, targeted to sites of myofascial pain. We suggest, however, that the paradigm we used may constitute a promising way to improve the outcome of CM patients treated with OnaA.This injection paradigm was elaborated with reference to the theory which assumes that the muscle sites with myofascial pain act as triggers to initiate or perpetuate migraine. In support of this hypothesis, it has been demonstrated that inactivation of cervical trigger points by anesthetic infiltrations or manual therapy resulted in reduced migraine number and intensity [, , \u2013]. The present study is to our knowledge the first to use OnaA injections targeted to pericranial muscle pain in CM patients. However, in a 2011 review paper, Gerwin mentioned his personal unpublished experience using a similar approach. He reports that he injects OnaA into the TrPs in the head, neck, and shoulder muscles identified by physical examination, and finds that 50% of patients treated in this way are headache-free, with an additional 30% significantly improved []. Both approaches are very close together. The only difference is that we did not target TrPs as the sites of injection. Identification of TrPs within a given muscle was just for us a mean to correctly select the muscles to inject. In a key paper about myofascial headache, Fernandez-de-las-penas [] suggests the crucial role of TrPs in generating muscle pain. Indeed, evidence supports that active TrPs release algogenic substances that are susceptible to promote the sensitization of muscle nociceptive nerve terminals, which may be responsible for muscle pain. In turn, the sensitized nerve ending liberate vasoactive neuropeptides such as calcitonine gene-related peptide (CGRP), Substance P and Glutamate, leading to a local neurogenic inflammation []. Since it is currently established that OnaA inhibits exocytosis of acetylcholine as well as multiple neurotransmitters including serotonin, dopamine, noradrenaline, gammaaminobutyric acid (GABA), enkephalin, glycine, substance P, ATP and calcitonin gene-related peptide (CGRP) [], we can assume that OnaA may act in CM through a reduction of the peripheral sensitization within the injected muscles. It is however unlikely that the action of OnaA is limited to a peripheral effect. Indeed, the fact that OnaA induces a reduction of migraine attacks frequency implies that a central action also exists in one way or another. Some studies have suggested that myofascial inputs may activate the trigeminovascular system and therefore trigger migraine attacks in migraine sufferers []. It can therefore be suggested that an indirect central effect may result from the reduction of nociceptive myofascial input towards central neurons. In addition, we cannot rule out a direct central effect through the retrograde transport of OnaA, which has been demonstrated in numerous preclinical studies []. It is unclear, however, whether OnaA axonal transport has a clinical relevance in humans.Cervicalgia present between migraine attacks was a major concern in our patients, present in 73.7% of cases. This is in keeping with studies that found a higher prevalence of neck pain disorders in patients with chronic rather than episodic migraine []. The significance of the cervical muscle tenderness observed in chronic headache is not fully understood, but is thought to result from myofascial pain []. We found that the combination of prodromal, percritic and intercritic cervicalgia was a predictive factor of response to OnaA treatment in CM patients, suggesting that the more severe the cervical myofascial disorder, the better the outcome. We also found that 94% of patients with neck muscle tenderness had a\u00a0\u2265\u00a050% reduction in cervicalgia intensity. Both findings are in line with our assumption that OnaA acts in CM at least partly by relieving the myofascial component of pain. We also showed that most patients reported a better efficacy of triptans after OnaA treatment. We think this supports the view that OnaA acts on myofascial pain, which is by definition unresponsive to triptan. Once relieved, the remaining pain is purely migrainous and is therefore triptan-responsive.We found that the latency of therapeutic effect was long-lasting, with a mean of 14.8\u00a0days (up to 30\u00a0days). This finding was unexpected, since the delay of action of OnaA is estimated around a few days in the classical indications of OnaA such as dystonia and spasticity []. To our knowledge, only one other study addressed this issue []. The authors found that the first signs of therapeutic effect started after a mean of 5.5\u00a0days, which suggests a progressive onset of improvement. The pattern of response in our patients was quite different. Patients reported a delayed, but rapidly occurring improvement. This difference may be due to the injection paradigm we used, targeted to sites of myofascial pain. If so, the long delay to onset we observed could be an argument supporting a central participation in the mechanism of action of OnaA in CM.Our results also raise the concern of the role of drug withdrawal in the management of CM. In the present study, we found that OnaA treatment itself led to a dramatic reduction of migraine rescue medication intake (81%). Thus, we propose that, in CM patients, (i) drug abuse may be a consequence of the ancillary myofascial pain rather than the cause of migraine chronicization, and (ii) OnaA treatment should be discussed before considering medication withdrawal.We conclude that specifically targeting myofascial pain sites with selective OnaA injections may be a safe and effective option in CM treatment. Further larger, placebo-controlled studies are needed to compare the present protocol with the fixed \u201cmultipoint-low dose per point\u201d PREEMPT protocol. If our results were confirmed by further studies, it could be suggested that myofascial pain and TrPs may contribute to headache pain in CM patients and constitute an important factor of migraine chronicization."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0780-8", "title": "Perceived stress in patients with migraine: a case-control study", "authors": ["Hye-Jin Moon", "Jong-Geun Seo", "Sung-Pa Park"], "publication": "The Journal of Headache and Pain", "publication_date": "21 July 2017", "abstract": "Perceived stress is the most common trigger for migraine. The objective of this study was to examine the clinical significance of perceived stress in migraine patients.", "full_text": "Migraine, a common disabling disease, accounts for a large proportion of non-fatal disease related burden worldwide []. In a review study on global burden of disease in 2013, migraine and mild-to-moderate mental disorders such as depressive and anxiety disorders were main causes of burden in this category for the Korean public []. Migraine has several comorbidities and modifiable risk factors. In published literature, vascular accidents, depression, anxiety, epilepsy, and sleep problems are commonly associated with migraine. Attack frequency, caffeine, medication overuse, obesity, snoring or sleep apnea, psychiatric comorbidity, and stressful life events have been suggested as modifiable risk factors for migraine complications such as vascular events and chronic migraine (CM) []. All these factors will reduce the quality of life (QOL) of migraine patients [, ]. In addition, psychiatric comorbidity and psychological distress may negatively affect the outcome of migraine patients [].It has been found that migraine patients have higher levels of perceived stress than healthy controls [, ]. In addition, identified stress levels are higher in migrainous women than those in migrainous men []. Moreover, stress can trigger migraine attacks. About 80% of migraine patients with identifiable triggers have reported that stress is a common trigger []. In a Korean hospital-based study, stress is the most common trigger for episodic migraine, followed by sleep deprivation and fatigue []. In addition, 57.7% of patients have indicated fatigue as a headache trigger []. It has been suggested that stress might be a predisposing factor in new-onset migraine []. Stress might also play a role in migraine chronification [].Although a stressful condition is likely to be associated with migraine, the level of perceived stress between episodic migraine (EM) and CM has not been delineated yet. Therefore, the first aim of this study was to determine the level of perceived stress in EM and CM patients. Factors associated with perceived stress in migraine patients have not been reported yet. If predictors for perceived stress of migraine patients can be identified, a guideline can be developed for clinicians to manage stress adequately. Therefore, the second aim of this study was to identify predictors for perceived stress in migraine patients. In addition, although it is known that migraine and comorbid disorders will reduce QOL of patients, the impact of stress on QOL has not been reported. Therefore, the third aim of this study was to delineate the impact of stress on QOL to provide information for clinicians.Our study revealed that the level of perceived stress was significantly higher in CM patients than that in controls. Although several factors including clinical and psychosomatic factors were associated with perceived stress, our data demonstrated that CM appeared to be a critical factor for perceived stress. Perceived stress was correlated well with migraine-specific QOL.While a higher level of perceived stress has been previously reported in migraine patients compared to that in healthy controls in two studies [, ], there was no difference in mean PSS score between migraine patients and controls after controlling for depression and anxiety in this study. This reveals that depression and anxiety are major determinants of perceived stress in migraine patients and controls. It is known that stressful events can cause depression and anxiety. In response to stress, corticotropin releasing factor (CRF) regulates the activity of hypothalamic-pituitary-adrenal (HPA) axis and triggers changes in serotonin receptors []. CRF is also known to influence anxiety responses with CRF receptor 1 being particularly important []. In a cohort study, it has been found that stressful events contribute to comorbidity of migraine and major depression []. Depression and anxiety can also aggravate stressful conditions. In epilepsy patients, depression and anxiety have direct effect on perceived stress []. Under these circumstances, depression and anxiety are not likely to be unique for perceived stress in patients with migraine.After investigating the relationship between migraine chronicity and perceived stress, it was found that CM patients had higher levels of perceived stress than controls. CM was selected as a critical factor for perceived stress after adjusting for depression, anxiety, and insomnia by multivariate analyses. It has been reported that CM patients are more likely to have depression, anxiety, sleep problems, and poor QOL compared to EM patients [, ]. These conditions might induce stressful conditions in CM patients to some extent. Our data demonstrated that CM appeared to be a migraine-specific factor for perceived stress. Stressful life events are likely to trigger migraine events [, ]. They might be risk factors for CM []. Repeated stress may lead to functional and structural alteration in the brain network. These changes in brain states may occur as a result of repeated migraine attacks through maladaptive coping mechanisms []. The cascade of these effects can lead to further deterioration of adaptation, causing transformation or chronification of the disease []. Therefore, clinicians should identify perceived stress by counseling migraine patients. They need to modify perceived stress through pharmacological or non-pharmacological interventions such as cognitive behavioral therapy and biofeedback to avoid transformation or chronification of migraine [].Our results revealed that the level of perceived stress was significantly associated with the role function and emotional function of migraine patients. Chronic stress may trigger migraine attacks [, ] or induce CM [, ], subsequently restricting or preventing participation in social or work related activities []. Therefore, chronic stress might affect emotions of migraine patients. That is the reason why clinicians should identify and modify stress.Our study has some limitations. First, subjects were from a single tertiary hospital. Therefore, our results cannot be generalized. Second, this was a cross-sectional study. Causal relationships between variables could not be confirmed. A longitudinal study is recommended to verify the causal relationship between perceived stress and CM. Third, the level of perceived stress was measured for the preceding month. Therefore, state of stress over a month was unknown. A long-term observational study is needed to evaluate the impact of CM on perceived stress.In conclusion, it was found that the level of perceived stress was significantly higher in CM patients than that in controls. Among several factors associated with perceived stress, CM appeared to be a critical factor for perceived stress. Significant negative correlations between perceived stress and migraine-specific QOL was found in this study."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0784-4", "title": "The impact of onabotulinumtoxinA on severe headache days: PREEMPT 56-week pooled analysis", "authors": ["Manjit Matharu", "Rashmi Halker", "Patricia Pozo-Rosich", "Ronald DeGryse", "Aubrey Manack Adams", "Sheena K. Aurora"], "publication": "The Journal of Headache and Pain", "publication_date": "1 August 2017", "abstract": "OnabotulinumtoxinA has been shown to reduce headache-days among patients with chronic migraine (CM). The objective of this analysis was to determine whether onabotulinumtoxinA has an impact on headache-day severity in patients with CM among those patients who were deemed non-responders based on reduction in the frequency of headache days alone.", "full_text": "Chronic migraine (CM; \u226515 headache days per month for \u22653 consecutive months and with \u22658\u00a0days/month of migraine-type headaches) [] is associated with significant personal, societal, and economic burdens [\u2013]. Compared with people with episodic migraine (EM; <15 headache days per month), those with CM experience greater headache intensity, increased pain severity and disability, [] higher rates of comorbid medical conditions, [, ] reduced health-related quality of life, [] greater economic burden, [] and reduced productivity. [, ].The Phase 3 REsearch Evaluating Migraine Prophylaxis Therapy (PREEMPT) clinical trial program established the safety and efficacy of onabotulinumtoxinA for CM [\u2013]. In PREEMPT 1 and 2, patients were randomized to double-blind treatment with onabotulinumtoxinA or placebo (24\u00a0weeks), followed by open-label treatment with onabotulinumtoxinA (32\u00a0weeks) [\u2013]. Treatment with onabotulinumtoxinA resulted in significant improvements in a variety of efficacy endpoints, including the change in frequency of headache days throughout the double-blind treatment period []. Nevertheless, anecdotal reports from treating clinicians have indicated that results from these trials do not fully reflect the patient benefits that are observed in clinical practice. Specifically, it has been suggested that onabotulinumtoxinA treatment may have an impact on other clinical characteristics such as headache-day severity.In this analysis, we assessed the effect of onabotulinumtoxinA on headache-day severity in patients with CM using pooled data from the PREEMPT clinical trials. Our analysis placed a particular focus on the effect on patients who did not experience a clinically meaningful reduction in the frequency of headache days.Study details have been reported previously, [, ] and will be only summarized here.In an earlier analysis of the PREEMPT data, it was observed that 49% of patients treated with onabotulinumtoxinA demonstrated a\u00a0\u2265\u00a050% reduction in headache-day frequency after 1 treatment cycle, and an additional 11% who did not respond after the first treatment cycle responded after treatment cycle 2 []. In the current analysis, we present data for the effect of onabotulinumtoxinA on headache-day severity among those with CM who did not have a reduction in headache-day frequency after two cycles of treatment (non-responders defined as <50% reduction in headache-day frequency at week 24). During the randomized, placebo-controlled, double-blind phase, patients treated with onabotulinumtoxinA demonstrated a greater reduction from baseline in the number of severe headache days per 28-day period than did those receiving placebo. In addition, compared with the placebo group, there were a lower percentage of patients receiving onabotulinumtoxinA with an ADHS score of severe, a lower percentage of severe headache days pooled across all patients within the onabotulinumtoxinA group, and a higher rate of at least 1-grade improvement in headache severity from baseline (severity responders). Among all severity responders, the proportion of HIT-6 responders (\u22655-point improvement from baseline) was greater for the onabotulinumtoxinA group than the placebo group at the end of the double-blind phase.The headache-day severity endpoints showed a noteworthy peak at week 24 in both onabotulinumtoxinA and placebo treatment groups. The key reason for this non-response peak is the selection criteria, since the population of interest was predefined as those without adequate treatment response in relation to headache-day frequency at week 24. It is interesting that in this group of non-responders, there was some response at other time points both before the arbitrary 24-week non-response point and after that time point.Open-label results where both groups were receiving onabotulinumtoxinA generally demonstrated lesser between-group differences, with no significant differences between the groups observed by the end of the 56-week study. Conversely, headache-day frequency, as previously reported by Aurora et al., [] was significantly reduced by onabotulinumtoxinA in the double-blind phase and continued to show between-group differences through to week 56, making a case for early treatment with onabotulinumtoxinA in patients with CM. This observation suggests that treatment with onabotulinumtoxinA produces significant reduction in headache-day severity that may compliment a reduction in headache-day frequency, since onabotulinumtoxinA treatment in the open-label phase was able to eliminate between-group differences in severity, but not frequency. These findings are worthy of further investigation to fully understand the potential therapeutic benefit of early treatment with onabotulinumtoxinA.Importantly, the current analysis demonstrated reduced headache-day severity at the first post-baseline assessment (week 4) in patients who were headache-day frequency non-responders at week 24, suggesting an important clinical response to treatment not captured by the measure of headache-day frequency reduction. Furthermore, the alignment of the HIT-6 response with the severity response suggests that the reduction in headache-day severity of at least 1 grade was clinically meaningful. Further study is required to determine whether this clinical response translates into a reduction in healthcare resource utilization and broader economic benefits.The large number of patients with CM included in these double-blind placebo-controlled studies makes these results particularly robust. The use of the voice interactive daily telephone diary encouraged high patient compliance with diary record keeping and captured data without the need for reliance on long-term recall. This would be expected to result in more accurate capture of patient data, as others have shown that current health status can have an impact upon a patient\u2019s recollection of the past [].The study is not without its limitations. The lack of an active comparator is a potential limitation. However, the lack of any approved prophylactic treatment for CM makes it difficult to identify an appropriate comparator. The time point to determine headache-day frequency nonresponse was set at week 24, which was an arbitrary time point. The selection of a different time point for the definition of non-response may have resulted in different outcomes. Further clinical trials may be required to understand if a different time point for the determination of headache-day frequency non-response has any clinically meaningful impact on the interpretation of the data presented here.Similarly, no optimal responder rate for the reduction from baseline in headache-day frequency has been defined for the CM population. Although both 30% and 50% cutoffs for headache measures have been suggested to be clinically meaningful, [] a 50% cutoff is more commonly used in migraine studies, and provided the rationale for the definition of non-responders for the current analysis. Analysis of the current data using a\u00a0<30% reduction in headache-day frequency as the definition for non-response produced similar results to those reported here, with those classified as non-responders achieving a significant reduction in headache severity (Additional file : Table S1, and Figures S1 and S2). This additional analysis using <30% as the cutoff further strengthens the findings of this analysis.The International Headache Society has published guidelines for clinical trial assessment of prophylactic treatment for CM, including guidelines on the selection of outcome measures []. The primary end-point outcomes recommended by these international experts include headache days with moderate or severe intensity, migraine days, or frequency of migraine episodes. However, despite this international accord on the selection of outcome measures to encourage robust clinical trials, the outcomes may not fully align with the patient\u2019s expectation of therapy. This study demonstrates that frequency day response alone may not be sufficient to determine a clinically meaningful response to therapy, and highlights the need for further work on developing key patient-endorsed outcome measures for the assessment of prophylactic treatment of chronic migraine in particular and headache disorders in general.Patients from the PREEMPT clinical trial program who received onabotulinumtoxinA and met our definition for headache-day frequency non-response (<50% reduction in headache-day frequency at week 24) demonstrated significantly reduced headache-day severity (compared with those receiving placebo). Among those who showed reduced headache-day severity, onabotulinumtoxinA also produced greater reduction in headache impact scores. These results suggest that patients with CM experience clinically meaningful relief from headache intensity following treatment with onabotulinumtoxinA, even among those who may not experience a clinically meaningful reduction in the frequency of their headaches."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0793-3", "title": "Single-dose botulinum toxin type a compared with repeated-dose for treatment of trigeminal neuralgia: a pilot study", "authors": ["Haifeng Zhang", "Yajun Lian", "Nanchang Xie", "Chen Chen", "Yake Zheng"], "publication": "The Journal of Headache and Pain", "publication_date": "10 August 2017", "abstract": "Several RCT studies including ours, seem to prove the role of Botulinum toxin type A (BTX-A) in the treatment of trigeminal neuralgia (TN), but no standardized dosing regimen has been established. In our study, we compare two different methods of administration: single-dose or repeated-dose strategy which was most frequently applied over the years in our centre.", "full_text": "Trigeminal neuralgia (TN) is severe and recurring pain distributed unilaterally along a branch of the trigeminal nerve. [, ] TN can be triggered by brushing the teeth, washing the face, drinking liquids, or shaving. Patients become fearful of performing these life activities, in anticipation of long-lasting stabbing pain. TN patients may display a haggard facial expression and depressive mood, as their quality of life and ability to work is compromised. The most common treatment is anti-epileptics such as carbamazepine and oxcarbazepine. However, these medications may have to be discontinued due to intolerable side effects. Neurosurgical interventions remain debatable for safety and efficacy concerns [].Botulinum toxin type A (BTX-A) is an exotoxin released by the gram-positive bacterium . Its initial medicinal use was in the management of blepharospasm and hemifacial spasm []. In recent years, BTX-A has been used for the relief of chronic migraine and many other types of headache [\u2013]. The application of BTX-A to relieve TN was first reported in 2002, and its safety and effectiveness was later confirmed by series studies [\u2013]. These findings suggest that local intradermal or submucosal injections of BTX-A may be a promising therapy for TN.In 2009, our group initiated a study of TN treatment using BTX-A, and a clinical database was established. Through several clinical trials, our group has shown that BTX-A can provide long-lasting relief of TN symptoms, with only mild or moderate side effects [, \u2013]. However, before clinical application of BTX-A for TN treatment can become routine, a standardized therapeutic regimen should be established. In particular, there is no consensus regarding dosage or treatment schedule, and relevant clinical trial data are scarce. In a randomized, double blind, placebo-controlled study in 2014, we reported that doses of 25\u00a0U or 75\u00a0U BTX-A were similar in short-term efficacy for treatment of TN []. Since the efficacy could not be improved by simply increasing the dose, we asked whether the efficacy may change if the dosing time was extended. To answer this question, for the present study we conducted an open-label trial to compare the efficacy, safety, and tolerability of single or repeated administrations of BTX-A in patients with TN.BTX-A is a -derived exotoxin that has been used widely in cosmetology and for treating dysmyotonia []. Recent years have seen an enormous interest in the application of BTX-A to relieve chronic migraine and other types of headache [\u2013]. In addition, BTX-A has been used for neuropathic pain, including post-herpetic neuralgia [], diabetic neuropathic pain [, ], and occipital neuralgia []. However, to date, no standardized dosing regimen has been established. We conducted this pilot study to evaluate the efficacy and safety of single-dose BTX-A compared with repeated-dose BTX-A in TN. We found during a 6-month follow-up that single and repeated dosing of BTX-A in TN patients have comparable efficacy and side effects.Since 2002, there have been many reports concerning BTX-A treatment for TN [\u2013]. Several randomized clinical studies have focused on efficacy and safety issues []. These findings have triggered much interest in clarifying questions such as the optimal dose, duration of therapeutic efficacy, and common adverse reactions []. Our group has also been working on determining the efficacy and safety of the clinical use of BTX-A [, \u2013].The impetus for this study was our clinical experience that, in some TN patients showing poor responses to BTX-A, repeated dosing may lead to better pain relief. In an earlier 14-month follow-up study involving 88 patients with TN, we found that the duration of therapeutic effect (i.e., reduction in VAS scores) may be due mostly to the first injection of BTX-A []. Therefore, we recommended a relatively low dose at the first injection, and, if necessary, an additional 1-to-3 doses to treat intractable TN, 2-to-4\u00a0weeks later.However, we still wondered if repeated dosing conferred a therapeutic advantage over a single dose. Thus, we determined to perform the present pilot open-label trial to compare the efficacy and safety profiles of single or repeated doses of BTX-A. An important factor in our study design was that a medium dose (50\u2013100\u00a0U) of BTX-A was used, because this is within the most common dosing range in our clinical practice. Moreover, a relatively larger dose was used in patients who received the repeated dose (100\u2013140\u00a0U compared to 70\u2013100\u00a0U in single dose group). Although our previous study indicated that such a difference in dosage may not influence efficacy and safety, at least in the short term, future studies are warranted to determine doses for single and repeated dosing regimens.Our present study suggests that both the single and repeated dosing regimens were similar in frequency rates and severity of side effects. Thus, it is reasonable to suggest that the occurrence of adverse reactions is more relevant to the injection procedure. This may require more attention in future practice. For example, the orbicular muscle of the mouth should be avoided, and a reduced dose may be required at each injection site (e.g., 1.25\u20132.5\u00a0U/point).The results of our study showed that repeated dosing of BTX-A did not contribute to improved clinical outcome in TN patients. Since repeated dosing is accompanied by increased cost and inconvenience, single dosing may be the best choice in management of TN. Nevertheless, for patients who respond poorly to the first injection, a second dosing may still be safely performed. Given the inherent limitation of this pilot study, a future study is anticipated to provide more comprehensive evaluation of the optimal BTX-A regimen for clinical management of TN.The single- and repeated-dosing BTX-A regimens were largely comparable in efficacy and safety. This study suggests that repeated dosing has no advantage over single dosing of BTX-A for TN. Dosing should be adjusted for the individual patient."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0789-z", "title": "Symptoms and clinical parameters of pediatric and adolescent migraine, by gender - a retrospective cohort study", "authors": ["Tal Eidlitz-Markus", "Avraham Zeharia"], "publication": "The Journal of Headache and Pain", "publication_date": "8 August 2017", "abstract": "The available data on gender differences in clinical migraine parameters among pediatric patients are based on relatively few studies, which did not use the current version of the International Classification of Headache Disorders (ICHD) of the International Headache Society. The aim of the present study was to compare between males and females, demographic and clinical characteristics of children and adolescents with migraines diagnosed according to the ICDIII-beta version.", "full_text": "Worldwide data show that migraines are two to three times more prevalent in women than in men []. In addition, clinical symptoms of adult migraine differ by gender, with women having a longer attack duration, greater intensity of headache attacks, higher rate of headache recurrence, greater disability, and a longer recovery time [, ]. Nausea, phonophobia and photophobia, and aura were all reported to be more frequent in women []. Differences were also demonstrated in brain high-field magnetic resonance imaging between adult female and male migraineurs, and between female migraineurs and healthy controls [].In children, migraines were found to occur at a similar rate in both genders before the age of 10\u00a0years but was more common in girls after the age of 11\u00a0years []. Accordingly, high-field magnetic resonance imaging studies of children and adolescents with migraine showed both gender and developmental differences in brain anatomy and structure compared to healthy controls []. However, the clinical data on the pediatric age group are derived from relatively few studies, all based on older versions of the International Classification of Headache Disorders (ICHD1) of the International Headache Society (IHS) [, ].The aim of the present study was to compare between males and females, demographic and clinical parameters in a large sample of male and female pediatric migraineurs diagnosed according to the 2013 ICHD III-beta version [].The database search identified 468 children and adolescents with migraines, 215 boys (45.9%) and 253 girls (54.1%), who attended the pediatric headache clinic during the study period. Mean age at admission to the headache clinic was 11.3\u00a0\u00b1\u00a03.6\u00a0years (range 3.8\u201318, median 11.1), and at onset of headache disease, 9.0\u00a0\u00b1\u00a03.8\u00a0years (range 3.25\u201317.92, median 8.7). The mean interval from onset of headache disease to presentation at the clinic was 26.6\u00a0\u00b1\u00a026.4\u00a0months (range 1\u2013124, median 18). Migraine without aura was documented in 313 patients (66.9%), and migraine with aura, in 127 (27.1%); the remaining 28 patients (6.0%) had probable migraines. Mean headache frequency was 13.5\u00a0\u00b1\u00a011.4 headaches per month (range 0.03\u201330, median 8); 155 patients (33.11%) had chronic migraines (\u226515 episodes/month) and 298 (63.6%) had episodic migraines. Fifteen patients did not report headache frequency. Psychiatric comorbidities were documented in 94 patients (20.08%), organic comorbidity was reported in 53 patients.Compared to children older than 6\u00a0years, children younger than 6\u00a0years had lower mean headache frequency 9.31\u00a0\u00b1\u00a011.31 vs 14.03\u00a0\u00b1\u00a011.32 per month, \u00a0<\u00a00.001; lesser mean duration of headaches before admission 11.26\u00a0\u00b1\u00a08.79 vs 28.63\u00a0\u00b1\u00a027.27\u00a0months, \u00a0<\u00a00.001; and similar mean duration of headache attacks 12.88\u00a0\u00b1\u00a017.2 vs 12.52\u00a0\u00b1\u00a015.11\u00a0h, \u00a0=\u00a00.77.Psychiatric comorbidity was associated with higher headache frequency,16.2\u00a0\u00b1\u00a012.1 headache attacks per months vs patients without psychiatric comorbidity12.8\u00a0\u00b1\u00a011.1, \u00a0=\u00a00.01; and a higher rate of migraine with aura, 30.9% for patients with psychiatric comorbidity vs 26.2% of patients without psychiatric comorbidity (\u00a0=\u00a00.02)The present study of gender differences in pediatric migraineurs shows that female patients were older than male patients at both clinic admission and onset of headache disease. Accordingly, more female patients were postpubertal at both time points; and had a higher rate of migraine with aura, greater migraine frequency, more chronic migraine, and a lower rate of vomiting.In a large study of 2982 adults aged 18\u201365\u00a0years with migraine, female patients were found to be prone to a significantly longer duration of headaches and greater intensity of headache attacks than male patients, with a higher prevalence, frequency, and intensity of nausea; and a higher prevalence of phonophobia and photophobia [].W\u00f6ber-Bing\u00f6l et al. [] reported on the prevalence of migraine headache symptoms in children by gender and found that the female patients had a higher frequency of migraine with aura, as in our study, and a higher rate of vomiting than the male patients, contrary to our study. Additionally, the male patients had a higher rate of phonophobia, contrary to our study. However, the study of W\u00f6ber-Bing\u00f6l et al. [] was based on the original IHS criteria (ICHD1), whereas ours was based on the revised criteria for children of the ICHDIII-beta version. Furthermore, the age range in the earlier study was 3\u201319\u00a0years and in ours, 3.8\u201318\u00a0years.The differences observed in the current study in migraine parameters and in symptom frequency between males and females may have an anatomical basis. This assumption is supported by brain magnetic resonance imaging studies of adult and pediatric migraineurs [, ]. In both age groups, gender differences were noted in neural mechanisms in regions involved in sensory, motor, and affective functions, between male and female migraineurs [, ]. The authors also suggested that the observed changes might be attributable to differences in the response to intermittent stress (migraine attacks) or to differential effects of gonadal hormones (testosterone versus estradiol or progesterone) on hippocampal function and brain circuitry [, ].However, clinical gender-related differences in pain perception are less clear. A 2012 meta-analysis of 126 adult studies of pain perception conducted in 1998 to 2008 concluded that 10\u00a0years of laboratory research had not produced a clear and consistent pattern of gender differences in human pain sensitivity, even with the use of deep, tonic, long-lasting stimuli to mimic the clinic setting [, ]. Additionally, no gender differences in pain perception results were found in pediatric studies of the perception of laboratory-provoked dental pain in children (mean age 6.7\u00a0\u00b1\u00a02.4\u00a0years) [] and of disease-related pain perception during acute attacks of sickle cell anemia in adolescents (median age 14.8\u00a0years) []. By contrast, in an evaluation of pain perception in 118 children and adolescents from grades 5 to 9, using the cold pressor task and a related pain questionnaire, Vierhaus et al. [] found that the female subjects had significantly higher pain intensity scores by both measures. The differences among the studies may be explained by the different ages of the populations and the different methodologies used, including differences in the study settings and in the questionnaires. Such factors may also account for the findings of more pronounced temporal summation, allodynia, and secondary hypoanalgesia in women in laboratory studies [], and for the lack of a significant difference in the rate of allodynia by gender in our pediatric survey study. Although questionnaires have demonstrated reliability in this setting, differences in the diagnosis of allodynia may have contributed to the findings []. Overall, the evidence supporting a possibly less efficient endogenous pain inhibitory system in females is mixed and does not necessarily apply to all pain modalities [].In our study, no difference of psychiatric comorbidity was found between genders. Psychiatric comorbidity was associated with higher headache frequency and with a higher rate of migraine with aura. Gender differences in response to pain were evaluated with anxiety and pain questionnaires by Fuss et al. [] in a survey study of 1006 children aged 11.6\u00a0\u00b1\u00a02.7\u00a0years; of whom 27% reported having an experience of pain that lasted 3\u00a0months or longer. Girls with a history of persistent pain expressed higher levels of anxiety sensitivity (\u00a0<\u00a00.001) and pain catastrophizing (\u00a0<\u00a00.001) than did both girls without a history of pain and boys, regardless of pain history. The authors concluded that boys and girls appear to differ in terms of the relationship between age and pain history, and in the expression of pain-related psychological variables [].Compared to male pediatric patients, female pediatric patients with migraines have a significantly higher rate of chronic migraine, a higher frequency of migraine with aura, older age of onset, a higher rate of onset after puberty, and a lower rate of vomiting. Some of these findings may be related to better verbal ability of females, which makes it easier for them to describe their symptoms and their pain. Estrogen levels may explain the higher rate of aura in pubertal (but not prepubertal) girls compared to boys. Further studies are needed in ambulatory settings to assess the generalizability of the findings."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0779-1", "title": "Subjective cognitive decline in patients with migraine and its relationship with depression, anxiety, and sleep quality", "authors": ["Sun Hwa Lee", "Yeonwook Kang", "Soo-Jin Cho"], "publication": "The Journal of Headache and Pain", "publication_date": "25 July 2017", "abstract": "Cognitive decline is a major concern in patients with migraine. Depression, anxiety, and/or poor sleep quality are well-known comorbidities of migraine, but available evidence on the subjective cognitive decline (SCD) is limited. This study aimed to investigate the presence and frequency of SCD and its relationship with anxiety, depression and sleep quality in patients with migraine.", "full_text": "Migraine is a common and disabling neurological disorder which leads to reduced quality of life and significant functional impairments []. In South Korea, a study investigating the prevalence of migraine over the course of 1\u00a0year reported that approximately 9.2% of women and 2.9% of men suffer from the disease []. The burden of migraine is compounded by other co-occurring disorders, including psychiatric disorders, sleep problems, epilepsy, and stroke []. Thus, it is important to identify manageable comorbidities for effective treatment of migraine.There is contradictory evidence of the association between migraine and cognitive impairment. Some case-control studies reported that patients with migraine exhibit poor psychomotor speed [], attention [], and verbal memory [] performance compared to non-migraineurs, even during postictal periods. Conversely, several longitudinal and cross-sectional studies have shown that migraine is not associated with cognitive dysfunction or decline and may even associate with better function or less decline [\u2013]. Despite such inconsistent research findings on objective cognitive impairment, it is common that patients with migraine complain cognitive impairment in clinical practice. The perceptions of cognitive abilities are driven by comorbid symptoms rather than actual cognitive decline in patients with chronic pain. Therefore, cognitive problems in migraineurs is worthy to be approached from the assessment of complaints and its association with other comorbidities [, ].Subjective cognitive decline (SCD), also known as \u201csubjective memory impairment\u201d in early studies, refers to any self-perceived or subjectively experienced worsening of cognitive function in the absence of impaired performance on cognitive tests []. SCD may represent the earliest symptoms of cognitive decline or preclinical Alzheimer\u2019s disease (AD) and is also associated with psychological factors such as depression and anxiety in cognitively healthy elders [] and cerebrovascular disease [\u2013]. Multiple factors seem to contribute to the development of SCD; however, most studies on SCD are on the elderly population. Some investigations on healthy adults report that memory complaints are common, regardless of age, and seem to be related to negative affect [, ]. However, these studies focused only on \u201cmemory complaints\u201d while difficulties in other cognitive domains, such as attention or executive function are also common in patients with migraine.There is strong evidence indicating an association between migraine and several psychiatric conditions, such as mood and anxiety disorders [, ]. In a study using cluster analysis for patients with chronic migraine, patients with high levels of affective temperamental dysregulation showed a higher risk of suicidal behavior []. Recent studies also supported genome wide association between migraine and bipolar disorder, therefore, it is important to identify manageable psychiatric comorbidities for effective treatment and improving life quality among migraineurs [, ]. However, an association between SCD and comorbid symptoms or pain severity has not yet been investigated with relevant cognitive tests among patients with migraine.We hypothesized that the migraineurs with SCD would complain of more severe pain and exhibit greater psychiatric comorbidities than individuals with migraine without SCD. Thus, the aim of the present study was to investigate the presence and frequency of SCD in patients with migraine, as well as to analyze the association of SCD with clinical features and headache impact, anxiety, depression and quality of sleep.Among a total of 669 first-visit patients for headache between January to November 2016, 481 patients were excluded due to following criteria (Fig. ): having primary or secondary headache disorders other than migraine (\u00a0=\u00a0199), age criteria (\u00a0=\u00a060), experiencing severe pain (\u00a0=\u00a068), and refuse to perform questionnaire or cognitive testing (\u00a0=\u00a0154). Among migraineurs, none was excluded due to abnormal range of performance on both screening tests. Amongst the total of 470 migraine patients, the age of included patients (\u00a0=\u00a0188, mean age\u00a0=\u00a038.09\u00a0\u00b1\u00a09.92) were significantly younger than the excluded patients (\u00a0=\u00a0282, mean age\u00a0=\u00a040.68\u00a0\u00b1\u00a014.43, \u00a0=\u00a00.032), because those who were aged <19 or >65 were excluded. However, the percentage of gender did not differ between excluded versus included migraine patients (\u00a0=\u00a00.610).A total of 188 participants with migraine, aged 38.1\u00a0\u00b1\u00a09.9\u00a0years, were included. The mean SCD-Q score was 6.5 (SD\u00a0=\u00a05.5), with 84 patients (44.7%) scoring higher than 7. Thus, 44.7% of participants were diagnosed as SCD in this study. Among the 188 participants, 106 (56.4%) scored higher than the cutoff on the GAD-7, 98 (52.1%) scored higher than the cutoff on the PHQ-9, and 154 (81.9%) scored higher than the cutoff on the PSQI.SCD-Q scores were correlated with the depression on the PHQ-9 (\n                        \u00a0=\u00a00.49, \u00a0<\u00a00.001), anxiety on the GAD-7 (\n                        \u00a0=\u00a00.34, \u00a0<\u00a00.001), sleep quality on the PSQI (\n                        \u00a0=\u00a00.33, \u00a0<\u00a00.001), sleep duration during weekdays (\n                        \u00a0=\u00a0\u22120.20, \u00a0=\u00a00.007), VAS (\n                        \u00a0=\u00a00.17, \u00a0=\u00a00.017), and the frequency of headache attacks per month (\n                        \u00a0=\u00a00.15, \u00a0=\u00a00.047). However, SCD-Q scores were not significantly correlated with the HIT-6, age, maximum pain duration, medication, sleep duration during weekends, nor cognitive screening test scores (data not shown).We investigated SCD amongst the patients with migraine using a standard questionnaire with relevant cognitive testing. The main findings of this study were 1) SCD is relatively common in adult patients with migraine, 2) migraineurs with SCD reported severe headache pain severity and headache impact, 3) migraineurs with SCD were more depressed and anxious, while experiencing lowered sleep quality and sleep duration during weekdays were shorter, 4) depression on the PHQ-9 and sleep duration during weekdays were associated with SCD after adjusting demographic variables, headache related variables and psychological variables.Previous studies found that patients with migraine showed normal range of performance on both the MMSE [] and MoCA []. Similarly, none were excluded due to abnormal performance on screening tests among migraineurs in this study. Considering the mean SCD-Q scores of 3.2 (SD\u00a0=\u00a03.7) in normal controls in the previous study, the mean SCD-Q score of 6.5 (SD\u00a0=\u00a05.5) in migraineurs was relatively high []. Not negligible number of young adult migraineurs seemed to complaint cognitive decline and, to our knowledge, this is the first report to identify SCD in adult patients with migraines. Moreover, the proportion of SCD (44.2%) is comparable to common comorbidities of migraine such as poor quality of sleep (81.6%), anxiety (55.8%) and depression (52.1%).SCD in preclinical AD received particular interest on aging studies since longitudinal data support SCD as a risk factor for future cognitive decline as well as AD dementia [, , ]. A review study on SCD suggested that the proposed age of onset in studies of preclinical AD is 60\u00a0years or older despite the age at SCD onset was not defined as a core criterion []. Unlikely to SCD in elderly, there is no conclusive evidence for that SCD or perceived forgetfulness in young adult is a risk factor for future cognitive impairment or dementia []. In studies of healthy adults, memory complaints are frequent regardless of age; however, the type of complaint varies and may be related to negative affect throughout adulthood [, ]. This study also suggested that SCD among migraineurs require more attention to psychological or sleep problem rather than close follow-up of cognitive decline.Subjective pain is a common symptom in patients suffering from depression and in turn, chronic pain may trigger a depressive state [, ]. Interestingly, migraineurs with SCD complained of more severe difficulties in subjectively reported measures, such as pain intensity and headache impact, compared to migraineurs without SCD, but this association was not persistent on logistic analysis. This implies such sensitivity to perceived pain or subjective difficulties may be related to psychological impairments and/or sleep problem among migraineurs with SCD.In line with previous studies, our data also revealed high levels of anxiety and depression in patients with migraine []. Not surprisingly, depression was significantly associated with SCD among migraineurs in this study. It is possible that SCD among migraineurs may either independently or conjointly account for the high levels of depression and anxiety. Moreover, biased perceptions of cognitive function may relate more to emotional state than objective ability, causing individuals to misinterpret or exaggerate problems by overestimating minor cognitive disruptions. According to Beck\u2019s cognitive theory of depression, depression is associated with a negative view of oneself, environment, and future []. Increasing the awareness patients with migraine have of possible misperceptions could help them understand their neurological and psychological state more objectively.Similar to a previous study, more than half of the total patients with migraine that were evaluated in the current study complained of poor sleep quality []; this percentage was even higher (86.9%) in migraineurs with SCD. This relationship may be explained by the high correlation between SCD-Q and PSQI scores. Indeed, a previous study revealed an association between perception of poor sleep quality and worse cognitive performance in healthy elderly individuals []. In the current study, the average amount of sleep achieved during weekdays for migraineurs with SCD was less than 6\u00a0h. This value is less than the recommended duration of 7 to 8\u00a0h for adults []. It has been reported that migraineurs who routinely sleep 6\u00a0h per night exhibit more severe headache patterns and sleep complaints than migraineurs who slept longer []. Thus, short sleep duration during weekdays may account for the poor perception of cognitive function and sleep quality observed in the current study.Several limitations to this study should be addressed. The main limitations of the study would be that it is based on retrospective review of headache registry records. Therefore, this study may be biased in its sampling procedure and we cannot rule out the possibility that individuals who perceived SCD were more inclined to agree to cognitive testing. However, variables in this study were collected prospectively according to the registry and the difference between excluded and included patients was not significant in demographical characteristics such as gender. In addition, we used brief screening tests (i.e., K-MMSE and K-MoCA) as cognitive measures. These were employed as it is difficult to recruit a substantial number of patients willing to undergo comprehensive neuropsychological tests in clinical settings. Although there were no group differences in any sub-items of both tests, more detailed cognitive tests may help reveal subtle cognitive characteristics that might not yet exceed age-, sex-, and education-adjusted normal ranges in migraineurs with SCD. Lastly, future studies should include a healthy control group to provide better standards for understanding migraineurs with SCD.In conclusion, SCD seems relatively common in adult migraineurs in terms of both gender and migraine subtype. Migraineurs with SCD reported severe pain and impact of headache, were more depressed, anxious, perceived poorer sleep quality, and shorter sleep duration during weekdays than those without SCD. Depression and shorter sleep duration significantly associated to the presence of SCD in migraineurs after adjusting other variables."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0791-5", "title": "Volumetric abnormalities of thalamic subnuclei in medication-overuse headache", "authors": ["Zhiye Chen", "Zhihua Jia", "Xiaoyan Chen", "Mengqi Liu", "Shuangfeng Liu", "Lin Ma", "Shengyuan Yu"], "publication": "The Journal of Headache and Pain", "publication_date": "14 August 2017", "abstract": "The thalamus exerts a pivotal role in pain processing and cortical excitability control and a previous voxel-based morphometry study confirmed increased volume in bilateral thalamus in medication-overuse headache (MOH). The aim of this study is to investigate altered thalamic subnuclei volume in MOH compared with normal controls, and to evaluate the relationship of each thalamic subnuclei volume with the clinical variables.", "full_text": "Medication-overuse headache (MOH) was defined as a headache occurring on 15 or more days per month developing as a consequence of regular overuse of acute or symptomatic headache medication for more than 3\u00a0months [] . MOH has a prevalence of 0.6\u20132.0% in the general population [, ], and was associated with mood disorders in 27\u201385% and anxiety disorders in 61\u201383%. MOH patients experience reduced quality of life compared with those who do not suffer from headaches []. A pre-existing headache disorder seems to be required to develop MOH []. It is well known that previous primary headaches such as migraine are the most important risk factors for the development of MOH, 50%\u201370% MOH have co-occurrence of migraine in population-based studies [, ]. Many psychosocial and socioeconomic factors which are prevailed in patients with chronic forms of headache are also associated with MOH. However, the mechanism behind how chronic exposure to abortive medication leads to MOH remains unclear. Alteration of cortical neuronal excitability, central sensitization involving the trigeminal nociceptive system have been suggested to play a part in the pathophysiology of MOH [].The thalamus contains third-order trigeminovascular nociceptive neurons and exerts a pivotal role in pain processing and cortical excitability control [, ]. Microstructural and functional alterations of the thalamus have been found in migraine patients [, ]. Significant volume reductions of the following thalamic nuclei densely connected to the limbic system were observed in migraineurs: central nuclear complex, anterior nucleus and lateral dorsal nucleus, supported that higher-order integration systems are altered in migraine []. Increased iron deposition and myelin content/cellularity in the thalamus of migraine with aura patients compared with migraine without aura patients and healthy controls were found, may underlie abnormal cortical excitability control leading to cortical spreading depression and visual aura []. A voxel-based morphometry (VBM) study identified increased gray matter volume in bilateral thalamus in MOH patients []. Although it was demonstrated that periaqueductal gray (PAG) volume gain [] and altered intrinsic functional connectivity architecture [] were confirmed in MOH patients in our previous study, however, it was not known that how the thalamic subfields volume changed in MOH up to now.Up to now, several documents had recognized that thalamic subnuclei were segmented based on diffusion tensor imaging [, ], and thalamic nuclei densely connected to the limbic system were observed in migraineurs []. Therefore, morphology analysis of thalamic subnuclei would provide more information in the understanding of neuromechanism of MOH.The main objective of the current study was to investigate the altered thalamic subnuclei volume in MOH compared with normal controls, and to further evaluate the relationship of each thalamic subnuclei volume with the clinical variables.Our study aimed to identify morphological changes of thalamic subnuclei in MOH and try to reveal more information about the neuromechanism of MOH. In our study, psychiatric evaluation revealed that the majority of patients had comorbid psychiatric conditions, containing both anxiety and depressive disorders, which is accordant with epidemiologic studies [, ]. A previous study showed MOH patients have a greater risk of suffering from anxiety and depression than episodic migraine, and psychiatric disorders occurred significantly more often before the transformation from migraine into MOH than after []. It deduced that these disorders may be a risk factor for the evolution of migraine into MOH. Another follow-up study identified several risk factors for MOH among people with chronic headache, including increased Hospital Anxiety and Depression Scale score []. However, depression and anxiety disorders are associated with both migraine and non-migrainous headache, and this was related to the headache frequency rather than headache diagnosis in another research, so the relationship between psychiatric disorders and MOH may be comobidity []. The cause-effect relationship needs further longitudinal study.Consistent with the previous study [], we found increased whole thalamus volume bilaterally in the MOH. An increase in GMV may reflect structural brain plasticity as a result of exercise and learning []. Gray matter volume increase in the thalamus has also been found in chronic pain conditions such as back pain [] and chronic post-traumatic headache []. Increased GMV in the thalamus might reflect central sensitization in chronic pain states. However, studies about the thalamic subnuclei volume in these chronic pain conditions have not been found. If the morphological abnormalities of thalamic subnuclei are specific to MOH or if the morphological abnormalities of thalamic can be normalized as cephalic, extra-cephalic pressure-pain thresholds and pain-related cortical potentials in MOH patients after withdrawal of the overused medication needs further study [, ]. Unlike the specific thalamic subnuclei decreases observed in migraineurs [], all the thalamic subnuclei presented increased volume in MOH. Each thalamus is divided into the following subnuclei according to the inner medullary plate (including plate core): anterior nucleus (AN), dorsomedial nucleus (DM), ventral anterior nucleus (VA), ventral lateral nucleus (VL), ventral posterior lateral nucleus (VPL) and ventral posterior medial nucleus (VPM). AN of the thalamus is a key component of the hippocampal system for episodic memory. Via its connections with the anterior cingulate and orbitomedial prefrontal cortex, the AN may also involve in emotional and executive functions []. Affective and anxiety disorders prevailed in patients with chronic forms or transform of headache and substance use than in patients with migraine alone []. Decreased AN volume in migraineurs may be related to the psychiatric disorders in migraine patients and suggest that the central reorganization after repeated, long-term nociceptive signaling. Increased volume of AN in our study may suggest pre-existed morphological abnormalities in MOH. Somatosensory-related thalamic structures can be broadly divided into lateral and medial subdivisions (VPL and VPM), which receive sensory inputs from the spinal cord or medulla to the thalamus directly through the spinothalamic tract or trigeminothalamic tract []. VPL and VPM then project to the dorsal part of thalamus and then sends axon projections to the cerebral cortex for a complete sensory transmission []. Increased gray matter volume in DM, VPL and VPM may indicate higher-order of pain are altered in MOH.In our study, we did not find a relation between the volumes of thalamic nuclei and clinical features, such as VAS or the duration of the disorder. It suggests that increased gray matter volume in thalamus may relate to the genetic background of patients with MOH. We observed negative associations between HAMD scores and gray matter volume in all the thalamus subnuclei in patients, suggesting that these structural changes may also be influenced by mood disturbances related to the disorder [].In conclusion, increased gray matter volume in the whole thalamus and all the thalamus subnuclei may reflect central sensitization and higher-order of pain alteration in MOH. These structural changes in the thalamus may also be influenced by mood disturbances related to the MOH. Whether the observed morphological abnormalities in MOH can be reversed after withdrawal of the overused medication remains unclear."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0792-4", "title": "The efficacy of transcranial magnetic stimulation on migraine: a meta-analysis of randomized controlled trails", "authors": ["Lihuan Lan", "\u2020", "Xiaoni Zhang", "\u2020", "Xiangpen Li", "\u2020", "Xiaoming Rong", "Ying Peng"], "publication": "The Journal of Headache and Pain", "publication_date": "22 August 2017", "abstract": "As a non-invasive therapy, whether transcranial magnetic stimulation (TMS) is effective on migraine. This article was aimed to assess the efficacy of TMS on migraine based on randomized controlled trails (RCTs).", "full_text": "Recently, the incidence of migraine is gradually rising and becoming one of the most common nervous system diseases in the world []. According to ICHD-3 (beta version), migraine is divided into migraine without aura, migraine with aura, chronic migraine, complications of migraine, probable migraine and episodic syndromes that may be associated with migraine. Particularly, chronic migraine cause serious damage in the quality of life. However, the effect of the drug therapies, include acute therapies (non-steroidal anti-inflammatory drugs, ergotamine preparations and triptans) and preventive therapies (\u03b2-blockers, anticonvulsants, tricyclic antidepressants and calcium channel modulators), are not significantly improved the clinical symptoms.Transcranial magnetic stimulation (TMS), which is a magnetic field created by an electrical current through a coil wrapped around the scalp or skull. The types of TMS include single-pulse TMS, pair-pulses TMS and repetitive TMS. In neurophysiology, TMS can measure neural conduction, facilitate or inhibit the electrical activity of cerebral cortex []. TMS is a noninvasive technology and the first transcranial magnetic stimulator was introduced to the world in 1984\u20131985 []. Nevertheless, using TMS for a therapy was firstly reported on drug-resistant depressed patients in 1996 []. After 30\u00a0years later, TMS now can be applied for a diagnostic therapy in many diseases including multiple sclerosis, movement disorder, stroke, epilepsy and so on [, ]. Meanwhile, TMS also can be used for a therapy. There has been reported a series of diseases covering psychiatric disorders (depression, acute mania, schizophrenia, bipolar disease, panic disorder, post-traumatic stress disorder, substance abuse) and neurologic disorders (Parkinson\u2019s disease, dystonia, tinnitus, epilepsy, stroke) improved by TMS [\u2013]. Furthermore, single-pulse and paired-pulse/double-coil TMS are safety for normal human subjects and patients who suffer from migraine [, ]. However, there is less randomized control trails (RCTs) to identify the efficacy of TMS in migraine at present. Recently, there are some papers reviewed the effect of TMS for migraine [, ], but lack a meta-analysis. Although there is a meta-analysis about noninvasive brain stimulation in migraine, it reached a conclusion that TMS did not reveal significant effects for any outcome [], moreover, some new RCTs have revealed that TMS is efficacy for migraine recently.For the exact mechanism of migraine does not exist so far. It may relate to neural and vascular causes, involving cerebral cell hyper excitability, sensitization of the trigeminovascular pathway, correlative predisposing genes and environmental factors. As for migraine with aura, cortical spreading depression (CSD) proved to be its pathogenesis [\u2013]. CSD, an inhibition zone of cortical activity after stimulating vertebrate\u2019s cerebral cortex, and the zone would move to adjacent cortex at a speed of 2-5\u00a0mm/min. CSD may change the cerebral blood flow and result in headache. Currently, there are evidence that single pulse-TMS can suppress CSD in animal experiment []. Correspondingly, some clinical trials are developed to verify whether TMS is effective for migraine. This article provides an update on the effect of TMS in migraine from randomized control trails.According to the Preferred Reporting Items for Systematic Reviews [], a protocol of study-search strategies, outcome measurements, and methods of statistical analysis was prepared in advance.Migraine is a kind of chronic headache relating to cortical excitability. As a noninvasive therapy, TMS can activate (or suppress) the cortex excitability. In 2013, a Statement from the European Headache Federation indicates that using of a noninvasive therapy in chronic headaches is not evidence based at present and a neurostimulator should be considered only all alternative drug and behavioural therapies as recommended by international guidelines have failed and medication overuse headache is excluded, due to the lack of proper RCTs []. Because there are limited drugs that can improve the quality of life for people with the migraine, and TMS as a promising therapy which can facilitate or inhibit the electrical activity of cerebral cortex and there are some existing RCTs reveal that TMS can relieved headache. Nonetheless, there are few meta-analyses about the effect of TMS for migraine. By combining RCTs and meta-analysis, we hope to evaluate whether TMS can relieve headache and to expand its clinical application. So we decide to assess the effect of TMS on migraine by synthesizing evidences.In this meta-analysis, 5 RCTs including 313 patients comparing the efficacy of TMS group with control group indicated that TMS was significantly effective for migraine. However, the doses and frequency of TMS in these RCTs were different. And which doses could help to improve the headache frequency of migraine doesn\u2019t reach common understanding. In an open labeled study, Usha et al. reported that high frequency repetitive transcranial magnetic stimulation (rTMS) was effective and well tolerated for migraine prophylaxis []. Besides, in other study, M Teepker et al. reported that no statistically significant difference between low-frequency rTMS with sham stimulation was found []. In the present meta-analysis, 5 RCTs used a higher frequency (\u2265\u200910\u00a0Hz) stimulation. So we considered that higher frequency stimulation may reach an obvious effect. However, the reasons for variability are not only the dose but also the side, location of stimulation, type of coil and the number of sessions. The 5 RCTs were generally delivered at different frequency with a figure-eight coil positioned over the left motor cortex. Due to the difference in the side, location of stimulation, type of coil and the number of sessions, the efficacy of magnetic signal on electrical activity of cerebral cortex is different. Nevertheless, there is not a common standard of TMS on migraine at present. Given that, future well-designed RCTs are needed to confirm which dose, side, and location of stimulation, type of coil or the number of sessions is more effective for migraine.Transcranial magnetic stimulation, a novel treatment method, is considered to be effective for migraine in this meta-analysis. However, there is an inevitable problem that these RCTs did not have a standard control group. In four RCTs, there were active TMS group and sham TMS group [, , , ]. In one RCT, there were active TMS group and botulinum toxin-A injection group []. Therefore, we have no idea that whether TMS is superior to conventional therapy. It is necessary to conduct more clinical trials to assess the efficacy of TMS on migraine in the future.When evaluated the effect of TMS on chronic migraine, we reached a conclusion that there was not statistically significant difference in effect between active TMS group and sham TMS group. In light of this, we put forward two hypotheses: firstly, chronic migraine is a chronic pathogenic process and the threshold of pain had been raised. Although TMS can change the excitability of cortex, it needs more time to do this. Secondly, due to the small sample, this conclusion was not definite. Future well-designed RCTs are needed to confirm this conclusion.Besides, this meta-analysis has some limitations as following: first, the main limitation is that we only included published data and there were 5 RCTs included in this article. The published bias comes to an unavoidable issue. Therefore, the conclusion came from synthesizing evidence should be considered with caution. And in order to improve the reliability of this meta-analysis, we only take RCT into account. Although a meta-analysis of RCTs can provide a more reliable result, due to the lack of studies, only 5 RCTs included in this meta-analysis prevented us from reaching a more authentic outcome. Second, for all studies included in the analysis, patients were not a grouped by severity of pathogenic condition, sexuality or age and so on. So the efficacy of TMS should be taken into consideration. Third, the patients included in this paper mostly came from general hospitals or major institutions, so the patients might not represent patient populations in the world. Fourth, due to the difference of original data on studies included in the analysis, this meta-analysis did not make full use of data in studies.In summary, this meta-analysis indicates that TMS is effective for migraine based on the studies included in the article. For the stimulation parameters, using figure-of-8-shaped coil over the left motor cortex with higher frequency may be effect based on the studies included in the article. However, because of above limitations, the efficacy of TMS on migraine should be tasted on more RCTs in the future."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0798-y", "title": "Screening of genetic variants in ", "authors": ["Caroline Ran", "Carmen Fourier", "Julia M. Michalska", "Anna Steinberg", "Christina Sj\u00f6strand", "Elisabet Waldenlind", "Andrea Carmine Belin"], "publication": "The Journal of Headache and Pain", "publication_date": "22 August 2017", "abstract": "We have genotyped a Swedish cluster headache case-control population for three genetic variants representing the most significant markers identified in a recently published genome wide association study on cluster headache. The genetic variants were two common polymorphisms; ", "full_text": "Cluster headache (CH) is a severe primary headache affecting around 0.1-0.2% of the Swedish population []. There are no known causes of CH today, but a genetic component in the etiology is suggested. For instance, having a first or second degree relative with CH implies an increased risk of developing CH []. Twin studies also provide an indication of genetic influence in CH. Apart from case-reports identifying occasional monozygotic twins with CH, Ekbom et al. reported two concordant monozygotic twin pairs in a population of over 30,000 individuals in an interview-based register study from 2006, indicating that there is a weak heritability for CH [, , , , ]. In addition, there have been several reports of genetic associations linking genetic variations in several candidate genes to CH. Hypocretin receptor 2 () and nitric oxide synthase () are examples of such genes, representing a link between the molecular pathways involved in the pathophysiology, and genetic risk-factors [, , ].  in particular has received a lot of attention due to its involvement in the regulation of sleep and pain, functions highly relevant in the CH pathophysiology []. Moreover, hypocretin-1 levels are reported to be reduced in the ceresbrospinal fluid in CH patients []. Recently the first genome wide association study (GWAS) on a CH cohort was published. Though underpowered, the GWAS data indicated a few genetic markers potentially of interest for CH pathophysiology which warrant more thorough investigation [].We selected the top three associations from Bacchelli et al.; , an intergenic variant on chromosome 14q21;  in adenylate cyclase activating polypeptide 1 receptor type 1 () and one rare mutation, , in the membrane metallo-endopeptidase gene (), and performed a replication study on a well characterized and large Swedish CH case-control population. Replicating GWAS findings in independent materials is a crucial step in the validation of GWASs. In particular when the discovery cohort is small, it is important to verify the association and evaluate the importance of the identified markers in patients from different genetic backgrounds.  and  are attractive candidate genes since they are both involved in pain signaling, and our study may shed more light on the possibility of these genetic markers being involved in the pathophysiology of CH.We genotyped 542 CH patients and 581 controls for two single nucleotide polymorphisms (SNPs);  and , and one rare mutation; . All experiments were approved by the regional ethical review board in Stockholm, Sweden. CH patients were recruited after informed consent at the Neurology Department at Karolinska University Hospital, or through collaboration with neurologists at other clinics in the central part of Sweden. Diagnosis was confirmed by a neurologist and complied with the guidelines of the 3rd version of the International Classification of Headache Disorders (ICHD III) []. Patients were requested to provide personal, clinical and lifestyle information through a questionnaire and give a blood sample. Of the CH patients 31.7% were female, average age 52\u00a0years, median age 53\u00a0years, 10.3% had chronic CH, age of onset was 31.6\u00a0years and 10.8% reported they had at least one first or second degree relative with CH (53 patients of 489 for whom the information was available). Control subjects were anonymous healthy blood donors, of whom 44% were females, between the age 18 and 65.DNA was prepared from blood using the Gentra Puregene Blood kit according to standard protocols (QIAGEN, Hilden, Germany). Genotyping was performed with TaqMan quantitative real-time PCR (qPCR) on an ABI 7500 Fast cycler (Applied Biosystems, Foster City, CA, USA). We used pre-made TaqMan SNP genotyping assays, C__32158964_10 for  and C__2056560_10 for , and TaqMan genotyping master mix (Applied Biosystems).TaqMan genotyping was unsuccessful for the rare mutation , therefore we used pyrosequencing to analyze the  mutation. We used primers (Thermo Fisher Scientific, Waltham, MA USA) designed in-house using primer3 and mfold software, as well as the NCBI online Blast tool (), sequence available on request [, , ]. The genomic region of interest was amplified with a touchdown PCR spanning from 45 to 55\u00a0\u00b0C using one regular primer and one biotinylated primer. The PCR fragments were denatured and the biotin containing fragments were annealed to streptavidin-sepharose beads using a PyroMark Vacuum Prep tool (Biotage AB, Uppsala, Sweden), washed and incubated with the pyrosequencing primer at 80\u00a0\u00b0C for 2\u00a0min, and then analysed in an automated pyrosequencing PSQ 96 System using PyroMark Gold reagents (QIAGEN).Chi-squared (\u03c7) test was used for statistical analysis using GraphPad Prism v5.04 (GraphPad Softwares Inc., La Jolla, CA, USA). Fisher\u2019s exact test was used for the additional dominant genotypic model. Both SNPs were tested for Hardy Weinberg equilibrium (HWE) using the Online Encyclopedia for Genetic Epidemiology studies HWE calculator [].We have performed a replication study based on the recent findings of the first published GWAS on CH. The three most significant variants, ,  and  were selected and genotyped in a Swedish CH case-control study population. We found no association for either genotypes or alleles with CH for the common SNPs, and further we found no CH patient carrying the mutated allele of . The discrepancy between our study and the discovery study can have several reasons. The control materials exhibit considerable differences. The Italian GWAS used 360 cigarette smokers, while we used anonymous blood donors, 14% of which are presumably smokers, as this is the proportion of smokers in the Swedish population. Moreover, there is a significant difference in gender ratio between the Swedish and the Italian material. The Italian cohort have an unusually high proportion of males (84%) compared to the Swedish cohort (68,3%). As a control experiment, we therefore verified our results in a gender stratified analysis conducted with male subjects only, successfully replicating a lack of association. Also, there might be differences in genetic background between the Swedish and Italian populations. The minor allele frequencies (MAF) for the two common SNPs differs between our study and the Italian report, in particular for  which is protective in the Italian study. In our material, although the difference is non-significant, cases have a higher MAF (0.245) than controls (MAF 0.216) which would be indicative of an increased rather than decreased risk. The association discovered in the Italian cohort might also reflect a linkage disequilibrium (LD) between these SNPs and other genetic variations truly linked to disease that are not present in the Swedish population. Since we did not genotype any additional marker at these loci, we could not control for a potential difference in the LD structure between the Swedish and Italian populations. Another limitation of our study is the possibility that other rare  mutations might be associated to CH in our material. Last, life style and environmental factors, e.g. seasonal variation has been reported to influence CH, smoking is a risk factor for CH, other plausible factors might be inflammations and diet, [, , , , ], and gene-environment interactions might cause genetic risk factors to differ in specific populations or various parts of the world.In conclusion  and  do not impact the risk of developing CH in the Swedish population. Also,  is a rare mutation which does not seem to be enriched within the Swedish CH patient group."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0796-0", "title": "Persistent post-traumatic headache vs. migraine: an MRI study demonstrating differences in brain structure", "authors": ["Todd J. Schwedt", "Catherine D. Chong", "Jacob Peplinski", "Katherine Ross", "Visar Berisha"], "publication": "The Journal of Headache and Pain", "publication_date": "22 August 2017", "abstract": "The majority of individuals with post-traumatic headache have symptoms that are indistinguishable from migraine. The overlap in symptoms amongst these individuals raises the question as to whether post-traumatic headache has a unique pathophysiology or if head trauma triggers migraine. The objective of this study was to compare brain structure in individuals with persistent post-traumatic headache (i.e. headache lasting at least 3\u00a0months following a traumatic brain injury) attributed to mild traumatic brain injury to that of individuals with migraine.", "full_text": "The majority of individuals with post-traumatic headache (PTH) have headache characteristics that are consistent with a migraine phenotype [\u2013]. The only clinical feature that then differentiates PTH from migraine is the head injury itself. The International Classification of Headache Disorders (ICHD) 3 beta criteria stipulate that a PTH must begin within 7\u00a0days of the head injury or within 7\u00a0days of being able to detect headaches following a head injury []. If the PTH persists for at least 3\u00a0months, it is classified as \u201cpersistent PTH\u201d (PPTH). The ICHD criteria do not include any headache characteristics that differentiate PTH from other headache types such as migraine or tension-type headache. For the patient without a history of migraine who has head trauma and develops headache immediately following the trauma, the diagnosis of PTH is rather straightforward. However, even in such a situation, it is not clear if the head trauma caused a unique headache type (i.e. PTH) with a unique underlying pathophysiology or if the trauma unmasked an underlying propensity toward the development of migraine. Identification of differences in the pathophysiology of migraine and PTH would support the notion that migraine and PTH are truly distinct headache types.The objective of this study was to compare measures of brain regional volume, cortical thickness, surface area, and brain curvature in a cohort of patients with PPTH attributed to mild traumatic brain injury (mTBI) to a cohort of patients with migraine. Differences in brain structure would serve as evidence that PPTH and migraine might be associated with different underlying mechanisms and thus should indeed be considered distinct headache types.Amongst the 28 individuals with PPTH, 21 (75%) had a phenotype that was consistent with a diagnosis of migraine (had the symptoms not been initiated by TBI), 4 with probable migraine, 2 with tension-type headache, and 1 was unclassifiable. Twenty-seven reported that their headaches were of moderate or severe intensity, 25 reported sensitivity to sound, 24 sensitivity to light, 22 nausea, 21 throbbing quality of headache, 19 worsening of headache with physical activity, and 11 vomiting. Five subjects had one TBI in their lifetime, 13 had 2 TBIs, two had 3 TBIs, one had 4 TBIs, two had 5 TBIs, and five had 6 or more TBIs. Of the TBIs leading to PPTH, 14 were due to explosions/blasts, 7 were due to sports injuries, 4 were due to motor vehicle accidents, and 3 were due to falls.To better interpret the differences in brain structure identified between PPTH and migraine, the seven regional structural measures that differed between the PPTH and migraine cohorts were compared between the PPTH group to a group of healthy controls and between the migraine group and the healthy controls. Amongst these regions, there were differences between the PPTH group and healthy controls for thickness of the right supramarginal gyrus (\u00a0=\u00a0.001), area of the right lateral orbitofrontal region (\u00a0=\u00a0.024), and thickness of the left superior frontal region (\u00a0=\u00a0.035). For all of these comparisons, PPTH was associated with less area and cortical thickness compared to the healthy controls. Comparisons between the migraine group and the healthy control group demonstrated no significant differences in the seven structural measurements that differed when comparing the PPTH group to the migraine group.The phenotypic similarities between migraine and PPTH justify the need for studies that investigate mechanistic similarities and differences between migraine and PPTH. The main finding of this study is that there are differences in brain structure between patients who have PPTH and those with migraine, perhaps suggesting that these two headache types are associated with distinct underlying pathophysiology despite their substantial similarities in symptoms.Brain areas that differed between PPTH and migraine were located in lateral orbitofrontal, superior and middle frontal, precuneus and supramarginal gyrus regions. The observation that only three of the seven brain regional measurements differed when comparing PPTH to healthy controls and none of the seven brain regional measurements differed when comparing migraine to healthy controls suggests specificity of some brain structural differences to the comparison of PPTH vs. migraine. We are not aware of prior studies that have compared brain structure or function in a group of individuals with PPTH to a group with migraine and thus are unable to compare our findings to others. However, there are a few studies that have compared brain structure and metabolism in individuals with PTH compared to healthy controls. For example, a longitudinal voxel-based morphometry study by Obermann and colleagues compared 32 patients with PTH attributed to whiplash injury to healthy controls []. During the first 14\u00a0days post-TBI, there were no differences in brain structure. However, at 3\u00a0months, PTH was associated with decreased gray matter density in the anterior cingulate cortex and dorsolateral prefrontal cortex. As headaches subsided by 1 year, the gray matter density normalized in these brain areas. Amongst those who developed PPTH, at 1\u00a0year there was in increase in gray matter in the midbrain, thalamus and cerebellum. Assuming that the patients who developed PPTH and those who did not had similar brain injuries, this study supports the notion that brain structural changes in patients with PPTH are related to the continued headaches and are not due solely to the inciting brain injury. A magnetic resonance spectroscopy study of 17 individuals with PTH attributed to mTBI (9 with acute PTH and 8 with PPTH) found that compared to healthy controls those with PTH had metabolic abnormalities in the anterior frontal lobes, anterior and posterior medial frontal lobes, and medial parietal lobes []. Although there was little power in this study to detect differences between patients with acute PTH compared to those with PPTH, those with PPTH had non-significant but lower N-acetylaspartate values (a marker for neuronal health). In our study, there were no significant correlations between the structural measurements of regions that differed when comparing PPTH to migraine with the number of years that individuals had PPTH. However, our patient population differed than those in prior publications in that our patients had a longer duration of PPTH at the time of imaging (mean of 8.5\u00a0years). Studies of mTBI have also demonstrated abnormalities of brain structure in several of the regions that we identified to have different structure in PPTH compared to migraine [\u2013]. Most of these previously published studies have not reported whether the participants had PTH and they did not attempt to disentangle the effects of the brain injury itself on brain structure vs. the effects of post-TBI symptoms on brain structure.The brain regions that differed in structure when comparing individuals with PPTH to those with migraine in this study have all been previously demonstrated to participate in pain processing. Frontal regions play roles in the affective and cognitive evaluation of pain [, ]. Frontal regions have previously been found to have abnormal structure, function, and functional connectivity in individuals with different headache types including migraine, cluster headache, and medication overuse headache [\u2013]. The precuneus, a core region of the default mode network that is responsible for self-referential processing and interoception, participates in the determination of pain sensitivity and pain thresholds [\u2013]. Prior studies have implicated the precuneus and the default mode network in headache disorders including migraine, medication overuse headache, and cluster headache [, \u2013]. The supramarginal gyrus is likely involved in cognitive evaluation of pain including pain empathy [, ]. The supramarginal gyrus has previously been demonstrated to have atypical function and structure in groups of individuals with migraine and medication overuse headache [, ]. Although there are sufficient data to support a role for these brain regions in pain processing and headache, the explanation as to why the structure of these regions differs in individuals with PPTH compared to those with migraine is yet to be elucidated. It is plausible that these are brain regions that are simply more susceptible to the effects of mTBI and once damaged they contribute to the initiation and persistence of PTH.The different measures used in this study (volume, area, cortical thickness, curvature) provide complementary information about brain structure. Regional volumes, surface area, cortical thickness and curvature are plastic measures of brain structure that commonly show alterations in the presence of aging, learning, and neurodegeneration related to disease. Brain curvature provides a measure of cortical folding, with increased curvature indicating areas of sharper cortical folds. Increased cortical curvature has been associated with white matter damage, aging, neurodegenderative disease, and mild TBI []. Although regional volumes have been most commonly used to measure brain structure and compare brain structure between subject cohorts, measurements of area and cortical thickness are likely to be more sensitive to small changes in brain structure. Regional brain area, cortical thickness, and volume have been previously identified to differ in cohorts of individuals with headache compared to healthy controls and to contribute to subclassification of headache types [\u2013].Limitations: Similar to individuals with migraine and PPTH in the community and in clinical practice, our research participants had co-morbid medical conditions, and they were utilizing medications. For example, patients with PPTH had higher anxiety and depression scores than patients with migraine and healthy controls. Although we controlled for many potentially confounding variables in our ANCOVA, including depression and anxiety scores, it is not entirely possible to decouple the effects of these co-morbid illnesses on brain structure from the effects of migraine and PPTH on brain structure. However, study results would be of little value in understanding migraine and PPTH if we excluded patients with typical disease and only enrolled the very rare individual who has migraine or PPTH in isolation. Inclusion of typical patients with migraine and PPTH makes the results from this study generalizable to the typical individual with migraine or PPTH. Furthermore, the subject cohorts were not exactly matched for age and sex. Although we attempted to do so, the realities of our patient populations resulted in enrolling more men with PPTH (mostly armed forces veterans) and more women with chronic migraine. Sex was controlled for in our ANCOVA and was significant for only one of the brain measurements that differed between migraine and PPTH, but it still could have impacted our results to some extent. Perhaps the biggest limitation of this analysis is the inability to differentiate the effects of TBI from that of PPTH on brain structure. It is not clear if the differences in brain structure identified in this study are due to PPTH or if the differences would be similar in individuals who have persistent post-TBI symptoms without headache. We are currently trying to enroll those rare patients who do not have headache but do have other persistent symptoms following a mild TBI for comparison to the PPTH group.In conclusion, although there was substantial overlap in symptoms between individuals with PPTH and those with migraine, the structure of several brain regions differed in individuals with PPTH compared to those with migraine. These structural differences suggest that the pathophysiology of PPTH might be different than that of migraine and support the classification of PPTH and migraine as distinct headache types. Additional studies are planned to confirm these imaging findings and to determine their specificity."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0788-0", "title": "Headache attributed to airplane travel: diagnosis, pathophysiology, and treatment \u2013 a systematic review", "authors": ["Sebastian Bao Dinh Bui", "Parisa Gazerani"], "publication": "The Journal of Headache and Pain", "publication_date": "16 August 2017", "abstract": "Headache attributed to airplane travel, also named \"airplane headache\" (AH) is a headache that occurs during take-off and landing. Today, there are still uncertainties about the pathophysiology and treatment of AH. This systematic review was performed to facilitate identification of the existing literature on AH in order to discuss the current evidence and areas that remain to be investigated in AH.", "full_text": "Headache attributed to airplane travel, also named \"airplane headache\" (AH) occurs in a population of passengers during airplane travels. The headache appears as an intense short lasting pain at landing and it is often located in the fronto-orbital region [\u2013]. Despite its occurrence rate and high impact, only limited is known about AH, and this type of headache has only been defined and included in the headache classification since 2013 by International Headache Society (IHS), which provides headache classifications and maintains related updates []. The first case of AH was described in 2004, and since then, number of publications on AH has been added into the literature [\u2013]. Previous reports, before inclusion of AH in classification, could be based on diversity in diagnosis, which makes it difficult to determine whether reported patients suffered from AH or other conditions [, ]. Despite the fact that some points are known based on these publications, there is still uncertainty around influence of ethnicity, gender or age on incidence or prevalence of AH. A male dominancy has been reported for AH [, , ]. Current knowledge about pathophysiology and treatment of AH is limited that calls for further investigation on both epidemiological aspects of AH, pathophysiology and treatment options.There is also a diverse range of hypotheses about the pathophysiological causes of AH. Previous studies have suggested vasodilation in the cerebral arteries or sinus barotrauma as a result of cabin pressure change in the airplane [, , , , , ]. These proposed mechanisms require further investigation to prove or falsify suggested theories. Besides, no specific treatment plan has been developed for AH, although several medications have shown beneficial effects, e.g. triptans []. Considering challenges and limitation of AH studies under real-time conditions, it might be an option to study this headache under controlled experimental conditions. This approach has also been used in studying other types of headaches []. An experimental model of AH has been developed recently [] that can help in further understanding of potential mechanisms underlying AH, or to examine AH under different circumstances, and identification of potential biological biomarkers. This model [] can also serve for testing treatment options for AH.To provide a better overview of existing literature on diagnosis, pathophysiology, and treatment of AH, this systematic review was performed. It was proposed that outcome of this review would highlight existing evidences, missing information, and stimulate further research in AH. Understanding AH pathogenesis and efficient targeting would ultimately help millions of passengers who suffer from this condition.Based on this systematic review, it is now evident that further studies are required to investigate AH systematically. Investigations may clarify unknown aspects of AH diagnosis that can be taken in updates of AH classification by IHS. Future experimental studies are also essential to further investigate proposed mechanisms underlying AH; barotrauma and vasodilation in the cerebral arteries, and also to investigate the biological effects of most used medications, ibuprofen, naproxen and triptans for alleviating of AH-attacks. These studies would advance our understanding of AH pathogenesis and value of treatment options that are not yet established. This would subsequently help millions of passengers suffering from this condition."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0794-2", "title": "Excessive daytime sleepiness in secondary chronic headache from the general population", "authors": ["Espen Saxhaug Kristoffersen", "Knut Stavem", "Christofer Lundqvist", "Michael Bj\u00f8rn Russell"], "publication": "The Journal of Headache and Pain", "publication_date": "16 August 2017", "abstract": "Excessive daytime sleepiness (EDS, defined as Epworth sleepiness scale score\u00a0>\u00a010) is a common symptom, with a prevalence of 10\u201320% in the general population. It is associated with headache and other chronic pain disorders. However, little is known about the prevalence of EDS among people with secondary chronic headaches.", "full_text": "Headache and sleep complaints are prevalent in the general population and often coexist in the same subject []. Excessive daytime sleepiness (EDS) is associated with neurological disorders and pain [, ]. Only a few studies have investigated EDS in headache [\u2013], and the results are not uniform, possibly due to differences in methods and patient populations [\u2013]. We have previously reported on the prevalence of EDS in primary chronic headaches in the general population [].To our knowledge EDS has not been evaluated in people with secondary chronic headache. Thus, in the present study we investigated the prevalence of, and factors associated with EDS in participants from the general population with different secondary chronic headaches.In total 93 of the 113 eligible participants (82%) completed the ESS. Respondents and non-respondents to the ESS did not differ in age, gender composition or the distribution of headache diagnoses (data not shown).A total of 36 people had CPTH, 19 people had CEH and 40 people had HACRS. Co-occurrence of CPTH and CEH was found in 7 people, while one person had co-occurrence of CPTH and HACRS. Six persons had other secondary chronic headaches, i.e. 3 post-craniotomy, 1 diving related, 1 pregnancy-related, and 1 post-meningitis.Seven of those with CPTH, seven of those with CEH and ten of those with HACRS reported EDS, respectively. Only one of those with other secondary chronic headache reported EDS.The people with CPTH and CEH were descriptive similar (gender, co-occurrence of migraine, medication overuse, ESS or EDS) and were merged for the purpose of statistical analyses. We included people with CPTH/CER and HACRS in the main analyses and excluded those seven persons with other secondary chronic headaches due to the low numbers and consequent statistical limitations.Applying the \n                        -test in non-adjusted analyses, no significant differences (data not shown) were found in those with and without EDS depending on socio-demographics, body mass index, smoking, alcohol, caffeine, other sleep disorders, anxiety/depression, comorbidity of other disorders or medication use for other conditions.In this large population-based study almost one out of four subjects with secondary chronic headache reported EDS. The main finding was that the prevalence of EDS did not differ between those with CPTH/CEH and HACRS. Furthermore, medication-overuse, or the composite propensity score (age, gender, headache frequency and co-occurring migraine) was not associated with EDS in this population.No previous study has investigated EDS for secondary chronic headache in the general population. The prevalence of EDS in men and women with secondary chronic headache corresponds to that for people with primary chronic headache in the general population (20.6% among women, 22.5% among men) and is comparable to data reported from the Norwegian general population (16.1% among women, 20.1% among men) [, ]. Here, we did not include a directly comparable headache-free control group and caution is therefore warranted in this comparison. Also, due to the limited sample size in the present study, the risk of type 2 errors must be considered.Little is known about the precise relationship between headache and sleep problems, when these occur concurrently. An association between EDS and different pain conditions has been reported [, ]. Pain may disturb sleep and give rise to EDS, but sleep loss and EDS may also contribute to pain. Some of these secondary headaches are poorly understood, thus, further research is warranted []. Studies suggest that CEH can be explained by local factors in the neck with dysfunction of the neck muscles and mechanical cervical spine pathology leading to limited cervical movements and projection of the pain []. Headaches attributed to head trauma and whiplash trauma have instead been suggested to represent an interplay between the physical injury, neuroinflammation, psychological disturbances and emotional stress of the accident [, ]. Finally, longstanding oedema of the nasal mucosa and rhinosinal inflammation result in chronic rhinosinusitis which may give chronic headache []. The present study reported that CPTH/CEH and HACRS had similar prevalence of EDS despite these different headache forms probably being caused by different pathophysiological mechanisms. Therefore, it may be the complex burden of pain, more than the specific condition that is associated with EDS. Furthermore, the prevalence was comparable to that of two other different headache entities; chronic migraine and chronic tension-type headache [].The population-based sample in the present study was large, and the high response rate should ensure that the sample was representative of the general population. Even though the sample size of secondary chronic headache may seem small, this is the largest sample of subjects with secondary chronic headache recruited from the general population.The diagnostic criteria of CEH and HACRS have been discussed for many years [, ]. When the study was conducted the more vague ICHD-II criteria for CEH were in use and HACRS was not recognized as a cause of chronic headache. Thus, to improve the diagnostic accuracy we used supplementary definitions. All patients diagnosed with CEH or HACRS in the present study fulfil the new ICHD-III\u03b2 criteria for these chronic headaches.Face-to-face interviews by headache experts, as in the present study, provide more valid headache diagnoses than questionnaire-based studies []. The ESS is a widely used, validated questionnaire for evaluating subjective daytime sleepiness, and the score is associated with clinically important outcomes, such as cognitive impairment, cardiovascular mortality, and injuries [, ].The 30\u201344\u00a0years age range in our study was chosen in order to ascertain a general population sample without much co-morbidity of non-headache disorders. Because EDS varies with age, our findings may not be generalizable to younger or older populations. The study lacked data on sleep quality and sleep duration that may be associated with sleepiness in headache [, , , ].The overall sample size limited the number of variables that could be analyzed as potential confounders, and this also lead us to dichotomize many variables for use in the analyses. Also, due to the small number in the present study, the risk of type 2 errors must be considered. Finally, the cross-sectional design in the present study does not permit any conclusions about causality.In conclusion, there was no difference in the prevalence of EDS between subgroups of different secondary chronic headache diagnoses."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0799-x", "title": "Altered insula\u2013default mode network connectivity in fibromyalgia: a resting-state magnetoencephalographic study", "authors": ["Fu-Jung Hsiao", "Shuu-Jiun Wang", "Yung-Yang Lin", "Jong-Ling Fuh", "Yu-Chieh Ko", "Pei-Ning Wang", "Wei-Ta Chen"], "publication": "The Journal of Headache and Pain", "publication_date": "23 August 2017", "abstract": "Fibromyalgia (FM) is a disabling chronic pain syndrome with unknown pathophysiology. Functional magnetic resonance imaging studies on FM have suggested altered brain connectivity between the insula and the default mode network (DMN). However, this connectivity change has not been characterized through direct neural signals for exploring the embedded spectrotemporal features and the pertinent clinical relevance.", "full_text": "Fibromyalgia (FM) is a common chronic pain disorder, with a prevalence of 2\u20134% in the general population []. FM is characterized by chronic, widespread pain along with various clinical symptoms that reflect a centralized pain state, including fatigue, insomnia, cognitive dysfunction, headache, and depression []. Because of its polysymptomatic nature, the prognosis of FM is highly distressful for the patients and cost intensive for the society [].The pathophysiology of FM remains unclear. Patients with FM are hypersensitive to painful and nonpainful stimuli and exhibit increased brain responses within the so-called \u201cpain network,\u201d including the insular cortex, anterior cingulate cortex (ACC), primary (S1) and secondary somatosensory cortices, and thalamus [\u2013]. Functional magnetic resonance imaging (fMRI) studies on FM have shown brain connectivity changes in this pain network, along with some intrinsic connectivity networks that exhibited synchronous activity in the resting state (i.e., resting-state networks), such as the default mode network (DMN) [, ], salience network [], and sensorimotor network []. Taken together, the chronic and persistent pain of FM appears to change a patient\u2019s brain responses during the processing of both externally driven information and internally generated thoughts.The DMN comprises a set of synchronous brain regions such as the medial prefrontal and posterior cingulate cortices that are active at rest and deactivated during task performance []. This resting-state network can modulate pain perception through autonomic and antinociceptive descending modulation networks []. A recent meta-analysis of voxel-based morphometry studies indicated that patients with FM exhibit gray matter atrophy in the left medial prefrontal and right dorsal posterior cingulate cortices, both of which are key regions within the DMN []. Moreover, the DMN is disrupted in the activation\u2013deactivation dynamics in the presence of chronic pain, suggesting that the DMN is a primary resting-state network affected by chronic pain []. Thus far, the mechanism underlying the interaction between the DMN and pain network brain regions in the context of chronic pain remains largely unknown []. Fallon et al. have used fMRI to investigate the blood-oxygen-level-dependent (BOLD) signal fluctuations in the DMN structures of patients with FM and demonstrated altered connectivity with various regions associated with pain, cognitive and emotional processing []. In other fMRI studies, Napadow and colleagues [] reported increased connectivity between the DMN and insula, a pivotal region of the pain network involved in multidimensional (sensory, affective, and cognitive) pain processing [\u2013]. Notably, the insula\u2013DMN connectivity was correlated with the individual level of spontaneous pain reported at the time of scanning, and it presented a corresponding decrease after the alleviation of pain following pregabalin treatment [] or acupuncture intervention []. This altered insula\u2013DMN connectivity could not be confirmed by another fMRI study on FM []; nevertheless, corroborating evidence of elevated coupling between the DMN and insula has been noted for other pain disorders, such as chronic back pain [], diabetic neuropathic pain [], and acute migraine headache [].Given its potential to encode clinical pain and serve as an objective measure of FM phenotypes, further characterization of the insula\u2013DMN connectivity is rucial. The DMN and insula are critically involved in pain perception and both structures may present correlated activities in a variety of tasks such as attention or self-recognition [, , ]. A previous resting-state fMRI study in FM also reported an association between individual ratings of pain sensitivity and the insula connectivity with midline regions of the DMN (posterior cingulate and medial prefrontal cortices) []. Thus, characterization of the insula-DMN connectivity may enhance our understanding towards the pathophysiology of FM.To date, most of the related fMRI studies in insula-DMN connectivity have measured very-low-frequency (<0.1\u00a0Hz) fluctuations of resting-state BOLD signals rather than directly recording neural oscillatory changes through a spectrotemporal analysis of a wide-frequency domain. However, chronic pain potentially changes the dynamic brain activities at specific frequency bands of >0.1\u00a0Hz [, ]. Some electroencephalography (EEG) studies on FM have shown spectral power changes at higher brain oscillation frequency bands (>1\u00a0Hz), especially within the theta range (4\u20138\u00a0Hz) [\u2013], but pertinent analysis for the connectivity change between specific brain regions remain inadequate, probably because of the constraint of EEG spatial resolution. Therefore, this study investigated the resting-state functional connectivity pattern between the DMN and insula across different frequency bands through magnetoencephalography (MEG), which enables the visualization of explicit neural oscillatory features, similar to traditional EEG but with finer spatial localization []. MEG has been used to characterize brain oscillatory changes in various chronic pain conditions, including chronic migraine [], phantom limb pain [] and complex regional pain syndrome []. In FM, a resting-state MEG study reported increased theta, beta and gamma oscillations in the prefrontal cortex []. We hypothesized that the insula\u2013DMN connectivity is altered by the chronic and persistent pain perception of FM, possibly reflecting the clinical phenotype of FM in a frequency-dependent manner.The main finding of this study is that patients with FM had decreased resting-state bilateral insula\u2013DMN connectivity at the theta band. When patients with FM and the controls were examined together, the insula-DMN theta connectivity was negatively correlated with tenderness. Moreover, in patients with FM, the insula\u2013DMN connectivity was also negatively correlated with tenderness at the beta band and with centralized pain-related symptoms (Symptom Severity Scale) and functional impairment (FIQR) at the delta band.Studies have reported the existence of intrinsic connectivity between the insula and DMN in healthy individuals [, ]. During pain processing, the insula has been proposed to serve as a switching core that relays sensory information into higher-order affective and cognitive modulation [\u2013], whereas the DMN has been linked to pain modulation through descending inhibitory pathways []. Thus, the present finding regarding disrupted insula\u2013DMN connectivity may implicate impaired pain modulation leading to the chronic pain of FM. Similarly, MRI studies on FM have reported that the DMN regions had decreased gray matter volume [, ] and functional connectivity with specific regions of the pain network [, ]. Moreover, a quantitative EEG study on FM demonstrated widespread hypocoherence in the frontal brain regions []. Overall, these overlapping brain changes may reflect the central sensitization mechanism underlying FM [].The present finding of disrupted insula\u2013DMN connectivity in FM appears to contrast with previous fMRI results demonstrating increased coupling of the BOLD signals between the insula and DMN [, ]. However, the discrepancy in the altered connectivity patterns could be explained by the fundamental methodological differences between fMRI and MEG [\u2013], the frequency-dependent oscillatory characteristics of the underlying neural network [, ], and the effects of autonomic regulation on BOLD responses [], which may be impaired in patients with FM [, ]. All of these apparently contradictory connectivity changes at different frequencies potentially characterize a common functional reorganization mechanism in FM. In agreement with this, several studies using different modalities to characterize the brain oscillatory change in the same disease have yielded contrasting patterns of connectivity change [, , ]. The present finding of altered insula\u2013DMN connectivity in FM is further supported by the similarity in the findings of bilateral insula-individual DMN areas (Additional file : Table S1 and Additional file : Table S2), absence of changes in V1\u2013DMN or S1\u2013DMN connectivity, and clinical relevance.Our results demonstrate that the insula\u2013DMN connectivity in FM was significantly decreased at the theta band. A recent review identified the theta oscillation as the main change that occurs in brain rhythm during chronic pain []. A quantitative EEG study that included patients with FM showed widespread hypocoherence in the frontal brain regions at low to middle frequencies, including the theta band []. In line, recent resting EEG and MEG studies in FM also showed altered theta oscillations in midline brain structure such as medial prefrontal cortex [, ]. Theta oscillation has been linked to working memory, attention, emotional arousal, and fear conditioning, all of which may be related to pain processing [, ]. Moreover, theta connectivity at the bilateral insula cortex has been reported to be correlated with pain perception. In a 64-channel EEG study using electrical stimulation at the threshold level, trials perceived as painful were characterized by a lower prestimulus theta connectivity, compared with trials rated as nonpainful []. We also found a negative correlation between tenderness and insular-DMN theta connectivity. Thus, the present findings of decreased theta connectivity between the insula and the DMN may reflect persistent pain encoding associated with the chronic pain state of FM.Despite its lack of correlation with clinical pain intensity, we also noted that the insula\u2013DMN connectivity was negatively correlated with tenderness at the beta band in patients with FM. In pain processing, the beta oscillation is associated with top-down attention modulation [, ] and the perceptual integration of sensory and contextual (cognitive, emotional, and motivational) information [, ]. Therefore, the increased tenderness in patients with FM may be justified by an inefficient attentional modulation or impaired recruitment of contextually appropriate brain networks, resulting in the widespread body pain phenotype. By contrast, the insula\u2013DMN connectivity at the delta frequency was negatively correlated with centralized pain-related symptoms (Symptom Severity scale) and functional impairment (FIQR) in patients with FM. The delta oscillation has been suggested to be the neuropathological hallmark of brain rhythm in mood disorders [, ], cognitive impairment [, ], pain attack [], and fatigue [, ]. Therefore, our present findings highlight the complex role of neural synchrony between the insula and the DMN in pain, emotional, and cognitive processing, as shown previously [\u2013]. Future studies should elucidate whether the delta synchrony of the insula\u2013DMN network serves as the common neural basis for the polysymptomatic nosology and multidomain functional disability in patients with FM.This study has several limitations. First, the anterior and posterior insula have been reported to be functionally segregated regions with different connectivity [\u2013]; however, we could not differentiate these subregions because of the constraint of the Colin27 anatomical labeling template. Nevertheless, the altered insula\u2013DMN connectivity in FM has been shown to involve both the anterior and posterior insula []. Second, a prior study proposed that no one-to-one correspondence occurs between any frequency component of brain activity and pain []. Notably, brain activity at different frequencies provides different and complementary information regarding pain, and the relationship between pain and brain activity may be variable and context dependent []. Thus, the present findings should be interpreted with caution when being generalized to other clinical contexts or pain disorders. Limited by its cross-sectional design, this study could not clarify the causal relationship of the connectivity changes in FM. Although we did not observe a clinical correlation between connectivity measures and disease duration (favoring the present findings as the consequences of FM), additional confirmatory longitudinal studies are warranted. Finally, the present finding of connectivity change may be problematic if such change is confounded by the ongoing pain perceived in patients with FM, despite a lack of correlation between the connectivity changes and the clinical pain intensity. Additional longitudinal studies controlling the intrasubject pain variation may therefore help re-confirm the \u201ctrue\u201d resting-state connectivity change.The insula\u2013DMN connectivity is associated with frequency-specific functional reorganization in patients with FM. The clinical relevance of this connectivity change may provide an objective measure of FM phenotypes and related functional disability. However, the confirmation of its causal relationship and potential as a neurological signature for FM requires further research."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0797-z", "title": "Volume expansion of periaqueductal gray in episodic migraine: a pilot MRI structural imaging study", "authors": ["Zhiye Chen", "Xiaoyan Chen", "Mengqi Liu", "Shuangfeng Liu", "Lin Ma", "Shengyuan Yu"], "publication": "The Journal of Headache and Pain", "publication_date": "15 August 2017", "abstract": "The periaqueductal gray (PAG) dysfunction was recognized in migraine, and the nonspecific PAG lesions were also observed in episodic migraine (EM) recently. However, the PAG volume change was not totally detected in EM up to now. Herein, the aim of this study was to investigate altered PAG volume in EM patients based on high resolution brain structural image.", "full_text": "The Migraine is a common type of primary headaches with a reported prevalence of approximately 5.7% in men and 17.0% in women [], and affect 12% of the population worldwide []. The neuromechanism of migraine has been the key focus of research []. Of all the target \u201cgenerator\u201d of migraine attacks, the PAG region has been the key observed brain structure.Periaqueductal gray (PAG) was a center with powerful descending antinociceptive neuronal network in midbrain [, ], and PAG activation was modulated by expectation of pain [] and placebo analgesia []. PAG could exert a dual control, including inhibition and facilitation, on nociceptive transmission in the dorsal horn and trigeminal nucleus [] by descending PAG-RVM (rostral ventromedial medulla) pathway contributing to central sensitization and development of secondary hyperalgesia [, ]. A previous study [] confirmed PAG dysfunction in migraine, and functional MRI studies demonstrated that the PAG dysfunction was associated with increased iron deposition, which may play a role in the genesis or pathophysiology of MOH [, , ] The PAG dysfunction changes might explain the neuromechanism of migraine, however, the PAG structure change was not elucidated completely.PAG abnormalities can be detected in migraine patients with brain T2-visible lesion using voxel-based morphometry (VBM), which mainly identified increased PAG density in migraine brain []. The altered PAG density indicated the volume change without modulation in VBM, which did not represent the true volume change []. Therefore, the true PAG volume abnormalities were not investigated in episodic migraine.Although PAG was a very small region in the midbrain, and the PAG volume changes had indirectly been assessed by VBM [, , ], which represent the volume changes in statistical level while not the true volume changes. Therefore, the PAG volume measurement was important for the accurate structural assessment of PAG. In our previous study, the PAG volume measurement using automated PAG segment had been applied to the medication-overuse patients [].In the current study, we hypothesize that migraine patients without T2-visible lesions may present PAG volume changes. To address this hypothesis, we prospectively obtained conventional T2WI and high resolution structural images from 18 episodic migraine (EM) patients, 16 chronic migraine (CM) patients and 18 age- and sex-matched normal controls without T2-visible lesions on the brain to calculate and analyze PAG volume change using an automated three dimensional volume mapping measurement.In this study, the individual PAG was created by applying the deformation field [] to the PAG template, and it could be used to compute the true PAG volume. Figure  provided a good profile for the PAG template and individual PAG segment, which was completely consistent with the actual PAG location and size.This study demonstrated that EM had the largest PAG volume, and it was significantly larger than that of NC, which indicated that the PAG volume expansion may take part in the migraine attack. Previous studies demonstrated that PAG lesions may lead to migraine attack [\u2013], and functional MRI studies also demonstrated that the PAG network was disrupted in migraine [, ]. Therefore, it could be considered that PAG structural change might be the cause of migraine, and PAG volume expansion might be the result of disrupted PAG network in migraine.Although there was no significant difference on PAG volume within NC-CM and EM-CM groups, the PAG volume of CM showed a slightly reduced tendency compared with EM and slightly increased tendency compared with NC. Therefore, it may speculate that PAG volume reducement may exist in the transformation of EM to CM, and the neuromechanism should be further investigated.Partial correlation demonstrated that only PAG volume in CM was negatively related to the VAS score, which indicated the PAG volume changes may be associated with VAS score. In EM patients, PAG volume showed no any correlation with the clinical variable, and this point indicated PAG volume expansion may be the direct impairment in EM and may be associated with pathological substrates []. The previous study presented that T2-visible load, age, and disease duration may be associated with gray matter volume by VBM methods. Therefore, this study provided a new viewpoint that PAG volume expansion in the migraine patients without T2-visible may be the specific imaging appearance in the midbrain, and it may be an independent brain changes in migraine, which was not infected by T2-load and other clinical factors. Herein, we could speculate that the gray matter changes in migraine may be classified as two patterns: PAG volume expansion and extra-PAG volume reducement based on the current study and previous studies [, , ].ROC curve demonstrated that PAG volume expansion may provide a fair level for the diagnosis of EM from NC (AUC\u00a0=\u00a00.731), and it was not enough to distinguish CM from NC and CM from EM because of lower AUC. Although PAG volume had a fair diagnostic efficacy for EM from NC, and it presented a slightly higher specificity (0.889) and a slightly lower sensitivity (0.556). Based on the Fig. , the overlap was observed in between NC and EM, which may decreased the sensitivity for PAG volume as a biomarker. However, it was reasonable to believe that the PAG volume expansion may be inclined to the diagnosis of EM.Although PAG is a very small structural in the midbrain, this study provided an automated PAG volume measurement methods, and which could be routinely used for the PAG volume measurement in clinical practice. PAG volume expansion could not only be considered as a potential diagnostic imaging biomarker for EM, but also might be considered as a treatment response prognosis for EM just as PAG volume reducement associated with treatment response in medication-overuse headache []. The main limit of this study was that the sample of this study was relative small, and it would be necessary to increase the sample size in the future study.In conclusion, PAG volume expansion may directly underlie the impairment evidence on the brain in EM, and could be considered as the imaging biomarker for diagnose and evaluation for the migraine."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-016-0034-x", "title": "Radar interferometry based settlement monitoring in tunnelling: Visualisation and accuracy analyses", "authors": ["Steffen Schindler", "Felix Hegemann", "Christian Koch", "Markus K\u00f6nig", "Peter Mark"], "publication": "Visualization in Engineering", "publication_date": "23 March 2016", "abstract": "The accurate, efficient and economical monitoring of settlements caused by tunnel boring machines, especially in regions of particular interest such as critical inner city areas, has become an important aspect of the tunnelling operation. Besides conventional terrestrial based methods to capture settlements, satellite based techniques that can accurately determine displacements remotely, are increasingly being used to augment standard terrestrial measurements. However, not much attention has been paid to analyse the accuracy of satellite based measurement data. In addition, there is also a lack of studies on how to visualise the resulting huge amount of data in the context of both the tunnel advancement and the existing building infrastructure.", "full_text": "The increase in traffic in areas of high population density (conurbation) has resulted in the erection of new underground traffic facilities as a means to alleviate potential congestion problems (ITA Commitee on Underground Space (ITACUS): www.itacus.ita-aites.org ). However, during the construction of such facilities using TBMs, the risk of damage caused to buildings and other structures rises with the progressive reduction in tunnel covering. Therefore, the methods and techniques currently used in the tunnelling process must be substantially improved to counteract these possible negative effects. An important task in research and practice is therefore, on the one hand, to minimize the resulting settlements by improving machine technology and, on the other hand, to control the magnitude of the settlements and their consequences by an area-wide, exact monitoring. Of the latter point, several very precise terrestrial measurement methods representing the state-of-the-art are available, nevertheless, these procedures are time and material consuming. Additionally, to implement these procedures it is necessary to install and operate instruments on-site in or at buildings over a long time period (Kavvadas ; Van der Poel et al. ).For some years now, technological progress in satellite based remote sensing using radar waves has made it possible to capture subtle displacements on the earth\u2019s surface with millimetre accuracy (Bamler et al. ; Arangio et al. ; Giannico & Ferretti ; Herrmann ). Thus, using this technology, expensive and labour-intensive on-site installations are not necessary. Within the framework of the collaborative research centre SFB 837 \u201cInteraction modelling in mechanized tunnelling,\u201d financially supported by the German Research Foundation (DFG), the aim is to implement and to scientifically evaluate an independent settlement-monitoring process at the construction site of the underground railway \u201cWehrhahn-Linie\u201d in D\u00fcsseldorf. For this purpose, the Institute of Concrete Structures at the Ruhr-Universit\u00e4t Bochum as Principal Investigator (PI) cooperates with the German Aerospace Centre (DLR), the state capital city of D\u00fcsseldorf and the TU Braunschweig (Mark et al. ).The focus is mainly on the question of accuracy, verified by several thousand comparative calculations with terrestrial measurements. In order to analyse possible resulting deviations from comparative calculations, a visualisation method will be used.This article presents the results of an accuracy analysis and methods that have been developed to analyse and visualise settlement data in a virtual reality (VR) environment. In particular, the next sections summarize the important basics of terrestrial and satellite based settlement-monitoring.The number of comparative figures is shown in Fig.\u00a0, distinguished by the evaluation method (Persistent Scatterer or Distributed Scatterer), and the specified spatial and temporal corridors. For the spatial corridor of 10\u00a0m, the evaluations were performed with just a time interval of 5\u00a0days. Under the conditions of the present project, the accuracy of the Distributed Scatterer is about half as good as the Persistent Scatterer.Due to the ongoing expansion of urban areas, solutions must be found to face the challenge of increasing mobility in an efficient and environment-friendly manner. One option is the extension of underground transportation systems using tunnels. However, during the construction of tunnel facilities using Tunnel Boring Machines (TBM), the risk of damage caused to buildings and other structures rises due to inevitable surface settlements. Research objectives are therefore, on the one hand, to minimize the resulting settlements by improving machine technology and, on the other hand, to control the magnitude of the settlements and their consequences by an area-wide, exact monitoring. Regarding the latter, facing the limitations of terrestrial measurement methods technological progress in satellite based remote sensing using radar waves has made it possible to capture subtle displacements on the earth\u2019s surface with millimetre accuracy (Bamler et al. ; Arangio et al. ; Giannico & Ferretti ; Herrmann ). However, there is still a lack of scientific analyses of the accuracy of satellite based settlement monitoring resulting in low acceptance of this technology in the daily application, for example in tunnelling.The contribution of this paper is twofold. First, this paper presents an accuracy analysis of remote satellite based settlement data in comparison with well-known terrestrial measurements within the frame of a mechanised tunnelling project. The evaluations presented in this paper demonstrate an accuracy of about \u00b11.5\u00a0mm in the measurement of building and ground deformation using the method of radar interferometry in urban areas. It is concluded that the high resolution of radar images of the TerraSAR-X satellite, in combination with conventional ground-based measurements, provides a very practical, economical and effective approach to tunnelling-induced settlement monitoring.The second contribution of this paper is a novel concept for visualising settlement data that incorporates (1) data correlating aspects within a tunnelling projects, e.g. visualising settlements within the context of buildings and the position of the boring machine, (2) different representations of discrete data points, e.g. point-oriented and area-oriented methods, (3) time-dependent settlement representation in correlation with operational boring machine data, e.g. 4D animation of correlated settlement values and thrust forces of the machine, (4) standard data formats, such as the Industry Foundation Classes (IFC), and (5) a 3D virtual reality environment implementation.Through the visualisation of settlements in accordance with the advancement of the TBM, a 3D tunnel product model can clearly show users possible interactions and influences, which would otherwise be difficult to detect. As presented in this paper, using a combination of visualisation and accuracy analysis, sources of error can be localized and interpreted, and the application of differential radar interferometry for settlement monitoring can be substantially improved."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-016-0036-8", "title": "VisArchive: a time and relevance based visual interface for searching, browsing, and exploring project archives", "authors": ["Keyun Hu", "Sheryl Staub-French", "Melanie Tory", "Madhav Prasad Nepal"], "publication": "Visualization in Engineering", "publication_date": "22 March 2016", "abstract": "Project archives are becoming increasingly large and complex. On construction projects in particular, the increasing amount of information and the increasing complexity of its structure make searching and exploring information in the project archive challenging and time-consuming.", "full_text": "Electronic data storage and database management systems offer simple and inexpensive ways to store digital information and documents of a construction project and enable project team members or software tools the ease and capacity to access project information remotely from anywhere. Construction documents such as meeting agendas, meeting minutes, schematic diagrams and computer-aided design (CAD) drawings, cost data, project schedule, design specifications contain rich information about different facets of a construction project. This information is typically archived in a shared digital storage repository. Unfortunately, even though this rich information can be chronicled and archived in a common repository accessible to all stakeholders or even integrated into a database management system for higher-level data processing, the increasing amount of information and the increasing complexity of its structure make searching and exploring information in the project archive challenging and time-consuming. In order for such repositories to be of practical use, construction professionals need to be able to rapidly retrieve and manipulate relevant information from the large and diverse collection of documents within project archives (Steed et al. ; Strotgen and Gertz ; Tory et al. ).Several recent studies, however, have shown that finding the right documents from a project archive in a timely manner can be very difficult. A \u2018project archive\u2019, in the context of this research is defined as a collection of files or information being generated or recorded historically throughout the project and stored in a common shared repository. For example, on several of the construction projects we have studied, design and construction teams used Autodesk Buzzsaw\u00ae (Buzzsaw ), a third-party application that is often used as a central repository for project information archiving, sharing and retrieval. Tory et al. () found that design and construction professionals using these existing tools had a difficult time searching and locating project files unless they were already familiar with the hierarchy structure and the name of the item they were searching for. Similarly, Demian and Fruchter () reported that construction professionals often asked colleagues for information rather than searching project archives. They argue that project archives may provide insufficient contextual information to help people understand the meaning of retrieved documents.Documents in construction project archives are typically organized and stored in hierarchical directories similar to other types of project archives. This allows individuals to access files by browsing directories and searching files using the meta-data (keywords, date, authors, etc.). Demian and Balatsoukas () argue that there are two unique issues in the design of information retrieval systems for the construction domain: (1) Engineers and construction professionals are unique in terms of their information needs and information-seeking behavior, and (2) construction project archives are organized differently than other types of document collections. Consequently, they conclude that there are unique design challenges for construction project archives and a need for research into the design of query-driven systems for this domain.Research into the design of such query-driven systems has shown the importance of revealing  about the results of a user\u2019s textual query rather than simply a relevance-ranked list of resulting documents (Demian and Balatsoukas ; Demian and Fruchter ). Specifically, users need to see resulting items in the  of their ancestors, descendants, and siblings within the file hierarchy, and understand the granularity of the item within that hierarchy (Demian and Balatsoukas ). Our work extends query-driven interface research by providing two new forms of contextual information to help users understand the set of documents resulting from their query: 1) , in the form of a timeline to show when each document was created and 2) , by showing exactly which of the entered keywords matched each document. In addition, we provide a new approach to reveal  for a particular item, so a user can explore which other users have accessed or modified a file. We introduce these ideas within a prototype query-driven interface, .\n                         employs visualization techniques to reveal the various types of contextual information, thereby off-loading cognitive effort onto the perceptual system (Card et al. ). It employs a combination of multi-scale and multidimensional timelines, color-coded stacked bar chart, additional supporting visual cues and filters to support searching and exploring historical project archives. The timeline-based interface integrates three interactive timelines as  visualizations (Steed et al. ; Pirolli et al. ). It implements and extends  (Ahlberg et al. ) and  principles (Ahlberg and Shneiderman ). The feasibility of using these visual design principles is tested in searching construction file archives of an educational building project. We also apply our tool to a software defects tracking system in the open-source software development domain to demonstrate its ability to search and visualize larger amounts of unstructured data.This section describes the unique challenges of working with construction project archives and the specific user needs for visualizing archived construction project information. The visualization design espoused in this section is primarily motivated by a common problem encountered in the construction domain. However, the concepts could also be applied to other domains that have non-spatial, metadata-based and time-oriented project data, such as source files of a software project.Construction projects generate voluminous data sets with significant heterogeneity of data files (structured, semi-structured and unstructured) (Russell et al. ) and types (Knowledge and Information Management ; Rezgui ). Electronic documents for a construction project are archived and stored over time in a central repository which we refer to as a \u2018project archive\u2019. The files can be moved, modified and accessed by different individuals from diverse backgrounds and from different organizations. Design and construction practitioners frequently search for relevant project information within the project archives on a regular basis as part of their intra- or inter- organizational decision making processes. As such, the ability to search, browse and explore project archives more easily and effectively is critical for the success of a construction project. Specifically, construction experts need an effective and efficient way to find and classify information (Caldas et al. ) and, more importantly, to explore relevant information in the project archive (Demian and Balatsoukas ).Previous research (Demian and Balatsoukas ) and our own studies of construction projects (Tory . )) have demonstrated that information seeking and retrieval from large, shared construction project archives is a very difficult and time-consuming process. In particular, these studies found that it is difficult for project members to search for and locate relevant documents, and that there is a lack of support to browse and explore search results in construction project archives. Practitioners can spend significant amounts of time searching for documents and in many cases, fail to find what they are looking for. For example, in one meeting we observed, the mechanical consultant spent over 10\u00a0min searching for images of the water filtration systems on his laptop, which significantly interrupted workflow in the meeting. In another meeting, the project team was unable to find the sustainability goals for the project and ended up spending significant amounts of time trying to identify the goals from memory and writing them out by hand on a white board.\n                         represents this contextual information using a combination of usable, visual, and interactive components to better support searching, browsing and exploring project archives. While the design of the current prototype is based on the requirements gathered from the construction domain, we believe that most of these problems and requirements are common to other domains as well.As a visualization-based interface,  utilizes and extends various visualization design ideas. In this section, we summarize existing design contributions related to \u2019s three context-specific design features: timeline based visualizations; visual indications of search relevance and matched keywords; and visual representations of user activity in shared repositories.This section describes the development of, , a visualization tool that implements a combination of standard visualizations and interaction techniques to solve the specific problem of searching project archives. We first provide a detailed description of the visualizations implemented, and then describe the relevance algorithm and the implementation details.In order to examine the feasibility of using  for searching, browsing and exploring information in project archives of different domains, two case studies were conducted. The case studies involved (1) a construction project archive, and (2) a software project archive (software defect tracking). Specifically, the design principles implemented in  were examined in relation to the three context-specific design features: (1) the temporal context shows timeline based visualizations; (2) the search-relevance context shows visual indications of search relevance and matched keywords; and (3) the usage context shows visual representations of user activity in shared repositories. The evaluation focused on the feasibility of the prototype to resolve complex use scenarios, rather than simple search using known file names or IDs.The interface of  was revised and modified based on these case studies, but the core features of the tools described in Section \u201c\u201d remained stable. Since a file contains more information than users often need, the interface for the construction project case study was designed to include a description viewer that allows users to view the details (e.g. file description or file path) when they click on a file from the information browser. The information browser for the software project case study was modified to display the summary for each software defect. Users can identify a software defect item by viewing its summary.This research extends query-driven interface research by providing three context-specific design features: timeline based visualizations; visual indications of search relevance and matched keywords; and visual representations of user activity in shared repositories. These design features were implemented in an interactive visualization tool called  that integrates multiple commonly used visualization and user interaction techniques to facilitate searching, browsing, and exploring information in historical project archives.  visualizes relevance-ranked search results with color-coded stacked bar charts in project timelines and uses additional supporting visual cues to distinguish search results based on search keywords. Two case studies from the construction and software project domains were used to demonstrate its applicability, usefulness and generality.Currently,  allows users to view the access history of a single item (e.g. a file or a defect record) in the project archive. It might be useful for users to also explore the access history of multiple items in the archives. Different visual representations might be used to achieve this goal in future research (e.g. multiple timelines could be used to represent the access history of different files in one display, or they could be aggregated into one timeline and use color-scale to distinguish different items). The interactive timeline visualization could also be more flexible to allow for different time frames. The current implementation represents each bar of the timeline in terms of the number of items created in 1\u00a0day. However, if the project archive covers a long period of time (e.g. the lower timeline of the software defect archive, visualizing around 1000 records over 2\u00a0years), the bar chart will be compacted, and the user will have difficulty clearly seeing the color-coded visualization in the lower timeline. Furthermore, the relevance-ranking algorithm for generating search results is based on keywords, which may limit the quality and reliability of the search results in other domains.The current prototype used manually extracted meta-data for demonstration purposes. However, key design ideas of  should be able to seamlessly integrate with existing archive management systems that provide well-designed content management and text search capabilities. For example, the defect tracking system Bugzilla allows users to create and edit defects with searchable content and meta-data such as related keywords. Since all the content and meta-data have been stored in database when the defect was created, they became searchable by the system.  could be embedded to the system in place of the conventional text based search results to provide better visualization and interaction capabilities. However, in a file or document based archive such as the construction archive, searching can be more challenging. To overcome this, searchable data such as textual content or meta-data needs to be extracted from the file or inserted by users when creating them (this is especially important if the file is not text based, such as image files). Well-designed archive management systems should be able to extract the text content automatically from the text-based files and make the keywords searchable in the system. The quality of the search results will depend on the searchable meta-data and the user\u2019s analytical ability.While the visual cues for both the historical context and the matching-level searches should make search efficient for users, the cues will only be successful if the users notice and understand them. To this end, more research is needed to confirm whether users can indeed intuitively understand the meaning of visual indicators implemented in . In addition, user studies are also needed to examine the effectiveness, usability and user experience of  in various domains. We believe that the design principles implemented in  can be applied to different domains as long as the information items in the archive have temporal information, such as creation date, and the relevant meta-data.  simply requires the access information (e.g. creation date, access date, and modification date) and searchable meta-data (e.g. summary, description, keywords, tags) to function. For example, research papers in the IEEE online archives (IEEE ) are associated with a publication date and meta-data for users to search. Instead of presenting the search results in a conventional list of papers,  could visualize a timeline-based overview of the published papers. Advanced filters and an information browser could be customized and modified based on users\u2019 needs in these different domains."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-015-0032-4", "title": "Optimization of a hybrid tower for onshore wind turbines by Building Information Modeling and prefabrication techniques", "authors": ["Laura Alvarez-Anton", "Manuel Koob", "Joaquin Diaz", "Jens Minnert"], "publication": "Visualization in Engineering", "publication_date": "8 January 2016", "abstract": "Nowadays wind energy is becoming increasingly significant in the planning, development and growth of new electricity supply systems. Special attention has been given to land-based turbines for ensuring the efficient economical operation of massive hubs rising 100m above the ground, based on the idea that the bigger the turbine, the more complicated are the transportation and assembly processes.", "full_text": "Shortly after the nuclear disaster in Fukushima in 2011, the German Federal Government decided to phase out all nuclear power plants and now plans to achieve this by 2022. The aim is to shut down the plants gradually. For this reason, Germany puts special focus on the development of renewable energies like wind power, solar energy, hydroelectricity, biomass energy, etc. Of these renewable forms of energy, wind power has the greatest potential for expansion. Countries like China, the U.S., Germany, Spain and India have a worldwide share of 72\u00a0% of wind power production. Consequently, they are the most important markets. In relation to the other top 20 countries which have installed wind power systems, Germany is in second place in terms of surface area (right behind Denmark) with 99\u00a0kW/km. The first non-European country is China (ranked 16) with 10\u00a0kW/km, followed by Japan and the U.S., each with 7\u00a0kW/km (Fraunhofer IWES, ). In 2013, 24.7\u00a0% of Germany\u2019s gross power consumption came from renewable energies. Wind power had the largest share with 33\u00a0% onshore and 1\u00a0% offshore wind turbines. Thus onshore wind turbines made the largest contribution to the overall power yield.It is well known that the bigger the wind turbine is, the more complicated are the transportation and assembly processes. Moreover, another important concern of their on-site construction is the quality control and the efficiency of the construction processes. Therefore, the research problem definition will be focused on how to improve this situation, trying to combine a new design with the implementation of existing methodologies in the construction industry.The research problem aim of this paper consists in addressing the complex processes involved in the transportation and assembly of wind turbine towers. A new and more efficient and economical design supported by industrialized and computerized processes could be an appropriate solution.In Germany today special focus is being put on the development of renewable energies, especially wind power. The increase in the hub height of wind turbines means that tower structures have to be more massive. This, in turn, leads to higher transport and assembly costs. The present paper describes the design and construction of the hybrid tower.Hybrid towers are currently being developed as part of a research project at the Technische Hochschule Mittelhessen (University of Applied Sciences) in the State of Hesse in Germany together with Oberhessisches Spannbetonwerk GmbH. The mission statement driving the development of the new hybrid towers for onshore wind turbines is associated with optimizing the performance of the supporting structure. The new tower concept reduces the amount of material used, and thus the weight of the tower itself decreases remarkably. Moreover, hybrid towers are designed to cut transport and assembly costs dramatically.Due to the high dynamic loads to which it is exposed, a wind turbine only has a lifetime of some 20\u00a0years. The traditional wind turbine is set for reconstruction and there will be widespread repowering. All this means that new and more efficient wind turbines will replace old wind turbines at the same location. Planning and cost estimations with respect to deconstruction and other tasks during the life cycle of a wind turbine will have to be dealt with. This is why good cooperation among stakeholders is so necessary. Such cooperation is facilitated by Building Information Modelling methodology.However, this is not the only advantage of BIM. The current research aims at implementing BIM methodology from the early design phases, so that the model becomes the centralized source of information. It remains to be said that BIM can enhance prefabrication processes and optimize the performance of a hybrid tower during its complete life cycle."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-016-0033-y", "title": "The mathematics of spatial transparency and mystery: using syntactical data to visualise and analyse the properties of the Yuyuan Garden", "authors": ["Rongrong Yu", "Ning Gu", "Michael Ostwald"], "publication": "Visualization in Engineering", "publication_date": "16 February 2016", "abstract": "One school of thought argues that the Traditional Chinese Private Garden's (TCPG) primary spatial property is that it features both freestanding buildings in space, and spaces freely positions in the landscape, creating a great sense of phenomenal transparency. In contrast, a more traditional interpretation of the TCPG stresses that its primary purpose is to evoke mystery, or provide places of isolation or for retreat.", "full_text": "Developed primarily in the sixteenth and seventeenth centuries, traditional Chinese private gardens (TCPG) are today recognised as a special type of landscaped space with a particular set of characteristic properties. In particular, a TCPG is distinguished by its rich spatial arrangement and its associated aesthetic properties, leading to being regarded as having unique experiential qualities (Peng ; Tong ). But what exactly these characteristics of the TCPG are, has been a subject of on-going debate. For example, some researchers argue that the primary feature of a TCPG is that it creates a space of mystery, evoking a sense of longing and providing opportunities for isolation or retreat (Zou ; Conan ; Han ). In contrast, other scholars have noted that the TCPG often possesses distinctly modern features, including open courtyard gardens and freestanding buildings, which emphasise the phenomenal transparency of its planning (Li ). But which of these interpretations are reasonable or appropriate?In this paper, we use computational and mathematical methods to examine the visual and permeable properties of one of the most famous TCPGs, the sixteenth century Yuyuan Garden (also known as the \u2018Yu Garden\u2019) in Shanghai. The purpose of this research is to compare the relative role played in its planning by spatial properties associated with transparency and mystery. The primary methods used for this analysis are two Space Syntax techniques which, like all such syntactical methods in architectural research, are supported by graph mathematics (Hillier and Hanson ; Hillier and Kali ; Ostwald ; Bafna ). However, such is the complexity of the data produced through this comparative analysis that data visualisation plays an important role in interpreting the results, and then understanding them in the context of the Yuyuan Garden.This paper commences with a brief overview to introducing the method that is used for the analysis of transparency and mystery in the Yuyuan Garden. At the conclusion to this section a series of hypotheses are framed about the planar spatial properties that are expected to be found in the Yuyuan Garden, how these will be tested and what mathematical indicators will be used to determine the relative validity of each hypothesis. Thereafter, the holistic results for the Yuyuan Garden, are presented and discussed, using a series of different data visualisation heat-maps, to support their interpretation. Finally, the mathematical properties of several distinct locations in the garden are compared to assess the three hypotheses.Two of the more intangible properties of architectural space and form are  and . Since the 1980s, a large body of research has been produced which has demonstrated how measures can be derived from an architectural plan to determine levels of efficient movement or surveillance, social control or integration, or predict pedestrian movement patterns, or even relative crime rates and property values (Bhatia et al. ; Hillier ; Lynch ; Ellard ). For all of these spatial correlates, robust measures, algorithmic models and visualisation techniques are available. However, for understanding spatial transparency and mystery, the available models mostly provide only indirect indicators of the essence of these properties. For example, it is argued that perceptions of transparency are associated with both visual and permeable properties, respectively, the ability to see through and to move through space (Rowe and Slutzky ; Vaughan and Ostwald ). Perceptions of mystery can be associated with both the lack of information available about a place (that is, a limited capacity to see or move) and the lack of intelligibility of this information (its complexity due to lack of order) (Hillier ; Ostwald and Dawes ). Thus, mathematical and computational models of these two spatial properties do exist in syntactical research, but they have not been subjected to the same level of optimisation as several other characteristics of architectural space and form, and they have rarely been used to analyse spaces that are renowned for possessing these properties. One of the reasons for this is that transparency and mystery are, almost by their very nature, more diffuse properties. Yet, they are regularly identified by designers and scholars alike when they describe the most important characteristics of environments (Appleton ; Hildebrand ). A case in point, in this context, is the TCPG which is celebrated for these properties. The following section describes the Space Syntax method, which is responsible for developing some of the few mathematical tools that are currently available for examining spatial properties associated with mystery and transparency.This paper presents a computational visualisation and analysis method based on the use of Space Syntax techniques for examining vision and movement potential in a complex garden environment. As demonstrated in the example of the Yuyuan garden, the method is effective for analysing and visualising spatial properties beyond the surface level of forms and shapes. Through the application of this method we have been able to provide a new insight into two of the most famous, but poorly understood perceptual properties of TCPGs; transparency and mystery.This paper has several practical and methodological limitations that are important to take into account. First, the paper is focussed only on two-dimensional representations of space, effectively considering only the plans of the Yuyuan Garden, not its three-dimensional form. However, the analysis does take into account contours when calculating visual accessibility. Thus, any artificial hills in the garden which are over 1.7\u00a0m in height do restrict vision. Second, the analysis doesn\u2019t differentiate between some properties of the gardens. For example, low-lying plants and small garden walls are treated in the same way, although in reality they are different things. In practical terms though, both of these features impede movement, but not vision; we can see over them, but moving through them is more difficult. Third, in both a cognitive and a phenomenological sense, the experience of spatial mystery and transparency is far more complex and contingent than the method adopted in this paper might suggest. The techniques used in the present paper provide a statistical and geometrically mapped generalisation of spatial and visual properties but they do not directly take into account individual spatial preferences of behaviours. Nevertheless, research cited in this paper does map these mathematical properties and results directly to human attitudes and activities, so the method adopted for this research is not without appropriate foundations in human perceptual and behaviour studies, although this aspect of the research is not developed in detail in the paper.The results of this research show that for the Yuyuan Garden, mystery is potentially more a product of structural permeability than visual accessibility. This confirms the first hypothesis. In practical terms the result means that, for example, when navigating in the Yuyuan Garden, people will often find that spaces or objects which they can see, are much more difficult to reach. Furthermore, this quality of spatial mystery is compounded and reinforced by the complexity of the garden space. This confirms the second hypothesis.The third hypothesis suggests that the sense of transparency in the Yuyuan Garden is a by-product of the visual \u2018pull\u2019 and \u2018directionality\u2019 experienced in major spaces. This result is also broadly supported by the data, but the relationship between area and transparency requires a more extensive study to produce a definitive outcome. The limited data used here suggests that there is a relatively consistent level of phenomenal transparency, which may be either heightened or undermined, by a less consistent level of literal transparency. Thus, for example, a space may be relatively large, and a person may be able to see a long way through it, but they are not always drawn to move through that space in a consistent, or even practical way.Ultimately, this paper uses quantitative methods to examine a previously qualitative subject of debate focused on seemingly intangible characteristics. The primary contribution of this research is to enhance our understanding of the spatial features of the TCPG, as represented in the Yuyuan Garden case study. The second contribution is to demonstrate that syntactical analytical methods are appropriate for examining the spatial features in complex landscape designs. The third is to emphasise the usefulness of data visualisation techniques for intuitively interpreting complex results. Future research arising from this paper will enlarge the sample size of the study to increase the generalizability of the results and include other spatial features in the TCPGs into the analysis."},
{"url": "https://thejournalofheadacheandpain.springeropen.com/articles/10.1186/s10194-017-0800-8", "title": "The K", "authors": ["Mohammad Al-Mahdi Al-Karagholi", "Jakob M\u00f8ller Hansen", "Johanne Severinsen", "Inger Jansen-Olesen", "Messoud Ashina"], "publication": "The Journal of Headache and Pain", "publication_date": "23 August 2017", "abstract": "To review the distribution and function of K", "full_text": "Adenosine 5\u2032-triphosphate-sensitive K (K) channel openers have been used in clinical trials for the treatment of hypertension and asthma. The most common side effect mentioned during treatment with K channel openers was headache (62, 64, 66\u201379) (Tables  and ). However, only little attention has been focused on the role of K channels in migraine pathophysiology.K channels were originally identified in cardiomyocytes [], but have also been found in several tissues, including pancreatic \u03b1- and \u00df-cells, smooth muscle, skeletal muscle and central neurons [, ]. The channels belong to the family of inwardly rectifying K channels that are inhibited at physiological intracellular levels ATP/ADP ratio. When intracellular ATP is reduced under conditions of metabolic challenges they open. K channels are critical in regulating insulin secretion, controlling vascular tone, and protecting cells against metabolic stress [, , ].Over the past three decades, some preclinical evidence has emerged indicating that K channels may play an important role in migraine pathophysiology. In particular, the vasodilation effect of K channels is relevant, since it is has been established that endogenous neurotransmitters that trigger migraine attacks are often associated with dilation of cranial arteries [].Here we review preclinical and clinical studies on K channels and discuss the K channel as a novel therapeutic target for migraine treatment.The K channel is a hetero-octameric complex that consists of four pore-forming K inwardly rectifying (Kir) subunits and four regulatory sulfonylurea receptor (SUR) subunits [].The Kir6.x subunit exists in two isoforms, Kir6.1 and Kir6.2. The SUR subunit belongs to the ATP-binding cassette (ABC) transporter family, regulated by sulfonylurea, with three isoforms, SUR1, SUR2A, and SUR2B [, ].A number of endogenous vasoactive signaling molecules have been implicated in migraine [], and K channels may interact with these molecules.In the late 80\u2019s there was a tremendous interest in developing novel K channel openers for hypertension, angina pectoris and asthma. Three pharmacological drugs were developed, pinacidil, nicorandil and levcromakalim. One of most common adverse events after treatment reported in these studies was headache [\u2013].The selective synthetic K channel openers levcromakalim and pinacidil have been shown to induce dilation in rat cranial arteries [, , ] and in isolated human cerebral arteries []. Moreover, the arterial dilation can be inhibited by synthetic K channel blockers like glibenclamide [, ] and PNU37883A [, ] (Fig. ). These findings suggest that high incidences of headache could be due to vasoactive effect of the K channel openers in pain-sensitive extra- and/or intracerebral arteries.Kchannels are expressed in migraine-related structures such as the cranial arteries, TG and TNC [\u2013, ]. K channels are also connected to a number of key molecules in migraine pathogenesis, particularly nitric oxide, CGRP, PACAP and PGI known to provoke migraine attacks [, \u2013]. Therefore, the K channels are interesting in migraine context.Human experimental models have demonstrated that the activation of the cAMP and cGMP pathways can trigger headache in healthy volunteers and migraine attacks in migraine sufferers [, , ]. The cAMP and cGMP signaling pathways are crucial in the activation of K channels, which result in the relaxation of smooth muscle [\u2013]. Furthermore, synthetic K channel openers like levcromakalim and pinacidil trigger headache in non-migraine patients [\u2013]. Although a detailed description of levcromakalim- and pinacidil-induced headache and accompanying symptoms are lacking, these data support a role of K channels in migraine headache. Because K channel openers were tested for other indications, there are no available data on the potential migraine-inducing effects of pinacidil and levcromakalim in migraine patients. It is conceivable that both headache and migraine are underreported as adverse events, as was found for the phosphodiesterase inhibitors, cilostazol and sildenafil [, ].In addition to the vasoactive effects, the K channels might also tap into other parts of the migraine cascade. For a number of patients, migraine attacks are associated with transient focal neurological symptoms called the aura [], possibly caused by cortical spread depression (CSD) []. During CSD K conductance is increased, and CSD may be inhibited by Kir antagonist []. The fact that K channels open under cellular stress, as seen during long lasting depolarizations, could provide a link between K channels, CSD and migraine aura.With regard to the migraine pain, it is worth noting that K channels are also found in peripheral nociceptive fibers [] and activation of these channels play a crucial role in anti-nociception at both spinal and supra-spinal levels [, ]. The exact role of these findings in the headache induced by K channel openers is unknown.If K channel openers are in fact able to trigger migraine, the next step to consider is whether K channel antagonists can relieve migraine. K blockers for the treatment of migraine should be selective for the Kir6.1/SUR2B subtype because of its dominant presence in vascular tissue (Table ). The necessity of a subtype specific blocker is unavoidable because of occurrence of different subtypes in different tissues. Glibenclamide cannot be used due to its high affinity to the Kir6.2/SUR1 subtype of K channels present in the pancreas with hypoglycemia as a side effect []. PNU-37883A is a Kir6.1 selective K channel blocker that was originally developed as a diuretic drug [, ]. The drug was not approved to human studies because of its cardiac depressant activity in animal studies []. This precludes further clinical development of PNU-37883A due to possible serious adverse events but may not exclude further investigations in other blockers against Kir6.1 subunit because it is not clear if all blockers against Kir6.1 subunit have non-favorable effects. These findings indicate that the SUR2B subunit and the Kir6.1 subunit should be a potential target for the treatment of migraine, but proof of concept studies are needed to examine this hypothesis.Emerging evidence suggests that K channels could be involved in the pathophysiology of migraine. K channels exist in structures which are believed to be linked to the pathophysiology of migraine, including cerebral and meningeal arteries and the trigeminal system [\u2013]. It is established that the cAMP signaling pathway and possibly cGMP signaling pathway are involved in the activation of K channels [\u2013]. This is interesting in migraine contexts, as the two signaling pathways are likely to be crucial in the development of a migraine attack.We suggest that the presented clinical and theoretical evidence support further studies of K channel openers in migraine context. Future human studies will help clarify the role of K channels in the pathophysiology of migraine."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-015-0031-5", "title": "Low-cost virtual reality environment for engineering and construction", "authors": ["Thomas Hilfert", "Markus K\u00f6nig"], "publication": "Visualization in Engineering", "publication_date": "7 January 2016", "abstract": "Presenting significant building or engineering 3D-models is a crucial part of the planning, construction and maintenance phases in terms of collaboration and understanding. Especially in complex or large-scale models, immersion is one of the major key factors for being able to intuitively perceive all aspects of the scene. A fully immersive system needs to give the user a large field-of-view with reduced latency for lifelike impression. Technologies such as VRwalls and shutter glasses can deliver high refresh rates, yet fail to give a large field-of-view. Head-mounted-devices for virtual reality fill this gap. Head tracking mechanisms translate movements of the user\u2019s head into virtual camera movements and enable a natural way of examining models. Unlike a stereoscopic representation with projectors, point-of-view tracking can be achieved separately for each individual user. Hardware costs for such systems were very high in the past, but have dropped due to virtual reality systems now gaining traction in the mainstream gaming community.", "full_text": "Head mounted devices (HMD) for Virtual Reality (VR) are currently experiencing a renaissance. In the past, these systems were only accessible for large companies at high costs (starting at several thousand Euros) and using specialized systems. Even then, the user experience was under par. Devices were lacking the refresh rate needed to smoothly translate head movements in the virtual world and had inadequate resolution and a low field-of-view (FOV) for realistic impressions. Sufficient refresh rates of displays and updates in the virtual world are key factors to consider when using VR, as the user will be otherwise likely prone to motion sickness on longer exposure.In contrast to traditional surface based stereoscopic methods, such as 3D shutter glass monitors or beamers, HMDs enable individual rendering of the user\u2019s perspective. Movements of the head and / or body are being translated into movements of the virtual avatar. The camera location and rotation will then be modified to the matching position. These techniques are similar to the field of Augmented Reality (AR), omitting the blending with the real world. AR will become more important in the future, but lacks the fully free designed environment without the boundaries of reality VR has.While a designer or engineer has learned to use his imagination during the design phase to visualize the final product, this is difficult for outsiders. Being able to present 3D models and scenes to a wide range of audience is therefore beneficial in engineering and construction. We chose to use a game engine as basis of our research, as they are trimmed to maximum performance at a high level of detail. Also, in contrast to using a pure graphics engine, sound playback, physics and networking abilities are already present. Having game developers targeting the engine for plugin development is also a plus, as ongoing feature extensions and bug fixes will be available.When creating 3D building models in construction, additional resources have to be assigned for the conversion into a realistic visual representation or the buildings have to be recreated from scratch for the different 3D formats for visualization, which are most likely incompatible among each other. If additional iterations of those models are created by the engineers, time consuming and costly effort has to be spent on adapting the newest changes into the VR scenes. Also, the visual representation of the entity in 3D space has to be defined after each import, so that realistic materials can be used in the visualization. Minimizing the complexity and automating this migration process into the VR presentations would be a great benefit for the construction industry.In this paper we present a solution for building a VR environment, consisting of different hardware and software components to achieve a deep level of immersion to users while minimizing the effort of creating 3D scenes for VR from building models. We show applications for the environment via three different, common use cases. Additionally, all of the components mentioned are cheap to buy, enabling even non-professionals access to such simulations. Every part can be used independently, but play well together on maximizing the immersion experience and interaction.In Sampaio and Martins () the application of VR to visualize the construction of a bridge using two different construction methods is described. The target of this implementation was to give students a deeper understanding about how bridges are built, as they can\u2019t have the same level of insight on a real construction site due to safety reasons. They target mainly the creation and deployment of 3D models over traditional teaching practice (verbal description / pictures / diagrams) and come to the conclusion that the interaction is one main benefit of using VR. However, they don\u2019t elaborate on proper input methods for natural interaction with the system and the high level of immersion a HMD would give.Grabowski and Jankowski () are testing VR training for coal miners using different HMD hardware setups in conjunction with joystick and VR glove input methods. They found out that the test subjects preferred high immersive VR, but the FOV was negligible. The reason may have been, the hardware used for the 110\u00b0 FOV is known to be prone to ghosting (artefacts from previously rendered frames still partly visible, causing motion sickness) and this may be diminishing the positive effects of having a high FOV. Nevertheless, the study showed that the use of a vision based system for detecting natural hand movements is better than wireless 3D joysticks and that the result of the training \u201c[\u2026] is maintained in the long term.\u201d (Grabowski and Jankowski, , p. 321).R\u00fcppel and Schatz () describe the creation of a serious game environment for evacuation simulation in case of a fire. They harness Building Information Modeling (BIM) data as a base for their building-modelling concept and describe the advantages of having material data included to simulate the structural damage. The VR-lab concept described uses ideally most human senses (visual, tactile, auditory, olfactory) and enables interaction with the scene. However, their proposal references expensive components for the environment, by means of VR-lab facilities. Using the low-cost equipment described in this paper may benefit creating an easier deployable and affordable setup with only minor drawbacks (missing olfactory or tactile senses), but with more immersive visual representations. Surround headsets and / or binaural recordings can replace the audio installation described in their paper.Merchant et al. () explore different publications on virtual reality-based learning with regard to the outcomes in education. During the assessment they found out there are many examples of VR being beneficial for learning and \u201cvirtual reality-based instruction is an effective means of enhance learning outcomes\u201d (Merchant et al., , p. 37). As stated, one reason for not having widespread VR in education is financial feasibility. Also, VR environments based on desktop 3D computers do not provide fully immersive experiences, but enhance the learners\u2019 engagement (Merchant et al., , p. 30). We conclude that having a low-cost VR environment at disposal, which works on a normal desktop level, may be favourable for the Architecture, Engineering and Construction (AEC) industry, as well as for education outside of this field. There may be some applications where regular serious games on a computer monitor may be better to use, as mostly everyone can use a mouse or keyboard to operate traditional user interfaces (e.g., entering numeric values). However, operations, such as pushing buttons or levers, may manifest more naturally in a students\u2019 memory if real world movements are usable and being tracked into VR.Roup\u00e9 et al. () support the aforementioned statement, in saying that \u201cbody movement can enhance navigation performance and experience\u201d (Roup\u00e9 et al., , p. 43), but the ability of tracking parts of the body usually needs expensive equipment. The authors describe a system using an Xbox Kinect sensor for navigating planned urban environments. By using body postures (leaning back and forth, turning the shoulders) participants are able to navigate through a VR-model of a city. They were able to perceive distances and scales inside the environment better when using body movements. As translating postures to VR navigation is a first step, an optimal solution would be to map walking movements directly with dedicated hardware, if possible.Edwards et al. () show the feasibility of using a game engine to include end-users in the BIM design process. They use an Autodesk Revit plugin for communicating with the Unity game engine and enable collaboration over a networked connection. They also extract BIM information via a local webserver started in Revit, but this is only a short-lived instance and is disabled, as soon as a dialog is closed. A central server for administrating projects and revisions would be beneficial to integrate the already existing design process. Furthermore, the implementation is missing the VR integration we aim to achieve.All studies show that there is demand for VR in many fields. It may have not been considered testing the actual deployment in some due to financial concerns of such a system. Providing a low-cost flexible environment which delivers safe testing and natural interactions is desirable. Especially in fields where hazards limit trial and error testing, VR can gain a foothold.The goal of our approach is to unify an environment for different use cases in engineering and construction. As BIM is being introduced throughout the construction industry, support for Industry Foundation Classes (IFC) is beneficial. The IFC enable data exchange between a wide range of software applications and design tools. IFC models consist of optional geometry information with attached metadata, such as materials used and product structure. By retaining this metadata information within the virtual environment (VE), interaction with the elements enables more profound experiences. If a user wants to check which material a certain wall is composed of, this is seamlessly possible without breaking immersion.A VE consists of a visual output element and one or multiple input elements to enable interaction with the system. Creating natural input methods are a hurdle at building such a system. They are either camera-based rigs with multiple cameras using visual detection algorithms, handheld devices or specialized depth sensors. Tracking the hands in relation to the virtual body position seems to be the most promising way to manipulate objects in the virtual world, as the learning curve for new users is modest and tracking reliability is not limited by hidden body parts due to an unfavourable camera position.Ideally, off-the-shelf equipment should be usable for the whole system, as this will lower the total cost and increase deployment options.For our development process, the Oculus Rift and Leap Motion are used as a base system to build upon to. UE4 is used as the central point for simulating VEs. Using the BIMServer as a central connection between clients, it is possible to load building data from multiple endpoints. Metadata about certain objects will be available to the user, such as the element ID of an IFC element. Additional information can be polled via the JSON-API. Extending the editor interface itself is easy, as source code examples are readily available. The BIMServer internally uses the ifcopenshell project (ifcopenshell, ) for parsing of IFC files.After parsing the data structure contained inside the binary file, we are able to create actors in UE4 to display the geometry. We are using a class \u201cIFCActor\u201d that inherits from UE4\u2019s own AActor class and extends with additional attributes and functions. Usually, UE4 uses static meshes for presentation, but there is an option of having procedural geometry (i.e. created at runtime) with the \u201cProceduralMeshComponent\u201d class. As no reference to static meshes can be stored, which is usually contained in UE4\u2019s \u201cuasset\u201d file format, the triangular data is retained inside the map we are importing into. This leads to larger map sizes, but simplifies asset management, as everything is stored at a single location.Using the Oculus Rift with UE4 is simple due to the engine having a plugin available. This will be loaded automatically, as soon as the user has a Rift connected. Visual corrections for chromatic abbreviation and distortion caused by the lenses and input mapping to a virtual camera works out of the box.Grabbing objects in 3D space can\u2019t be achieved with regular collision modelling, as the object would bounce back and forth between fingers. Gestures or special finger combinations can be used to attach objects to certain slots on the user\u2019s character. A Leap Motion gesture could be moving certain parts of the hand in a circle, swiping in a direction or pressing a finger forward / downward. Gesture based input can be modelled with Blueprints only, giving non-programmers a possibility to extend the logic.As UE4 is one of the major game engines available, support for alternative HMD devices, especially the HTC Vive, is possible by using the SteamVR-API (supported from UE4 version 4.8 onwards), which aims to be a standard for interfacing with virtual reality hardware. Tool based input, such as the tracked joysticks of the HTC Vive, is beneficial when trying to grab objects, as pressing a dedicated button on the handle of the stick is easier and more reliable to detect than a grabbing gesture.In the following segment we present some possible applications of virtual reality in conjunction with the Unreal Engine and the aforementioned VR hardware. While some of the scenarios may not be completely novel, the usage in a low cost VE with HMDs is well worth a consideration.With the new devices (Oculus Rift CV1 / HTV Vive) the requirement rises to 90 FPS for more immersive experiences, which should be achievable with our proposed approach. Especially as newer graphics cards in combination with Windows 10 have support for DirectX 12, which is reducing draw call overheads with an optimized graphics pipeline.Special implementations, such as special control schemes and interactions with the 3D world still need initial development effort, as no \u201cout of the box\u201d solutions exist. Complex and high level architectural scenes, such as seen in Fig.\u00a0, still require a capable 3D artist for content creation or a developer experienced with the game engine. As immersion inside a scene does not solely depend on the realism of graphics, professional developers without deep understanding of modelling can also do fast creation of environments. Several free 3D model sites are available on the Internet, which can be used as a source of models for a scene. Full body joint detection can be implemented using Microsoft\u2019s Kinect or similar hardware. Users are then able to move every part of their body and see the results in the VE.Hand detection using the Leap Motion is good, but tends to give false positives when not having free space in front of the user. Also, misdetection of the left and right hand are possible. This is an issue that is currently being worked on by the manufacturer and developers. Combining the Leap data with the Kinect system seems promising for now.Regarding the fire example, we propose to further elaborate the scripting qualities of the UE4. Depending on material properties, a fire spread rate could be calculated, minimizing the set up time for the test operators and giving a more realistic environment.As of now, several developers are porting their software to SteamVR and the HTC Vive, where the development kit preorder phase ended recently. Setting up the lighthouse trackers for the Vive and clearing a room of obstacles may be still a thing for enthusiasts, but tracking the hands with tools seems more reliable, as the optic detection of the hands is still not consumer-ready. In reasonable time, Sony will step into the market with their own solution (Morpheus headset) and this will be a big step forward for the whole VR community, as more customers are aware of the possibilities tied to VR.HMDs today are getting more useful for a wide range of applications in construction and engineering, while costing less than in the past. Modern game engines, such as the UE4, enable even non-programmers to generate logic procedures and levels for presentation. VEs enable the users to experience complex models or control schemes instead of having to comprehend a complex explanation or offline rendered 2D/3D images.We have shown the feasibility of automating major parts in the VR creation process from BIM as a starting point and the simplification by using the proposed methodology and software, as no parallel designing process for visualization is needed. BIMServer as a central server for storing BIM data is beneficial when using the proposed workflow and plugin for UE4. Geometry does not have to be recreated and can be imported. We plan to further extend the functionality and simplify the workflow in the future to create an easy importer for VR experiences based on real building data.Different use cases for utilizing a VE are evacuation plan testing, expert training and accessibility validation of environments. The supplied scenarios are only a small selection of what can be done using a VE for construction and engineering. Especially when using intuitive and natural control schemes, it may be easier for users to interact with the virtual surroundings.Also, we are looking forward to future HMDs that may include eye-tracking mechanisms. Rendering from two different point-of-views is computationally expensive, even more, when considering increasing resolutions for HMD displays in the coming years. Eye tracking can limit this impact, by only calculating high-resolution parts of the VE where the user is looking at and giving more approximate representations at the peripheral vision.Having the graphics card manufacturers change their architectures and drivers to be optimized for VR seems to be an indicator that this technology is here to stay and will be further improved in the future."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-016-0039-5", "title": "Research on urban landscape design using the interactive genetic algorithm and 3D images", "authors": ["Seiki Koma", "Yuichiro Yamabe", "Akinori Tani"], "publication": "Visualization in Engineering", "publication_date": "20 February 2017", "abstract": "Generally, there are different optimal solutions with regard to urban landscape planning depending on the area and the opinions and characteristics of community residents. Furthermore, when considering urban landscape and/or city-planning regulations, it is important to include residents\u2019 opinions based on voluntary activities like participation in town development on a regional scale and its management. However, residents\u2019 opinions are difficult to quantify, as many do not have specialized knowledge. Therefore, when an administrative body plans a city, a system to include residents\u2019 opinions on urban landscape options is required.", "full_text": "The urban landscape comprises the cityscape and scenery inherent to a region and has various characteristics. Local governments enact urban landscape regulations and must consider residents\u2019 opinions during voluntary activities in the development of urban landscape and/or city-planning. When urban landscape design is understood to be an optimization problem, the optimal solution may differ depending on the region and is influenced by the opinions and characteristics of the residents. However, many community residents do not have specialized knowledge; therefore, when a given administration plans a city, a system to include residents\u2019 opinions regarding urban landscape options is required. In the field of urban planning, previous research has focused on the basis for decision support in inner-city development (Seifert, M\u00fchlhaus, and Petzold ), computer aided zoning and urban planning (Garyaev ), a 3D visualization system (Tan, Fan, and Deng ), and the role of procedural modeling (Luo and He ). Moreover, studies have been conducted into the optimization and development of support tools to solve complex urban planning or landscape problems. For example, in building design using the Genetic Algorithm (GA), previous studies have examined floor shape optimization for green building design (Wang, Rivard, and Zmeureanu ) and three-dimensional shape generation for low-energy architectural solutions (Caldas ). However, this research concerned environmental aspects rather than subjective evaluations as the constrained condition. Research has also been conducted into the conceptual design of commercial buildings using GA (Miles, Sisk, and Moore ), concerning, for example, the floor plan and layout of columns based on a large number of criteria, including lighting requirements, ventilation strategies, limitations introduced by the available sizes of typical building materials, and the available structural systems. GA has also been used to devise a solution to the unequal area facilities layout problem (Wang, Hu, and Ku ). Thus, it can be seen that previous GA research has focused on the shapes, column layout, and facilities layout of buildings, the design of building facades including the multi-criteria decision-making process (Raphael ), and the development of support tools for decision making. Finally, Kawano and Tsutsumi () developed the design idea generation support system for the facades of office buildings. However, studies aimed at the streetscape of office buildings or a wider range of urban landscape elements have not yet been conducted. Therefore, when an administrative body is planning a city, it is necessary to develop a support system that targets not only a specific building but also the entire streetscape in order to create consensus between administrative bodies and residents who lack architectural knowledge, based on the subjective evaluation of the latter. In this research, in particular, such a system is developed based on evaluation from a pedestrian perspective. To create consensus between administrative bodies and residents, this system presents images and the appearance of a completed urban landscape, and facilitates the administrative bodies and residents to share complex images. In addition, the system enables both groups to easily point out a problem or an improvement because they can share the image in the process of consensus. It is envisaged that, in the future, when an administrative body plans a city, they may use this system to consider residents\u2019 opinions.In Case 1, the optimal results were almost achieved because the average user satisfaction value was 4.2 out of a possible 5, as shown in the questionnaire results in Table\u00a0. The rate of textures for the first generation was relatively equable as shown in Table\u00a0. Textures No. 4 and No. 3 were selected with a similar frequency, as shown in Table\u00a0.In Case 2, the optimal results were also nearly achieved because the average user satisfaction value was 4.2, as shown in the questionnaire results in Table\u00a0. In Table\u00a0, textures No. 7 and No. 1 were selected approximately 40% of the time. However, the selected ratio of textures in the first generation was biased unlike in Case 1 because of the disproportionate rate of textures for the first generation. It is necessary to examine whether the texture that was finally selected was chosen. As shown in Table\u00a0, the number of evaluations for wall positions was 2.3, for heights was 1.8, and for textures was 1.9. The wall positions evaluated at the beginning were evaluated the most times; thus, it is considered that users may have been too tired or bored to evaluate the urban landscapes as a whole as the number of evaluations increased. That is to say, if the overall number of evaluations was high, the number of evaluations of a given element decreased over time; thus, it is necessary to examine the evaluation order and evaluation method because the evaluation order may have affected the optimal results.In Cases 2\u20132 and 2\u20134, four of the height values (Average, Standard deviation 1, Median value, and Standard deviation 2) were relatively close, as shown in Table\u00a0. In particular, standard deviation 2 in Cases 2\u20132 and 2\u20134 displayed a large difference compared to the other cases. Furthermore, the median values in Cases 2\u20132 and 2\u20134 were high. The urban landscape in Cases 2\u20132 and 2\u20134 had large differences in height as shown in Figs.\u00a0 and . Many high buildings were selected in Cases 2\u20132 and 2\u20134. Furthermore, the height values of Cases 2\u20137 and 2\u20139 were relatively close, as shown in Table\u00a0. The average and median values of Cases 2\u20132 and 2\u20139 were also relatively close. However, the standard deviation 1 and 2 in Cases 2\u20137 and 2\u20139 showed a large difference compared with Cases 2\u20132 and 2\u20134. If the average heights of buildings were close, while, on the other hand, the difference in adjacent buildings was large, users might perceive a different image with respect to their urban landscapes. The urban landscape in Cases 2\u20137 and 2\u20139 comprised small height differences and more gradual changes in height, as shown in Figs.\u00a0 and . In Cases 2\u20132 and 2\u20136, the difference between the average, standard deviation 1, median value, and standard deviation 2 height values was large, as shown in Table\u00a0. In Case 2\u20132, the heights of buildings in the urban landscape were perceived as jagged as shown in Fig.\u00a0. However, for Case 2\u20136, the building heights of the urban landscape were perceived as low and their changes were small as shown in Fig.\u00a0. For heights in Cases 2\u20139 and 2\u201310, the average and median values that mean average heights of buildings in Case 2\u20139 were larger than their values in Case 2\u201310. However, standard deviations 1 and 2, which mean variation in the heights of buildings in Case 2\u201310, were larger than their values in Case 2\u20139, as shown in Table\u00a0. In Case 2\u20139, the building heights in the urban landscape were perceived as high and their changes were small as shown in Fig.\u00a0. However, in Case 2\u201310, high buildings were also chosen although low buildings were also frequently chosen as shown in Fig.\u00a0. For this reason, the standard deviations 1 and 2 in Case 2\u201310 were larger than in Case 2\u20139.The questionnaire results showed that both feelings of tiredness and ease of making choices were slightly higher for Case 2. Therefore, while individual evaluations were considered easier than simultaneous evaluations, on the other hand, individual evaluations require many generations to reach convergence. Simultaneous evaluations like Case 1 do not require many generations to reach convergence and it is easy to use the system because three parameters are evaluated simultaneously. The characteristics of each case like usability and ease of making selections were identified.In addition, it was noted that the participation of residents in urban planning gives rise to problems concerning consensus, the system used by the residents to participate in urban planning, and inhabitant consciousness. As the first step in addressing these problems, the setting of a target site and factors was simplified, a system that reflects users\u2019 opinions regarding urban landscapes was built using IGA, and the characteristics of selected urban landscapes were identified in this study. Since this study considers only the simple evaluation of users and does not spare any thought for cognitive psychology (for example, Gestalt psychology), cognitive psychology could not be introduced in this study. Nonetheless, it is considered that cognitive psychology may be necessary. On the limitation of this study in building a decision support system of subjective human opinions, there is consensus. For example, it is considered that a system providing urban landscapes that can satisfy all residents probably cannot be built; however, it is possible to build a system that provides urban landscapes that can satisfy 70\u201380% of residents. Finally, by implementing the optimization system among multiple users and creating consensus, residents\u2019 participation in urban planning can be made easier, the lack of expert knowledge on the part of residents and difficulties in reflecting their opinions can be overcome, and both residents and administrative bodies can share information and concept values. Therefore, it is considered that better town development can be carried out using this system."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-016-0035-9", "title": "V3DM+: BIM interactive collaboration system for facility management", "authors": ["Wan-Li Lee", "Meng-Han Tsai", "Cheng-Hsuan Yang", "Jhih-Ren Juang", "Jui-Yu Su"], "publication": "Visualization in Engineering", "publication_date": "24 February 2016", "abstract": "Facility management (FM) is an important phase in the building lifecycle. Although building information modeling (BIM) technology has been widely used in the design and construction phase, efforts to transfer information to the FM phase are still in the nascent stage. A good BIM-to-FM tool should be able to inherit and integrate the building information built in the design and construction phase as well as be able to provide a platform for managers to browse the information, discuss potential problems, and arrange maintenance work.", "full_text": "Facility management is an important phase in the lifecycle of a building. The operation and maintenance costs of a building are considerably greater than the construction costs (Gallaher et al. ). By providing appropriate project information to facility managers, these costs can be greatly reduced. In 1983, the National Academy of Sciences first noticed the importance of useful facility management data for operation and maintenance of buildings (Scarponcini ).In the past, facility managers used 2D graphics to acquire basic information. However, 2D graphics contain several symbols that only professionals would understand, thus leading to difficulty in interpretation of the information for managers (Tsai et al. ). Currently, many researchers have been attempting to implement building information modeling (BIM) for facility management (FM). BIM visually presents spatial data through 3D models, which can enhance managers\u2019 level of comprehension of the information (Tsai et al. ). The following sections review studies on BIM for FM and the challenges in using BIM in the operation and maintenance phase.Yu et al. () developed a framework for computer-integrated FM and pointed out that the project data should be selectively transferred into the building FM phase. Vanlande et al. () developed an extension of BIM technology, which allowed the creation of an information system dedicated to the entire building lifecycle. A visual BIM-based approach for evaluating the coverage of a CCTV system in public areas associated with a building was developed by Chen et al. (). Lee et al. () conducted a case study of applying BIM coordination technology to a high-speed rail station. Edwards et al. () attempted to apply game engine technology to BIM to build a collaborative and interactive platform for end users of the building. YouBIM, a spin-off international building information modeling service firm, developed a software named YouBIM, which provides a cloud-based solution to manage the building activities in a simple way in the FM phase (YouBIM ).Data inheritance and integration is one of the big challenges in implementing BIM technology for FM. BIM has been widely used in the design (Lee et al. ) and construction (Chen et al. ) phases. Although several types of software exist in the market for architects and constructors to build BIM models, interoperability of the software is limited. Therefore, data transformation and integration is difficult (Wetzel et al. ). To address the issue of how to fully utilize existing data in the FM phase, some standards have been developed for information exchange. The Industry Alliance for Interoperability (IAI) developed a data transformation standard, called the Industry Foundation Classes (IFC), which allows exchange of 3D models and information data among specific BIM tools (NIST ). According to buildingSMART, which publishes the IFC data model standard, IFC can currently be supported around 150 software applications worldwide (buildingSMART ). Furthermore, East () reported in Construction Operations Building Information Exchange (COBIE): Requirements Definition and Pilot Implementation Standard that the development team of the National Building Information Model Standard (NBIMS) released a standard known as COBIE for selecting information generated in the design and construction phase and providing the information to the operation and maintenance phase.Besides data inheritance and integration, the design of existing BIM tools is another challenge. Existing BIM tools are typically designed for the construction industry and have the main functionality of model and data construction, with user interfaces designed to support this activity. However, FM involves multidisciplinary activities, and hence, a BIM tool for FM should be able to provide diverse functionalities, such as visualization, analysis, and discussion (Becerik-Gerber et al. ; Singh et al. ).In summary, BIM is an advanced approach for the architecture, engineering, and construction (AEC) industries that aids in the exchange and interoperability of information for design, construction, and management (Eastman et al. ). BIM technology has been successfully used in the design and construction phase of the AEC industry. However, as Wetzel et al. () described, \u201cwith all the success that BIM has experienced during the design and construction phase, efforts to transfer information to the facility lifecycle phase is in its infancy.\u201d Moreover, some problems remain to be resolved for optimizing BIM technology for FM. Therefore, this research aims to develop a system that can inherit and integrate building information from the design and construction phase and utilize the information in FM. In addition, a visual interactive platform is designed for data collaboration, which allows users to browse through the facility information, discuss a facility\u2019s potential problems, and arrange operation or maintenance personnel.In this research, a BIM interactive collaboration system was developed, which can help administrators integrate the 3D models and information obtained from constructors and discuss issues related to facilities on a discussion platform. In the following sections, we discuss the modules of the system and their functionalities.We developed a BIM interactive collaboration platform, named V3DM+, for assisting FM. To implement the proposed system, we combined HTML 5 and Unity 3D, a game engine that supports 3D environments, virtual navigation, and a web player. HTML 5 is used for visualizing information data and handling discussions. Unity 3D directly displays the 3D models on the website. By applying Unity 3D and HTML 5, building managers can use this system without considering hardware limitations. The following sections will introduce the actual user interface and operating procedure for V3DM+.An accessibility analysis was conducted to test the developed facility management system using a real project of the Tamsui Civil Sports Center (TSSC) as a case study. We first analyzed the management demands of the TSSC. Second, we optimized and implemented V3DM+ on the sports center by taking completed construction reports and models as the data source. Finally, we received results and derived ideas from the case study.In this study, we developed a BIM interactive collaboration system, V3DM+, for FM. Through an accessibility experiment on a real project, we verified that V3DM+ can greatly contribute to FM. However, some limitations exist for implementing this system in a real case.In this study, we designed and implemented an innovative system, V3DM+, for facility management. This system provides an interactive collaboration platform for the maintenance and operation phase. Managers of a building can browse through the data, initiate discussions, or assign maintenance tasks directly in V3DM+. For the users to manage facilities without the need to use authorized software, a data arrangement module was developed, in which an API library is used for importing models built using different BIM tools. For connecting all objects in the models with their related information data, specific IDs were assigned to both the object and its related data. In addition, a game engine was applied for presenting a model in a realistic 3D rendering environment. Using the game engine to display the special information, the user can browse through the model intuitively in a visual way.Through an accessibility analysis, we verified that the system can be implemented in a real project, and the results confirmed that V3DM+ is an effective tool for facility management. Users can import data into V3DM+ regardless of the file format. Using V3DM+ to manage facilities is easier than using existing BIM tools. The analysis results showed that V3DM+ has a great potential to elevate the effectiveness and reduce operational maintenance costs for buildings In future work, automatic importing and presentation of data will be realized by implementing a data transformation module."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-016-0037-7", "title": "Application of fractal algorithms of coastline echo\u2019s generation on marine radar simulator", "authors": ["Shuguang Ji", "Zhang Zhang", "Hongbiao Yang", "Dan Liu", "Rapinder Sawhney"], "publication": "Visualization in Engineering", "publication_date": "12 May 2016", "abstract": "Marine radar simulator is a useful approach endorsed by International Maritime Organization (IMO) to train the seafarers on how to operate marine radar equipment and use marine radar equipment for positioning and collision avoidance in laboratory. To fulfill all of the marine radar simulator training requirements, a high performance simulator is necessary. However, imperfections with currently available marine radar simulators require simulator developers to make improvements.", "full_text": "The International Convention on Standards of Training, Certification and Watchkeeping for Seafarers 78/95 (STCW Convention 78/95) of International Maritime Organization (IMO) requires using a marine radar simulator to train seafarers. A marine radar simulator is the only acceptable approach in the laboratory for seafarers to learn how to operate radar equipment and use radar for positioning, navigation, and anti-collision. Upon completing all requirements set forth in the training, the trainee will receive certification for qualifications to work on board. Currently, marine radar simulators are widely used by the members of IMO as one of many useful tools for seafarer education and training (; ; ; ). However, due to the limitation of the simulation technology, the marine radar simulators on the market are unable to replicate the performance of real radar equipment. Taking the simulation of coastline echoes as an example, the coastline echo is generated by raw data, which are acquired from the digitalized chart and consist of a collection of coordinate points. By connecting two adjacent coordinate points, a straight line is generated to approximate a real coastline echo. This method works well for generating a coastline echo under a large radar range (say 6 nautical miles (NM)). However, when the radar range is adjusted to a smaller radar range (such as 0.25 NM), the shape of coastline echo will lose its natural structure and look quite artificial (; ). In addition, it should take around three seconds for the scan line of the marine radar to rotate a round. By adopting a traditional generation method for the coastline echo under small radar range, the time for the scan line to rotate around is much more likely to exceed three seconds, since extra sampling coordinate points (if available) have to be inserted to generate a high quality coastline echo. Imperfections like the artificial coastline shape and slow rotation of the radar scan line may have negative impacts on the training effectiveness for users (; ). In order to overcome the problems associated with the conventional coastline echo simulation approach, we apply fractal theory to the coastline echo simulation process, since fractal theory is widely used as a graphics tool for generating natural-looking shapes like coastlines, rivers, mountains, and other natural features (). The simulation results are evaluated and scored by 30 experienced mariners to validate that the coastline echoes generated by fractal algorithms look more natural than those generated by conventional method. Furthermore, an improved fractal algorithm is designed to guarantee the scan line can finish a round of rotation within three seconds, which is difficult to be achieved using the conventional method, especially under a larger radar range.The STCW Convention provides required components for seafarer training, which use the radar simulator as a tool of training and assessment. These highlights include (; ; ): factors affecting performance and accuracy; detection of misrepresentation of information, including false echoes and sea returns; setting up and maintaining displays; range and bearing; plotting techniques and relative motion concepts; identification of critical echoes; course and speed of other ships; time and distance of the closest approach to crossing, meeting or overtaking ships; detecting course and speed changes of other ships; effects on the changes of the own ship\u2019s course or speed or both; and application of the International Regulations for Preventing Collisions at Sea. To fulfill all of these training requirements, a high performance marine radar simulator is needed. In the current marine radar simulator market, the major developing teams include Nautical Software (), Bridge Command (), Kongsberg Maritime (), Landfall (), and Dalian Maritime University Institute of Navigational Technology (). In addition, some previous research investigates methods to improve the marine radar simulator. For example, Arnold-Bos et al. developed a versatile bistatic and polarimetric marine radar simulator. In their simulator, realistic sea surfaces are generated using the two-scale model on a semi-deterministic basis, so as to incorporate the presence of ship wakes in the simulation (). Yin et al. designed a radar simulator using a PC to generate radar echoes and a radar interface board to generate radar signals. Their simulator has a more flexible and realistic operation interface than other simulators (). Zhang et al. put forward a coastline echo intensity algorithm based on RGB and HIS color models and applied this algorithm on the marine radar simulator. The simulation results from this model are consistent with the electronic chart ().In this study, we incorporate fractal theory, a branch of non-linear mathematics, to improve coastline echo simulation. The research targets of fractal theory are irregular objects and non-linear systems in the nature. The term \u201cfractal\u201d? was first used by mathematician Benoit Mandelbrot in 1975 to extend the concept of theoretical fractional dimensions to geometric patterns in nature (). In the 1980s, fractal theory was applied into the signal processing for radar because the echoes reflected into radar system have many fractal patterns (; ). Even though fractal theory has been widely applied in fields such as virtual reality, image processing, and time series analysis, etc. (; ), there are few studies to apply it into the simulation of coastline echo for marine radar simulator. This research aims to close this gap. Partial of findings reported in this article were originally presented at the 94th Transportation Research Board Annual Meeting. We improved the research methodology in Ji et al. () in this article. Especially, a full control of the physical parameters involved fractal function, Weierstrass-Mandelbrot function (WMF), is used to simulate coastline echoes. In addition, a quantitative validation of the simulation results is designed to assess the fidelity of the simulation outcomes and comprehend possible values of the introduced parameters among the simulation algorithms.The echo reflection on radar simulator can be classified into three types (; ). A Type I Echo is the echo reflected by artificial architectures such as berth and breakwater. Type I Echoes have regular shape and can be used for positioning because of its clear boundaries and fixed position. A Type II Echo is the echo reflected by rocky coast. Type II Echoes have a realistic pattern as well as fixed position. A Type III Echo is an echo reflected by flat coast such as sand coast. Type III Echoes have a large echo reflection zone and relatively weak reflection. Additionally, the shape and position of a Type III Echo will change with the motion of the waves. This study focuses only on simulation of Type II Coastline Echoes because of its natural fractal features. Three different fractal algorithms are adopted to simulate coastline echoes. A comparison among these three simulation algorithms is included.In this paper, fractal algorithms are applied into the simulation of coastline echoes on marine radar simulator. The simulation outcomes from different methods are compared as well. In order to guarantee the rotating speed of radar scan line, threshold value L is introduced into the simulation process. Based on our evaluation, the improved FBM algorithm is the best choice for the simulation of coastline echoes on marine radar simulators. Natural-looking coastline echoes generated by the algorithms introduced in this study can improve the quality of the training significantly. The fractal algorithms developed in this paper are packaged into a dynamic link library (dll) with well documented application programmable interface (API), which means that the algorithms are decoupled from the simulator program. This brings great benefits and convenience when transplanting the algorithm to other simulators, as long as the simulator is able to load a dll library. Since a dll library is supported by most of Windows based programs, we believe that the adoption of our algorithms on other simulators will be effortless. One limitation of this study is that we only simulated Type II coastline echoes. Another limitation of this study is that we aim at applying the fractal algorithms into the simulation of the coastline echoes in radar simulator rather than finding the best parameters of the fractal algorithms for the simulation. Last but not least, since the authors are unable to collect real data from marine radar on board, a survey method is employed to evaluate the simulation results by fractal algorithm. In the future, simulation of other radar echo types will be considered; how to choose the parameters for the fractal algorithms should be investigated; and a more objective evaluation approach should be designed to evaluate the simulation results."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-016-0038-6", "title": "A review of tertiary BIM education for advanced engineering communication with visualization", "authors": ["Amarnath Chegu Badrinath", "Yun\u2010Tsui Chang", "Shang\u2010Hsien Hsieh"], "publication": "Visualization in Engineering", "publication_date": "16 June 2016", "abstract": "Today, the architectural, engineering, construction, and operation (AECO) industry is motivated to employ graduates educated about Building Information Modeling (BIM) tools, techniques, and processes, which help them to better integrate visualizations and data into their projects. In line with today\u2019s AECO industry necessities and government mandates, globally active BIM educationalists and researchers are designing BIM educational frameworks, curricula and courses. These educationalists and researchers are also generating solutions to the obstacles faced during integration of BIM education into tertiary education systems (TESs). However, BIM researchers have taken few efforts recently to provide an overview of the level of BIM education across the globe through review and analysis of the latest publications associated with BIM education in TESs. Hence, this study attempts to fill this gap by providing a review of the efforts of globally active educationalists and researchers to educate AECO students about BIM in the context of advanced engineering education with visualization.", "full_text": "Traditional Computer-Aided Design (CAD) drawings (i.e., graphical entities such as dots, lines, and curves) and 3D models (i.e., 3D based presentations, rendering, walk-through, etc. to enhance model-based visualizations) have evolved into a new paradigm: intelligent Building Information Modeling (BIM). This tool consists of data-rich smart objects (defined in terms of building elements and systems such as spaces, walls, beams, and columns) being aggregated for the digital representation of physical and functional characteristics of facilities. Intelligent BIM has multiple dimensions from 3D to D\u2014such as 3D-visualization, 4D-scheduling, 5D-estimation, 6D-facility management applications, and 7D-sustainability\u2014offering multiple benefits such as BIM model use throughout the building life cycle (Computer Integrated Construction Research Group ; Succar ). Hence, intelligent BIM provides an opportunity for Architectural, Engineering, Construction, and Operation (AECO) industry stakeholders to evaluate possible solutions and identify potential problems of the final product before the start of actual construction. The most common use of intelligent BIM is visualization, and the most essential part of visualization in engineering is communication. Visualization can enhance the communication between AECO industry stakeholders, and result in better understanding of what a client is asking for. Advanced visualization techniques also improve the efficiency of information exchange in the context of AECO education in tertiary education systems (TESs), assisting AECO students in solving geometric tasks. Hence, the use of CAD and intelligent BIM, technological advances in spatial representation, and conceptual skills by which users can make intuitive decisions about spatial problems are all essential to delivering better education for AECO students. Moreover, introducing AECO students to modern BIM technology, tools, and related processes will allow them to be further competitive and flexible in a rapidly changing Information Technology (IT) environment (Hsieh et al. ).Based on today\u2019s AECO industry expectations and government mandates, many educational institutions across the globe are investigating how to incorporate BIM in TESs (Becker et al. ; Salman ; Rooney ). In addition, globally active BIM educationalists and researchers have invested huge efforts in delivering BIM educational frameworks, designing BIM curricula, conducting BIM courses, and developing new strategies for overcoming the obstacles faced during BIM implementation. Relatedly, a few BIM educationalists and researchers have delivered overviews of BIM educational trends in the past (Barison & Santos , ; Wong et al. ; Lee & Dossick ). Recently, NATSPEC, a non-profit organization published an update on the state of BIM awareness and adoption in countries such as the USA, Canada, the Czech Republic, Finland, the Netherlands, Norway, the UK, South Africa, China, Hong Kong, Singapore, Japan, Australia, and New Zealand. NATSPEC\u2019s study revealed that BIM education and its uptake are still at different levels of implementation across the globe, and provided an outline declaring that current BIM education tends to focus on the use of particular BIM software. In the end, NATSPEC\u2019s report emphasized the need for education connected to open BIM, BIM management, and a collaborative working environment for them (Rooney ). Open BIM and BIM management in academic BIM education refers to educating AECO students on how students of different disciplines need to collaboratively design, construct, and operate buildings based on open standards and workflows. However, NATSPEC\u2019s study failed to document completely the status of BIM education and awareness in each country. Another drawback was that the report was purely based on the responses provided by a global group of parties with an interest in BIM. Moreover, no recent efforts have been undertaken by BIM researchers to review and analyze the latest BIM publications in order to provide an overview of the state of BIM education worldwide."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0042-5", "title": "Data completion in building information management: electrical lines from range scans and photographs", "authors": ["Ulrich Krispel", "Henrik Leander Evers", "Martin Tamke", "Torsten Ullrich"], "publication": "Visualization in Engineering", "publication_date": "21 March 2017", "abstract": "The concept of building information management (BIM) is based on its holistic nature. This idea pays off, if all relevant information is fused into one consistent data set. As a consequence, the completeness of data is vital and the research question on how to complete data automatically remains open.", "full_text": "Studies conducted on the productivity of the construction industries show that the industry could improve efficiency using a standardized workflow across its stakeholders, particularly in the field of electrical construction companies. According to these studies the implementation of building information modeling (BIM) is a solution (; ); its application would reduce conflicts and improve coordination. A survey conducted in 2015 showed that the implementation of BIM has not yet reached the majority of companies (). The study by Hanna et al. () points out the deficits of actual implementations of BIM in the electrical construction field. Other sectors of the construction industry have increased their productivity using BIM (). The globally accepted Industry Foundation Classes (IFC) standard () includes a complete set of electrical entities. The lack of BIM implementation in the field of building electricity is surprising, as shown by a survey conducted in the United States (). This study shows a ranking of BIM features used, e.g. clash detections, visualization of electrical design or space utilization. Out of the partaking companies, 21% use BIM and report positive savings in time and cost. The remaining 79% respond that they are not using BIM due to not knowing about BIM, lack of technological experience, software incompatibility, and implementation costs. In addition, 59% of the companies which actually use BIM have only three or less years of experience with this technique. Nevertheless, these studies show that there is an interest in the utilization of BIM for electrical construction. There seems to be a hesitation of participants of the electrical construction sector to engage with BIM. This is at least partially grounded in the fact that BIM data is generally not available for projects that deal with the existing building stock which accounts for the biggest share of the building market, e.g. in the European Union for 75% (). Most of buildings date to a pre-digital area (). Project planning for such buildings requires extensive and expensive documentation of the as-built state. In Europe, 61% of the overall building stock consists of pre-1980 buildings. The need to upgrade these buildings in terms of energy efficiency () demands investments in a range between 584 and 937 billion euro until 2050.The general compliance of the electrical installation to the framework built by norms provides a relatively secure ground for professionals to estimate where wires could be routed, but assured data is usually not available. This situation provides the base scenario of this paper and other recent research: In the work of Petkova et al. (), the location of sockets in 2D floorplans of buildings were used to estimate the amount of copper wire in the walls, a valuable resource and a real asset in the evaluation of a buildings value before demolition. This method integrates the prior knowledge of wiring placement directly into the algorithm, however, the norms differ between countries and been revised over time. In contrast, the method presented in this paper proposes to encode this knowledge using a knowledge management approach.The main idea of knowledge management is to represent prior knowledge and expert knowledge in a self-contained, machine-readable manner, independent of the usage scenario. A commonly used approach to represent knowledge is based on machine learning. It \u201cteaches\u201d a machine to solve a problem using a training dataset; i.e. a set of input data with corresponding, labelled output data. Many computer vision problems, such as segmentation, detection, recognition, and matching, can be solved by machine learning techniques (). Although many problems can be solved by machine learning, its application has some important limitations. Most machine learning techniques require a sufficiently large, labelled training dataset \u2013 a crucial precondition, which may be elaborate to meet. From the knowledge management point of view another problem of machine learning algorithms may arise: the \u201clearned\u201d knowledge can seldom be imported or exported in a human-readable way. As a consequence, prior knowledge, which may be available in form of specifications, standards and norms cannot be reused and the question what has been learned remains unanswered. Therefore, we chose a rule-based approach to represent knowledge (). The main advantage of procedural modeling techniques is the included expert knowledge within an object description; e.g. classification schemes used in architecture and civil engineering can be mapped to procedures. For a specific object only its type and its instantiation parameters have to be identified.Such grammar systems were originally developed in formal language theory, and are commonly used in compiler construction. They are also a common tool used by generative modeling techniques (). For a review on generative modeling techniques we refer to (). The proposed grammar ruleset can be adapted to reflect changes in norms, or encode different norms, typically by a domain expert. The method is implemented as a pipeline that uses indoor 3D scans of buildings as a starting point. These scans consist of range scans and image information. From this data, a 3D representation of the scanned spaces is generated via a preprocessing step, and is further processed to determine the configuration of installation zones and a possible wiring hypothesis for the acquired data.The proposed approach is divided into four main concepts: data acquisition, data preprocessing, the detection of visible endpoints of electrical installations (sockets and switches), and eventually the hypothesis of installation zones and a possible wiring inside the walls.Although the final result of the pipeline is non-visual (i.e. a semantically enriched BIM model), all steps produce a visual output which is used for debugging the pipeline, for the verification of the results and for publication purposes. The point clouds have been rendered using CloudCompare; the room geometry is exported as X3D and rendered with Blender; the endpoint detection returns JPEG for visual inspection; the hypothesis modules generates SVG files for visualization.The following sub-chapters describe each part of the pipeline in greater detail.This work presents an approach that synthesizes a hypothesis of electrical lines from range scans and photographs. It uses a new method that utilizes a formal grammar to represent the guidelines that govern the placement of electrical lines.The main contribution of our new approach is the formalization of norms, standards and prior knowledge in a machine-readable manner. The presented application in the context of building information management shows the potential of the generative paradigm. Unfortunately, the machine-readable representation in form of a grammar system differs significantly from human-readable, descriptive norms. Future research will focus on a representation that closes the gap between machine-readable and human-readable knowledge representations.Besides scientific contributions the new approach has several benefits: it enables the adaption of rules to different norms, up to specifics that concern only one building. The labeling of detected sockets with confidence values from the detection stage proved to be a viable approach in this project. Concerning future developments practitioners which evaluated our solution remarked that it would be necessary to show users that the generated data is an estimate and not the sole truth, to counter the blind believe, which stakeholders often demonstrate in the face of external data. The representation of an installation zone probability as a heat map would be a way to present this information. Practitioners also stated that this feature would be helpful in the design of new buildings. In later BIM based planning stages, the number and locations of sockets and switches per room are known, but no automated means to estimate the cable length and the attached costs are available.For a productive usage the following issues have to be addressed. Just like any acquisition and reconstruction method, our approach also relies on the input data quality and completeness (a wall which has not been photographed cannot be analysed); the preprocessing step assumes a single floor with straight walls only; the endpoint detection can only detect what has been learned (in our examples we did not train high voltage sockets); the hypothesis cannot reproduce cases where the actual wiring does not follow the rules \u2013 a scenario often found in old buildings. Nevertheless, this proof of concept already shows the applicability of our approach.Source code for the proposed pipeline for the modules orthophoto generation, electric endpoint detection and installation zone generation as well as wiring hypothesis is freely available at the GitHub repository  in Modules ,  and .\n                \n                \n              \n                \n                \n              \n                \n                \n              \n                \n                \n              "},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0043-4", "title": "A logic-based representation and tree-based visualization method for building regulatory requirements", "authors": ["Jiansong Zhang"], "publication": "Visualization in Engineering", "publication_date": "16 March 2017", "abstract": "Many research and development efforts have been made for automated compliance checking of building designs with regulatory requirements, but there is a lack of a non-proprietary and user-understandable representation of building regulations to support automated compliance checking in the construction domain that is experimentally tested for understandability and reading speed.", "full_text": "The automation in building code compliance checking has a long history dating back to the 1960s when Fenves et al. () digitized the American Institute of Steel Construction\u2019s (AISC) specifications into decision tables. Since then, a number of efforts have been made to explore the automated compliance checking of building designs with various types of code requirements using various types of computable representations. For example, Garrett and Fenves () presented the automated compliance checking of structural design with structural codes using decision tables and information network to represent structural requirements from design standards; Delis and Delis () examined the automated compliance checking of building architectural features (e.g., space configuration) with fire codes using IF/THEN rules to represent fire code requirements; Han et al. () discussed the automated compliance checking of building components with accessibility requirements using simple simulations and rules to represent accessibility requirements; and Tan et al. (, ) proposed automated compliance checking of building envelope design using decisions tables to represent building codes and design regulations.Some commercial automated compliance checking systems were developed. As surveyed by Eastman et al. (), several automated compliance checking projects have utilized rule-based checking platforms such as the Solibri Model Checker (SMC), the Express Data Manager (EDM) Checker, and FORNAX. Research efforts have also been designated for the representation of regulatory requirements for automated compliance checking purposes. For example, Lau and Law () described a representation of regulations, standards, and codes using eXtensible Markup Language (XML) as well as ontology; Yurchyshyna et al. () used a representation of regulatory requirements in the form of SPARQL Protocol and RDF Query Language (SPARQL) queries; Pauwels et al. () proposed a representation of regulatory requirements using Notation 3 (N3) Logic; Hjelseth and Nisbet () developed a Requirement, Applies, Select, and Exception (RASE) method for marking up regulatory requirements to support further processing into computable representations; Beach et al. (; ) extended the RASE method to a regulatory information representation at both the paragraph level and the word group level that can be converted to Semantic Web Rule Language (SWRL) rules; Dimyadi et al. () adopted a representation of regulatory requirements using the Drools Rule Language (DRL); and Zhang and El-Gohary () developed a representation of regulatory requirements using first order logic. Compared to the methods used in commercial systems, these research efforts strive for a more flexible representation that potentially can be used to represent a variety of code requirements.Automated code compliance checking is an important application of modeling and computing technology in the architectural, engineering, and construction industry. Many efforts have been made in academia, government, and industry to develop automated compliance checking methods and systems. In spite of the many efforts in automated compliance checking research and development, there is still a lack of a non-proprietary and user-understandable representation of building regulations to support automated compliance checking. Little has been done toward testing or improving the user-understandability of regulatory requirements in the building and construction domain to support automated compliance checking. To address this research gap, this paper demonstrates a logic-based representation and tree-based visualization method for regulatory requirement. The method leveraged B-Prolog rules to represent regulatory requirements where the conditions of the B-Prolog rule represent the premises of the regulatory requirement, and the conclusion of the B-Prolog rule represents the compliance status with the regulatory requirement. Each concept or relation in a regulatory requirement is represented as a predicate in the B-Prolog rule, with the original names of the concepts and relationships used as the names and arguments of the predicates. This consistency in naming helps users understand the meanings of the concepts and relations simply by looking at the logic rule representation. In the tree-shaped visualization method, a tree structure is used to illustrate the concepts and relations. The tree representation used visual cues both in the horizontal dimension and the vertical dimension to provide a better visualization of the regulatory requirements compared to the pure logic-based representation.To test the understandability and reading speed of the proposed visualization method and how it compares to the original text representation and the logic representation, a survey was conducted during which 300 people with various backgrounds were contacted and 93 of them participated. The survey collected background information of the respondents\u2019 sex, age, education, and knowledge level on building codes, and tested the respondents\u2019 understanding and reading speed of the different representations of two regulatory requirements. Between the two regulatory requirements one is relatively simple and the other is relatively complex. The testing results were statistically analyzed. Results showed that: (1) the visual representation of regulatory requirements using the tree-based visualization method was significantly better than the logic-based representation in terms of understandability and reading speed for both simple and complex regulatory requirements; (2) the original text representation of regulatory requirements was slightly better than the visual representation in understandability when the requirement was simple (i.e., significant at the 90% and 95% confidence levels but not significant at the 99% confidence level). the text representation was slightly better than the visual representation in understandability when the requirement was complex, and the text representation was slightly better than the visual representation in reading speed both for simple and complex requirements (i.e., only significant at the 90% confidence level, or at the 95% confidence level for simple regulatory requirement). But none of these differences were significant unless otherwise specified; (3) the original text representation of regulatory requirements was significantly better than the logic-based representation in terms of understandability; (4) the original text representation of regulatory requirements was significantly better than the logic-based representation in reading speed. This result shows that the proposed tree-based visualization method can significantly improve the understandability and reading speed of the logic-based regulatory requirement representation, and it is at a comparable status with the original text representation of regulatory requirements in terms of understandability and reading speed. Further analysis into the different levels of knowledge on building codes showed that high level of knowledge on building codes may help with the understanding (of simple regulatory requirement) and reading speed (of complex regulatory requirement) of the logic representation, but the difference between visual and logic representation is significant in most cases.This research contributes to the body of knowledge in three main ways: (1) before this research, little has been done for testing or improving the user-understandability of regulatory requirements in the building and construction domain to support automated compliance checking, as far as the author is aware. This research is among the first to experimentally compare the understandability and reading speed between different representations of building regulations in a quantitative manner; (2) there is a lack of a non-proprietary and user-understandable representation of building regulations to support automated compliance checking in the construction domain; the proposed logic-based representation and tree-based visualization method serves as one possible non-proprietary and user-understandable representation of building regulatory requirements; (3) this research shows that when given an effective visualization method, the computable representations of regulatory requirements can achieve understandability and reading speed that are comparable to the original text representation (i.e., with no significant difference); and (4) this research reveals that the tree-based representation of regulatory requirements improve the understandability and reading speed of regulatory requirements as compared to the computable logic-based representation.Two main limitations of the current work are acknowledged. First, the work presented in this paper only serves as an initial investigation on the potential effects of visualization on computable regulatory representations in terms of understandability and reading speed, and this investigation focused on a specific type of computable regulatory information representation (i.e., Prolog-based representation) and a specific type of visualization technique (i.e., tree-based visualization). How the tree-based visualization performs for other types of computable regulatory information representations and how other types of visualization technics compare with the tree-based visualization can be conducted by follow-up research efforts. Second, there are regulatory requirements that are much more complex than the ones used in the experiment in this research; how the different types of representations perform on these significantly complex requirements still need to be investigated. In future work, the author plans to scale up the experiment to include other types of computable regulatory information representations and other types of visualization methods as well as incorporating the investigation of scalability of those methods to the much more complex regulatory requirements."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0040-7", "title": "Barriers and facilitators for BIM use among Swedish medium-sized contractors - \u201cWe wait until someone tells us to use it\u201d", "authors": ["Petra Bosch-Sijtsema", "Anders Isaksson", "Martin Lennartsson", "Henrik C. J. Linderoth"], "publication": "Visualization in Engineering", "publication_date": "21 March 2017", "abstract": "The pace of diffusion of BIM (Building Information Modelling) use is considered to increase with governmental initiatives in which public clients in countries like Finland, Singapore, United Kingdom, and Sweden begin requiring BIM as a part of the project delivery. Currently, larger contractor firms use BIM to a certain extent. However, BIM use by mid-sized contractor firms (that is, firms with 50\u2013500 employees that can successfully compete with larger contractors on projects costing a maximum of 50 million Euros) is relatively unknown. Hence, the aim of the paper is to explore current use and perceived constraints and driving forces of BIM-implementation with respect to mid-sized contractors.", "full_text": "BIM has been launched as one of the most promising developments in the building and construction industry (Eastman et al. ) and is even considered as a new paradigm (see Azhar et al., ). Additionally, Lee and Yu () claim that several researchers and practitioners seem to agree on BIM\u2019s potential applicability in and benefits for construction. The confidence in BIM as a means to increase industrial efficiency is further expressed by governmental initiatives in countries including Great Britain, Singapore, Finland, and Sweden, where public clients have started to require BIM as part of project delivery. This development is supposed to increase the pace of diffusion of BIM use. However, critics of this indicated that:  (Dainty et al., ). Thus, there is a gap between optimistic predictions on the positive effects of BIM in the industry and the actual implementation and use of BIM wherein several challenges are identified. Becerik-Gerber and Rice () suggested that it is difficult to evaluate the benefits of BIM use with quantitative measures because the claimed benefits are often intangible. Fox () questioned the positive effects of BIM use and proposed that expectations of BIM benefits could be excessively optimistic. Additionally, Vass and Gustavsson () indicated that BIM professionals do not perceive any business value from BIM currently, although it holds future promise. Finally, Demian and Waters (), Hartman et al. (), and Linderoth () argued that the temporary nature of construction projects create challenges when BIM use diffuses to consecutive projects.Thus, previous research focused on identifying the benefits of BIM use and also identified constraints and questions as to whether the claimed benefits were achieved. However, in order to enable BIM to have a transformative effect on the industry, it is necessary for the use of BIM to diffuse in an encompassing and integrated use among actors in the construction process. The scope of BIM use and its application is not studied to any lager extent. Scattered success stories from single projects and a diffused discourse suggest that technical consultants and larger contractors use BIM. However, previous studies did not examine BIM use among medium-sized contractors with 50\u2013500 employees. It is interesting to study this group because these firms can successfully compete with larger contractors in projects up to 50 million Euros although they lack resources for in-house research and development when compared with larger contractors. It is important to examine the manner in which competitive capabilities of medium-sized companies change in a case where it becomes mandatory to deliver a BIM-model as a part of the contract. Thus, the issue pertains to the extent of BIM use in the companies, the purposes for which BIM is used, and the perceived facilitators and constraints. Hence, the aim of the paper is to explore current use and perceived constraints and driving forces of BIM-implementation with respect to mid-sized contractors. A mixed method approach was used. First, interviews with mangers in medium size contractors were conducted, and this was followed by a survey that was administered to Swedish contractors with 50\u2013500 employees.A mixed method approach was considered appropriate, as the present study constitutes an explorative study. That is, the data was collected by both qualitative methods (interviews and cases) and a quantitative method based on a survey. In order to gain insights with respect to the benefits and barriers of BIM use for medium-sized contractor firms, seven in-depth interviews involving representatives of medium-sized contractor firms were conducted. The interviews involved questions related to the use of BIM in the firm, the manner in which BIM was used, and the types of benefits and barriers involved in BIM use. All the interviews were taped, transcribed, and coded thematically. Two CEOs, three construction managers, and three site managers were interviewed. The selection of representatives made it possible to cover multiple perspectives concerning BIM use in medium-sized contractor firms. The interview results in combination with extant research were used to develop the survey questions.The quantitative data used in this study was collected through a telephone survey administered to CEOs of mid-sized contractor firms in Sweden. The target population for the study involved mid-sized contractor firms, with 50 to 500 employees. Firms belonging to this group were first identified through the membership directory of the Swedish Construction Federation (the trade association for private construction companies in Sweden). Second, a search based on industry codes in the Retriever Business (a database containing financial information on every limited liability firm in Sweden) was performed. Following a manual screening of the list of firms, the firms that had terminated or could not be seen as a contractor or by any other reason did not belong to the target population were removed, the total population consisted of 104 contractor firms.A preliminary version of the questionnaire was tested with potential respondents and also discussed in the reference group. The final questionnaire was then controlled and scripted by the Kantar Sifo, who also collected the data based on telephone interviews with the respondents. Kantar Sifo is one of Sweden\u2019s largest and most respectable marketing research companies (). The use of an external professional research firm improved validity and reliability through several levels of quality control in the data collection process. To increase the validity of our measures the questionnaire was re-verified with respect to language and ease of understanding the questions by Kantar Sifo. Inter-rater reliability was increased by using randomly selected and experienced professional callers. The interview process was monitored and recorded to ensure the consistency and quality of responses. This ensured that all problems in the manuscript (including misunderstandings) were captured in the monitoring process.In this section the results from the interview study are presented first. There after the results from the survey study are presented in order to gain a more fine grained understanding of the results from the interview study.A closer scrutiny of the BIM-use applications confirmed that the visualizing capabilities of BIM were evidently the most commonly used applications when compared to the analysis and simulation, co-ordination and communication, and data extraction and transfer capabilities. However, the visualizing capabilities could also be considered as a proxy for the co-ordination and communication capabilities, because the visualization capabilities could be used to facilitate co-ordination and communication. This was confirmed in the interviews to a certain extent wherein improved communication and problem solving - for example better communication among actors - were stated as two benefits of BIM use.The interviews elicited a wide array of technical, organizational, and environmental constraints. In contrast, driving forces were mentioned to a lesser extent. Conducting the survey made it possible to extract more fine grained knowledge with respect to the perceived constraints by analyzing the perceived strength of constraints and differences between users and non-users. There was a consensus among the interviewees that the major constraints for using BIM included lack of demand from clients, lack of knowledge, and cost versus benefits issue. The survey results indicated that the biggest perceived constrains involved the lack of any environmental pressure from clients, partners, and regulatory bodies. It was somewhat surprising that non-use among partners was perceived as a major obstacle because medium-sized contractors often co-operate with the same partners as larger contractors that use BIM. The question arises as to whether some of the partners worked with BIM or 3D-models, and this was either not communicated to the mid-sized contractors or they did not demand BIM. The lack of internal demand as a driving force for BIM use (see Table\u00a0) could indicate that the question as to whether or not partners use BIM was never raised. A response recorded during an interview indicated that this could in fact be the issue. A site manager was asked if they used BIM during the detailed design and he indicated that he did not think that BIM was used. However, a contract manager joined the interview later and stated that BIM was often used in the detailed design. This evidence of a lack of internal communication and information transfer in a company could be a strong indicator that contractors actually did not really know the extent to which BIM was used by their partners in the detailed design. Evidently, this is a topic for further research.When the survey results were scrutinized in detail and the user and non-user categories were separately analysed, a more complex picture emerged with respect to the perceived constraints and included details such as the significant differences between perceived constraints (Table\u00a0). Non-users perceived a lacking internal demand and a lacking client demand as the biggest obstacles. In a separate analysis of this data we could also see that there was a highly significant correlation between these two variables (with a Pearson correlation coefficient of .617 and -value <,000). Hence, it could be claimed that the lack of internal demand was a result of a lack of client demand. Conversely, these obstacles were not perceived as strong by the users when compared with the non-users (Table\u00a0). Instead, the users perceived a combination of organizational and technological factors as the biggest obstacle. The perception of the need for \u201chigh investments in hard- and software\u201d could be considered as a proxy for organizational resources (see Orlikowski and Iacono ) in conjunction with the fact that costs exceeded the benefits. It is necessary to investigate the perception of the need for \u201chigh investments in hardware and software\u201d. The major investments in \u201cBIM-equipment\u201d is made by technical consultants involved in the detailed design, and contractors who want to use the model for visualization purposes could download free software. The perception could be grounded in the opinion that it is necessary to upgrade the company\u2019s computer capacity if it would be possible to use BIM in a more encompassing and advanced manner. However, it was remarkable that learning cost in terms of time was not perceived as a big constraint (see Table\u00a0). In an interview, a managing director of a non-user company confirmed that the lack of client demand was the biggest constraint, but when clients began demanding BIM, it would be relatively easy for the company to accommodate the changing customer demands.However, among the users, the relative advantage of the technology factor was perceived as the strongest driving force. This was expressed by the perception that BIM is a means to follow the technological development and that BIM provides competitive advantages to the company. The non-users did not have these types of strong perceptions (Table\u00a0). In this sense, it could be claimed that the role of the lack of demand as a constraint diminished for users because the users perceived that they could gain competitive advantages by following the technological development. This assumption was supported by a strong correlation (600; -value <,01) between the two variables follow technological development and competitive advantage.As previously stated, advocates for BIM see it as a process that will have a significant impact on the industry (e.g., Eastman et al. ) and it is claimed that the client would be the greatest beneficiary (Olofsson et al. ). However, the process perspective was less obvious from the results of the study. The results indicated BIM was perceived foremost as a tool for visualization. However, the perception that BIM could provide the company with a competitive advantage contributed to a more multi-faceted view of BIM. Moreover, the question arises as to whether the low score with respect to the competitive advantage of non-users raised questions as to how these companies perceived the discussion of public clients for whom BIM would be a compulsory part of the project delivery. Finally, users and non-users had considerably different perceptions with respect to BIM. Users focused on the technology and the relative advantages of using BIM, while non-users focused on the (non-existing) environmental pressure. Thus, another topic for future research involves a study of the underlying reasons for the differences in the perceptions of BIM.The results indicated that more than half of medium-sized contractors in the sample used BIM in some projects. The use was limited foremost to utilizing the visualizing capabilities of BIM, but the visualization can also facilitate BIM coordination and communication among actors. An observation of the perceived constraints for BIM use (Tables\u00a0 and ) indicated a few significant observations. Statements that measured subjective norms (i.e., external pressure) were given the highest scores and statements that measured attitudes (i.e., how respondents perceived BIM) were awarded the lowest scores. For instance, the question arose as to whether the lack of internal or external demand was the main reason for non-users to use BIM. Although the respondents appeared to agree that BIM was generally user friendly and that the complexity of the model was not a major obstacle, there were significant differences between users and non-users in this aspect. Users to a much higher degree than the non-users of BIM believed that BIM-use required high investments in hardware and software.With respect to the main driving forces for BIM use, attitudes related to the technical development and achievement of competitive advantage were observed as the main forces. Norms (internal and external demands) were given the lowest scores in terms of a driving force. Several statements of respondents about attitudes and norms also showed significant differences between users and non-users. As expected, users generally valued driving forces much higher than non-users.The findings indicate the main obstacles for BIM-implementation is related to the lack of normative pressure while the main driving forces were primarily driven by the subjective positive or negative evaluation of BIM by individuals, instead of external pressure from clients and partners or with respect to the internal capacity and knowledge to use BIM."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0044-3", "title": "Resource-loaded piping spool fabrication scheduling: material-supply-driven optimization", "authors": ["Jing Liu", "Meimanat Soleimanifar", "Ming Lu"], "publication": "Visualization in Engineering", "publication_date": "11 April 2017", "abstract": "As offsite prefabrication and modular construction continue to gain momentum into the future, material supply chain becomes increasingly complex for modern construction projects. Pre-engineered material supply presents itself as a driver for planning crew installation operations on site that involve skilled labor and heavy equipment.", "full_text": "The construction industry is a vital component to the national economy for many countries. In Canada, 2015, construction industry accounted for 7.18% of the Gross Domestic Product (GDP) and provided 7.64% of the national employment (Statistics Canada ; Statistics Canada ). The construction industry in the province of Alberta is unique due to the high proportion of mass capital projects, extreme cold weather, labor shortage, and special contractual strategies. Over the past few decades, modular construction method has been widely implemented in lieu of the conventional stick-build method in developing industrial projects across the globe.However, the majority of these raw materials and pre-fabricated components are increasingly sourced from offsite or even offshore fabrication facilities. The balance between material supply and production plan needs to be delicately maintained in order to minimize the idling time of material and labors, and the project duration (Koskela et al. ). The concept of supply chain management (SCM) is to integrate all the activities related to the flow and transformation of products from raw material to end user, in order to better satisfy the needs of end-customers and benefit all members in the chain (Walsh et al. ). However, due to the broad variability in field productivity and the difficulty in predicting material demands by field activities, it is more challenging to achieve efficiency in supply chain management in the construction industry than in the manufacturing industry (Dubois and Gadde ; Tommelein et al. ). The complicated material supply chain in the construction industry generally makes just-in-time material delivery practically impossible. Song et al. () found that 20% of the spools in fabrication shops and 60% of equipment modules in module assembly yards experienced material shortages. Thomas and Sanvido () concluded that poor material management could cause 50 to 130% schedule slippage by conducting three case studies.The optimization of supply chain management in logistics or operation research (OR) aims to streamline the material flow by removing all the non-value adding processes and material wastes (Arbulu and Ballard ). The optimization objectives are generally set to minimize the inventory cost, or minimize buffer sizes and time of material waiting prior to being further processed (Tserng et al. ; Walsh et al. ; Xu et al. ). However, research in supply chain optimization in the context of construction planning has yet to address the project scheduling problem by (1) incorporating the supply of materials as explicit constraints alongside labor constraints and technology constraints; and (2) simultaneously achieving the efficiency of supply chain management and the efficiency of labor resource utilization through optimization.The purpose of this paper is to demonstrate the applicability of the developed material-supply-driven project planning and control scheme in mitigating the negative impact of material delays upon industrial project schedules, specifically in the context of piping spool fabrication. This paper is organized as follows: Literature reviews are conducted first on spool fabrication and module assembly in industrial sector along with potential problems present in the complex supply chain. In the subsequent section, a framework for scheduling piping spool fabrication under the constraints of material supply, contractual deadline, limited labor availability, and technology relationships is proposed. Next, a spool fabrication case study is conducted to demonstrate the effectiveness of the proposed framework in mitigating material delays and achieving the targeted delivery time for a particular spool.Where \n                         is the total number of paid worker-hours (WH);  is the total project duration;  is the total number of available labor resources. \n                         is the scheduled start time of activity \n                         and \n                         are the scheduled start time and duration of the immediate predecessor of activity ; _ is the total number of its immediate predecessors. \n                         is the set of activities which are in process (already started but not finished), or ready to be started at time ; \n                         is the amount of required resource  for activity ; J is the total number of resource types. __ is the total number of activities for fabricating spool ; \n                         is the preset deadline for spool k; the total duration for fabrication spool  is determined by the latest finish time of activities of spool .  is the set of activities which are finished, in process, or ready to be started at time , \n                         is the required amount of material  for activity .  is the cumulative supply amount of material  at time period  is the total number of material types.The mathematical model can be readily solved by using constraint programming engine software, which has been proved to be very powerful in industrial control applications (Fromherz et al. ). The constraint programming engine software integrates multiple techniques in operations research (OR), artificial intelligence (AI) and graph theory (Rossi et al. ) for solving complicated scheduling problems with efficiency and cost-effectiveness (Bockmayr and Hooker ). Constraint programming has also been employed in addressing construction scheduling problems of practical size and complexity. Liu and Wang () took advantage of constraint programming to enhance the computing efficiency for scheduling linear construction projects with multi-skilled crews. Menesi et al. () demonstrated the capability of constraint programming to handle the time-cost trade-off problem for large-scale projects involving thousands of activities. In this study,  () was utilized to implement constraint programming algorithm in search for the optimum solution to the mathematical model.A realistic case study is presented in the following section in order to validate the effectiveness of the proposed application framework for scheduling the spool fabrication based on the project material-supply-driven project planning and control optimization algorithm.To evaluate the effectiveness of the proposed method in solving the scheduling problem, computer execution times on the three scenarios was recorded based on a desktop with a 3.2-GHz CPU and 8\u00a0GB random-access memory (RAM). The computer execution time includes (1) time to generate the model, (2) time for the engine to solve the model and (3) time to display the results. The model of the case study includes 32 activities, 64 variables and 97 constraints. The execution time averaged over 100 independent runs for each scenario is as follows: 0.0311\u00a0s for generating the baseline schedule, 0.0377\u00a0s for generating the updated schedule with the shortest duration, and 0.0336\u00a0s for generating the updated schedule with the earliest finish time of Spool 3, respectively. The computing performance in optimizing schedules in the case study demonstrates the potential of scaling up the application of the proposed method in coping with material delays and making detailed resource-loaded crew-job schedules in a piping fabrication shop, which generally serves multiple projects and multiple clients simultaneously in real world settings.With more and more prefabricated materials and assemblies, the construction material supply chain becomes increasingly complicated and uncertain. Material delay is commonly encountered in spool fabrication shops serving the needs of multiple industrial construction projects. Therefore, this paper proposes an application framework for implementing material-supply driven project planning and control in order to improve the schedule performance of spool fabrication projects subject to time-dependent material supply information collected in material management systems. The application framework consists of four major components, which are (1) data sources, containing detailed design drawings, contract information, resources availability and time-dependent material supply patterns; (2) extracted information serving as constraints for optimizing spool fabrication schedules; (3) constraint programming optimization engine; and (4) two forms of output visualization, i.e., the interactive activity Gantt chart and the material supply-demand chart, which guide decision-making processes subject to material delays. Three sample spools from a real oil and gas expansion project in Alberta are selected to build a case study for demonstration and validation of the proposed application framework. In addition to the baseline schedule, two alternative schedules with different priorities in defining scheduling optimization objectives, namely, the shortest fabrication duration and the earliest finish time of the particular spool, are produced, lending effective decision support for practitioners in coping with material delays and making detailed resource-loaded crew-job schedules in a piping fabrication shop which generally serves multiple projects and multiple clients simultaneously."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0045-2", "title": "Impact of linear correlation on construction project performance using stochastic linear scheduling", "authors": ["Ricardo Eiris Pereira", "Ian Flood"], "publication": "Visualization in Engineering", "publication_date": "27 April 2017", "abstract": "In the construction industry, the productivity of all trades is directly impacted by uncertainty and variability. For repetitive projects, smooth work flow of productive resources is necessary to minimize or eliminate interruptions and idle time with the objective of reducing costs. An ideal or near optimal solution requires careful planning of the sequence, timing and resource allocations for each activity. Earlier research has demonstrated that uncertainty in the duration of repeated activities can have a significant impact on what is determined to be the optimum project plan. This suggests that correlation in the duration of repeated activities (where durations are stochastic) may also be important in determining the most favorable plan.", "full_text": "Many of the activities performed in construction are repetitive in nature. Activity repetition is most prevalent at a low level in a work breakdown, such as the cycling of equipment in an earthmoving operation or the laying of bricks, but it is also common at intermediate and high levels, such as the laying of utility lines or the construction of many similar floors in a high-rise building. Repetitive activities can be either discrete or continuous processes but most planning tools are limited to one or other perspective. The critical path method (CPM), for example, treats all activities as discrete units - this is convenient for activities that are inherently discrete, but activities that are continuous in nature (such as the operation of a tunnel boring machine) must be converted into a series of discrete units of work.Planning projects where there is significant repetition of activities becomes challenging using traditional activity network methods (such as CPM) because of the difficulty of ensuring continuity in resource utilization (Harris and Ioannou ) and the consequently large number of activities and dependencies that must be defined and maintained. As a result, alternative planning methodologies have been considered in construction such as the Linear Scheduling Method (LSM) which represents work as discrete-continuous activities plotted across space and time. LSM provides a visually insightful framework for representing activity progress and understanding how interactions between activities impact that progress.Regardless of the planning methodology adopted, modeling repetitive activities requires careful attention to ensure accuracy since a small error in the estimate of a single repetition translates to a large error over many repetitions. Moreover, effects such as learning and forgetting (Gates and Scarpa, ) in repetitive activities can be dramatic and if not properly addressed can lead to significant errors in the estimation of project performance. Uncertainty in activity performance must also be taken into account since it can significantly impact the accuracy of project performance estimates. Ignoring uncertainty (using a deterministic analysis) leads to optimistic estimates of project performance for concurrent interacting processes, the so-called . The PERT method is a relatively popular tool used for modeling uncertainty in construction schedules, but it only considers uncertainty along the deterministically derived critical path and therefore underestimates both project uncertainty and project duration. Consequently, the PERT method, while simple to use, is only suitable for projects that have a dominant critical path with a low probability of other paths becoming critical. Indeed, interactions between construction processes are usually sufficiently complicated that stochastic effects can only be modeled accurately using statistical sampling techniques, the most popular of which being the Monte Carlo method.Recent years have seen an interest in developing optimization and satisficing methods for planning repetitive construction work. Ioannou and Srisuwanrat (, , , ) proposed and evaluated a technique for planning a smooth work flow for productive resources operating in conditions of uncertainty, set within a linear scheduling (LSM) framework.Trofin () and Flood et al. () implemented a Monte Carlo analysis using the LSM framework to assess the impact of uncertainty on project duration, and activity idle time. It was shown that increasing the level of uncertainty not only increased the expected project duration but also changed the optimal schedule.Rachmat et al. () investigated stochastic simulation on repetitive projects to incorporate activity performance uncertainty in look-ahead scheduling. A case study was undertaken for the construction of a pipeline, where real data were collected in the field, fitted to a statistical distribution, and processed by a simulation package that took into account uncertainty using Monte Carlo sampling. The output from these models was a variable production rate linear schedule of all the activities that comprised the project. In this analysis it was concluded that including uncertainty on linear schedules improves the forecasting capability of project performance and thus helps a scheduler anticipate problem areas and formulate new plans that improve project performance.Processes that are naturally stochastic can also demonstrate correlation between the duration of repeated activities. Positive correlation means that if one activity (or repetition of an activity) takes longer than expected then the correlated activities (or repetitions of that activity) are also more likely to also take longer, and vice versa. Work on correlation between construction activities (repeated or otherwise) is minimal, but it is easy to demonstrate that positive correlation affects the statistical performance of a project by increasing kurtosis, meaning that more of the variance in the performance of a project results from occasional larger deviations as opposed to more frequent smaller deviations. An outstanding question, however, is whether the effects of correlation significantly impact the optimality of a plan. This paper reports on ongoing research into this question. It introduces the questions being investigated and their rationale, the proposed approach to resolving them, and the results from a series of experiments designed to assess the potential impact of correlation on project plan optimality. If correlation is found to impact plan optimality, then this will justify further work into the development and validation of more accurate models of correlation.The paper provides a review of the concept of activity correlation in . This is followed by a description of the modeling approach adopted in this study (), and the experimental plan (). The results and their analysis are then presented in , followed by a summary of the conclusions and recommendations for future work in the final section.The results of the experiments described above indicated that lower levels of correlation, between 0.00 and 0.80, did not show a significant impact on either  or . However, for higher levels of correlation both variables were found to increase geometrically. Therefore, an additional 1000 LSM scenarios were generated for each level of correlation ranging from k\u2009=\u20090.80 to 1.00 in increments of 0.025, to provide a higher resolution in the results for the region where performance was found to change most dramatically.The impact of correlation between activities on the performance of construction projects is not well understood. Moreover, existing models of correlation are limited in sophistication and largely untested in terms of their accuracy. Before investing resources in the development of more appropriate models of correlation for construction it was decided to first test whether correlation may affect project performance significantly. Specifically, this study had the goal of determining whether the optimality of a project plan is prone to disruption by unaccounted correlation. Project performance was assessed in terms of two optimality indicators:  and project . The results showed that both performance indicators are significantly impacted if the level of correlation is high (between k\u2009=\u20090.8 and k\u2009=\u20091.0), in the worst case having an expected crew idle time of 7% of crew active time and an expected extension to the project duration of 12%. These results are applicable to problems that fall within the scope of this study.These results provide justification for investing resources in developing our understanding of how correlation can best be modeled in construction. These studies would, in part, be aimed at extending the study beyond the limitations listed in . For example, studies are required to determine whether correlation between the durations of construction activities should use linear or non-linear relationships, and whether correlation between repetitions of an activity is best modeled as a dependence between immediate repetitions or between the first and current repetition. Future research must also include real-world data from a comprehensive range of construction project types.Alternative indicators for project optimality should also be considered. In particular, there is a need to determine the extent to which the  and  could be reduced if an accurate assessment of the level of correlation was available and used to determine the optimal plan - the question is not straightforward as the systems performance is subject to stochastic variance. Finally, work is required to determine how different levels of uncertainty in the duration of an activity affects the relationship between the level of correlation and plan optimality."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0046-1", "title": "GIS-based visualization of integrated highway maintenance and construction planning: a case study of Fort Worth, Texas", "authors": ["Jojo France-Mensah", "William J. O\u2019Brien", "Nabeel Khwaja", "Loyl C. Bussell"], "publication": "Visualization in Engineering", "publication_date": "26 April 2017", "abstract": "This paper reports on a case study of the use of visualization of geospatial data that is distributed across data sets and requires integration over time and space to aid decision makers. Like many State Highway Agencies (SHAs) in the United States, the Texas Department of Transportation (TxDOT) is organized along the traditional functional lines of planning, design, construction, maintenance, and operations. It has historically relied on experience and longevity of its staff to efficiently and effectively plan its construction and maintenance projects. Although functional boundaries of maintenance and construction are fairly clearly defined, there tends to be some overlap in projects that can be executed by either of the functional groups. The department currently does not have a robust integrated information system for identifying potential planning conflicts between its construction and maintenance projects. This has led to suboptimal use of resources, including overlapping plans for maintenance and mobility enhancement projects.", "full_text": "Successful highway infrastructure planning and maintenance requires significant investments in terms of time, human resources, and money. Every year, billions of dollars are spent on maintaining highway infrastructure via new construction projects, road maintenance, and rehabilitation activities (Lee et al. ; Zhang et al. ). Increasing urbanization has led to a growing demand for highway infrastructure resulting in transportation systems becoming more complex in response to the demand (O\u2019Brien et al. ; Podgorski and Kockelman ). Consequently, the need to optimally allocate limited resources to maintain and improve the state of transportation infrastructure cannot be overemphasized. These factors among others, significantly affect public funds expenditure on highway infrastructure development, thus drawing increased public scrutiny to budget planning and funds allocation for highway infrastructure (Sanchez ).Today, the critical focus on and the need for efficient management practices in transportation planning and policy are underscored by key federal laws passed in the last three decades. These include the Surface Transportation and Uniform Relocation Assistance Act of 1987, the \u2018Intermodal Surface Transportation Efficiency Act of 1991 (ISTEA),\u2019 and the recent \u2018Moving Ahead for Progress in the 21 century Act (MAP-21).\u2019 These laws have exhibited an increasing emphasis on integrated management practices and efficient use of federal funds. They also highlighted the need to invest in transportation systems by looking at the issue from the perspective of economic, socio-cultural, technological, and sustainable systems. Central to these considerations, is a push towards the use of data to drive highway agencies in making more informed decisions concerning highway planning and management (Thill ).Such advancement notwithstanding, the transportation planning process is a continuous and arduous task involving data, models, and users. As part of the planning process, decision-makers need to use a vast collection of data and information to address a number of substantial issues like traffic management, construction scheduling, Right of Way (ROW) acquisition, public communication, and others (Nobrega and O\u2019Hara ; Sankaran et al. ; Woldesenbet et al. ). Highway agencies often collect a plethora of data and information about the nation\u2019s network of highways. The sources of such data vary widely and their forms range from drawings, pictures, maps, tables, text descriptions, to accounts of personal experience (Flintsch et al. ). However, highway agencies usually have to deal with fragmented databases, multiple incompatible models, redundant data acquisition efforts, and sub-optimal coordination between the various agencies or departments operating on the same highway facilities (Chi et al. ; Ziliaskopoulos and Waller ). To compound this problem, multiple independent legacy information systems usually co-exist within the same agency (Chi et al. ; Thill ).In spite of huge investments in data collection and archiving efforts, the amount of information and knowledge generated from data repositories are minimal and less supportive of informed decision making. Additionally, both practitioners and researchers have questioned the efficiency of data programs in meeting the needs of users for highway infrastructure planning purposes (Flintsch and Bryant ; Woldesenbet et al. ). Transportation professionals still face the onerous task of organizing highway data into suitable forms to support decisions concerning highway maintenance, rehabilitation, traffic control, highway monitoring, and projects prioritization. These issues have given rise to a surge in the demand for effective practices and tools that can integrate, manage, and analyze highway data (Parida and Aggarwal ).Over the past two decades, many State Departments of Transportation (DOTs) have explored the use of digital information systems for highway management decision support (Kang et al. ; Lee et al. ). Accordingly, TxDOT relies on several information systems; these include but are not limited to the Pavement Management Information System (PMIS), a Maintenance Management Information System (MMIS) referred to as COMPASS, and the Design and Construction Information System (DCIS). The challenges associated with accessing and leveraging data from multiple information sources highlights the need for an integrated system that can fuse projects data and support applications to aid Maintenance and Rehabilitation (M&R) planning. While TxDOT has made progress to integrate some highway data, there is a lack of an automated process to visualize and integrate data from the individual information systems to better support highway project planning decisions.The need for such a system has grown for metropolitan districts since they have significantly more lane-miles of on-system highways under their responsibility and consequently more projects in various phases of development and delivery at any given time. The funding mechanism for maintaining, rehabilitating and upgrading the existing system is complex. It has become further complicated since TxDOT\u2019s funding is dependent on revenue from multiple sources with different permissible uses. Moreover, the planning process is fiscally-constrained at the category level; the amount of funding available determines the number of projects that can be planned within specific categories. Metropolitan planning organizations\u2019 (MPO) policy boards have responsibility for certain funding categories requiring concurrence from TxDOT. In addition, at any given time, several projects are in various phases of construction. The actual cost of construction can vary from the budgeted costs that affect category funds available going forward. Construction costs can vary from budgeted costs at the time of bidding or throughout the construction phase owing to change orders, unexpected conflicts needing additional right-of-way, costs to relocated existing utilities, and many others. Furthermore, there are instances when existing roadways that were not expected to be rehabilitated within the planning horizon, have to be rehabilitated owing to faster deterioration in condition. This leads to reactive maintenance to maintain safety and pushing lesser priority projects down the list. The combined effect of these factors (and many more) creates a need for an integrated planning process leveraging modern visualization tools which will allow the integration of temporal and spatial data which can be viewed, reviewed, and updated in a dynamic setting.The rest of the paper is organized as follows. The next section describes the general challenges in performing highway planning tasks and how GIS has been previously used to address some of the identified challenges. In the \u201cObjective and Methodology\u201d section, a research framework based on a case study is presented. Following this, the \u201cCase Study\u201d section points out district-specific barriers to planning tasks and how a GIS-based tool was developed to integrate data from multiple information systems used by the district. A formalized presentation of the benefits of GIS integration and visualization to M&R planning follows. Finally, the paper ends with conclusions on the findings of this study and the directions for future work in the \u201cConclusions\u201d section.NCHRP studies (Neumann ; Systematics ) also reiterated the presence of these problems across many DOTs in the nation. According to NCHRP Report 798 (D\u2019Ignazio et al. ), planners need to project long-term transportation needs, identify funding strategies and sources, and optimize resources available to attain the best value for highway investments. In conducting lifecycle planning for transportation assets, they also need to incorporate maintenance operations while planning for new capital construction projects. This process involves having functional groups within the same highway agency working across silos to comprehend and integrate agency-wide thinking; the key to this process being information flow.Highway infrastructure problems in many cases involve spatial relationships between objects and events. Accordingly, visualization of highway data has been expedited by current advances in information technology and data collection technologies (Khattak and Shamayleh ). Given the spatial characteristics of road network data, the rational way to store and use such data should be via a consistent spatial referencing system such as a GIS (Medina et al. ). GIS is primarily used for storing, querying, analyzing, visualizing, and interpreting geographic data. It helps to reveal patterns and trends of objects that relate to one another in space and time (Ford et al. ).With GIS, a user can integrate information from various sources and spatially connect that information to study aspects of an infrastructure system that was hitherto unapparent. By leveraging the capabilities of GIS, many highway agencies today are integrating highway data to improve highway operations and planning tasks (Flintsch et al. ). Transportation agencies are using GIS tools to collect data, communicate information about asset condition and needs, select and prioritize treatments, plan and manage work, and aid with disaster recovery (Rasdorf et al. ). In most cases, however, these applications are limited to collecting and displaying data and involves limited use of targeted spatial analysis functions (Hall ). Over the past two decades, several research efforts have demonstrated the applicability and value-added impact of GIS to highway pavement management and highway planning (Flintsch et al. ; Hall ; Sanchez ). Many other research efforts have explored this area, emphasizing the integration of decision-support tools and methods (Beiler and Treat ; Parida and Aggarwal ; Zhang et al. ; Zhou et al. ).There are two components to the objective of this paper. The first involves documenting the existing M&R project planning procedures and identifying planning challenges that exist within districts. The second involves integrating projects data from disparate sources in a GIS environment to address information needs of highway planners and other functional groups within the district.The initial part of the study involves an extensive literature review and input from highway agency staff on the planning challenges that they face. The agency staff who contributed to this phase included highway professionals who have worked in the construction and maintenance planning roles for at least a decade. After identifying the challenges faced, the research team focused on addressing a technological challenge that could aid in also addressing some of the organizational challenges. A data integration approach was proposed and a GIS-based tool was built to demonstrate the practicality of the framework. Finally, based on input from agency staff, researchers identified the practical benefits of this tool to M&R project planning.The \u2018 highway network excludes county roads, local city streets, and functionally classified city streets, which the district office is usually not responsible for. While there are instances where the district collaborates with city agencies to maintain city streets, the projects involved usually remain under the purview of the city\u2019s planning agency. Also, the user interface mainly includes a \u201cQuick Access\u201d toolbar, \u201cCustom Functions\u201d toolbar, and feature layers displayed to the left of the screen (default in ArcMap application of ESRI\u2019s ArcGIS Software).This case study highlights how GIS technology can improve, through visualization, the decision-making process for planning highway projects. The primary functions of GIS in this regard include among others, the ability to integrate data from heterogeneous highway data sources and visually displaying projects in a way which is more useful for agency staff. GIS also affords highway agencies the opportunity to execute both spatial queries and conflict analyses that would, hitherto, have been difficult to achieve in a solely database environment. The benefits enumerated below present ways in which GIS was, and can be used to support highway project planning.The maintenance and rehabilitation (M&R) planning process is complex; multiple sources of funding with restrictions, different planning periods, and multiple stakeholders within and outside the DOT. For many DOTs \u2014 particularly with rapidly growing urban and metro areas \u2014 the process is challenging to manage with existing toolsets and databases. This study reports on the use of a GIS tool to integrate disparate data sources into a single, visual interface. The visualization tool makes identification of problems and opportunities more straightforward by; shortening the planning time and increasing the agency\u2019s abilities to make better use of limited funding.The merits of GIS include valuable visualization, contextualization of information, and integrated database management. This study has evaluated the benefits associated with integrating highway projects data with GIS. It indicates that such integration efforts could be used to better support construction and maintenance operations; improve the planning process; save time and resources, and ensure that accurate information is available to different functional groups within the department.In this case study, the automation of data processing and geoprocessing of projects data was complicated by a myriad of data quality issues; including missing data, inaccurate data, asymmetric data granularity, and semantic interoperability issues. More work is needed to delineate practical data integration challenges with GIS and to propose recommendations for improved data management practices for the district. Additionally, future work may also include the integration of highway conditions data, accidents data, and traffic information to provide a central source of information for the district to aid in project planning.The authors recognize that every DOT is different and the highway planning practices may differ from one agency to another. However, this paper presented practical issues that are familiar to most DOTs and highway agencies in the domain of highway project planning. The findings of this study highlight an opportunity for highway agency staff to take advantage of GIS to improve integrated planning of highway infrastructure. The key to this process is the need for DOTs to, first, identify existing challenges with their current planning process and, second, employ GIS where necessary to address data-related issues. As highlighted by this case study, the opportunities for improvement are substantial and warrant investment in new tools and supporting processes."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0048-z", "title": "Reasoning about accessibility for disabled using building graph models based on BIM/IFC", "authors": ["Barbara Strug", "Gra\u017cyna \u015alusarczyk"], "publication": "Visualization in Engineering", "publication_date": "15 June 2017", "abstract": "Building Information Modelling (BIM) is becoming a standard in Architecture, Building and Construction (ABC) industries. It provides a method for a unified representation of information about building throughout its lifecycle \u2013 from the design phase to the building maintenance. At the design phase the focus is usually on fulfilling the requirements and applicable norms and standards. One of the issues that is considered to be very important deals with the accessibility of buildings. While the design tools usually support the legal requirements for accessibility for disabled persons, little effort has been observed to address the quality of the access routes in terms of time, length and convenience of the route to be taken.", "full_text": "Architectural building designs are contemporary created with the use of CAD tools. BIM technology used for CAD applications enables to represent syntactic and semantic building information with respect to the entire life cycle of designed objects. The 3D object model is created using such elements as parameterized walls, ceilings, roofs, windows or doors (Eastman et al., ). The file format IFC (buildingSMART, ) is an interoperable BIM standard for CAD applications, which supports a full range of data exchange among different disciplines and heterogeneous applications. Information retrieved from IFC files is used in applications estimating construction cost for tendering in China (Ma et al., ) managing construction sites (Hu & Zhang, ) or evaluating design solutions (Jeong & Ban, ).IFC specifies virtual representations of building objects as well as their attributes and relationships. It includes most types of geometry, supports many classes of attributes (Eastman, ). Although IFC is an open standard, its complex nature makes information retrieval from an IFC model difficult (Zhang & Issa, ). The richness and redundancy of the full IFC model constitute an obstacle for particular use cases, as it cannot be fully supported by many specialized applications, and useful IFC subsets should be identified. Model View Definition (MVD), the Information Delivery Manual (IDM), and Semantic Exchange Modules (SEM) concepts have been introduced to efficiently reduce the scope of the full IFC model (Zhang et al., ). There is the on-going research on semantically precise and reusable model views and their implementations in real projects.At the building design phase the focus is usually on fulfilling the requirements and applicable norms and standards (Vreeker et al., ; Trinius & Sj\u00f6str\u00f6m, ). One of the issues that is considered to be very important deals with the accessibility of buildings (Bright & Giulio, ). While the design tools usually support the legal requirements for accessibility for disabled persons (Iwarsson & Stahl, ; Sakkas & Perez, ), little effort has been observed to address the quality of the access routes in terms of time, length and comfort of the route to be taken (Andrade et al., ; Ozer & Sener ). There is also a lot of research related to searching for routes in respect to evacuation from buildings in case of a disaster (Cepolina, ; AlShboul et al., ; Papinigis et al., ; Yatim, ; Ronchi & Nilsson, ).In this paper the problem of searching for the best route for people moving around in wheelchairs in public buildings is considered. The presented approach differs from methods described in the above mentioned works as our procedure does not search for the quickest/shortest way to a secure or appointed area (Zarrinmehr et al., ) but for the path first of all accessible, and as convenient as possible. Therefore in finding optimum routes for disabled such elements as existence of ramps, door types and door opening directions are considered.In this paper reasoning about accessibility of selected rooms is based on building graph models extracted from Building Information Modelling (BIM) process. In (Pu & Zlatanova, ) a concept for evacuation route calculation based on 3D geometrical/topological models of buildings is described. In (R\u00fcppel et al., ) an approach to model emergency situations in buildings based on BIM is described. However the process of generating graph networks out of BIM, on which route calculation can be performed, is difficult as it consists of merging several separately generated graphs. Therefore the proposed approach offers an effective way of creating a graph model of the considered building on the basis of the information retrieved from IFC files.The proposed application is aimed to aid people to decide about the routes they should take using mobile devices. The application extracts the information about the building from the external IFC file and searches for best routes to a chosen room starting from all possible building entrances.The proposed interactive application offers intelligent knowledge-based navigation, which is based on the graph representation of the building and its features. The approach described above was implemented and tested on example IFC files for the part of the office building shown in Fig.\u00a0. The given IFC file is converted into an internal representation in the form of the graph. The application searches for best routes to a chosen destination room starting from all possible building entrances.The standard algorithms used to search for shortest paths in building layouts return results which are less costly, but the obtained routes are often not accessible by disabled or too difficult to pass. The shortest route found by the standard algorithm, i.e., without taking into account semantic information related to passing through doors and stairs, from the  to  leads through ,  and . However this route is not accessible for people moving on wheelchairs as the stairs between the two halls are not equipped with a ramp.As mentioned earlier, the designers are well aware of the need to provide accessible routes for disabled persons. At the same time there is no requirement to make these routes comfortable or easy. The applications used by designers do not provide any visual clue to how the most comfortable or easy access for a disabled person would actually look like. To help designers solve this problem we designed a prototype of a user interface that if incorporated into design software could increase the awareness of the problem among designers.The prototype of the interface has been implemented using the Axure prototyping tool which allows to generate an interactive and responsive interface which can be run as the web application within the application of choice. For this paper the prototype has been run within the Firefox application to test it.The proposed method allows us to include semantic knowledge about IFC entities in the graph representation of floor layouts. The information about existence of stair ramps, existence and style of doors between spaces, and the direction of their opening is very important in the process of searching for routes both accessible and easy to be traversed. The information about the distances which should be covered between different entry/leaving points in each space is also used. The described application not only helps the designer to include special requirements during the design process but also offers the knowledge-based intelligent navigation which proposes the best routes connecting selected places and comfortable for disabled persons.This paper describes our prototypical application which aids disabled people to find accessible routes to selected rooms using mobile devices. The information about the building layout is extracted from the IFC file and stored in the graph representation together with information about the accessibility between spaces. As for people moving around on wheelchairs easiness of passing a route between two places can depend on the route direction, the directed graphs are used to represent layouts.The paper contributes to the field of computational methods applied to searching for routes inside buildings. It proposes a methodology based on extracting topological and semantic information from IFC files describing building layouts. Building-specific knowledge is used to search for accessible routs by means of the modified one-source minimal-cost search algorithm, where costs of passing through different spaces and between them is considered. In our method we assume that the input is in the form of the IFC compliant STEP (Standard for Exchange of Product Model Data) data file (ISO 10303\u201321, ). Although BIM tools produce data in various formats, an open standard IFC data model developed by the International Alliance for Interoperability allows the building information to be exported from these tools to the standard IFC compliant format (Dhillon et al., ). Many systems can implement converters between the IFC schema and their native data models to export and import IFC instances to exchange information with each other (Zhang et al. ).Moreover, in our approach the information about style and types of doors (left, right) and the existence of stair ramps is indispensable. It is assumed that during the design phase the designer has properly modelled such information into building models before exporting them to the IFC format.One of the practical challenges of the proposed method is determining the coefficients which are assigned as cost of opening doors in the lookup tables. Up to now we assume that this cost corresponds to the distance which would be covered if the doors were opened automatically in the time interval needed to open them manually. Moreover the application should adapt itself to dynamical changes in the building. For example it should take into account the situation when one of the doors are temporarily out of order.In future we intend to give the possibility of adding more elements which can be treated as different obstacles in moving around the building. The graph representation of information extracted from IFC files will be also used to search for accessible egress routes in case of emergency."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0047-0", "title": "Rear-screen and kinesthetic vision 3D manipulator", "authors": ["Chao-Chung Yang", "Shih-Chung Jessy Kang", "Hsiang-Wen Yang", "Tzong-Hann Wu"], "publication": "Visualization in Engineering", "publication_date": "15 June 2017", "abstract": "The effective 3D manipulation, comprehension, and control of 3D objects on computers are well-established lasting problems, which include a display aspect, a control aspect, and a spatial coupling between control input and visual output aspect, which is a debatable issue. Most existing control interfaces are located in front of the display. This requires users to imagine that manipulated objects that are actually behind the display exist in front of the display.", "full_text": "3D computer graphics technology allows people to display 3D models on computers. As the technology advances, it has become widely used in various industries including animation, gaming, and computer-aided design. However, the limitations of display and control devices still introduce difficulties when comprehending and interacting with 3D models. Further, the spatial coupling between a perceived visual location and a manipulating location of models is still a debatable issue.The first issue is the two-dimensional limitation of display devices. Although models are in three dimensions, it still takes efforts to present them stereoscopically. To make models \u201cpop out\u201d of screens, 3D viewers commonly use the technique of presenting two offset images separately in different eyes, requiring extra head-worn devices (Eckmann ). Another way to enhance stereoscopic perception is by using \u201cmotion parallax\u201d effects, which is the relative displacement of viewed models by changing observers\u2019 positions (Rogers and Graham ). On the other hand, Projection Augmented Model utilized a physical model, which is projected with computer images. This method present 3D models in a realistic looking. However, the pre-defined geometry shape and high precision of objects tracking and projecting is required (Raskar et al. ).The second issue is the limitation of control devices. Dominant 2D input devices, which allow fine control of two-dimensional motion, are inappropriate for 3D manipulating due to the limited number of degrees-of-freedom (DoF). As a result, a mouse with virtual controllers for 3D manipulating has been discussed and evaluated in conjunction in several previous studies (Chen et al. ) (Khan et al. ). To overcome the limited DoF, controllers with three or more DoF are also developed for enhancing usability in 3D interactions (Hand ).The last issue is coupling between control input and visual output spaces. Humans process visual cues received from eyes and proprioception from hands guide the movements of hands to reach and grasp models; this is called eye-hand coordination (Johansson et al. ). Good eye-hand coordination can reduce the mental burden during manipulation. However, most motion controllers decouple the perceived visual space (which is behind the display) and interactive space of models in front of the display (so called \u201cfront-screen\u201d in the following chapters). Some people consider that, although this method follows the usual method of computer use, it may separate eye-hand coordination. Users\u2019 brains need to make a semi-permanent adjustment of the spatial coupling between these spaces (Groen and Werkhoven ). This adaptation leads to negative after-effects of eye-hand coordination (Bedford ). To discuss these issues, some related works about spatial coupling problems are reviewed in the next section.Figure\u00a0. Grab time for Front-Screen Interaction (FI), Rear-Screen Interaction (RI) and Rear-Screen Interaction with Kinesthetic Vision (RIK) Variants of the rear-screen and kinesthetic vision 3D manipulator. Error bars represent +/- SEM (Standard Errors of the Mean.)Figure\u00a0 and . Coordination ratios across the Front-Screen Interaction (FI), Rear-Screen Interaction (RI) and Rear-Screen Interaction with Kinesthetic Vision (RIK) Variants of the rear-screen and kinesthetic vision 3D manipulator: (a) Coordination in all directions; (b) Coordination in the Z-direction. (Error bars represent +/- SEM.)We propose a rear-screen and kinesthetic vision 3D manipulator, which is a novel 3D object manipulation method with a simple setup. Users are allowed to interact with a virtual object directly behind the screen. The components of the rear-screen and kinesthetic vision 3D manipulator are described and implemented in this research. Finally, experiments are conducted to evaluate the design.The experimental results show there is a significant difference in coordination in the z-direction between FI, RI and RIK. Therefore, objects whose trajectory is in the depth direction are more efficiently manipulated using the rear-screen and kinesthetic vision 3D manipulator than using the standard setup. In general term, the kinesthetic sense improves users\u2019 depth perception. The finding shows the possibility and value of installing sensors for use in the design review and gaming domains."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0049-y", "title": "Stochastic forecasting of project streams for construction project portfolio management", "authors": ["Alireza Shojaei", "Ian Flood"], "publication": "Visualization in Engineering", "publication_date": "19 June 2017", "abstract": "Construction companies typically work on many projects simultaneously each with its own objectives and resource demands. Consequently, a key managerial function is to allocate financial, equipment, and human resources between these concurrent projects in a way that satisfies the individual project constraints while optimizing the company\u2019s overall objectives.", "full_text": "The focus of research so far has been the selection and prioritization of projects among a pool of known projects, ignoring the opportunities and needs of unknown future projects. It is by definition a short term planning strategy and has no guarantee of satisfying a company\u2019s longer term goals. The current horizon of strategic planning which covers selecting the projects for bidding and planning for their contractual needs and necessary resources for execution are limited to the extent of advertised projects in the market.In the proposed approach, it is argued that future unknown projects can be represented statistically and by bringing those into the strategic planning process companies can devise more appropriate medium and long term strategies. In the construction context, where projects are advertised by clients and won by companies by bidding for them or other procurement methods, it is fundamentally wrong to use a company\u2019s past and current portfolio to predict unknown future projects. The correct approach is to use the historical data from the market to forecast upcoming projects (all the available projects in the future).A preliminary study is underway developing, validating and testing a project stream generator for design-bid-build highway construction projects let by the Florida Department of Transportation (FDOT). The outputs from the generator are those parameters most critical to a company, namely the occurrence and letting date of a project, its expected duration, and its expected cost.The first step is modeling the main variables through univariate modeling methods such as autoregressive modeling (AR), moving averages (MA), autoregressive moving average (ARMA), and exponential smoothing. More sophisticated approaches such as artificial neural networks can also be implemented considering the availability of the necessary data size. After establishing a benchmark, potentially relevant predictors were identified to populate a pool of candidate independent variables based on a literature review and cognitive theories. This brings in the environmental uncertainties into the forecast with the aim of improving the accuracy of the simulation. These variables will not necessarily have a causal relationship with the main variables; the only concern here is to be helpful in forecasting the dependent variable. This paper reports the results of this research up to this stage.The second step is exploratory data analysis. It starts with a graphical comparison of the independent and dependent variables such as scatterplots of pairs of variables. Relevant test include Pearson correlation, unit root (stationary or non-stationary test), Granger causality (helpful for short term forecasting), and cointegration (suitable for long term forecasting) tests.The last step is to choose a set of multivariate modeling approaches based on the results of the exploratory data analysis and investigate whether including explanatory variables and models that are more complex can improve the accuracy of the forecast. The range of the models should test for linear and non-linear relationships based on the results of the previous step along with variable selection (pruning), parameter optimization and finding the appropriate lag between variables. The authors suggest using two different variable selection methods and comparing the results to provide further explanatory insight into each variable\u2019s importance. The first method is univariate feature selection using the Granger causality test as an identifier of the appropriate explanatory variables. The second and more robust method is using recursive feature elimination with a greedy optimization algorithm. This iterative method builds models and separates best and worst variables at each step. This process continues until all the variables have been considered. The result is the ranking of the variables based on their order of elimination. It is crucial to embed a cross validation method within the recursive variable selection method to avoid overfitting.This paper has outlined an extension to PPM that allows future streams of both known and unknown (but statistically quantifiable) projects to be taken into account in the strategic planning of a project portfolio. On-going work is concerned with developing, validating and testing a project stream generator based on FDOT historic data. This generator will work stochastically, producing samples of streams of future FDOT projects (in terms of time of occurrence, expected duration and expected cost) based on historical data and economic indicators. The next step of the project will be to combine the generator of streams of future projects with a sample portfolio to show the model capabilities and effectiveness. The complete framework will give users the opportunity to try different bidding and project selection strategies to see how these affect their future resource demands so they can plan ahead and find an optimal strategy and optimal resource distribution for the future. An ability to accurately predict future project streams, and to take into account uncertainties in these streams, may help reduce the extent of the continuous adjustments required to a company\u2019s portfolio plan, as well as help optimize the selection and management of the portfolio. The types of resources that may be considered are anything required by a business to conduct its operations, including human resources, financing, equipment, and production facilities."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0052-3", "title": "Crowdsourcing BIM-guided collection of construction material library from site photologs", "authors": ["Kevin Han", "Mani Golparvar-Fard"], "publication": "Visualization in Engineering", "publication_date": "14 July 2017", "abstract": "With advances in technologies that enabled massive visual data collection and BIM, the AEC industry now has an unprecedented amount of visual data (e.g., images and videos) and BIMs. One of the past efforts to leverage these data includes the Construction Material Library (CML) that was created for inferring construction progress by automatically detecting construction materials. CML has a limited number of construction material classes because it is merely impossible for an individual or a group of researchers to collect all possible variations of construction materials.", "full_text": "To fully leverage these visual data, researchers have worked on developing data analytics that could potentially automate construction performance monitoring - progress, quality, and safety (Bosch\u00e9 et al. , Han et al. , Kim et al. ,, ). For progress detection, Han and Golparvar-Fard () proposed an appearance-based progress detection based on a machine learning method proposed by Dimitrov and Golparvar-Fard (). As part of these efforts, the Construction Material Library (CML) consisting of over 3000 construction images and 22 material categories (denoted as classes) were collected and used as a training dataset for the proposed machine learning method. These were enough dataset to be applied to small scale case studies (the buildings with the similar architectural style, such as brick exterior walls () and progress detection of concrete structures ()). However, there are many more construction materials to be added to CML to be practically useful. The purpose of CML is to evolve over time by more and more researchers contributing to creating a much larger dataset that can be shared and used as a benchmark. For that reason, it is shared online with the research community.However, it is merely impossible for a small group of researchers to collect images of every construction materials. Adding more images and classes to CML requires a series of tasks that are very labor intensive. First, someone has to a visit construction site (s) and take photos. Then, someone has to go through every image and annotate predetermined material classes manually. This physical constraint of limited human resources barred the authors and Dimitrov and Golparvar-Fard () from having more material classes and images.Therefore, this paper proposes a streamlined data collection process for cognitive computing research in the AEC industry by utilizing IIMs. Cognitive computing research in this paper refers to computer vision and machine learning research that require the training and testing phases. The authors intend to attract practitioners to share their project data and in return provide visualized IIMs online where they can access the IIMs via web browsers. The authors have created a web-based platform for managing IIMs and crowdsourcing collection of CML from the images in these IIMs. Amazon Mechanical Turk (MTurk) is used for creating annotations. This approach can easily outnumber 3000 images of the current CML and is more suitable for expanding the number of material classes.Moreover, by sharing it with the whole research community in the AEC industry, the whole research community can benefit from the dataset that is larger and has more material classes. The authors have observed a growing interest from researchers in applying computer vision techniques in the AEC domain. These researchers can also contribute by sharing their IIMs. They can also use the platform for various machine learning research projects. The platform provides a generic tool that allows users to annotate and label construction materials.A typical range of images used for image-based 3D reconstruction varies from a few hundreds to a few thousands. These visual data can add up to a significant size throughout the construction duration. For instance, Han and Golparvar-Fard () collected about 30,000 images (about 108 GB) in 11 months just using unmanned vehicles (both ground and aerial). As more practitioners and researchers collect visual data for creating IIMs to improve construction performance monitoring and management, there will be an abundance of visual data that are valuable to cognitive computing research for automating construction management processes. The main goal of this paper is partial automation in data processing and collection with minimal administrative efforts, enabling mass-generation of annotated visual data of construction materials, which can potentially advance cognitive computing within the domain of construction engineering and management.Over the past decade, the Computer Vision research community has experienced significant advances in object recognition with help of large image databases (Bell et al. ; Dana et al. ; Deng et al. ; Endres et al. ; Liu et al. ; Hu et al. ; Russell et al. ; Torralba et al. ; Xiao et al. ) to which the community had access. The researchers now have access to a vast amount of training and testing data. Many of these efforts are shared on their project websites and are used as benchmarks.These datasets are primarily for image classification of objects and materials. The datasets by (Deng et al. ; Endres et al. ; Russell et al. ; Torralba et al. ) contains large collections of objects and scenes with relatively smaller collections of materials. These are more suitable for studies on object recognition rather than material recognition.On the other hand, the datasets by (Bell et al. ; Hu et al. ; Liu et al. ,) focus primarily on material recognition and therefore consist of the collections of close-up photos of objects and scenes that are suitable for collecting annotated materials. Han, Dimitrov, and Golparvar-Fard (,) have worked on creating a dataset of construction materials (CML). Due to the limited resources, their work resulted in a relatively small numbers of material classes and patches. This paper presents a possible solution for further expanding CML with relatively fewer efforts on the researchers.Crowdsourcing, especially using MTurk, has gained popularity among image classification research projects. The work by (; ) have included tools that utilize MTurk for crowdsourcing of creating annotations. Similarly and more closely related to the AEC industry, Liu and Golparvar-Fard () have used MTurk for creating annotations of construction workers. MTurk enables quick annotations of a vast amount of images at relatively low cost. However, it still requires an admin person to manage web servers (e.g., interface for annotation, updating image sets), annotators, and quality of annotation.The main characteristics that differentiate the proposed work from the above-mentioned literature are: 1) focusing on construction materials, 2) using BIM to guide users with the areas to be annotated (i.e., automated BIM-based segmentation using IIMs), and 3) its adaptiveness to new classes through the last step in Fig. .To test and validate quality of annotations that are submitted by the annotators, a case study of three projects were created. For each project, three duplicate projects were generated to compare the accuracies of three different annotators on the same projects. This test also serves as a mean for improving the interface and formulating a quality control mechanism suitable for a sustained data collection effort. Therefore, the MTurk Sandbox was used. The Sandbox is a simulated environment for testing the generated HITs prior to actual deployment to the marketplace. The main benefit is that the interface can be tested and be improved before the deployment. Once HITs for the Sandbox is generated, the authors can directly hire annotators and share the URLs to the generated HITs. To simulate realistic cases, three non-construction annotators were chosen. They were assigned to work on the three projects. At the end of the testing, their feedback will be used for future improvement before actual deployment to the marketplace.Crowdsourcing CML for the research community in the AEC industry is presented. The presented approach will benefit from the practitioners and researchers interested in creating and using IIMs in practice and research. In return, the contributors will have access to their IIMs in a web-platform. Moreover, the AEC research community will have access to more construction material data that can be used for various machine learning related research.The current version uses BIM to guide users by annotating BIM elements visible on images. If the material information from IFC files are extracted, the developed platform can further guide users by providing a less number of material types to label. These features are not included in the current version because of inconsistencies created by different BIM tools and by different designers. Efforts like IFC Standards can potentially minimize and avoid these inconsistencies in the near future. Then, the second version will be updated to read the material information from IFC files.Moreover, an additional study on computing percentages of overlapping annotations with the same labels and discrepancies among annotators could be conducted. This study will enable the calculation of a confidence level and reliability of the labels per annotator and per material class.The proposed method is designed to grow CML in size (both number of annotations and classes) over time. The developed platform requires administrators (the authors in this case) who help contributors create IIMs, generate HITs, and review additional materials to be added to existing CML. Continuously maintaining this effort and limited resources to pay the MTurk users are the remaining challenges. The overall system can be improved to automate manual processes. However, dealing with paying monetary rewards to annotators and quality controls of annotations cannot or perhaps should not be automated.One possible solution would be deploying to the cloud (the platform is currently running in the authors\u2019 server) and having researchers who will be using this platform to contribute and add to CML along the way. The research community, in this case, will serve as administrators and annotators. Gamifying the platform could be a potential solution for motivating more people to contribute while reducing the cost associated with paying annotators. However, an obvious challenge of these solutions would be the cost associated with maintaining the cloud space as CML grows. Until securing the necessary funding to fully deploy the platform to the cloud, the developed platform will be continuously used and managed by the authors and the annotated projects will be publicly available upon request."},
{"url": "https://viejournal.springeropen.com/articles/10.1186/s40327-017-0053-2", "title": "A semi-automated approach to generate 4D/5D BIM models for evaluating different offshore oil and gas platform decommissioning options", "authors": ["Jack C. P. Cheng", "Yi Tan", "Yongze Song", "Xin Liu", "Xiangyu Wang"], "publication": "Visualization in Engineering", "publication_date": "14 July 2017", "abstract": "Offshore oil and gas platforms generally have a lifetime of 30 to 40\u00a0years, and platform decommissioning is a major issue because many of the existing offshore oil and gas platforms are reaching the end of their service life. There are many possible options for decommissioning offshore oil and gas platforms, and each decommissioning option can be implemented using different methods and technologies. Therefore, it is necessary to have a clear understanding and in-depth evaluation of each decommissioning option before commencing platform decommissioning. 4D and 5D building information modeling (BIM) has been commonly used in the building industry to analyze constructability and to evaluate different construction or demolition plans. However, application of BIM in the oil and gas industry, especially for the platform decommissioning process, is still limited.", "full_text": "Due to the excessive OOGP decommissioning in the next decade and many possible options can be used to decommission OOGPs, studying potential decommissioning options is greatly needed. In addition, OOGPs are usually located at sea, which requires a more detailed and accurate program for decommissioning compared with the platforms on the land. Therefore, having a better understanding of the selected OOGP decommissioning option can ensure a smooth process in decommissioning, resulting in reduced idle working time and a better control of the project.In the building industry, 4D building information modeling (BIM) has been used to simulate the construction or demolition plan. Using 4D BIM, different plans can be compared by visualizing the work sequences and duration of each task. Unreasonable logic in the sequence and task duration can be detected by professional project managers and engineers. Previous efforts have studied the application of 4D BIM for the monitoring of construction operations (Han & Golparvar-Fard ), for reconstruction (Kacprzyk & K\u0119pa ), for construction progress measurement (Hu et al. ; Kim et al. ), for workspace conflict visualization (Moon et al. ) and for safety analysis (Hu et al. ; Zhou et al. ). However, 4D BIM application for OOGP decommissioning is understudied. The advantages of 4D BIM have been shown from the building industry, almost no studies talked about 4D BIM application in OOGP decommissioning. The differences in 4D BIM application between OOGP decommissioning and the building industry may be the reason. For example, the components and structures are different in the two industries, and multi-trucks can be used in building industry to carry the removed elements, while only one heavy lift vessel is usually used during OOGP decommissioning, requiring carefully planning.In addition to the schedule, cost is another important concern for OOGP decommissioning. Equipment, materials, and labor are main resource costs in such projects. By incorporating resources, 4D BIM simulations turn into 5D BIM simulations, which can help understand resource utilization over time. 5D BIM has been studied and improved by a few research efforts (Lu et al. ; Mitchell ; Scheer et al. ). By using 5D BIM models for OOGP decommissioning, timing and costs can be better planned and monitored to make sure projects are on track. The objective of this study is to develop a new approach to create multiple 4D/5D BIM models in a semi-automated manner for evaluating various scenario options of OOGP decommissioning.This paper is organized as follows. Potential decommissioning options for OOGP will be first studied and summarized in  section. In  section, a proposed approach to create multiple 4D/5D BIM models in a semi-automated manner for evaluating OOGP decommissioning options will be proposed. An example will be presented in  section to illustrate the proposed approach, followed by the discussions and conclusions in  section.Based on the systematic literature review, all potential OOGP decommissioning options are divided into three categories: reuse, recycling, and disposal. Usually, reuse has the top priority considering the sustainability as its energy consumption is relatively small compared to recycling (Zawawi et al. ) and more environmentally friendly than disposal. An OOGP can choose one decommissioning option for the whole structure. For example, the platform can be transferred to a Liquid Natural Gas (LNG) terminal or an offshore hotel (Zawawi et al. ). In addition, different decommissioning options can also be applied to the different parts of an OOGP. For example, the topsides can be removed to shore for recycling, the jacket can be toppled down for an artificial reef, and the pipeline can be left in place for disposal after clearance.The choice of decommissioning options usually depends on the condition of OOGP. However, sometimes regional regulations may also impact the option decision. For example, in the North Sea, OSPAR Decision 98/3 requires any platform should be completely removed for further consideration, like reuse, if its jacket weight is less than 10,000 tons in air (). Therefore, the Rig-to-Reef program, which allows obsolete, nonproductive OOGP to be converted into artificial reefs to support marine habitats, has never been applied in the North Sea. On the contrary, many states in the United States like Texas, California, and Florida currently implement the Rig-to-Reef program for some of their platforms in the Gulf of Mexico, as well as the offshore southern California platforms in the near future.According to the summarized OOGP decommissioning options tree, there are 9 options when considering an OOGP as a whole, and a total of 168 potential options exist when considering an OOGP by sections. Therefore, many decommissioning options are available for an OOGP when it reaches its end of service life. In addition, each option can be realized by different methodologies. For example, when topsides is considered to be removed to land, it can be removed as a whole in a single lift by using huge lift vessel, or it can be removed using the reverse installation method, which removes the topsides in modules following the reverse process to its installation. As the working environment of OOGP decommissioning is more demanding compared to the related work onshore, it is important to evaluate different methodologies for the selected decommissioning option. Since 177 possible OOGP decommissioning options are available, the traditional 4D/5D BIM model creation method, which is usually manually conducted, is time consuming. Therefore, in this study, a semi-automated 4D/5D BIM model creation approach was proposed to reduce the model creation time.This paper presents and demonstrates a new approach to create multiple 4D/5D BIM models in a semi-automated manner for evaluating various scenario options of OOGP decommissioning. An OOGP 4D/5D BIM model relationship database that contains different decommissioning schedules (or schedules and resource) for OOGP parts such as topsides, jacket, and substructure. 3D BIM models and combined 4D/5D BIM model relationships can be automatically matched to generate 4D/5D BIM models for OOGP decommissioning options. With the created 4D/5D BIM models, Rig-to-Reef and Removal-to-Shore options and their methods can be clearly visualized, and the model creation time was reduced. The visualization provides a good reference for project managers to plan and execute OOGP decommissioning, and potentially unreasonable working sequences and lengthy activity durations can be detected and resolved. 5D BIM models of OOGP decommissioning can visualize and track resource utilization, helping the development of detailed resource plans. In addition, 5D BIM models can help monitor the cost of an OOGP decommissioning project.According to results of the illustrative example, the proposed new approach of semi-automated 4D/5D BIM model creation for OOGP decommissioning can facilitate a better understanding of different disassembly plans for offshore platforms, including the lifting sequence, resource utilization, and duration during decommissioning. Potentially unreasonable sequences in the decommissioning activities and unnecessary downtime can be detected during 4D/5D BIM simulations and comparisons. In addition, according to the finalized working plan, the resource utilization at different stages can be used as a reference for equipment, material and labor plan, which reduces the possibility of work shutdown as the offshore traffic is more demanding compared to that onshore. The simulation results can also be used as a benchmark for the decommissioning project control and as training materials for workers on the platform. Finally, the illustrative case shows that using the proposed semi-automated approach can reduce the 4D/5D BIM model generation time by 58.8% compared with the traditional method.However, the proposed approach also has some limitations. As mentioned in the illustrative example, the data schema for representing OOGP in BIM is still lacking. Currently, IFC, which is the main neutral file format for data exchange among AEC/facility management software, does not include OOGP entities such as integrated function modules, pipes, valves, and other elements on offshore platform. Using building elements to represent OOGP elements creates many problems (Cheng et al. ). The BIM model for 4D/5D simulations used in the illustrative example was not represented with defined IFC entities such as , , and , they were all represented with , which is used to represent entities that have not been defined in IFC. Surface model that consists of points in 3D environment is usually used by  to represent the undefined object. The lack of BIM applications of the oil and gas industry, especially offshore oil and gas platforms, may be one possible reason. Therefore, if the oil and gas industry wants to benefit from BIM technology, necessary efforts should be conducted on IFC extension to the oil and gas industry or new data models need to be developed in the future."},
{"url": "https://cancer-nano.springeropen.com/articles/10.1186/s12645-015-0009-y", "title": "Exploratory use of docetaxel loaded acid-prepared mesoporous spheres for the treatment of malignant melanoma", "authors": ["Sameer Kaiser", "Maximilian B MacPherson", "Ted A James", "Albert Emery", "Page Spiess", "Albert van der Vliet", "Christopher C Landry", "Arti Shukla"], "publication": "Cancer Nanotechnology", "publication_date": "24 January 2015", "abstract": "Five year survival for metastatic melanoma (MM) is very low at <10%. Therapeutic options have been limited secondary to systemic toxicity. As a result there has been a growing movement towards developing targeted drug delivery models. Prior research of this group has demonstrated the effectiveness of acid-prepared mesoporous spheres (APMS-TEG) in delivering chemotherapeutic agents at a lower effective dose than systemic administration. This study aims to assess the ability of the previously developed APMS-TEG particles to deliver therapeutic doses of docetaxel for the treatment of melanoma.", "full_text": "The incidence of melanoma has been steadily increasing over the past few decades. In the Caucasian population, the rate has tripled over the past 20\u00a0years []. Melanoma has become the 6 most common cancer in the US, with an estimated 76,250 new cases in 2012 []. The true incidence however, may be even higher given underreporting of outpatient managed cases to cancer registries []. While melanoma accounts for only 4% of all skin cancers, it is responsible for more than 74% of all skin cancer related deaths [].Five year survival for those with metastatic melanoma (MM), 2-5% of melanoma diagnoses, is very low at <10% [,]. Therapeutic options for metastatic disease have also been limited. The mainstays of adjuvant therapy consist of dacarbazine and high dose interleukin-2 [-]. Chemotherapeutic drugs examined have included docetaxel (DOC), paclitaxel, temozolomide (TMZ), vinorelbine, and others [-]. Despite the number of chemotherapeutics that have activity against melanoma, these agents have had limited clinical use due to their systemic toxicities. The past few years have seen the arrival of ipilimumab and vemurafenib, which have improved survival in metastatic melanoma [,]. However, even these agents have limitations in their applicability due to their toxicity profiles.As with other metastatic diseases, there has been a growing movement towards developing targeted drug delivery models in order to reduce systemic toxicity and enhance tumor toxicity. Docetaxel, which binds and stabilizes microtubules leading to mitotic arrest and eventually apoptosis, shows promise as an agent for targeted drug delivery due to its potent activity against melanoma. To date, there has been work from only two groups exploring the use of nanoparticles and nanomicelles to deliver docetaxel for MM [,]. Zheng, et al. created a polymer based nanoparticle as a delivery vehicle for DOC []. Though they show efficacy in killing melanoma cells, the particles are limited as they can only be administered via intratumoral injection. Ma, et al. describe preliminary work using nanomicelles loaded with chemotherapeutic agents, including DOC [].Nanoparticles show promise for drug delivery due to their small size, but carry the risk of systemic toxicity as well. By virtue of their size, these particles can enter and disrupt functioning of cellular organelles []. In order to address the issue of systemic toxicities, our prior research has focused on the development of a micro particle (1 \u2013 3\u00a0\u03bcm diameter) that can be utilized for targeted drug delivery due to its porous nature []. These acid-prepared mesoporous spheres tagged with tetra ethylene glycol (APMS-TEG or APMS) have been proven to deliver chemotherapeutic agents to tumor cells at a lower effective dose than systemic administration []. Additionally, these particles have been demonstrated to show minimal systemic toxicity [,]. This study aims to assess the ability of the previously developed APMS-TEG particles to deliver therapeutic doses of DOC for the treatment of melanoma.Our study demonstrated that DOC loaded APMS-TEG particles significantly reduce the growth of melanoma. The high potency against melanoma cells makes DOC a suitable choice for loading into APMS-TEG particles. In addition, scanning electron microscopy demonstrates that these particles are readily taken up by melanoma cells in a relatively short time frame.When we load APMS-TEG particles with docetaxel, we see clear evidence of dose dependent reduction in cell growth. The particles do not demonstrate saturation with drug at doses as high as 20 nM. We see peak release of drug within 1\u00a0hour and this is sustained thereafter. The fact that we do not see all of the drug released in the 20 nM loaded particle is potentially a result of particles reaching equilibrium with the media it is suspended in. However, further studies are required to fully elucidate the release kinetics of DOC from the particles.The toxicity profile of the APMS-TEG particles and mesoporous silica in general, is favorable when examining reviews of the existing literature. Nanoparticle formulations of mesoporous silica have been shown to be non-toxic with no evidence of inflammation in rat tendon, kidney, heart, and liver over 30\u00a0days after injection []. It can be safely administered subcutaneously, intravenously, and via intra-peritoneal routes with good bioavailability []. Mesoporous silica does not have carcinogenic potential []. These APMS-TEG particles have previously been shown to be non-immunogenic and non-toxic [].At this juncture, future directions for this model are twofold. The first goal is to move into animal models to validate  activity against melanoma cells. The second will be to characterize the pharmacokinetics and pharmacodynamics of the particle. This will include attaching functional moieties to the particle that will allow for targeted delivery of the particles to melanoma cells.APMS-TEG particles have previously demonstrated effectiveness at delivering chemotherapeutic drugs for other cancers, notably mesothelioma []. These early studies demonstrate that APMS-TEG particles can be used to deliver chemotherapeutic agents which could be used for the treatment of melanoma. Because these particles have been shown to be non-toxic and do not elicit an immune response, the problem of systemic toxicity associated with nanoparticle based delivery systems is reduced. Docetaxel loaded APMS-TEG particles demonstrate significant activity against malignant melanoma and thus offer a novel approach to the treatment of the disease."},
{"url": "https://cancer-nano.springeropen.com/articles/10.1186/s12645-016-0015-8", "title": "Nanotechnology and cancer: improving real-time monitoring and staging of bladder cancer with multimodal mesoporous silica nanoparticles", "authors": ["Sean K Sweeney", "Yi Luo", "Michael A O\u2019Donnell", "Jose Assouline"], "publication": "Cancer Nanotechnology", "publication_date": "27 April 2016", "abstract": "Despite being one of the most common cancers, bladder cancer is largely inefficiently and inaccurately staged and monitored. Current imaging methods detect cancer only when it has reached \u201cvisible\u201d size and has significantly disrupted the structure of the organ. By that time, thousands of cells will have proliferated and perhaps metastasized. Repeated biopsies and scans are necessary to determine the effect of therapy on cancer growth. In this report, we describe a novel approach based on multimodal nanoparticle contrast agent technology and its application to a preclinical animal model of bladder cancer. The innovation relies on the engineering core of mesoporous silica with specific scanning contrast properties and surface modification that include fluorescence and magnetic resonance imaging (MRI) contrast. The overall dimensions of the nano-device are preset at 80\u2013180\u00a0nm, depending on composition with a pore size of 2\u00a0nm. ", "full_text": "Transitional Cell Carcinoma (TCC) of the bladder is the fifth most common malignancy in the United States (Siegel et al. ). Although the majority of human bladder cancer is superficial at the time of detection, the recurrence rate and the risk of progression to advanced disease are high (Hall et al. ; Kaufman et al. ; Faba et al. ; Nargund et al. ). Currently, the most common screening method is cystoscopy (Liu et al. ), which includes white light as well as fluorescent or narrow band cystoscopy (Kriegmair et al. ; Filbeck et al. ; Joudi and Konety ). The fluorescent and narrow band methods aim to highlight or differentiate the tumor from normal bladder epithelium wall using broad range/non-specific fluorescent dyes or stains (Filbeck et al. ; Joudi and Konety ). These generic labeling methods are not aimed at any particular molecule, but rather colorize the entire bladder with the hope that some differential labeling may occur. Macroscopic tumors can be seen with these methods, but smaller lesions may be missed (Zaak et al. ; Grossman et al. ). In addition, suspicious smaller lesions need biopsy for cancer staging, as cystoscopy alone remains insufficient (Cheng et al. , ; Mitra et al. ).The development of diagnostic tools for TCC has been slowed by the lack of an optimal animal model for the disease, due to the anatomical inaccessibility and the relative lack of molecular and bioinformatics data on the genetic diversity of the specific tumors. A variety of experimental models have been tested with mixed success, including mice (Chan et al. ; Zhang et al. ), rabbits (Nemoto et al. ), and primates (Cozzi et al. ). The current standard is a mouse orthotopic tumor model in which a known TCC cell line (MB49) is implanted following chemical disruption of the normal epithelium (Soloway et al. ; Bockholt et al. ; Newton et al. ). This cell line has similar features and replicates the pathobiology of human homologues (Luo et al. ). Although the presence of reporter genes has allowed for some measurements of non-invasive real-time tumor growth, no methods have been described capable of providing 3-dimensional details of growth, including heterogeneities within the tumor, in real time.In addition to diagnostic tools, therapeutic options for TCC are limited by anatomical challenges. After initial resection, many patients undergo bacillus Calmette-Guerin (BCG) immunotherapy; however, the recurrence rate for BCG is up to 50\u00a0% (Hall et al. ; Faba et al. ). Furthermore, many patients cannot complete the therapy due to the risks of sepsis and other side effects (Lamm et al. ; Brausi et al. ). Thus, groups have attempted to package an attenuated form of the BCG cell wall skeleton into nanoparticles to more safely induce an immune response, with mixed results (Ochiai et al. ; Nakamura et al. ). Many other forms of nanoparticles are used as carriers of chemotherapeutics; (Connolly et al. ; Huang et al. ) these, however, often cannot be visualized/detected and have poor retention in the bladder. Very few examples exist of chemical agents that can be used both for detection and therapeutic solutions. For example, radioactive agents produce quality PET/SPECT images, (Hoilund-Carlsen et al. ; Bouchelouche and Choyke ) but have short half-lives, necessitating repeated doses for longitudinal studies, further increasing the patients\u2019 exposure to ionizing radiation. Thus, there is a need for an advanced compound which would provide a good image using lowered-cost equipment (ultrasound, MRI), with no ionizing radiation and with good retention in the bladder for follow-up growth evaluation. The development of a single agent capable of providing sequentially valuable information from MRI as well as ultrasound contrast [image-guided surgical techniques (Nyland et al. ; Gkritsios et al. )] would be highly desirable.Mesoporous silica nanoparticles (MSNs) are often studied as vehicles for drug delivery due to their large ratios of pore volume and surface area to weight relative to other nanomaterials (Giri et al. ; Gruenhagen et al. ; Slowing et al. ; Chen et al. ; Benezra et al. ). Once injected, MSNs are bioinert, non-toxic and well-tolerated at clinically relevant concentrations (Slowing et al. ; Lu et al. , ), unlike biodegradable polymers that can generate toxic byproducts (Athanasiou et al. ; Semete et al. ), or nanomaterials with reactive elements such as quantum dots (Kirchner et al. ; Liu et al. ). In addition, the use of MSNs in imaging applications is an area of active research. In particular, MSN are used to label and track cells through functionalization with fluorophores (Giri et al. ; Chen et al. ; Benezra et al. ), paramagnetic materials for magnetic resonance imaging (MRI) (Chen et al. ; Hsiao et al. ; Larsen et al. ), or electron dense materials for computed tomography (CT) (Chen et al. ; Luo et al. ).Here, we describe the application of a novel MSN tool for noninvasive and longitudinal tracking of bladder cancer, using an established in vivo mouse model for the disease. Murine bladder cancer cells (MB49) were found to take up the MSN in vitro; in the mouse bladder, the MSN were taken up preferentially by tumor cells more readily than healthy bladder epithelium. The MSN were further functionalized with a peptide found to bind specifically to bladder cancer cells, thus improving specificity. The MSN facilitated the use of tumor imaging/staging in vivo using MRI, and ex vivo using fluorescent microscopy. This study presents the potential of these MSN to improve tumor visualization and staging, thereby improving patient outcomes.This study represents a unique example of the synergistic use of MRI, nanoparticles and computational imaging to visualize TCC in a mouse model in 3 dimensions, as a prototype for similar use in human TCC. Our approach allows for a significantly improved detailed window into the pathogenesis of the tumor, including invasion of the bladder wall and heterogeneities within the tumor. These pathological hallmarks are frequently observed in human-excised tissue. Currently, MRI 3-D reconstruction of the bladder is less common than traditional cystoscopic evaluation despite the fact that it is less invasive than cystoscopy, and important observations, including tumor staging, may be missed. The challenge in translating this technology to humans, where the tumor will be much smaller relative to the entire bladder at the time of imaging, will be improving the specificity for TCC to obtain the strongest signal possible. However, we predict that through additional research, a combination of MRI and fluorescent cystoscopy aided by MSN will make it possible to more accurately assess human TCC, and ultimately, if caught at a sufficiently early stage, improve patient prognosis."},
{"url": "https://cancer-nano.springeropen.com/articles/10.1186/s12645-016-0013-x", "title": "Bio-conjugation of antioxidant peptide on surface-modified gold nanoparticles: a novel approach to enhance the radical scavenging property in cancer cell", "authors": ["Sushma Kalmodia", "Suryanarayanan Vandhana", "B. R. Tejaswini Rama", "Balasubramanyam Jayashree", "\u2020", "T. Sreenivasan Seethalakshmi", "\u2020", "Vetrivel Umashankar", "Wenrong Yang", "Colin J. Barrow", "Subramanian Krishnakumar", "\u2020", "Sailaja V. Elchuri", "\u2020"], "publication": "Cancer Nanotechnology", "publication_date": "9 February 2016", "abstract": "Functionalized gold nanoparticles are emerging as a promising nanocarrier for target specific delivery of the therapeutic molecules in a cancer cell, as a result it targeted selectively to the cancer cell and minimized the off-target effect. The functionalized nanomaterial (bio conjugate) brings novel functional properties, for example, the high payload of anticancer, antioxidant molecules and selective targeting of the cancer molecular markers. The current study reported the synthesis of multifunctional bioconjugate (GNPs-Pep-A) to target the cancer cell.", "full_text": "Functional nanomaterials are used for various biomedical applications, and it is important to devise a safe and cost-effective alternative approach for targeted cancer therapy. Functionalized gold nanoparticles (GNPs) as an emerging and promising nanocarrier, particularly for the application in nanoparticle-mediated targeted therapy, have shown a significant progress in cancer therapy and diagnosis. Among all nanomaterials, GNPs are more promising due to their unique physicochemical properties, such as biocompatibility, surface plasmon resonance (SPR), and ability to functionalize with amine and thiol groups (Choi et al. ; Koshevoy et al. ). The therapeutic efficacies of GNPs were demonstrated by various studies which include the multifunctionality and specific interaction of target moieties, such as peptide, antibody, DNA/RNA, and drug with cellular molecular markers (Delong et al. ; News and Views in ; Thundimadathil ). The peptide favors therapeutic application due to its small size, ease of synthesis, tumor-penetrating ability, bioavailability, and good biocompatibility (Agyei and Danquah ; Gu et al. ; Prades et al. ). Peptides such as cell-penetrating peptides, therapeutic peptides, and antioxidant peptides have been reported earlier for cancer therapeutic applications (Thundimadathil ; Flora and Pachauri  ; Farkhani et al. ). The antioxidant molecules increase hydrogen peroxide levels, scavenge reactive oxygen species (ROS), and induce cancer cells to undergo apoptosis (Turkevich ). Thiol-based antioxidant systems are present in mammals to provide antioxidant property and, therefore, antioxidant molecules are important molecules for disease prevention (Valko et al. ; Zeng et al. ).The redox imbalance in a cancer cell is correlated with the oncogenic stimulation and important indicator of the disease progression. Therefore, it has considered as a pro-tumorigenic signaling molecule that, involved in the initiation, progression and metastasis of cancers (Maeda ; Valko et al. ; Storz ; Droge ; Finkel ; Kimura et al. ). Hence, modulating the redox state of the cancer cell is an interesting area of study for cancer therapy. Several studies reported the nanocarrier-based targeting of the cancer biomarkers such as, transforming growth factor-beta-1 (TGF-\u03b21), transmembrane receptor P185(HER2) for targeted cancer therapy, to our knowledge, none of these studies utilized to target the ROS using antioxidant peptide (Pep-A) (Prades et al. ; Tsai et al. ; Kodiha et al. ). Hence, it is imperative to develop a suitable biocompatible functionalized nanomaterial for specific and effective targeting the cancer tissue.In this study, we used RB cancer model for targeting the ROS, by means of the antioxidant peptides (Pep-A) which contain alternative aromatic or sulfur-containing amino acid. The side chains of Pep-A are believed to contribute to strong radical scavenging activities of peptides in the cancer cell. Toward achieving this aim, we design novel multifunctional strategies to attain more specific and effective response. The application of antioxidant molecule alone for targeted therapy has limited its application due to bioavailability and stability in biological environment, limitations of which prompted this research of utilizing nanocarrier-mediated delivery, as it could play a pivotal role of being an effective therapeutic agent. Hence, GNPs-Pep-A composite was synthesized using thiotic acid (TA), an antioxidant linker molecule. Thiotic acid improves the metal-chelating capacity, radical scavenging, and improves the antioxidant effect. The GNPs synthesized by  chosen for the delivery of Pep-A due to several advantages associated with the natural polyphenols used in the synthesis processes: (1) antioxidant and anticancer property, (2) reduced chemical and metal toxicities (3) synergistic improvement in the antioxidant effect of Pep-A, and (4) most importantly, decrease in the nanoparticle-mediated ROS induction (Sharma et al. ). The current findings indicate that bioconjugate (GNPs-Pep-A) shows therapeutic efficacy by quenching the ROS species in the cancer cells. The antioxidant potency of bioconjugate to target the ROS is owing to the co-existence of different phases in a conjugate (GNPs-Pep-A).Although a number of nanocarrier-based approaches have emerged for cancer therapy, this study is an aid for ROS-mediated targeted therapy. It is an effective and safe approach for the diseases like cancer which are highly heterogeneous in nature. Thus, the current study provides a rational plan with an alternative to conventional peptide-functionalized targeted therapy, which would be an effective strategy in the clinical management of cancer.Functionalized nanomaterials have revolutionized the field of targeted cancer therapy. Cancer biomarkers or targets such as onco-proteins are targeted with the functionalized nanomaterials. This study emphasizes on targeting the ROS in Retinoblastoma (RB) cancer cells, and also on an in vitro model to deliver the antioxidant peptides. The results have demonstrated an effective system for ROS scavenging activity which induces the apoptosis in RB cancer cells. The observed results confirm the synergistic effect of the GNPs-Pep-A. The novel, multifunctionalized GNPs can be alternative candidates for cancer therapeutic strategy \u2014such as those demonstrated in case of current study\u2014the self-therapeutic GNPs (such as GNPs) and antioxidant peptides. Thus, the antioxidant-based therapy may improve response to the therapy, and therefore, in future, these novel GNPs-Pep-A can be promising nano biocomposites for targeting cancer cells. Although the application of antioxidants is controversial in cancer treatment, the effect of antioxidant can minimize the side effect of chemotherapy, which in turn, can reduce the chemical toxicity, and may, therefore, enhance the degree of success during the course of treatment."},
{"url": "https://cancer-nano.springeropen.com/articles/10.1186/s12645-016-0023-8", "title": "Profiling lung adenocarcinoma by liquid biopsy: can one size fit all?", "authors": ["Harry W. Clifford", "\u2020", "Amy P. Cassidy", "\u2020", "Courtney Vaughn", "Evaline S. Tsai", "Bianka Seres", "Nirmesh Patel", "Hannah L. O\u2019Neill", "Emil Hewage", "John W. Cassidy"], "publication": "Cancer Nanotechnology", "publication_date": "22 November 2016", "abstract": "Cancer is first and foremost a disease of the genome. Specific genetic signatures within a tumour are prognostic of disease outcome, reflect subclonal architecture and intratumour heterogeneity, inform treatment choices and predict the emergence of resistance to targeted therapies. Minimally invasive liquid biopsies can give temporal resolution to a tumour\u2019s genetic profile and allow the monitoring of treatment response through levels of circulating tumour DNA (ctDNA). However, the detection of ctDNA in repeated liquid biopsies is currently limited by economic and time constraints associated with targeted sequencing.", "full_text": "Cancer is a disease of the genome; one which is initiated by nanostructural perturbations in the structure and function of DNA (e.g. somatic mutations, epigenetic modifications, etc.) and driven by the sequential accumulation of these perturbations (Hanahan and Weinberg ). The study of genomic aberrations and the identification of somatic mutations that drive a particular malignancy are, therefore, fundamental to the understanding of tumour biology. In addition, targeted therapies developed to inhibit the growth of a tumour are almost exclusively stratified to patients harbouring specific mutational profiles (Huang et al. ). For example, cetuximab, an anti-epidermal growth factor receptor (EGFR) therapy, is only truly effective in patients with EGFR amplifications (Yang et al. ). Tumour genotype information is needed by clinicians on a per patient basis.Resistance to targeted therapies often emerges during a treatment regimen. Pre-existing resistant populations in a treatment-na\u00efve tumour and induced-resistant populations acquired de novo during therapy have both been described as mechanisms of resistance. Bhang and colleagues have recently traced the emergence of erlotinib resistance in a model of lung adenocarcinoma, identifying a pre-existing -amplified clonal population responsible for in vitro recurrence (Bhang et al. ). In a separate lung cancer model, Hata et al. showed that EGFR mutations could be acquired during navitoclax therapy and drive the inhibitor-resistant phenotype (Hata et al. ). Thus, including temporal resolution in cancer genomic information will better inform treatment decisions.Because of the clinical importance of tumour genomics, it is unsurprising that the sequencing of tumour biopsies prior to, and during, treatment regimens has become commonplace over the past several years. However, spatial heterogeneity within a tumour can lead to an under-representation of intratumour heterogeneity and an inaccurate reporting of tumour genotypic information gleamed from punch biopsies (Sottoriva et al. ; de Bruin et al. ). Moreover, such biopsies are relatively invasive for solid tumours. Thus, many researchers and clinicians alike have turned to so-called \u2018liquid biopsies\u2019 in an attempt to identify circulating mutant tumour DNA (ctDNA) in a patient\u2019s blood (Newman et al. ; Ma et al. ). By deep molecular characterisation of this ctDNA across multiple sequential biopsies, it is hoped that researchers and oncologists will gain a better picture of cancer\u2019s genetic makeup and how this evolves over time, without the considerations associated with spatial heterogeneity.Typically, profiling of ctDNA is achieved through deep or targeted amplicon sequencing (Newman et al. ). However, this approach is limited in terms of cost and throughput. For some of the more immediate clinical applications of ctDNA, such as tracking treatment response, temporal resolution of a tumour\u2019s evolution may be as useful as a deep understanding of its molecular drivers. Thus, many approaches for sequential monitoring of ctDNA have focussed on high-throughput techniques such as droplet and digital PCR to trace individual mutations in a patient\u2019s blood over time (Zheng et al. ). In this study, we sought to determine whether a panel of recurrently mutated genomic loci (hereafter \u2018hotspots\u2019) could be developed which would give suitable coverage over the entirety of the intertumour heterogeneity seen in human malignancies. As a test case, we focus on lung adenocarcinomas: a malignancy that is not well suited to typical punch biopsy techniques and that has substantial genomic heterogeneity amongst the clinical population.Over the past several years, renewed effort in cancer research has yielded a myriad of molecular drivers of and contributors to tumour progression. Alongside the most often cited contributors, there are changes in stromal cell infiltrates (Kalluri and Zeisberg ), alterations in receptor prevalence or cell signalling (O\u2019Neill et al. ), and nanotopographical changes to the cancer cell\u2019s niche (Cassidy ; Cassidy et al. ). However, cancer is fundamentally a disease of the genome and only by understanding the patterns of clonal dynamics and evolution of genomic clones will the disease be fully understood.As the need for accurate and temporally specific genomic information makes its way into the clinical setting, we must adopt new methodologies of profiling a tumour\u2019s genome in a non-invasive and low-cost manner. Analysis of ctDNA has shown much promise in this regard, being used in many pioneering studies for monitoring treatment response, predicting relapse, and profiling intratumour heterogeneity (Bettegowda et al. ; Ma et al. ; Zheng et al. ). However, analysis of ctDNA is often initially based on targeted sequencing, which is both expensive and time consuming. Typically, specific primers can be designed after initial sequencing and ctDNA levels in the blood can be followed by less-demanding techniques, such as droplet digital PCR (Zheng et al. ). In this study, we set out to identify a panel of recurrent mutations in lung adenocarcinoma that would cover the majority of patients. Primers could then be designed and optimised for this panel ready for \u2018off-the-shelf\u2019 use in molecular diagnostic laboratories.Lung adenocarcinoma is particularly heterogeneous and, even with a panel of 400 recurrent hotspots, coverage of 1\u00d7 was only possible in ~80% of patients (Fig.\u00a0b). This is particularly problematic as many of these mutations are likely passengers and therefore not necessarily clonal to the whole tumour. Thus, with a coverage of 1\u00d7 we could not be sure that ctDNA levels were truly representative of the tumour bulk as a whole. However, this panel could be substantially refined in the future given the prevalence of recurrent copy number aberrations in driver genes seen in Fig.\u00a0, and the recurrent promoter methylation in lung cancer (Belinsky ) which is recapitulated in ctDNA (Mishima et al. ; Warton et al. ). Care should also be taken to include likely \u2018truncal\u2019 genomic aberrations common to the tumour as a whole and not restricted to minor subclonal populations. Differences in TCGA and Broad datasets (Fig.\u00a0) reflect tumour heterogeneity in lung adenocarcinomas and suggest that recurrently methylated CpG sites may also require inclusion in such panels. Although if such efforts relied on bisulfide conversion of CpG islands, we may see a loss of resolution for \u201cC to T\u201d SNVs at these sites.The need for rapid identification of ctDNA in the time- and cost-constrained environment of clinical oncology is clear, and lung adenocarcinoma is of particular interest due to the difficulty in collecting recurrent solid biopsies. Our study aimed to identify a targeted hotspot panel for lung adenocarcinoma. We described mutation patterns in known genetic drivers of lung adenocarcinoma and profiled genome-wide recurrently mutated loci. Moreover, this work has identified several novel recurrent mutations in genes not typically associated with lung adenocarcinoma, which are each present in a significant subset of TCGA lung adenocarcinoma patients (Fig.\u00a0a, e.g.  and ). Whilst our panels were informative, they did not provide sufficient coverage and depth to be clinically useful. Future work should refine our initial panel to include recurrent copy number aberrations and hyper-methylated promoter regions.ctDNA shows great promise for low-invasive serial monitoring of tumour burden and heterogeneity through treatment cycles. However, current ctDNA detection techniques rely on next-generation sequencing which is time consuming, expensive and requires bioinformatics expertise and access to specialist sequencing facilities. Tracing ctDNA through serial biopsy is better suited to high-throughput and low-cost techniques such as digital droplet PCR. In this scenario, a molecular diagnostics laboratory would first deeply sequence a patients\u2019 ctDNA and then design primers for subsequent digital droplet PCR. In this study, we sought to define a panel of common hotspot mutations in lung adenocarcinoma to allow molecular diagnostic laboratories to design and optimise primers to cover the majority of patients. Although our 400-hotspot panel showed good coverage and depth in the TCGA dataset, all patients could not be covered. The difficulties in finding hotspots common to all patients reflect the profound intertumour heterogeneity seen in all cancers (Cassidy and Bruna ) and in particular lung adenocarcinomas. Further work is needed to optimise the panel design prior to use in the clinic, alongside continued collection of whole genome sequencing data from lung adenocarcinoma patients. Beyond mutations, efforts should be made to include recurrently methylated CpGs and copy number aberrations in such panels.Primary mutational analysis was carried out using cBioPortal (cbioportal.org) (Cerami et al. ; Gao et al. ). Lollipops were constructed using the R package \u2018lollipops\u2019 (github.com/pbnjay/lollipops), with pathway data obtained from Cytoscape 3.2.1 (cytoscape.org) (Lopes et al. ). Called somatic mutations (SNVs) and clinical metadata were downloaded from the TCGA Data Portal (tcga-data.nci.nih.gov) (Network ). Validation dataset from the Broad Institute was downloaded from dbGAP (Imielinski et al. ). Mutation annotation format (MAF) files were manipulated in R Studio (Mac) 0.99.484 (rstudio.com). Combined data were analysed in Microsoft Excel (Mac 14.4.3) and R Studio with results plotted in GraphPad Prism 6 (Mac) and R (3.3.1 Unix; r-project.org)."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0165-y", "title": "Molecular recircumscription of ", "authors": ["Kuo-Fang Chung", "Wen-Hsi Kuo", "Yi-Hsuan Hsu", "Yi-Hsuan Li", "Rosario Rivera Rubite", "Wei-Bin Xu"], "publication": "Botanical Studies", "publication_date": "21 February 2017", "abstract": "Despite being a relatively small genus, the taxonomy of the paper mulberry genus ", "full_text": "Prior to Corner ()\u2019s circumscription,  L\u2019H\u00e9r. ex Vent. was known as a genus of three species distributed in East Asia and continental Southeast Asia: the type species  (L.) L\u2019H\u00e9r. ex Vent.,  Siebold, and  Siebold (Ohwi ; Liu and Liao ), with a hybrid between  and  known from Japan (Kitamura and Murata ; Yamazaki ; Okamoto ) and Korea (Yun and Kim ). Corner () expanded the generic concept by combining  Thwaites as  sect.  (Thwaites) Corner, stating that \u201c (i.e., sect.  and sect. ), \u201d (Corner ). Currently,  sect.  comprises four species:  (Baill.) C.C. Berg of Madagascar,  (Hook. f.) Corner of China (Yunnan), India (Assam), Myanmar, and Thailand,  (Blanco) Bureau of the Philippines and Sulawesi, and  (Thwaites) Corner of Sri Lanka (Corner ; Berg ; Zhou and Gilbert ; Berg et al. ). Based on Corner ()\u2019s circumscription,  is characterized by membranous stipules, globose syncarps, drupes covered by thickly sets of slender stalked bracts of various shapes, crustaceous to ligneous endocarps, and conduplicate to plane cotyledons. Although Corner ()\u2019s expanded concept has been followed by most authors (e.g., Berg ; Rohwer ; Chang et al. ; Zhou and Gilbert ; Berg et al. ) except for Capuron () who sustained the generic status of , the monophyly of  sensu Corner () has not yet been tested (Zerega et al. ; Clement and Weiblen ) and much about the taxonomy of the genus remains unsettled.Although paper mulberry has long been introduced to Europe (Barker ), it is Kaempfer ()\u2019s plate (\u201c 471.  472\u201d) depicting paper mulberry (as \u201c\u201d) in Japan cited by Linnaeus () that was lectotypified (Florence ) for  L., the basionym of . In Japan where paper mulberry is known as \u201cKajino-ki\u201d (Okamoto ),  has long been regarded as non-native (Schneider ), also introduced for paper making around ca. 610 A.D. (Matthews ; Barker ). Quite confusingly, the name Kajino-ki was taken by Siebold () for , a name long applied to a small \u2018monoecious\u2019 shrub with \u2018globose\u2019 staminate catkins ca. 1\u00a0cm across known as Hime-k\u00f4zo in Japan (Ch\u00fbj\u00f4 ; Kitamura and Murata ; Yamazaki ; Okamoto ). Elsewhere,  is also widely found in China (Chang et al. ; Zhou and Gilbert ), Taiwan (Liao , , ), and Korea (Yun and Kim ). The natural hybrid between Hime-k\u00f4zo and Kajino-ki known as K\u00f4zo in Japan (as \u00a0\u00d7\u00a0; Kitamura and Murata ; Okamoto ) and Daknamu in Korea (Yun and Kim ) has also been long cultivated and favored by Japanese and Korean farmers for traditional paper making for centuries (Yamazaki ). In 2009, this natural hybrid was further named \u00a0\u00d7 M. Kim (Yun and Kim ). The third species, , is a \u2018dioecious\u2019 lianascent climber with \u2018spicate\u2019 staminate catkins ca. 1.5\u20132.5\u00a0cm long distributed in Japan (known as Tsuru-k\u00f4zo), central to southern China, and Vietnam (Ohwi ; Yamazaki ; Zhou and Gilbert ; Okamoto ), with a controversial record in Taiwan (Suzuki ; Kanehira ; Liu and Liao ; Liao , , ).In the article titled \u2018A speciograhical revision on \u2019, Suzuki () studied a set of highly variable specimens akin to \u201cHime-k\u00f4zo\u201d collected from Taiwan first identified as  sensu Forbes and Hemsley () by Hayata (). After comparing with specimens collected from Japan, Suzuki () concluded that  and  are different species and that all the Taiwanese specimens should be collectively recognized as a distinct taxon, which he named  var.  T. Suzuki. However, Suzuki ()\u2019s treatment was not cited in Kanehira (), the most influential pre-World War II work on the woody flora of Taiwan (Li ). Instead, Kanehira () followed Hayata ()\u2019s treatment, identifying the entity as  and stating that the species is dioecious. Interestingly, although a majority of the treatments of Kanehira ()\u2019s \u2018Formosan Trees\u2019 were followed in the first edition of the Flora of Taiwan (Liu and Liao ) and its predecessor (Liu ), both Liu () and Liu and Liao () treated the species as  with  var.  synonymized under  [though mistakenly typed as  \u201c\u201d Sieb. var.  Suzuki in Liu and Liao ()]. Subsequently, Yamazaki () revisited the issue. Yamazaki () emphasized the differences in leaf shapes, adopting Suzuki ()\u2019s treatment by circumscribing  var.  as a variety endemic to Japan and  var.  a variety distributed in southern China, Taiwan, and Vietnam. Yamazaki ()\u2019s treatment was adopted by most treatments of the Chinese floras (e.g., Chang et al. ; Zhou and Gilbert ; Liu and Cao ) with rare exceptions such as Cao () in which  var.  was treated as a synonym of . The taxonomic status of  var.  was further complicated when Liao (, , ), in addition to , reported  from Taiwan, with  var.  again treated as a synonym of . Liao (, , )\u2019s treatment has been followed by all subsequent works of Taiwan (Liu et al. ; Yang et al. ; Lu et al. ) as well as local online blogs (e.g., Nature Campus ). In a recent assessment of the conservation status of the flora of Taiwan,  is listed as a \u2018vulnerable\u2019 species with its small and declining populations (Wang et al. ).Given the complicated taxonomy of these names, it is rather surprising that none of the abovementioned authors had attempted to examine and clarify type materials of the two names described by Siebold () as well as  var. . After lectotypifying Siebold\u2019s\u00a0Japanese plant names (Akiyama et al. ), Ohba and Akiyama () revised the taxonomy of  of Japan. Surprisingly, the specimen of Siebold\u2019s collections of Japanese plants that matched best to the protologue of  and thus lectotypified (M-0120984) turned out to be K\u00f4zo (Akiyama et al. ; Ohba and Akiyama ), the natural hybrid between Hime-k\u00f4zo and Kajino-ki cultivated for traditional paper making. Consequently,  Hance, the next valid name long synonymized under  (e.g., Zhou and Gilbert ) becomes the correct name for Hime-k\u00f4zo (Ohba and Akiyama ). For , the plate of \u2018\u2019 in Kaempfer () was lectotypified (Akiyama et al. ). Based Ohba and Akiyama ()\u2019s treatment, the four species of  in Japan are  (Tsuru-k\u00f4zo), \u00a0\u00d7 (K\u00f4zo),  (Hime-k\u00f4zo), and  (Kajino-ki).Because Ohba and Akiyama () dealt only with Japanese materials, this study attempts to clarify the distribution range of  and resolve controversies surrounding the name  var.  based on herbarium work, field observation, and molecular data. We also sampled species of  sect.  which thus far has never been sampled (e.g., Zerega et al. ; Clement and Weiblen ) to test the monophyly of  sensu Corner ()."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0166-x", "title": "Meso- or xeromorphic? Foliar characters of Asteraceae in a xeric scrub of Mexico", "authors": ["Patricia Rivera", "Jos\u00e9 Luis Villase\u00f1or", "Teresa Terrazas"], "publication": "Botanical Studies", "publication_date": "23 February 2017", "abstract": "The anatomical traits associated with water deficit are also observed in plants growing in poor soils. The species may resist water deficit through three main strategies: escape, avoid or tolerate. The Pedregal de San \u00c1ngel Ecological Reserve (REPSA), Mexico, is an environment with low nutrient soil and low water availability. It is set on the basalt formation derived from the Xitle volcano eruption. The main vegetation type is characterized as xerophytic shrub. Thus we expect that species growing in this community will show leaf xeromorphic traits and may have any of the three response strategies. We analyzed the foliar anatomy of 52 species of the Asteraceae family at the REPSA because it is the most abundant angiosperm family in the site, showing a wide variety of growth forms and anatomical variation.", "full_text": "Plants growing under xeric conditions can have a set of anatomical, physiological or phenological adaptations allowing them to escape, avoid or tolerate water stress (Santos and Ochoa ; Fahn and Cutler ; Dickison ; Valladares et al. ). Drought-escaping species generally are annual or biannual herbs that develop their life-cycle in short time periods when conditions are favorable (Kramer ; Santos and Ochoa ; Fahn and Cutler ; Valladares et al. ) or they usually possess perennation structures, such as bulbs, rhizomes or runners that remain latent until conditions are favorable again. Plants showing the avoidance-of-stress strategy possess some features enabling them to minimize or compensate for water loss or to increase water uptake and avoid desiccation. Indicator traits of the drought avoidance mechanism include deep roots, small or strongly lobulated leaves, mesophyll cells smaller in size, thick cell-walls and cuticles, multiseriate epidermis, strongly developed palisade parenchyma, stomata sunken or in crypts and abundant trichomes (Esau ; Kramer ; Santos and Ochoa ; Gibson ; Dickison ; Valladares et al. ; Fang and Xiong ). Drought tolerating plants can sustain a certain level of physiological activities under stress conditions. Tolerating plants show tissues resistant to dehydration and characters such as stress-induced leaf abscission, succulent leaves, lignified cell-walls, mucilage accumulation and the occurrence of buliform cells (Kramer ; Santos and Ochoa ; Dickison ; Valladares et al. ; Fang and Xiong ).Traits of stress-avoiding and stress-tolerating plants have also been related to soil nutrient deficit (Dickison ; Fonseca et al. ; Wright et al. ). In fact, soil nutrient availability explains better the distribution of foliar characters in plants around the world than mean annual temperature, mean annual precipitation and irradiance together (Ordo\u00f1ez et al. ). It has been hypothesized that xeromorphism has evolved from the adaptation of mesomorphic plants to low nutrient soil (Fahn and Cutler ) or to both, the lack of water and poor soils. These two conditions are found at the Pedregal de San \u00c1ngel Ecological Reserve in the Basin of Mexico.The foliar anatomy of Asteraceae has been studied mainly in species with economical value (Ragonese ; Ferreira et al. ; Freire et al. , ; Mil\u00e1n et al. ). The leaves are usually dorsiventral, hypo- or amphistomatic and have anomocytic stomata. However, it is difficult to assert generalizations regarding the foliar anatomy of Asteraceae because there is so much variation in characters such as stomata distribution, trichome density and type, hypodermis development, presence of secretory structures and parenchymatic vascular bundles sheaths (Anderson and Creech ; Metcalfe and Chalk ; Freire et al. ). The usefulness of foliar characters for taxonomic classification at the genus or species level (Ferreira and De Oliveira ; Castro et al. ; Luque et al. ; Delb\u00f3n et al. ; Adedeji and Jewoola ) or for phylogenetic and ecological studies (Boeger and Wisniewski ; Horn et al. ) has been overlooked because most studies have focused exclusively on examining the epidermal surface (Ferreira et al. ; Freire et al. , ; Adedeji and Jewoola ; Gil et al. ; Redonda-Mart\u00ednez et al. ) and ecological or leaf evolution traits are missing for Asteraceae as they have been generated for other plant families (Luckow ; Schmerler et al. ; Brodribb et al. ).We hypothesize that Asteraceae species at this ecological protected area (REPSA) will show anatomical traits for avoiding or tolerating stress, the aim of this study was to characterize the foliar anatomy of a sample of 52 species belonging to 41 genera and 13 tribes of Asteraceae living in the xeric environment of this reserve.The present study was conducted in the REPSA, within the central campus of the National Autonomous University of Mexico (UNAM, Fig.\u00a0a). Samples were collected during two rainy seasons from August 2008 to December 2009. Three to six fully developed leaves showing no evidence of injury were removed from three individuals of each selected species. All the leaves were photographed with a digital camera. Leaf area was determined using an image analyzer according to the procedure described by Garnier et al. (). After being photographed, two to three the leaf laminas per species were dried in oven at 60\u00a0\u00b0C for at least 48\u00a0h to constant weight and then weighed on an analytical balance to determine the dry mass of the leaf. Specific leaf area (SLA) was calculated as the ratio between the leaf area and leaf dry mass [SLA\u00a0=\u00a0leaf area (cm)/dry leaf mass (g)]. The remaining leaves (1\u20134 per species) were fixed with a formaldehyde-glacial acetic acid-ethyl alcohol solution (Ruzin ). They were then rinsed with tap water to remove fixative residues and stored in a glycerin-ethyl-alcohol-water solution (1:1:1) until sectioning. Voucher specimens information is given in Additional file .Portions of the middle region of the leaf, including the intercostal area from the midvein to the margin were cut, rinsed and dehydrated in increasing concentrations of tert-butanol (10\u2013100%) with a Leica automatic tissue processor (TP1020) remaining for 24\u00a0h in each concentration. The tissues were embedded in paraffin (melting point: 56\u00a0\u00b0C) and transverse and paradermal sections\u00a010\u201312\u00a0\u00b5m in thickness were cut with a Leica rotatory microtome (RM2125). The resulting sections were stained with safranin-fast green (Johansen ) and mounted on synthetic resin. In the paradermal sections, the cuticle, guard cell length and epidermal cell shapes were examined. In the transverse sections, the thickness of the following features was measured: the total leaf, mesophyll, palisade parenchyma, spongy parenchyma, cuticle, and height of the abaxial and adaxial epidermis cells. The measurements were obtained through an Olympus microscope BX-51 attached to an image analyzer (Image Pro-plus version 6.1, Media Cybernetics 2006). For each variable, 25 measures were recorded. Other features, such as the number of palisade parenchyma layers or the spongy parenchyma type (open or compact), were also determined. Laminar size classification followed Webb (), detecting 6 categories: nanophyll (0.25\u20132.25\u00a0cm), microphyll (2.25\u201320.25\u00a0cm), notophyll (20.25\u201345\u00a0cm), mesophyll (45\u2013182.25\u00a0cm), macrophyll (182.25\u20131640.25\u00a0cm) and megaphylly (>1640.25\u00a0cm). Sclerophylly index (SI) was calculated following Boeger and Wisniewski () where SI\u00a0=\u00a0leaf dry mass (g)/2 leaf area (cm), considering SI\u00a0>\u00a00.6 as sclerophylly and SI\u00a0<\u00a00.6 as mesophylly). Stomatal pore area index (SPI) was calculated following Tian et al. () where SPI\u00a0=\u00a0stomatal density*stomatal length*10. Leaf lamina and midvein descriptions followed Dickison () and tribe classification followed Funk et al. ().Characters were square root and log10-transformed prior to statistical analyses. Pearson and Sperman correlations were calculated between pairs of leaf traits. In addition we produce a principal component analysis (PCA) to evaluate which leaf traits explained the variation in this site. Analyses were performed with R (R Core Team ) and SAS (Statistical Analysis System software version 9, SAS Institute 2002).The foliar anatomy of Asteraceae was variable. In the epidermis, differences were found in the shape of epidermal cells, seen in surface view, and in stomata location, as well as epidermis and cuticle thickness. The cell types constituting the mesophyll were variable as well as their abundance. These observations were consistent with the high foliar variation reported for the family (Anderson and Creech ; Metcalfe and Chalk ; Ferreira et al. ; Freire et al. , ; Mil\u00e1n et al. ; Adedeji and Jewoola ). The striae in the leaf cuticle have been previously observed by other authors (Adedeji and Jewoola ), who have suggested that the striae could be used to separate species. The observed stomatal types and the stomata being preferably distributed on the abaxial surface agrees with previous reports for the family in other works (Metcalfe and Chalk ; Adedeji and Jewoola ).It was difficult to find a defined anatomical pattern for the family. However, we found that the combination of anatomical features is usually more homogenous within the tribes than the family level. For example, most examined species show bifacial leaves, except for some members of the Bahieae, Coreopsidae and Tageteae tribes, which had equifacial leaves. It should also be stressed that all the members of Astereae had amphistomatic bifacial leaves with striated cuticles, while striae were absent in all members of Gnaphalieae. These observations should be confirmed by examining more Astereae and Gnaphalieae members.The foliar anatomy of Asteraceae was variable, even among members of the same tribe growing in the same locality. If the observed variations were the result of adaptation to a xeric and poor-soil environment, then the observed features would correspond to non-succulent xeromorphic leaves but it was not the case. The majority of the studied species possess mesomorphic leaf features as simple lamina, single-layered epidermis, and soft (mallacophyllous) large-size leaves with high SLA. Although some characters of drought resistance can be observed as for well-developed palisade and parenchyma bundle sheaths. The aforementioned character combination made difficult to classify the species studied within one of the three main response strategies to water stress. We suggest that the occurrence of specific character combinations in each species is due to a very fast evolutionary process involving the growth form, the adaptation to the environment and the phylogeny of the family. A character evolution analysis in an Asteraceae phylogeny including genera endemic to Mexico is needed in order to support this hypothesis (Terrazas, on going research).The combined study of morphological and anatomical traits and its correlations is important to understand the constraints imposed over the diversity generated by phylogeny and adaptation. For example, although leaf anatomy is not xeromorphic in the studied species, it is advantageous for the species living in the REPSA to have the combination of traits mentioned. Those traits allow them to survive in full sun during the short rainy season having efficient photosynthetic capacity. Assuming that species from the forests around the Xitle colonized the REPSA, it is not surprising that they retained some of their mesic characters and also adapted to the low water environment. There were also species with Neotropical and Nearctic affinities that probably had to modify their leaves to colonize new environments. These species probably retained the capacity to adapt to dry environments, and that is one of the reasons for their success. It is known that in ecological succession, pioneer species need to have low nutritional requirements and efficient metabolisms to survive. The invasiveness of Asteraceae species is probably related to their capacity to grow in this poor-soil environment."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0172-z", "title": "Cambial variations of ", "authors": ["Sheng-Zehn Yang", "Po-Hao Chen"], "publication": "Botanical Studies", "publication_date": "29 March 2017", "abstract": "Cambial variations in lianas of Piperaceae in Taiwan have not been studied previously. The stem anatomy of seven ", "full_text": "Lianas usually have two stages of development, the autologous support stage and the climbing stage (Schnitzer and Bongers ). Internally, the autologous support stage is characterized by a few narrow vessels and many thick fibers. The climbing stage involves secondary growth of the stem, which can be a standard or anomalous (Isnard and Silk ). Morphological variation in liana stems is primarily associated with the geometry of the phloem and xylem, and irregular shapes within the stems are classified into cambial variants (Angyalossy et al. ). The diversity of liana stem shapes, structures, and cambial variants has been recently reviewed (Angyalossy et al. ), including those of Piperaceae.Members of Piperaceae represent about five genera and 3700 species around the world (Christenhusz and Byng ), and many are economically important because of their medicinal and culinary uses. They are erect or scandent shrubs, small trees, or succulent, terrestrial herbs, with nodose stems. The leaves are entire, alternate to opposite or verticillate, petiolate or infrequently subsessile, palmately nerved or penninerved, pellucid dotted, and sometime aromatic (Souza et al. ). The genus  L. is represented by about 1050 species distributed primarily in the tropics (Mabberley ). The most outstanding anatomical character in the Piperaceae is the nature of the vascular bundles in the stem. In all species of Piperaceae studied to date in the genera ,  Trel., and  Blume, vascular bundles are organized in two or more concentric rings, a characteristic that is not present in other genera in Piperaceae (Trueba et al. ).Several studies have characterized cambial variants in certain  species.  C. DC. has a typical arrangement of tissues (Tepe et al. ), with a parenchymatous pith, medullary vascular bundles, a sclerenchymatous cylinder, peripheral vascular bundles, and a vascular cambium. The stem of  L. had an inner irregular circle of primary vascular bundles interspersed with a large mucilage canal in the pith, more mucilage canals in the inner cortex, an undulating wall of sclerenchyma, and an outer ring of smaller cortical bundles. The primary vascular (medullary) bundles occupied the pith (Beck ). Raman et al. () showed that the stem of  has a ring of mucilage canals located between cortical and medullary rings of primary vascular bundles, a central mucilage canal in the pith, secretory cells in the cortex, cortical fibers, and an endodermis with a Casparian strip. The descriptions of  here do not match each other owing to the age of the stem. Beck () showed that the stem of  G. Forst. had two medullary bundles that occupied the very center of the pith. The species had two cylinders of vascular bundles. The central, irregularly grouped primary vascular bundles were enclosed by a thick cylinder of secondary xylem capped by phloem that is separated by wider medullary rays of primary parenchyma.Raman et al. () studied  Roxb. The stem of  has a large central mucilage canal, medullary vascular bundles surrounded by parenchyma in the pith, a ring of cortical vascular bundles, and a discontinuous ring of collenchyma in the outer cortex. Saraswathy et al. () indicated that the stem of  Vahl. was circular in cross section, with a large central mucilage canal surrounded by individual bundles scattered in a parenchymatous cortex, which was encircled by a wavy ring of sclerenchyma followed by a ring of vascular bundles and medullary rays. A pericycle and collenchyma formed the outer cortex. In a broader study of woodiness in Piperales and Trueba et al. () presented images of transverse sections of  Trel. and Yunck. and  C. DC.  produced wide, lignified rays, with solitary or clustered vessels, but the center and edge of the stem were not included. A growth ring was visible in the rays and vascular tissue. The peripheral vascular bundles in  gradually elongated through secondary growth, exceeding the medullary vascular bundles, while the growth of the medullary bundles was minimal, and wide rays were formed by primary parenchyma cells. Santos et al. () showed that in  L., a large parenchymatous pith is surrounded by an inner circle of approximately nine medullary bundles, a ring of sclerenchymatous fibers, and an outer circle of approximately 30 vascular bundles, some of which have bundles of fiber capping the adjoining phloem.According to these studies and others,  typically has an inner series of vascular bundles separated from an outer series by a sclerenchymatous ring. This pattern has been observed in  L. and  Link (Ravindran and Remarshree );  Kunth (Souza et al. ); , , , , . , .  (Miq.) C. DC. (Pessini et al. ); .  Kunth (Albiero et al. ); .  Kunth (Albiero et al. ); .  Sw. (Albiero et al. ); .  (Souza et al. ); and  (Kunth) Steud. (Duarte and Siebenrock ).Taiwan is located in the subtropical zone, with a warm and humid climate. Piperaceae are represented in Taiwan by two genera,  Ruiz and Pavon and . Eight scandent species and two herbs are recorded in the genus  (Lin and Lu ; Hsu and Chung ). However, data regarding the internal stem anatomy of seven of the climbing species are lacking. Cambial variations are diverse within the family, so the present study attempts to (1) provide detailed photographs of features discussed, (2) specify the developmental stages observed, and (3) provide a key based on anatomical characters of transverse sections to facilitate the identification of irregular cambial activity in lianescent species of the genus  in Taiwan.Fresh stems were cut into pieces about 5\u00a0cm long, and a freehand cross section of each stem was made with a razor blade. The stem surface was immediately photographed using a Nikon D7100 SLR digital camera (Lens AF Micro Nikon 60\u00a0mm 1:2.8D, Nikon Corporation, Tokyo, Japan), and qualitative and quantitative anatomical traits were determined using Image-J software (Ferreira and Rasband ). The specimens were dried in an oven (60\u00a0\u00b0C) for 4\u20135\u00a0days and were then stored at \u221240\u00a0\u00b0C for 3\u20134\u00a0days. All specimens were deposited in the Provincial Pingtung Institute (PPI) herbarium at the National Pingtung University of Science and Technology, Pingtung, Taiwan, for subsequent identification. The nomenclature follows Flora of Taiwan Volume 2 (Lin and Lu ).The stem anatomy of  and  has been reported in earlier studies (Raman et al. ; Yang and Chen ), but all photographs and observations presented in this study are new. The climbing mechanisms of the lianas (adhesive, hook, or twining) follow Chen et al. (). Terminology for the description follows Chiang (Chiang ), Metcalfe and Chalk (, ), Tepe et al. (), Beck (), Raman et al. (), Saraswathy et al. (), and Santos et al. () (see Additional file : Appendix), and the stem shape determined by a length/width ratio of between 1.5 and 2.0 followed the Systematic Association Committee for Descriptive Terminology ().The vessel arrangement in the secondary xylem was diffuse porous in  (Fig.\u00a0a), and ring porous in  (Fig.\u00a0e). A ring of sclerenchyma separated the rings of medullary vascular bundles and peripheral vascular bundles and was continuous in all species except  (Fig.\u00a0c).  developed a deeply furrowed xylem cambial variant by parenchyma proliferation, which is not found in the other six species. The mucilage canals varied from absent ( and ) to present in the center of the pith (, , , and ; Fig.\u00a0a, d\u2013f) or present in the pith and inner cortex (; Fig.\u00a0a). The maximum and minimum width of rays differed from each other in all species except , which had rays of a uniform width (Fig.\u00a0g). The medullary vascular bundles persist but their growth is eventually minimal, while the peripheral vascular bundles continue to develop as the stem increases in diameter, and secondary thickening is restricted to the peripheral vascular bundles (Fig.\u00a0a\u2013d) due to the fascicular cambia that are located between the phloem and xylem of the peripheral vascular bundles.By comparing different sizes of stems in  (Fig.\u00a0a\u2013d) and  (Fig.\u00a0e\u2013h), two different series of vascular bundle development were observed. In the first type of development, much of the stem\u2019s volume is parenchyma, with one central mucilage canal is located in the pith, and eight additional mucilage canals in the inner cortex (Fig.\u00a0a). The medullary vascular bundles experience secondary growth from a fascicular cambium until they reach a certain size, while the number of medullary vascular bundles increases (Fig.\u00a0b), the cortical vascular bundles do not grow appreciably, and a sclerenchymatous ring thickens (Fig.\u00a0c). Then the growth of medullary vascular bundles is essentially minimal, while the peripheral vascular bundles produce xylem and phloem from a fascicular cambium, eventually exceeding the medullary vascular bundles, and wide rays fully separate the sclerenchymatous ring, which becomes discontinuous (Fig.\u00a0d). The species  showed this type of development.In the second type of development, much of the stem\u2019s volume is also parenchyma, and only one mucilage canal is present, located in the center of the pith (Fig.\u00a0e). Approximately ten medullary vascular bundles and a sclerenchymatous ring are in the inner cortex, and all the cortical vascular bundles apparently develop. Both medullary vascular bundles and cortical vascular bundles continue to grow (Fig.\u00a0f), and a ring of sclerenchyma gradually forms (Fig.\u00a0g). Then, the growth of medullary vascular bundles becomes minimal, while the peripheral vascular bundles continuously grow from a fascicular cambium. The medullary vascular bundles scarcely change while the peripheral vascular bundles soon exceed the medullary vascular bundles (Fig.\u00a0h), wide rays fully develop, and the sclerenchymatous ring remains continuous. The species  belongs to this type.Cambial variants of liana stems are divided into two types: those that originate from a single cambium and those from multiple cambia (Angyalossy et al. ). Members of Piperaceae such as  (Miq.) T. Arias, Callejas and Bornst produce the external vascular cylinders type (Angyalossy et al. ); in the present study, all seven  species also developed two series of primary vascular bundles separated by a ring of sclerenchyma. The outer irregular series of primary vascular bundles, or cortical vascular bundles, always developed near the cortex or periderm. The inner irregular series of vascular bundles, the medullary vascular bundles, were always found around the center of the stem. The stem of  produced wide, lignified rays, with solitary or clustered vessels (Trueba et al. ), and the stem of  has a discontinuous ring of collenchyma in the outer cortex (Raman et al. ); these characteristics were not observed in this study. The stem of  has a typical arrangement of tissues (Tepe et al. ), but lacks mucilage canals in the pith or in the inner cortex.The stem of  had also two cylinders of vascular bundles, but two medullary bundles, rather than a mucilage canal, occupied the very center of the pith (Beck ). The characteristic of a thick cylinder of tracheary elements that formed radially extended regions from cambium activity in  (Beck ),  (Santos et al. ), and  (Trueba et al. ) was also seen in . The stem of  was circular in transverse section (Saraswathy et al. ), unlike the oblate, transversely elliptical stems of the seven species in the present study.The stem of  has a ring of mucilage canals located between the cortical and medullary rings of primary vascular bundles (Raman et al. ); in this study, eight additional mucilage canals were found in the inner cortex. The stem of  just has a central mucilage canal in the pith, as described in  by Beck (). Though we did not observe the development process of the other five species in Taiwan, the two diagnostic features, a continuous sclerenchymatous ring and only a central mucilage canal (see Table\u00a0), were also found in the these species, indicating that their development process is similar to that of . However, the development of the other five species was not observed, and although the two diagnostic features were the same, further study is needed to confirm that the same pattern of development occurs.We observed that the growth of medullary vascular bundles continues but gradually becomes minimal, while the peripheral vascular bundles continue to grow in these seven  species. This result is consistent with a previous report (Trueba et al. ) that secondary thickening is actually restricted to the peripheral vascular bundles in species undergoing secondary growth. The variations in the activity of the medullary vascular bundles and the peripheral vascular bundles result in unusual distribution patterns of xylem and phloem, which influences stem shape based on the shape of the peripheral vascular bundles (Beck ). The function of the flattened stems of lianas is related to their climbing habit, where they are either appressed to or leaning on the supporting tree\u2019s trunk; this habit is entirely restricted in epiphytic lianas (Obaton ). The seven  species climb via adventitious roots and form a cambial variant type with flattened stems or oblate in that the broad face of the flattened axis contacts branches of trees (Carlquist ). The flattened stems as well as adventitious roots seem related to an epiphytic habit. More than two vascular bundle rings or two development types might be seen in stem of larger diameter, so it is valuable to observe more details in further studies.Cambial variants in Piperaceae include successive cambia, xylem in plates, xylem dispersed by parenchyma divisions, intraxylary phloem, included phloem (interxylary phloem), phloem arcs/wedges, and secondary growth of external primary vascular bundles (Metcalfe and Chalk ; Caball\u00e9 ; Carlquist , , ; Isnard et al. ; Acevedo-Rodr\u00edguez ; Jacques and de Franceschi ; Angyalossy et al. ). In woody vines, certain segments of the interfascicular cambium produce secondary parenchyma whereas a fascicular cambium produces typical proportions of xylem and phloem, as in  (Aristolochiaceae) (Beck ). The xylem is furrowed by arcs or wedges of phloem derived from portions of the cambium that produce less xylem and more phloem (Angyalossy et al. ). However, furrowed xylem may develop from phloem wedges or secondary parenchyma proliferation. Carlquist () defined rays as having very wide, thin-walled cells with limited subdivision during secondary growth. Wide rays develop in Piperaceae, and during secondary growth, the rays do not subdivide very much. Indeed, there may be little difference between them. Wider rays in lianas allow the stems to experience torsion without sustaining damage to vessels and contribute to liana stem flexibility (Putz and Holbrook ). The functional significance of wider rays in these seven  species needs to be checked.In the present study, we found that deeply furrowed xylem in  was produced from parenchyma proliferation (Fig.\u00a0c). This deeply furrowed xylem cambial variant has not been reported previously. Twenty-two families develop xylem in plates (also called radiating plate xylem), including Piperaceae. Carlquist () stated that in Piperaceae, combinations of cambial variants were common, such as secondary growth of external primary vascular bundles combined with xylem in plates, so two cylinders of vascular bundles combined with deeply furrow xylem and xylem in plates in  is another special cambial variant type. In the stems we examined, the xylem was beginning to furrow in , , and , indicated that these three species will develop deeply furrowed xylem in older stems. In summary, the seven  species in Taiwan produce cambial activity that results in large, radially extended regions of secondary vascular tissues, which are separated from each other by wide, medullary rays of primary parenchyma.The arrangement of two concentric cycles of bundles separated by a sclerenchymatous ring is typical of  in Taiwan. The genus displays a cambial variant with external primary vascular bundles combined with xylem in plates and wide medullary rays. The stems of these seven species are irregular in conformation and generally oblate, or flattened in cross section. A continuous sclerenchymatous ring is present in all species except , where it is discontinuous, with an additional ring of mucilage canals. The mucilage canals varied from absent, central in the pith, or present in the pith and inner cortex; mucilage canals may be diagnostic for certain species. The deeply furrowed xylem cambial variant only appears in  and is characterized by an interfascicular cambium that forms secondary parenchyma."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0170-1", "title": "Taxonomic placement of ", "authors": ["Yung-I Lee", "Mei-Chu Chung", "Kongmany Sydara", "Onevilay Souliya", "Sulivong Luang Aphay"], "publication": "Botanical Studies", "publication_date": "29 March 2017", "abstract": "\n                           ", "full_text": "The genus  comprises about 80 species, extending from the Himalayas and southern China through Malaysia to Guadalcanal (Braem ; Cribb ). The beautiful flowers of  species are shaped like the slipper of Aphrodite and hold a place in the affections of orchid hobbyists in the world. In the wild,  populations are found in relatively restricted areas, and most  species are endangered or even facing extinction because of the over-collection and the destruction of their habitats.  can be classified into three subgenera including subgenus , subgenus  and subgenus  based on morphological, cytological and molecular phylogenetic data (Cribb ; Chochai et al. ). In addition, the subgenus  could be divided into five sections: , , ,  and .The limestone mountains of Indochina are home to a great diversity of endangered  species. During the past two decades, a number of amazing  species with very limited distribution were discovered in this area, such as  and  (Averyanov et al. ; Liu et al. ). For now, the central region of the Indochina, particularly the territory of Laos, contains the largest part of the Indochinese limestone mountain which remains to be investigated. These inaccessible areas, undoubtedly, are home for numerous unknown plant species, particularly for strictly endemic orchids. A few years ago,  was discovered and described from the limestone mountain in north-western Vietnam near the Laotian border (Averyanov ; Averyanov et al. ). The distinct flower morphology led some taxonomists to propose a new section  under subgenus  (Averyanov et al. ), or even a new subgenus  (Braem and Gruss ) to accommodate this species. Afterward, based on the cytological data, phylogenetic analyses using plastid and nuclear genes and morphological characters, Gorniak et al. () suggested the status of the separate subgenus  within the genus  as proposed by Braem and Gruss (). In 2014,  was identified as a new species from smuggled plants under the name of  from Laos (Gruss et al. ). Later, more plants were found on the rocky limestone in Northern Laos. Although the tiny plants with marbled leaves look similar to , the other morphological characteristics of flower, such as staminodial shield, lip, and petal/sepal ratio and color are distinct from  and species in the other sections/subgenera. Therefore, more detail studies are required when we are going to propose the taxonomic status to accommodate .\n                         has been characterized by the significant chromosome variation, ranging from 2n\u00a0=\u00a026 to 42 (Duncan and Macleod ; Karasawa ; Karasawa and Aoyama ). The changes of chromosome number and karyotype are suggested to be caused by Robertsonian translocation, e.g. the fission of metacentric chromosomes at the centromeric region to generate more telocentric chromosomes (Karasawa and Saito ; Jones ). In addition, results from FISH mapping of ribosomal rRNA genes indicates that the duplication of 25S rDNA loci occurred independently in subgenus  and the sections  and  of subgenus , while the duplication of 5S rDNA loci can be detected only in subgenus  (Lee and Chung ; Lan and Albert ). Together, these data (in combination of chromosome number, karyotype and rDNA site) provide valuable information for cytotaxonomy in the subgenus/section level of . This study aims to provide cytological, molecular and morphological data which could cast new light on the discussion on the taxonomic position of  within the genus.To assess taxonomic position of  within the genus , we compared the cytological, molecular and morphological data obtained from the representative species of each subgenus in  according to the report by Gorniak et al. (). Furthermore, we investigated the distribution patterns of rDNA signals in  and  for cytotaxonomic reference. The major significant characters among the subgroups are summarized in Table\u00a0.\n                         is characterized by the miniature plants with tessellated leaves, a single-flowered inflorescence, a flower having a helmet shaped lip with a V-shaped neckline, and a semi-lunate staminode with an umbo and tri-dents (Figs.\u00a0, ). These features distinguish  from all of the other known sections/subgenera of . The subgenus  forms a monophyletic group based on the combined analysis, and both  as well as  are embedded in this clade. Moreover, in  and , the comparative studies on karyomorphology and the patterns of rDNA FISH also suggest a closer relationship to subgenus . At the present time, based on its specific morphological traits, we propose a new section  under the subgenus  to accommodate , and describe it below. Furthermore, since  is also embedded in the subgenus , we suggest to change the status of subgenus  to section  under the subgenus .The new classification should be as follows:Genus: PaphiopedilumSubgenus: PaphiopedilumSection Laosianum Lee, Chung, Sydara, Souliya & Luang Aphay, sect. nov.Type:  O. Gruss, N. Rungruang, Y. Chaisuriyakul et I. Dionisio.This is a monotypic section containing only . The section is characterized by its single-flowered inflorescence and the miniature plant with tessellated leaves. Although both of  and  have miniature plants with tessellated leaves, there is a great difference between their flower morphologies. The lip is helmet shaped with incurved lateral lobes and a V-shaped neckline, and the petal is oval shape and intensively red\u2013purple veins.  has a semi-lunate staminode with an umbo and tri-dents that looks an intermediate morphology between those in sections  and . The chromosome number of  is 2n\u00a0=\u00a026.Section Megastaminodium (Braem & O. Gruss) Lee, Chung, Sydara, Souliya & Luang Aphay, stat. nov.\u2014Type:  Aver. & O. Gruss."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0175-9", "title": "\n                     ", "authors": ["Yu-Hsin Tseng", "Young-Dong Kim", "Ching-I Peng", "Khin Myo Htwe", "Seong-Hyun Cho", "Yoshiko Kono", "Kuo-Fang Chung"], "publication": "Botanical Studies", "publication_date": "7 April 2017", "abstract": "A new species, ", "full_text": "\n                         L. (Begoniaceae), comprising more than 1800 species classified into 68 sections (Doorenbos et al. ; Hughes et al. ; Christenhusz and Byng ), is one of the largest genera of vascular plants. With more than 760  species in Asia, Doorenbos et al. () recognized 18 sections [ C.B. Clarke,  Warb.,  A. DC.,  A. DC.,  Irmscher,  (Lindl.) A. DC.,  (Klotzsch) A. DC.,  Irmscher,  (Klotzsch) A. DC.,  A. DC.,  (A. DC.) Warb.,  A. DC.,  (Klotzsch) A. DC.,  (Klotzsch) A. DC.,  (Klotzsch) A. DC.,  (Klotzsch) A. DC.,  Irmscher, and  (Hassk.) Warb.]. Thereafter, four additional Asian sections were proposed [ (T.C. Ku) Y.M. Shui,  T.C. Ku,  T.C. Ku, and  (Warb.) G. Forrest & Hollingsw.] (Ku ; Shui et al. ; Forrest and Hollingsworth ; Ku et al. ). These 22 Asian sections are highly unequal in species numbers: eight of the large sections (, , , , , , , and ) comprise 95% of Asian  species and the rest 14 sections each with less than five species (Thomas ). Several molecular phylogenetic studies have demonstrated the paraphyly or polyphyly of these large sections, suggesting homoplasy of morphological characters used for current sectional delimitations (Tebbitt et al. ; Thomas et al. ; Chung et al. ). However, few studies have tested the monophyly of small Asian section of  thus far [but see Rajbhandary (); Rubite (); Thomas ()].Myanmar is botanically a most interesting country, but there have been no critical floristic surveys for nearly half a century. Thus far about 60 species of  have been recorded from Myanmar (Hughes ; Tanaka and Hughes ; Tanaka and Hayami ; Peng et al. ; Tanaka and Peng ). During the fieldwork in western Myanmar on 2 February 2012, the second author (YDK) collected an unknown  with only one developed wing in ovary/fruit, which is the key character of  sect.  sensu Doorenbos et al. () first delimited by de Candolle () as  sect. . Presently, only two species,  Warb. and  Warb., are recognized in sect.  (de Candolle ; Doorenbos et al. ). , the type species of sect. , is native to Bhutan, Nepal and India (Fig.\u00a0; Doorenbos et al. ; Hughes et al. ). Its chromosome number was reported to be 2\u00a0=\u00a016 (Legro and Doorenbos ), with an uncertain chromosome count 2\u00a0=\u00a028\u201342 by Sharma and Bhattacharyya (). , occurring in Bhutan and India (Fig.\u00a0), is characterized by lanceolate to oblong leaves with subcordate base. Chromosome number of  was documented as 2\u00a0=\u00a022 (Doorenbos et al. ). Based on recent systematics and phylogenetics of , sect.  is nested within the - clade (Rubite ; Thomas ; Rajbhandary et al. ; Leong ).Although morphology of the 1-winged ovary/capsule of the undescribed  should be assigned to sect. , it differs from  and  significantly the leaf shape, leaf size and distribution. In this study, we described it as a new species. We also provide detailed morphological data and molecular phylogenetic analysis to elucidate the sectional assignment for this species.Studies of morphology, molecular phylogenetics and cytology support the recognition of the new species, , which is fully described and illustrated. Our results also indicate that  is not closely related to species previously assigned to sect. , suggesting that the fruit morphology of a single developed wing in the ovary/fruit characterizing sect.  is homoplasious."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0169-7", "title": "Assimilates mobilization, stable canopy temperature and expression of expansin stabilizes grain weight in wheat cultivar LOK-1 under different soil moisture conditions", "authors": ["Mahesh Kumar", "Susheel Kumar Raina", "Venkadasamy Govindasamy", "Ajay Kumar Singh", "Ram Lal Choudhary", "Jagadish Rane", "Paramjit Singh Minhas"], "publication": "Botanical Studies", "publication_date": "21 March 2017", "abstract": "Grain yield of wheat is primarily determined by both grain number and grain weight, which often influence each other in response to environmental stimuli. Some of the genotypes are capable of maintaining high single grain weight (SGW) across the environments. Understanding mechanisms and factors associated with the superiority of such genotypes over others is necessary to enhance productivity of wheat.", "full_text": "To meet the food demand of the world population projected to reach over 9 billion by 2050, production of wheat ( L.) needs to be increased by 70\u2013100% (Godfray et al. ). This has to be accomplished with available natural resources including the land vulnerable to abiotic stresses caused by supra-optimal ambient temperature and soil moisture deficit. Since the predicted climate change can amplify the frequency and magnitude of these stresses, enhanced efforts are needed for genetic improvement of stress tolerance in temperate crops like wheat. The key components of wheat grain yield viz., grain number and grain weight are highly vulnerable to high temperatures and soil moisture deficit (Dreccer et al. ; Vignjevic et al. ). However, some genotypes of wheat can maintain high SGW across the varying environments (Mohammad et al. ). Several attempts have been made to elucidate the basis of variation in yield components, particularly the size of the grain (Lopes et al. ; Bustos et al. ; Rebetzke et al. ; Aisawi et al. ). Often, the seed weight has been associated with both source and sinks limitations (Reynolds et al. ; \u00c1lvaro et al. ). This can also be influenced by differences in assimilates supply from current photosynthesis and pre-anthesis assimilates stored in the stem (Blum ; Rane et al. ; Fischer ; Vignjevic et al. ). Other traits such as cooler canopy also facilitates better grain development and hence the productivity of wheat (Mason and Singh ).Recently, genes contributing to SGW have been demonstrated in crops like rice, barley and wheat (Ashikari et al. ; Weng et al. ; Zalewski et al. ; Zhang et al. ). Starch is the major storage reserve of wheat grains. Grain weight is primarily determined by the photo synthetic productivity, assimilation and translocation of starch in the developing grain during the crop growth (Jenner et al. ). In higher plants, chlorophyll is the major pigment contributing to photosynthesis which is pre-requisite for starch biosynthesis. The stability and net content of chlorophyll can be modulated by cytokinin, a phytohormone (Chang et al. ). -, a rice orthologue of  (), has been associated with grain weight in wheat (Zhang et al. ). Specific suppression of  enhanced tiller number and grain weight in transgenic rice (Yeh et al. ). Amylopectin, the more abundant polymer of starch is synthesized by the coordinated actions of AGPase, soluble starch synthase (SS), starch branching enzyme (BE) and starch debranching enzyme (DBE) (Kang et al. ). Other genes coding for expansins have also been implicated in wheat grain development. Lizana et al. () reported abundance of  transcripts in the pericarp and endosperm of wheat at 187 thermal time (\u00b0Cd) (10\u00a0days after anthesis) close to the peak expression detected by RT-PCR. All these reports prompted us to investigate the role of genes like -, expansin and starch synthase in contributing to higher grain growth in LOK-1 cultivar of wheat. However, many of the hypotheses related to grain weight cannot be generalized due to genotype by environment interaction and location specific trait evaluation.LOK-1, one of the popular cultivars of wheat, in India for the last 20\u00a0years owing to its superior quality grains. This cultivar yields stable and large grains across the environment though the yield potential is relatively less than the recently released cultivars for the region (AICW & BIP Report 2012\u201313).We conducted the experiments to elucidate the physiological and molecular basis of differences between SGW of LOK-1 and that of other recently released cultivars. The main objective was to identify factors responsible for high SGW in the former genotype by assessing contribution of source, sink, current photosynthesis assimilate, pre-anthesis stem reserves, canopy temperature and genes associated with the grain development in wheat.Several studies have shown that the higher the temperature the lighter the weight of grains at maturity (Sofield et al. ; Chowdhury and Wardlaw ; Stone and Nicolas ; Wardlaw ; Savin and Nicolas ; Rane et al. ). In addition, soil moisture stress also severely affects the development of grain weight in wheat under different environmental conditions (Konopka et al. ). However, despite adverse environmental conditions some wheat genotypes maintain their grain weight. For our investigations, we choose bold grained cultivar LOK-1, one of the mega cultivars of wheat being grown in hot and dry conditions prevailing in Central and Peninsular part of India for more than two decades although it has now turned susceptible to diseases. Though its grain yield potential is less, grain weight of LOK-1 is relatively higher than those of other recently developed cultivars even under hot and dry environment. Since the grain weight and its stability are critical determinants of final grain yield we probed possible reasons for higher and stable grain weight of LOK-1 relative to three other cultivars, which are popular in these regions."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0168-8", "title": "Anti-fungal activity, mechanism studies on \u03b1-Phellandrene and Nonanal against ", "authors": ["Ji-hong Zhang", "He-long Sun", "Shao-yang Chen", "Li Zeng", "Tao-tao Wang"], "publication": "Botanical Studies", "publication_date": "14 March 2017", "abstract": "Essential oils from plants have been reported to have wide spread antimicrobial activity against various bacterial and fungal pathogens, and these include \u03b1-Phellandrene, Nonanal and other volatile substances. However, biological activities of \u03b1-Phellandrene and Nonanal have been reported only in a few publications. Further investigations are necessary to determine the antimicrobial activity of these compounds, especially for individual application, to establish the possible mechanism of action of the most active compound.", "full_text": "Many plant species, including tomato, synthesize and store numerous volatile terpenoid compounds during normal leaf development (Buttery et al. ; Par\u00e9 and Tumlinson ). Tomato is a constitutive emitter of low amounts of mono- and sesquiterpenes under non-stressed conditions, but these emissions become greatly enhanced under stress (Jansen et al. ; Maes and Debergh ). The volatile blends from  leaves detected with SPME GC\u2013MS were mainly terpenoids (i.e., \u03b1-Phellandrene), fatty acid derivatives (i.e., Nonanal) and aromatic compounds (Zhang et al. ).Nonanal has been reported to exhibit antimicrobial activity against gram-positive and gram-negative bacteria in the concentration of 100 to more than 800\u00a0mg/kg (Muroi et al. ). Nonanal is reported to have a MIC of 0.2\u00a0\u03bcg/mg against  (Kubo et al. ). \u03b1-Phellandrene showed weak inhibitory effects against all tested bacteria at the concentrations of 1 to >4\u00a0mg/mL (Demirci et al. ; Iscan et al. ). \u03b1-Phellandrene, \u03b2-Phellandrene, ocimene, limonene, myrcene, and \u03b1-caryophyllene have shown in vitro activity against  sp., , , , and  (Perez et al. ; Costa et al. ). Essential oils are usually mixtures of monoterpene and sesquiterpene, and their oxygenated derivatives. Their composition and proportion depended on species as well as the extraction and separation methods (Fisher and Phillips ). Essential oils are aromatic oily liquids, their antimicrobial properties have been empirically recognized for centuries, but scientifically confirmed only recently (Dorman and Deans ). \u03b1-Phellandrene and Nonanal are present in large quantities in many species such as canola, soybean, , ,  and  (Inouye et al. ; Fernando et al. ; Al-Burtamani et al. ; Nurettin et al. ; Rodriguez-Burbano et al. ; Hern\u00e1ndez et al. ; Pandey et al. ). These essential oils had a broad-spectrum antimicrobial activity against various bacterials and pathogenic fungi including\u00a0 sp.,  (Inouye et al. ; Al-Burtamani et al. ; Nurettin et al. ; Hern\u00e1ndez et al. ; Sharma et al. ).The lipophilicity of essential oils enable them to preferentially partition from an aqueous phase into membrane structures of the fungi, resulting in membrane expansion, increased membrane fluidity and permeability, disturbance of membrane-embedded proteins, inhibition of respiration, alteration of ion transport processes in fungi and induced leakage of ions and other cellular contents (Burt ; Fadli et al. ; Khan et al. ; Oonmetta-aree et al. ).Biological activities of \u03b1-Phellandrene and Nonanal were reported only in a few publications. Further investigations are necessary to determine the anti-microbial activity of these compounds, especially for individual application, to establish the possible mechanism of action of the most active compound to combat resistant pathogenic fungi. This study aims to analyze \u03b1- Phellandrene and Nonanal on the mycelial growth of . The effects of different concentrations of \u03b1-Phellandrene and Nonanal on surface morphology, cell membrane permeability, and release of cellular material were investigated to elucidate their anti-fungal mechanisms.\u03b1-Phellandrene and Nonanal exhibited strong antifungal activity against . The inhibitory effect was positively correlated with the concentration of \u03b1-Phellandrene and Nonanal. These results were consistent with those of previous studies describing the antifungal activity of these volatile compounds (Fernando et al. ; Rodriguez-Burbano et al. ; Pandey et al. ; Sharma et al. ). At a relatively low concentration (0.1\u00a0mL/L), Nonanal reduced the mycelial growth of  by half, making it a promising antifungal substance. In addition, the inhibitory effect of Nonanal on  was more efficient than that of \u03b1-Phellandrene on . The phenomenons were not observed in \u03b1-Phellandrene and Nonanal, indicating that the aldehyde compounds are more effective than alcohols and olefine in controlling postharvest pathogens (Droby et al. ). Among aldehyde constituents, cinnamaldehyde showed the highest activity, followed by citral, and then perillaldehyde, octanal and Nonanal (Inouye et al. ).The potential mechanisms underlying the anti-microbial activity of aldehydes and terpenes are not fully understood, but a number of possible mechanisms have been proposed. Gram-positive bacteria are known to be more susceptible to essential oils than Gram-negative bacteria (Farag et al. ; Smith-Palmer et al. ). The weak antibacterial activity against Gram-negative bacteria was ascribed to the presence of an outer membrane (Tassou and Nychas ; Mann et al. ), which possessed hydrophilic polysaccharide chains as a barrier to hydrophobic essential oils. In the current experiment, in vitro antifungal activity enabled us to hypothesize that the potential antifungal activity of \u03b1-Phellandrene and Nonanal against  could be closely correlated with the physiology of the hyphae. SEM analysis showed that the volatile compounds could alter the morphology of  hyphae, disrupting the membrane integrity (Yahyazadeh et al. ; Tyagi and Malik, ).These changes generally occur because of an increase in the permeability of cells, and such changes commonly result in the leakage of small molecular substances, ions, and formation of lesions (Bajpai et al. ). The leakage of cytoplasmic membrane was analyzed by determining the release of cell materials including nucleic acid, metabolites and ions which was absorbed at 260\u00a0nm in the suspensions (Oonmetta-aree et al. ). After the addition of the \u03b1-Phellandrene and Nonanal visibly increased with increasing volatile compound concentration. The maximum release of cell constituents was observed in  treated with \u03b1-Phellandrene at MFC. The methyl ester is able to penetrate to the hydrophobic regions of the membranes and the carboxyl groups pass through the cell membrane, perturbing in the lowering of internal pH and denaturing of proteins inside the cell (Marquis et al. ). From the results of the present study combined with the previous studies, we can conclude that the two volatile compounds apparently induced the leakage of intracellular protons. These findings suggest that irreversible damage to the cytoplasmic membranes of  occurred, and the ions inside the cells leaked, ultimately leading to apoptosis of the fungus in the presence of volatile compounds.In addition to cell wall and plasma membrane alteration and disruption, exposure of the hyphae of  to \u03b1-Phellandrene or Nonanal resulted in K leakage. Our results are in agreement with the reported by Helal et al. (). The phenomenon could be explained that the release of ions was only based on their size and/or due to formation of holes or lesions of lipid bilayer of the plasma membrane (Prashar et al. ). The fatty acid composition of microbial cell membranes affects their ability to survive in various environments (Ghfir et al. ). The decrease in lipid content suggested that membrane stability decreased while the permeability of water-soluble materials increased (Helal et al. ). Fumigation of  with  essential oil, induced alterations in both the lipid content and the fatty acids methyl esters composition of the cells (Helal et al. ). In the present study, the addition of \u03b1-Phellandrene and Nonanal significantly decreased the lipid content of . The results have shown that the two volatile compounds had the ability to penetrate lipid structures of the cells and disrupt the cell membrane integrity.In conclusion, this study showed that \u03b1-Phellandrene and Nonanal could significantly inhibit the mycelial growth of  cells, disrupt their cell membrane integrity and result in the leakage of cell components. Our present study suggested that \u03b1-Phellandrene and Nonanal might be used as fungicides to fight against postharvest fungal diseases.\u03b1-Phellandrene and Nonanal significantly inhibit the mycelia growth of . These changes disrupt the integrity of the fungal cell membrane, leading to the leakage of cell constituent and potassium ions, and triggering an increase of the total lipid content, extracellular pH and membrane permeability. \u03b1-Phellandrene and Nonanal might be used as biological fungicides for the control of  in postharvest tomato fruits in the future.\n"},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0180-z", "title": "The effects of local variation in light availability on pollinator visitation, pollen and resource limitation of female reproduction in ", "authors": ["Guo-Xing Cao", "Bi-Xian Wu", "Xu-Jian Xu", "Xie Wang", "Chun-Ping Yang"], "publication": "Botanical Studies", "publication_date": "2 June 2017", "abstract": "Light availability may have direct effects on reproduction through resource availability, and indirect effects on female reproduction by influencing plant-pollinator interactions. Floral display size, pollinator visitation per flower, resource and pollen limitation of fruit and seed production were quantified in a forested patch and an adjacent open patch of two populations of the perennial herb ", "full_text": "It is commonly observed that not all flowers produce fruit and not ovules in a flower mature seeds in flowering plants (Stephenson ; Wiens ; Sutherland ; Lee ). Two main proximate limiting factors, pollen limitation and resource limitation, have been proposed to explain these common phenomena (Haig and Westoby ). Pollen limitation, because of insufficient pollinator service, is usually tested by the addition of supplemental pollen to stigmas experimentally; reproduction of plants is considered pollen-limited if fruit or seed set is elevated by hand pollinations relative to natural pollinations (Burd ; Larson and Barrett ; Ashman et al. ; Knight et al. ; Wesselingh ). Resource limitation can be tested by the comparison of fruit and seed production between plants of different size (Griffin and Barrett ) or plants grown at different levels of physical resources (Lee and Bazzaz ).Light availability is potentially an important resource that may constrain reproduction, because light intensity can affect net photosynthetic rate (Kitajima ). Experimental studies have shown that overall plant size is higher for plants grown with sun exposure compared to those grown in shade (Cai ; Zhao et al. ). Light environment can also affect the pattern of resources allocated to reproductive components; plants in low light environments have been shown to divert resources away from reproductive components to parts which can increase light capture, such as stems and leaves (McConnaughay and Coleman ). As a result of reductions in plant size and/or relative allocation to reproduction, flower and fruit production and the number of seeds that are matured may decrease with decreasing light availability (Niesenbaum ; Cunningham ; Kato and Hiura ; Cao and Kudo ).Light availability may affect the degree of pollen limitation by plant-pollinator interactions and thus female reproduction indirectly. Because insect thermoregulatory capacity is higher in open than shaded habitats, pollinator visitation rate to flowers may be greater in the former (Herrera ; Kilkenny and Galloway ). Light conditions can also induce changes in floral display size (number of flowers open at one time on a plant), and thus may indirectly affect visitation rate to flowers. Plants occur in better light environments may produce larger floral displays, and pollinator visitation rate per flower have been shown to increase with increasing floral display size in some studies (Klinkhamer et al. ; Grindeland et al. ; Kilkenny and Galloway ). These differences in visitation rate between habitats, if large enough, could translate into greater pollen limitation of fruit and seed production in shaded habitats unless female reproduction of plants in shaded habitats is resource limited.Plants of the same species often occur in locally contrasting light environments. When variation in light conditions is sufficient, female reproductive success may be directly affected by light availability, as well as indirectly affected by plant-pollinator interactions. In the present study, variations in pollinator visitation rate, pollen and resource limitation of female reproduction were simultaneously evaluated in the perennial animal-pollinated herb , which grows in adjacent forested patches and open patches. Such simultaneous investigation could partition direct effects of light availability on plant reproduction through reproductive resource from indirect effects through changes in pollinator visitation rates. Specifically, the following questions were addressed: (1) Is pollinator visitation rate lower in forested patches than in open patches? (2) And if so, is the degree of pollen limitation of fruit and seed production greater in forested patches than in open patches? (3) Is fruit and seed production lower in forested patches than in open patches in the absence of pollen limitation?The present study evaluated the effects of the local light environment on pollinator visitation, pollen and resource limitation of female reproduction in two populations of . . The results showed that light environment could directly influence pollinator behavior, although the relative importance of resource-mediated effect and pollinator-mediated effect on female reproduction may vary among populations.\n"},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0176-8", "title": "Microarray meta-analysis to explore abiotic stress-specific gene expression patterns in Arabidopsis", "authors": ["Po-chih Shen", "Ai-ling Hour", "Li-yu Daisy Liu"], "publication": "Botanical Studies", "publication_date": "16 May 2017", "abstract": "Abiotic stresses are the major limiting factors that affect plant growth, development, yield and final quality. Deciphering the underlying mechanisms of plants\u2019 adaptations to stresses using few datasets might overlook the different aspects of stress tolerance in plants, which might be simultaneously and consequently operated in the system. Fortunately, the accumulated microarray expression data offer an opportunity to infer abiotic stress-specific gene expression patterns through meta-analysis. In this study, we propose to combine microarray gene expression data under control, cold, drought, heat, and salt conditions and determined modules (gene sets) of genes highly associated with each other according to the observed expression data.", "full_text": "Facing the challenge of climate change, raising crop production to feed enough people indicates to increase the tolerance of plants to severe environments (Ronald ). Plant is a sessile organism and must maintain a complex system of genetic expression to accommodate the impacts of different environments in order to survive with success (Trewavas ). When the plant is subjected to a stress, genes coded on its DNA usually take the initial actions required to trigger proper self-defensive mechanisms (Sachs and Ho ). It is therefore straightforward to monitor gene expression patterns and their interactions as the first step to deciphering the underlying mechanisms of a plant subjected to stresses. Embraced by rapidly developed biotechnologies, it has become very convenient to accurately monitor global gene expression under different circumstances in living organisms (Ritchie et al. ). Although the sequencing of messenger RNA by the latest generation of sequencing technology (also known as RNA-Sequencing or RNA-Seq) is more straightforward, sensitive, and accurate in terms of the quantification of gene expressions, the systematic error rates and costs of said technology remain high compared to those of microarray technology, which has been in use for more than two decades (Mantione et al. ).After collecting a global set of gene expressions, finding differentially expressed genes is the first step in deciphering the underlying mechanisms of a plant that copes with stress. In addition, biologists have recently been asking more about the systematic explanations of gene expression patterns. (e.g., Atkinson and Urwin ; Hahn et al. ; Priest et al. ). Such inquiries have motivated the advancement of gene set analysis and the utilization of microarray data to make inferences regarding genetic networks. Gene set analysis concerns the disturbed gene sets instead of individual genes whereas the gene sets of interest are predetermined (e.g., the co-expressed genes, the genes in the same category of the gene ontology, the genes involved in the same metabolic pathway, etc.) (Kaever et al. , ; Rest et al. ). Network inference, which is the focus of this study, links genes with edges that indicate potential associations to depict the possible interactions among the chosen set of genes (Todaka et al. ; Rasmussen et al. ; Nakashima et al. ).Along with long-term development of the technology, a huge amount of results from a wide range of microarray experiments has been accumulated. As of August 18, 2016, data from a total of 179 microarray experiments had been included in the Arabidopsis Information Resource (TAIR) database. Typical experiments have consisted of several treated samples under a particular condition and several controlled samples as the background of comparison for the reasons of purification and simplicity. However, analyzing the expression patterns under one stressed condition versus those under control conditions can only reveal a corner of a huge puzzle. One is not able to depict an overview of the entire system or of the interactions between the impacts caused by different stress sources on the living organisms. Therefore, in this study, we combine all possible samples from different stress conditions and perform a meta-analysis of the gene regulatory network on the combined dataset.To that end, the coefficient of intrinsic dependence (CID), instead of the typical Pearson correlation coefficient, is used to measure the association between genes because the Pearson correlation coefficient only measures the linearity of the gene associations. However, past studies (Liu ; Liu et al. ) have shown that a nonlinear relationship between the expressions of two associated genes might occur in some cases. The CID does not require distributional and functional assumptions regarding the data and is useful for analyzing noisy microarray data. Relatedly, while systematic errors are well controlled in highly developed microarray technology, samples from different experiments contribute noise to each other when a meta-analysis is conducted due to the fact that the expression patterns from different experiments have a wide range of variation (Ramasamy et al. ; Campain and Yang ). The CID had been applied to investigate gene regulatory events incorporating the Galton\u2013Pearson correlation coefficient (Liu et al. , ), to identify associations among multivariate variables (Liu and Tsai ), and to select relevant features on a step-by-step basis according to their importance in relation to the target variable (Hsiao and Liu ). In this study, we strictly followed the definition and methodology of CID described in Hsiao and Liu () and focus on utilizing the CID in measuring the magnitude of association in general between genes based on microarray gene expression data.A CID matrix is used to construct the weighted gene co-expression network produced by a WGCNA. According to the weighted network, gene modules containing genes with similar expression patterns are then identified. The WGCNA further performs a principle component analysis (Pearson ; Hotelling ; Zhang and Horvath ) on the expression matrix of each gene module and uses the first principle component (designated as \u201cEigen gene\u201d) as the representative of the gene module. The expression of the Eigen gene is the linear combination of the expressions from all genes in the gene module, which has been utilized to identify quantitative-trait-associated gene modules by computing the Pearson correlation coefficient between the observed values of the quantitative trait and the expression levels of the Eigen gene (Zhang and Horvath ; Langfelder and Horvath ). In this study, it is the qualitative variable (i.e., the treated conditions of the sample) that is of interest. We propose to utilize the analysis of variance for identification of stress-related gene modules by using the expression levels of the Eigen gene as the dependent variable and the treatment conditions of the sample as the independent variable.The following steps to construct the weighted gene co-expression network were analogue to those of the well-known WGCNA package in R (Langfelder and Horvath ). The pairwise CID values for the expression levels of g DE genes were computed to form the g*g CID matrix, C\u00a0=\u00a0{c}, where c\u00a0=\u00a0CID(g|g) for two DE genes, g and g, by setting the subgroup size of g to be 20 (Liu et al. ). Unlike the correlation matrix, the CID matrix is asymmetric; that means c is not necessarily equal to c. The g*g adjacency matrix, A\u00a0=\u00a0{a}, was set as C taken to the power of three to make the connectivity of the DE genes follow the scale-free property of the biological network (Albert ; Pavlopoulos et al. ). Specifically, in this study, the connectivity of gene g for the asymmetric adjacency matrix was defined as \u03a3a\u00a0=\u00a0\u03a3(c), i,j\u00a0=\u00a01, \u2026, 2281. The adjacency matrix, A, was analyzed by WGCNA with minModuleSize\u00a0=\u00a060 to obtain the gene modules (Langfelder and Horvath ).By overlaying the values of the first two PCs in * and * together in a two-dimension plot (known as the \u201cbiplot\u201d), we can visualize the effects of the conditions and the modules and their interactions more easily (Gabriel ; Yan and Kang ). According to the inner-product property of a biplot (Yan and Kang ), the projection of a module vector on a condition vector represents the magnitude of the effect of the gene module on the condition, or vice versa. If the angle between two vectors is less than 90\u00b0 (greater than 90\u00b0), the gene module is up-regulated (down-regulated) in the condition. We further connect two points of two conditions (for example, the blue line in Fig.\u00a0 represents such a connection) on the biplot and draw a straight line passing through the origin and running perpendicular to the line connecting two condition points (for example, the red line in Fig.\u00a0). Any module vector having a small angle to the perpendicular line (the red line in Fig.\u00a0) cannot effectively distinguish the two conditions, meaning that similar expression levels (i.e., the projection lengths) on the gene module have been observed in the two conditions. Therefore, we claim that the gene module is specific for a condition if it fulfills two criteria: (1) the length of the gene module vector projected on the condition vector is relatively long, and (2) the gene module vector is almost perpendicular to the connected line (e.g., the blue line in Fig.\u00a0) between any other two conditions. The genes in the stress specific gene modules were verified through gene ontology (GO) analysis using agriGO analytic tools (Du et al. ).\n                         (Arabidopsis) microarray expression data consisting of 22,810 probe sets (genes) and 216 samples under five conditions (control, cold, drought, heat, and salt) were analyzed. The data were first preprocessed in order to fulfill the normality assumption of ANOVA (Additional file : Figure S1), which were run separately on the root and shoot samples. The most significant 2281 genes under different conditions were collected for further analysis. We had observed the tissue-specific responses of the genes under different conditions; only 554 genes were included in the top 10% lists for both the root and shoot samples. Furthermore, there were 813 genes that were differentially expressed only in the shoot samples and 914 genes that were differentially expressed only in the root samples.We further identified Eigen genes which were specifically regulated only under the cold, heat, or salt condition by drawing reference lines on the biplot. A line connecting the ends of any two vectors of conditions, C and C, was made; this line was called L(C,C) for convenience. Then we made another line, called P(C,C), passing through the origin and running perpendicular to L(C,C). A \u201cstress-specific\u201d module for the condition C would be almost parallel to both P(C,C) and the vector of C. The angles between the Eigen gene vectors and all three perpendicular lines, P(C[OLD], S[ALT]), P(H[EAT], S[ALT]), P(C[OLD], H[EAT]) are also shown in Tables\u00a0 and . When searching for the stress-specific Eigen genes, their lengths were also taken into consideration to identify the Eigen genes that were more affected by the condition of interest; a shorter Eigen gene vector implied that the expression levels of that Eigen gene did not change much under different conditions. The stress-specific Eigen gene vectors with a length greater than 0.25 are marked in italics and underlining in Tables\u00a0 and .In shoot samples, two Eigen gene vectors (ME8 and ME14) under the cold condition (Fig.\u00a0b) had very sharp angles (less than 15\u00b0) to P(HEAT, SALT). This implies that the expression patterns of these two Eigen genes are similar under the heat and salt conditions but very different under the cold condition in shoot samples (Fig.\u00a0). Similarly, ME9 and ME17 were heat-specific Eigen genes only in shoot samples. Four Eigen genes (ME2, ME6, ME12, and ME18) were identified as salt-specific only in root samples. Two Eigen genes, ME4 and ME11, were identified as salt-specific and heat-specific, respectively, in both root and shoot tissues.Of three heat-specific gene modules, M9 and M17 were specifically identified in shoot samples, while M11 was identified in both root and shoot samples. Similarly, we expected that shoot-only M9 and M17 exclusively enriched heat-related GO terms. The three common GO terms among the 55 enriched in either M9 or M17 were more general GO terms including \u201cresponse to chemical stimulus\u201d (GO: 0042221), \u201cresponse to stimulus\u201d (GO: 0050896), and \u201cplasma membrane\u201d (GO: 0005886) (Fig.\u00a0b). Combining the results from three modules, there were several heat-related GO terms that drew our attention, including \u201cresponse to heat\u201d (GO: 0009408), \u201cprotein folding\u201d (GO: 0006457), \u201cresponse to high light intensity\u201d (GO: 0009644), \u201cresponse to oxidative stress\u201d (GO: 0006979), \u201cresponse to radiation\u201d (GO: GO: 0009314), and \u201cresponse to cadmium ion\u201d (GO: 0046686) (Additional file : Table S3B). This implied that the plant may simultaneously suffer adversities from light, radiation, and cadmium toxicity when under heat stress.Under the salt stress, genes in the M4 module were specifically disturbed in both root and shoot samples. The three GO terms enriched only in M4 were \u201cresponse to inorganic substance\u201d (GO: 0010035), \u201cresponse to metal ion\u201d (GO: 0010038), and \u201cresponse to cadmium ion\u201d (GO: 0046686) (Fig.\u00a0c; Additional file : Table S3C). The module M6 contained genes in response to osmotic stress (GO: 0006970), especially salt stress (GO: 0009651). This meant that the enriched GO terms of M6 mostly overlapped with those enriched in the other salt-specific modules. Genes in the M2 module were more involved in the processes of localization (GO: 0051179), cellular component biogenesis (GO: 0044085), macromolecule biosynthesis (GO: 0009059), and vitamin biosynthesis (GO: 0009110). A large portion of genes in the M12 module were expressed in response to wounding (GO: 0009611), biotic stimulus (GO: 0009607), organic acid metabolism (GO: 0006082), and cellular amino acid metabolism (GO: 0006520). The M18 module contains genes related to reproduction (GO: 00000003), fruit and seed development (GO: 0010154 and GO: 0048316), and embryonic/post-embryonic development (GO: 0009790 and GO: 0009791).The inferred stress-specific gene regulatory events were partly supported by experimental results through literature search. For instance, we compared the AGI locus identifiers of the 98 genes in the cold-specific gene module M8 to those known as cold-temperature responsive genes in the Gramene Pathway Browser () and only two genes,  (AT4G25480) and  (AT3G50260), were matched. They belong to CBF1 homolog and RAP2 homolog, respectively, in the Gramene pathway modules.  was the intermediate between the well-known cold induced transcription factor, ICE1, and the cold responsive genes,  and  (Yamaguchi-Shinozaki and Shinozaki ), while  would repress the expression of  and  under the cold stress (Tsutsui et al. ).In this study, we identified abiotic stress-specific modules after conducting a weighted gene correlation network analysis (WGCNA) using the analysis of variance and biplot visualization. Our first step in doing so was to differentiate the relevant gene module(s) according to different categorical traits (the different stress conditions, in our case) by the analysis of variance, something which might not have been accomplished by an analysis of correlation. Furthermore, the geometric interpretation of a biplot aims to utilize mRNA levels to point out to plant physiologists a plausible direction for further in vitro validation of tissue-specific and/or stress-specific mechanisms. The readers need to be aware of the fact that the heatmaps and biplots presented in this study were constructed based on the expression levels of the \u201cpseudo\u201d Eigen genes, which represent the diverse gene expressions in the gene modules specified by the WGCNA. It is thus possible that none of the genes in a given module exactly matches the expression patterns of the Eigen gene. The results of the analyses only provide hints about the underlying biological processes, which need to be further confirmed, as for example, by the gene ontology analysis in this study. In conclusion, our approach has the potential to further elucidate stress-specific mechanisms in plants via meta-analysis of massive amounts of microarray data. It can be used to complement the conventional bioinformatics analyses associated with the studied phenotypes."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0179-5", "title": "\n                     ", "authors": ["Yo-Jin Shiau", "Jenn-Shing Chen", "Tay-Lung Chung", "Guanglong Tian", "Chih-Yu Chiu"], "publication": "Botanical Studies", "publication_date": "30 May 2017", "abstract": "Soil organic carbon (SOC) and carbon (C) functional groups in different particle-size fractions are important indicators of microbial activity and soil decomposition stages under wildfire disturbances. This research investigated a natural ", "full_text": "Soil organic carbon (SOC) is one of the most important indicators of soil quality (Reeves ). It improves soil physical properties such as holding soil water and reducing soil bulk density (Manns and Berg ) and also helps in the development of the microbial community (Beyer ). The chemical composition of SOC in particle-size fractions may also affect soil microbial activity and decomposition rate of SOC (Beyer ). This information can be valuable for determining changes in the SOC pools with changes in plant cover or climate (Rossi et al. ).Wildfire is one of the severe impacts that degrade SOC and alters vegetation (Fernandez et al. ) by affecting their content and composition (Czimczik et al. ; Knicker ). Labile carbon (C) compounds could be preferentially lost and lead to unalterable SOC during wildfire (Gonzalez-Perez et al. ) and further affect the physical, chemical, mineralogical, and biological properties of soil (Certini ; Shrestha and Chen ). The effect of wildfire on SOC content and properties depend on fire type (Mataix-Solera et al. ), vegetation (da Silva and Batalha ), climate (Birkeland ), and soil development (Certini ). Wildfire also decreases humic substance content and affects the aromaticity of humified fractions (Vergnoux et al. ).Solid-state C nuclear magnetic resonance spectroscopy with cross-polarization and magic-angle spinning (CP-MAS C NMR) has been found as a useful tool to determine the composition of SOC (Golchin et al. ; Faria et al. ). It is also useful to evaluate changes in SOC pools and humification under different environmental impacts. For example, Rossi et al. () used C NMR spectroscopy to characterize the change in composition and structure of SOC after fire disturbance. Faria et al. () used the NMR spectroscopy technique and found that wildfire increased the aromaticity of the topsoil SOC in forest in Portugal. Similarly, Lopez-Martin et al. () measured the changes in SOC pools after wildfire burnt a mountain forest in Andalusia and found that fire-affected soils retained similar C and N content but showed higher aromaticity as compared with adjacent unburnt forest soils. However, previous research on composition of SOC pools of fire-affected soils were mostly based on whole bulk soils, and the composition of SOC in particle-size fractions may be worth studying.High easily decomposable substances (O-alkyl-C) was found in the litter of grassland (i.e. bamboo), which may reduce the humification degree (alkyl-C/O-alkyl-C ratio) of the grassland soils (Wang et al. ). This easily decomposable litter may potentially remediate the impact of wildfire to the SOC pools and may also be worth studying.The previous studies found higher amount of SOC as well as fungal and bacterial respiration rates in a  forest than a nearby fire-affected grassland (Imberger and Chiu ). As well, the diversity of the bacterial community was greater in the grassland than  forest soil (Lin et al. ). Microbial (fungi and bacteria) biomass appeared to be greater in large (>205\u00a0\u03bcm) than small particle-size fractions (Chiu et al. ). Moreover, the composition of SOC from the nearby forests showed greater humification degree in forest soil than dwarfed bamboo soil (Chen and Chiu ).This research further determined the change in SOC in an original subalpine forest soil with vegetation succession after wildfire using a transect study. By determining the composition of SOC in various particle sizes along a sampling transect using C NMR spectroscopy, the study aims to better understand the effect of forest wildfires on the change of SOC in different soil particle scales. The hypothesis of this research is that higher humification degree and aromaticity would be found in the fine particle-size fractions as recalcitrant substances should accumulate in the fine fractions of soils.Wildfire rapidly oxidized organic matter at topsoil horizons and caused depletion of active C pool in an ecosystem (Gonzalez-Perez et al. ). Among the three sampling locations in present study, soil TOC appeared to be lower in burnt grassland and transition zones than in forest soils. In addition, Robichaud () suggested that soil permeability and hydraulic conductivity were significantly decreased in fire-induced soil. This suggestion may explain the greater clay content in grassland than forest soil we found.Distribution of C functional groups in fire-induced grassland\u00a0soils showed similar patterns to that in forest soils. The major component of SOC in the three sampling locations was O-alkyl-C, which was mainly contributed by carbohydrate-derived structures.The second greatest C functional group in the soil was alkyl-C, with higher content in forest than grassland soil (\u00a0=\u00a00.002). The content is mostly from recalcitrant substances such as fatty acids and waxes (Mahieu et al. ). Jien et al. () found coniferous vegetation with high content of alkyl-C, which was attributed to selective preservation of alkyl-C from lipids and aliphatic substances (Tegelaar et al. ).Forest soils typically contain richer SOC and provide more aromatic-C and alkyl-C than grasslands because of the higher aromatic-C content (Golchin et al. ). By comparison, grassland (dwarfed bamboo) litter contains more O-alkyl-C, which can be more easily decomposed than that in coniferous forest (Wang et al. ). Moreover, previous research also showed that labile C can be significantly increased in the bamboo soil (Shiau et al. ; Wang et al. ). This fundamental difference in litter composition between the plants may remediate the wildfire affected SOC pools in the grassland. As the humification degree is calculated by alkyl-C/O-alkyl-C, higher O-alkyl-C provided by bamboo litter may result in the lower humification as we found in bamboo soils (\u00a0=\u00a00.008). This observation also showed the impact of wildfire on the humification degree of SOC may recover after 50\u00a0years of succession.The aromaticity (aromatic-C/total C functional groups ratio) was similar between the forest and grassland soils. Several studies found that wildfire and incomplete combustion increased the soil aromatic-C content and aromaticity (Vergnoux et al. ; Faria et al. ; Rossi et al. ). However, aromatic-C is dominant in recalcitrant substances such as cutins, lignin, lipids, resins, surface waxes and tannin (Wang et al. ), and is usually found in woody forest soils. The potential increase in aromatic-C content via combustion in the studied grassland soil might offset the high aromatic-C content originally in the forest soils and litters, since little difference in aromaticity was observed between the forest and grassland soils.Characterization of particle-size fractions is useful for process-oriented research into SOC (Mathers et al. ). Soluble organic C is readily utilized by soil microbes, whereas particulate SOC is a more important nutrient source for microbial activity (Mahieu et al. ; Chen and Chiu ). The results from all our sampling locations showed decreased O-alkyl-C peaks and increased alkyl-C peaks with decreasing soil particle size. This observation implied that the undecomposed recalcitrant substances tended to accumulate in the fine fractions of soils. The observation was also found in previous research in that the recalcitrant soil organic compounds were typically found stably binding with fine clay minerals (Calabi-Floody et al. ), whereas coarse particle-size fractions contained the major proportion of O-alkyl-C and aromatic-C materials (Kavdir et al. ). In addition, the finding of high alkyl-C spectra was consistent with low C/N ratios in the fine particle-size fractions, which suggests that the decomposition degree of organic materials was highest in the fine (<0.4\u00a0\u03bcm) particle fraction.Fire-affected grassland soil appeared to have lower TOC than forest soils in all soil particle-size fractions in our study. The humification degree was higher in forest than grassland soils. While the aromaticity was similar in forest and grassland soils, which might be attributed to the fire-induced aromatic-C content in the grassland that offsets the original difference in those characteristics between the forest and grassland.The fine particle-size fraction contained a high amount of alkyl-C and high humification, which implied that the undecomposed recalcitrant substances tended to accumulate in fine particle-size soil fractions. In addition, the low C/N ratios of the fine particle-size fractions was supported\u00a0with high alkyl-C in fine particles."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0173-y", "title": "Small agarics in Taiwan: ", "authors": ["Yi-Yin Chang", "Yu-Ming Ju"], "publication": "Botanical Studies", "publication_date": "4 April 2017", "abstract": "Small agarics are poorly documented in Taiwan, with previously reported species either rudimentarily described or lacking a description or diagnosis in most cases. A survey on small agarics in a lowland forest of Taiwan revealed two species previously unrecorded.", "full_text": "Small agarics with a pileus less than 1\u00a0cm diam pose a challenge for mycobiotic surveys, because they are easily overlooked and, after collected, their delicate, fragile basidiomata need to be measured and recorded in a timely fashion. Most of these small agarics belong to  (Pers.) Roussel Fr., and  Earle, each of which contains hundreds of taxa. Some of the small agarics can be found in genera such as  Massee and  Qu\u00e9l. They are saprophytes or in close association with mosses (Davey et al. ), while a few of them have been reported as being mycorrhizal (Zhang et al. ) or parasitic (Dennis ; Baker and Holliday ; Sequeira ; Bayliss ). These overlooked small agarics are actually underestimated. Notable plant pathogenic species include  (Berk. & M.A. Curtis) Sacc., which causes the well-known American leaf spot that decreased annual yields of coffee crops by 20% (Sequeira ; Rao and Tewari ), and  Stahel, which causes witches\u2019 broom disease on cacao in South America (Baker and Holliday ; Evans ). Certain  species contain laccases and other enzymes capable of degrading aromatic compounds, lignin, and \u03b2-carotene (Dedeyan et al. ; Scheibner et al. ). Noticeably, more than 30 species of  are known bioluminescent (Desjardin et al. ).There are over 500  species described in the world (Desjardin et al. ). In Taiwan, 21  species have been reported, but, in most cases, a species is merely furnished with a rudimentary diagnosis or lacks a description/diagnosis entirely. Shih et al. () described a bioluminescent species  Shih et al., which represents the only novel species of the genus documented in Taiwan thus far. Despite the great potential and special features that these small agarics may possess, without extraordinary characters such as bioluminescence and pathogenicity, their taxonomy hardly becomes a research interest for mycologists in Taiwan.In the present study, , which is characterized by a white, minute, hairy pileus and a conspicuous cup-shaped basal disc at the stipe, is described as new. It has characteristics of  section  K\u00fchner ex Sing. in general, but its basidiospores are inamyloid. In addition,  the type species of the genus, is reported and represents a newly recorded genus in Taiwan. Both  and  are tiny, growing on substrates, such as fallen leaves or twigs, in humid forests.A list of  species known in Taiwan can be found in  (Shao ), to which references to the listed species are referred. Five of the species lack a local description and are denoted with an asterisk (*).  Kominami is an invalidly published name, which appears in Sawada () and is excluded from this key. The key was adapted mainly from descriptions in Maas Geesteranus (), Desjardin () and Aronsen and L\u00e6ss\u00f8e ()."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0171-0", "title": "Investigation of floristic similarities between Taiwan and terrestrial ecoregions in Asia using GBIF data", "authors": ["Chi-Cheng Liao", "Chih-Hui Chen"], "publication": "Botanical Studies", "publication_date": "27 March 2017", "abstract": "Floristic compositions of non-endemic plants of continental islands were related to the neighboring continents because non-endemic plant species had historically migrated to continental islands from source areas. This study attempts to identify source areas of a continental island by means of floristic analysis and to assess possible migration routes on the basis of geographical distribution ranges of plants. Large quantities of angiosperm data records were downloaded from the Global Biodiversity Information Facility (GBIF). Similarity index and cluster analysis were used to identify the floristic similarities among 22 geographical localities of Taiwan (GLTs) and 34 terrestrial ecoregions in Asia. Geographical distribution ranges of non-endemic angiosperm species in Taiwan (NEASTs) were evaluated to mirror the possible migration routes from different source areas to Taiwan.", "full_text": "Oceanic islands possess disproportionately high plant species richness and numbers of endemic taxa (Bramwell and Caujap\u00e9-Castells ; Brooks et al. ; Carlquist ; Chen and He ; Cowie and Holland ; Kier et al. ; Kreft et al. ; Krupnick et al. ; Malcolm et al. ; Waldren et al. ). On the contrary, continental islands possess relatively lower proportion of endemic taxa and floras of continental islands are closely related to floras of neighboring continents (Bramwell and Caujap\u00e9-Castells ; Fern\u00e1ndez-Palacios et al. ; Heaney ; Hsu and Wolf ). In East Asia, plant species richness of continental islands are closely related to the Eurasian continents and neighboring regions (Chiang and Schaal ; Hiramatsu et al. ; Nakamura et al. ; Ota ; Setoguchi et al. ). The extent to which non-endemic species of continental islands are related to the neighboring regions remains elusive.Taiwan locates at the eastern border of Eurasian continent and was formed by the collision between Luzon Arc and Eurasian continent during 2\u20133\u00a0million years ago (Chen and Liu ; Hsieh et al. ; Liew and Hsieh ; Teng , ; Voris ; Wei ; Zeng ). Land bridge connections between Taiwan and Eurasian continent (Hsieh et al. ; Liew and Hsieh ) allowed immigration of plants from neighboring regions to Taiwan (Chiang and Schaal ). The proportion of endemic vascular plant species of Taiwan is 26.1%, while more than 73% of vascular plants in Taiwan are not endemic (Hsieh ). Non-endemic plant species might have migrated to Taiwan from the neighboring regions, or the source area. Nonetheless, that where had been the source areas of plants in Taiwan is an unanswered question.Since few decades ago, tens of botanists had interested on the source areas of angiosperm species in Taiwan. Several studies had addressed on the phylogenetic relationships of plants among Taiwan and neighboring regions, including Japan, Ryukyu archipelago and China, to evaluate the immigration of plants from neighboring regions to Taiwan (Chen et al. ; Chiang and Schaal ; Huang et al. ; Kokubugata et al. ; Wei et al. ). Some studies had focused on the floristic compositions to explore the species richness of Taiwan and floristic relationships among Taiwan and neighboring regions (Chao et al. ; Feroz and Hagihara ; Hsu and Wolf ; Liao et al. ; Tang et al. ). All the phylogenetic and floristic studies attempted to answer two questions: where were the source areas of flora of Taiwan and how many plants had migrated from tropical, subtropical or temperate regions to Taiwan, respectively. The questions remain unanswered because continuous vegetation coverage extends from Malay Peninsula northward to the Arctic of Siberia and floristic compositions of vegetations change from tropical to temperate regions (Fang et al. ; Ni ; Ohsawa , ; Olson et al. ). The question which vegetation types or which regions are the important source areas of insular non-endemic plant species in Taiwan have never been investigated.Taiwan is characterized by the massif of a central mountain system with the highest peak of ca. 4000\u00a0m above sea level (ASL) and Tropic of Cancer crosses the southern part of the island. The northern and southern part of Taiwan belongs to the subtropical and tropical climate zones, respectively. Climates change from southern to northern areas and from low to high elevations in Taiwan (Chen ). Diverse climatic conditions in Taiwan are influencing the plant distributions and differentiation of floristic compositions within the island (Chiou et al. ; Su , ). Six geographical areas was divided in Taiwan in terms of climatic environments, plant distributions and floristic compositions (Su , ). Floristic differentiation among six geographical areas in Taiwan leads to two assumptions. The first is that different floristic composition of geographical areas in Taiwan had presumably related to different source areas in Asia. The second is that there had been probably several migration routes for plants to migrate from different source areas to Taiwan.This study attempts to identify the source areas of non-endemic angiosperm species in Taiwan (NEAST). Floristic similarities between Taiwan and neighboring terrestrial ecoregions in Asia were analyzed to identify the possible source areas. Geographical distribution ranges of the NEASTs in Asia were explored to mirror the possible migration routes of angiosperms from different source areas to Taiwan.Recently, studies on large scale biodiversity pattern are available because of rapid accumulation of global data records (Beck et al. ; Flemons et al. ; Guralnick et al. ; Ingwersen and Chavan ; Saarenmaa ). GBIF is the largest online provider of global data records (Flemons et al. ; Padial et al. ). Large quantities of biodiversity data are inevitably spatially biased due to uneven effort of data collections and it is problematic to use species occurrence data in species distribution models (Beck et al. ). Subsampling distribution data had been used to reduce spatial bias of data and improved model quality (Beck et al. ). Our study attempts to minimize the bias error of species occurrence data by transforming georeferenced data into presence-absence data. Our results of floristic relationships between GLTs and ecoregions in Asia are acceptable because of few reasons. The first is that the numbers of scientific names are higher at subtropical ecoregions in this study. The species richness pattern presented by the numbers of scientific names in Table\u00a0 agreed with the proposed latitudinal pattern of species richness in Asia. Subtropical region in East Asia were proposed to have high species richness along the latitudinal gradient (Feng et al. ; L\u00f3pez-Pujol et al. ; Qiu et al. ; Zhu ). In this study, higher numbers of scientific names in two subtropical ecoregions agreed with the proposed latitudinal change of species richness, despite the numbers of data records are not the highest (Table\u00a0). In addition, not all the angiosperm species of ecoregions were utilized in this study but only the species that are common between Taiwan and ecoregions, the NEASTs. The utilization of species common between Taiwan and ecoregions diminished the effects of spatial bias of data collections and minimized the study error caused by spatial bias of database.Interestingly, our study has implied an extraordinary species richness pattern along latitudes in East Asia. A latitudinal pattern of angiosperm species richness appears while numbers of scientific names are compared among ecoregions of four groups (Table\u00a0). Two subtropical ecoregions in South China possess the highest number of scientific names in Asia and there are 12,271 and 11,696 scientific names of ecoregions 20 and 17, respectively (Table\u00a0). High numbers of scientific names in subtropical ecoregions are expected to have high angiosperm species richness.Generally, species richness is highest at tropical regions and decreases toward higher latitudes (Barthlott et al. ; Pianka ; Qian et al. , ). To our knowledge, higher angiosperm species richness in subtropical regions than in tropical regions is a novel species richness pattern along latitudes. Higher species richness at subtropical region observed in this study might be false because some factors may lead to a false species richness pattern along latitudes. The factors include bias data collections, synonyms in lists of ecoregions, various sizes of ecoregions\u2019 areas, etc. Although these factors do not support our observation, our observation cannot be rejected because of some reasons. The first, subtropical region had been proposed as one of the biodiversity centers in Asia since several decades ago (Wang , ). The subtropical region had served as a refuge for angiosperms in Asia during ice age in the Quarternary (Qian and Ricklefs ; Qian et al. ; Qiu et al. ; Tiffney ). These studies had offered evidences supporting high species richness of subtropical regions in Asia. The second, a recent study had also implied higher plant species richness at subtropical regions. The study proposed that species richness of family Fagaceae is higher at subtropical regions than at tropical or temperate regions in East Asia (Liao and Chen ). The study of Fagaceae species richness is similar to our results in Table\u00a0. In summary, angiosperm species richness along latitudes in Asia is worth to be investigated to understand the patterns and causal factors of species richness along latitudes in Asia.Latitudinal patterns of angiosperm species richness in Asia are important on the angiosperm specie richness in Taiwan because latitudinal range and climatic environments of Taiwan are similar to the subtropical regions in continental Asia. However, subtropical region is likely not the only source area of insular non-endemic angiosperm species in Taiwan. Some phylogenetic studies investigated population genetic variations to evaluate historical migration processes of plants from neighboring regions to Taiwan and five hypothetical migration routes have been proposed (Fig.\u00a0) (Huang , ; Shen ; Wang , ). The five routes provided pathways for plants to migrate from tropical islands (Route I), Indochina (Route II), southern China (Route III), northeast China (Route IV) and temperate islands (Route V) to Taiwan (Fig.\u00a0). Our study attempts to identify the extent to which the five migration routes affected on the angiosperm species richness of Taiwan.Our results of floristic similarities and geographical distributions both demonstrated that angiosperm species richness of Taiwan are closely related to tropical ecoregions at Indochina and subtropical ecoregions at southern China. The results support that the Route II and Route III are important on the angiosperm species richness in Taiwan. The Route II provided pathway for the migration of plants from tropical ecoregions or Indochina through Southern China Sea to Taiwan (Huang ; Shen ) and angiosperms of Group B, C, and D was able to migrate to Taiwan through Route II. The Route III is from the eastern slope of Qinghai-Tibet Plateau through southern China to Taiwan (Matuszak et al. ; Wang , ). The Route III provided the pathway for the migration of plants from subtropical ecoregions to Taiwan and angiosperms of Group C, D, E, and F were able to migrate to Taiwan through Route III.The plants of tropical islands had presumably migrated to Taiwan via long distance dispersal and angiosperms of B-II probably migrated to Taiwan by using Route I. However, tropical islands in south Asia, include Philippine and New Guinea, are not as important as Asian continents on the angiosperm species richness of Taiwan because of two reasons. The first, the number of NEAST is lower in tropical islands than in Asian continent. The second, floristic relationships between Taiwan and tropical islands are not as close as that between Taiwan and Asian continents. Land bridge connections had never existed between Philippine and Taiwan in the Quaternary (Voris ; Zeng ); therefore, plants migrated from tropical islands to Taiwan were probably dispersed by currents, winds or birds. It is likely because of that migration of plants through long distance dispersal is more difficult than that through land bridge connections.In contrast to tropical and subtropical ecoregions, temperate ecoregions are far less important on angiosperm species richness in Taiwan. The Route IV and Route V provided pathways for the migration of plants from temperate ecoregions to Taiwan (Fig.\u00a0). The Route IV is from northeast China through Korea, Yellow Sea, East China Sea to Taiwan (Shen ). The Route V is from Japan through Ryukyu archipelago to Taiwan (Huang and Lin ). The angiosperms of Group D and F extend distribution ranges from tropical or subtropical to temperate ecoregions had probably migrated to Taiwan through Route IV or V. However, it is questionable whether angiosperms of Group D and F had migrated from southern to northern latitudes or vice versa. A published paper had proposed several hypothetical migration routes from southern to northern latitudes in China (Wang , ). Angiosperms had migrated from Southwest China toward north, northeast, or east to northern China, northeast China, Japan or Taiwan, respectively (Wang , ). Therefore, angiosperms with distribution from tropical or subtropical to temperate ecoregions (Group D and F) had probably migrated from southern to northern latitudes or from southern China to Taiwan. The studies of Wang (, ) had implied that most of the plants with distributions from tropical or subtropical to temperate regions had probably migrated from southern to northern latitudes. Therefore, Route IV and V were not supported by the studies of Wang (, ). Although few evidences had indicated migrations of gymnosperms from temperate regions to southern latitudes (Li et al. ), more evidences are necessary for the identification of Route IV and Route V for the migration of angiosperms.Angiosperms of Group G have not been observed at tropical or subtropical ecoregions in Asian continent. These plants had probably migrated from northern latitudes to Taiwan or vice versa. Migration of plants from northern latitudes to Taiwan was supported by the phylogenetic studies of  and  var.  (Li et al. ). Migration of plants from northern to southern latitudes might have been caused by global cooling in the late Tertiary and Quaternary that had forced many plants to migrate southward (Chung et al. ; Denk ; Huang and Lin ; Huang et al. ). Meanwhile, phylogenetic study had identified that  had expanded from Taiwan to northern latitudes after ice age (Huang and Lin ). Migrations of plants from Taiwan to northern latitudes or vice versa were supported by some evidences of published documents. Analysis on the population genetics is suggested to provide evidence for identifying the migration of plants from northern latitudes to Taiwan or vice versa; to date, few study has focused on this topic. Route IV and V had provided pathway for plants to migrate from temperate regions to Taiwan, whereas there are only 150 species of Group G. The number of Group G indicated that Route IV and V are less important on angiosperm species richness in Taiwan.Our results concluded that tropical and subtropical regions at Asian continent were most important on the angiosperm species richness in Taiwan because of high floristic similarity between GLTs and tropical and subtropical ecoregions. Tropical and subtropical ecoregions are most likely the most important source areas of angiosperms in Taiwan. The tropical islands at South Asia are the second important. Angiosperms with distribution ranges over tropical and subtropical ecoregions are the dominant members of the vegetation from low to high elevations in Taiwan. Most of the angiosperms at high elevation in Taiwan are the species with tropical and subtropical distributions and, therefore, temperate ecoregions in Asia are less important on the angiosperm species richness in Taiwan. Infraspecific genetic variations of angiosperms in Asia is suggested to be investigated to determine the historical migration of angiosperms from tropical or subtropical regions to Taiwan."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-017-0012-5", "title": "Medical 3D printing: methods to standardize terminology and report trends", "authors": ["Leonid Chepelev", "Andreas Giannopoulos", "Anji Tang", "Dimitrios Mitsouras", "Frank J. Rybicki"], "publication": "3D Printing in Medicine", "publication_date": "17 March 2017", "abstract": "Medical 3D printing is expanding exponentially, with tremendous potential yet to be realized in nearly all facets of medicine. Unfortunately, multiple informal subdomain-specific isolated terminological \u2018silos\u2019 where disparate terminology is used for similar concepts are also arising as rapidly. It is imperative to formalize the foundational terminology at this early stage to facilitate future knowledge integration, collaborative research, and appropriate reimbursement. The purpose of this work is to develop objective, literature-based consensus-building methodology for the medical 3D printing domain to support expert consensus.", "full_text": "Three-dimensional (3D) printing offers plentiful opportunities for personalized and precision based interventions. The collective technologies have reduced costs and improved outcomes in essentially every industry in which they have been applied. In medicine, 3D printing has already revolutionized how we consider and treat patients in multiple clinical scenarios while offering hope for regenerative medicine [\u2013].An integrated assessment of available literature is crucial to establish a common, medicine-specific, vocabulary needed to facilitate collaborative research, knowledge integration, and ultimately reimbursement. Little formal consensus exists in the literature on fundamental concepts, including the naming of the field of medical 3D printing, with domain-specific publications using alternative terms such as \u201crapid prototyping\u201d and \u201cadditive manufacturing\u201d among many others. In light of the rapid growth in the number and diversity of research efforts in medical 3D printing, it is imperative to standardize specific terminology in the peer-reviewed literature before knowledge \u201csilos\u201d create significant barriers to collaborative efforts.Centralized medical publication repositories and data mining technologies have enabled rapid large-scale analyses of entire scientific domains. Natural language processing and semantic web technologies allow medical publications to be recast as machine usable knowledge, facilitating objective integration between medicine and other disciplines such as chemistry, biology, and epidemiology [\u2013]. These technologies provide a tremendous opportunity in enabling rapid and objective integration of all published medical 3D printing data.The purpose of this study is to develop and apply an objective scientific basis in order to standardize 3D printing terminology that will facilitate scientific, clinical, and regulatory communications. Using this basis, we present the evaluation of 3D printing research trends within published work to date, organized by medical discipline as well as geographic distribution. We then derive the dominant terminology within the domain to propose a common term to represent a collection of technologies that produce physical medical models. With this approach, we intend to facilitate more detailed analyses to further define trends and focus studies, better integrating 3D printing technologies and cultivating collaboration, better recognition, and ultimately supporting reimbursement.Published papers pertaining to medical 3D printing up to January 2016 were identified within PubMed using the following search query:(rapid[All Fields] AND prototyping[All Fields]) OR (additive[All Fields] AND manufacturing[All Fields]) OR (\u201cprinting, three-dimensional\u201d [MeSH Terms] OR (\u201cprinting\u201d [All Fields] AND \u201cthree-dimensional\u201d [All Fields]) OR \u201cthree-dimensional printing\u201d [All Fields] OR (\u201c3d\u201d [All Fields] AND \u201cprinting\u201d[All Fields]) OR \u201c3d printing\u201d[All Fields]) AND \u201chumans\u201d [MeSH Terms]All full text publications were manually retrieved and screened to ensure that papers having only marginal relationship to medical 3D printing were not included. Specifically, screening included a rapid survey to ensure either of the major terms and derivatives were present with simple text search-based screening. Papers yielding no matches for any of the direct or synonymous terms to those searched above in the full text were discarded. The remaining papers were analyzed using software developed within our group. The purpose of this software was threefold: i) to create a text corpus (i.e., collection) amenable to computer query and analysis, ii) to discover recurrent domain-specific terms, and iii) to analyze publication metadata such as date and geographic location in relation to domain-specific term use. To accomplish these tasks, the software first converted full text papers from PDF to plain text, extracted available article metadata, and isolated article text by removing references. The software subsequently extracted sentences, phrases, and individual terms to generate a text corpus. This text corpus then underwent the following four separate analyses.Our formalized methodology supports the standardization of terms derived from literature consensus. We demonstrate the development of closely linked publication concept clusters (Figs.\u00a0, ) that reflect major research directions but can also become fertile grounds for terminological isolation in the absence of a commonly accepted language. Furthermore, we provide evidence of the disparity in terminological use at the level of entire medical disciplines (dentistry, otolaryngology) even for the most important top-level terminology (Fig.\u00a0). These differences have likely arisen from historical chance factors, since qualitative assessment of imaging modalities (Fig.\u00a0), applications (Fig.\u00a0), and specific printing technologies (Fig.\u00a0) does not reveal a significant disparity to justify the observed terminological use difference. To avoid fragmentation of research efforts and terminological isolation in the setting of a lack of objective reasons for terminology use difference, common literature-supported terminology should be established.Our analysis establishes the dominance of the term \u201c3D printing\u201d to represent a collection of technologies that produce physical medical models. The success of medical 3D printing technologies will be more readily pursued by universally adopting the term \u201c3D printing\u201d that already encompasses most of the total use in the recent literature. We thus propose that \u201c3D Printing\u201d be formally adopted.Both \u201crapid prototyping\u201d and \u201cadditive manufacturing\u201d have historical significance. The term rapid prototyping originally denoted all technologies that could rapidly produce a physical prototype of a virtually designed object.  was established to distinguish it from the dominant Computer Numeric Control (CNC) subtractive methods, where raw materials in sheet or block form are secondarily processed with numerically controlled cutting and milling tools to  the unnecessary parts, leaving behind a whole prototyped object, or parts of the object that could be assembled into one.  therefore encompassed  and  []. Additive manufacturing then encompassed technologies ranging from electron beam melting to fused filament fabrication, with \u201c3D printing\u201d abbreviated as 3DP originally relegated only to a set of technologies emerging at the Massachusetts Institute of Technology that produced models by binding powder using an adhesive, much like an inkjet printer [].Our analysis demonstrates that over time, the meaning of these technologies has evolved in medicine to first make 3D printing nearly synonymous and interchangeable with the other two terms. Over time, \u201c3D Printing\u201d surpassed all others in literature. We recognize that more data can influence the use of terminology over time. We also recognize that, in theory, there may be specific medical uses for which the term \u201c3D Printing\u201d may not be scientifically accurate. In such rare cases, the most accurate terms should be used. However, the adoption of the term 3D Printing should be used for other cases so as to provide standardization and the ability of the field to mature with a standard lexicon.Our analysis did reveal alternative terms, including, \u201c\u201d. While this term has significance in general industrial applications, it is not significantly used relative to the other three terms in medical applications. Within our study, this term was observed in minority niche use, primarily within bioprinting research, specifically in relation to the 3D printing of hydrogels and tissue scaffolds (Additional file : Appendix 2). As demonstrated in concept cluster analysis (Figs.\u00a0 and ), bioprinting is a vibrant field that is relatively distanced from the numerous other concept clusters in the 3D printing domain. The use of the relatively rare term \u201c\u201d to describe 3D printing technologies may therefore be regarded as another line of evidence of terminological fragmentation and knowledge \u201csiloization\u201d in development.One limitation of this work is that many 3D printed models used in medicine do not reach the peer-reviewed literature, and \u201cniche\u201d laboratories are contributing largely to patient care, but to date are not recognized by the scientific community. Thus, the actual practice may not be faithfully reproduced, even with an exhaustive analysis of the literature. Within these limitations, our data demonstrates that the greatest impact made by 3D printing so far is in musculoskeletal and cardiovascular domains where it is used in support of procedure planning and device creation in support of operative interventions and postoperative recovery. Bioprinting is also evolving rapidly, not only for printing skeletal structures for which allografts were used for decades prior to the advent of widespread 3D printing, but also for expanding the field to include printing soft tissues such as the liver and the supporting vasculature.While we have established evidence for \u201c3D printing\u201d as the discipline name, the remainder of the domain terminology is not yet organized. This requires not only a deeper analysis using the methodology described here, but also a shift in the perception of publication value in medical 3D printing. Guidelines must be established for reporting 3D printed models, including those that call for complete description of the technologies used []. Individual 3D printed models and their applications should be recognized as case reports in medical literature, and models themselves should be stored in repositories using a single common format, in a manner similar to PubChem and the Protein Data Bank in chemistry and biochemistry, respectively.The methodology described has impact beyond the identification of \u201c3D Printing\u201d as the dominant recommended terminology in medicine. This objective approach should be used as a foundation of a discussion to obtain a single ontology, that is, a formalized collection of standard terms for all medical 3D printing, including terms developed in the future. That is, we can organize and describe 3D printing terminology into logically consistent categories that can be referenced, followed, and extended as the technology grows. The most important terms will become part of the frontline medical lexicon for 3D printing as the field grows, and this ontology would be used by computers to standardize and facilitate data analyses and integration, thereby avoiding research duplication and confusion. The risk of not formalizing terminology in this stage of rapid growth is \u201csiloization\u201d and fragmentation of research and clinical applications. It will be increasingly important to develop, validate, and adhere to standards throughout 3D printing, beginning with and including those related to terminology.Medical 3D printing continues rapid expansion in scope and number of publications. Within this rapid expansion, however, evidence of terminological research isolation has emerged. We demonstrate an objective methodology for rectifying terminological discrepancies and apply it to identify the dominant name for medical 3D printing research. The term \u201c3D Printing\u201d is the dominant term in the domain and should be formally adopted as the principal discipline name moving forward, to the exclusion of other synonyms and alternative names where appropriate. This analysis should be expanded to other terms and controversial concepts and used to support expert consensus in establishing a common ontology and a means of objective unbiased discussion of the terminology needed to enable collaboration, seamless knowledge integration, and medical reimbursement."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-017-0010-7", "title": "Accelerated workflow for primary jaw reconstruction with microvascular fibula graft", "authors": ["Elisabeth Goetze", "Matthias Gielisch", "Maximilian Moergel", "Bilal Al-Nawas"], "publication": "3D Printing in Medicine", "publication_date": "14 February 2017", "abstract": "Major facial defects due to cancer or deformities can be reconstructed through microvascular osteocutaneous flaps. Hereby CAD/CAM workflows offer a possibility to optimize reconstruct and reduce surgical time. We present a retrospectiv observational study regarding the developement of an in-house workflow allowing an accelerated CAD/CAM fibula reconstruction without outsourcing.", "full_text": "Advanced tumors or progressive chronic inflammation of the jaws frequently require segmental resection. Thereafter reconstruction by free microvascular bone transfer represents nowadays the method of choice in patients with acceptable health status [\u2013]. For reconstruction of the upper and particularly the lower jaw the microvascular fibula flap is mostly utilized for extended bone defects and regularly allows integration of a skin paddle p [, ]. The basic concept in raising free fibula flaps was first described by Taylor in 1975 but has evolved in parts over the last decade [, ]. Surgery can be supported by computer aided design (CAD) based planning and preoperative manufacturing (computer aided manufacture, CAM) of surgical templates [\u2013]. A CAD/CAM workflow allows preoperative definition of cutting paths and angles at the resection site, modeling of the graft as well as the shape of the osteosynthesis material resulting in an easy composable and placeable reconstruct []. The overall assembly time consisting in intraoperative cutting, positioning and refinement of the graft is reduced by the CAD/CAM workflow [, ], thereby substantially reducing risks concomitant with long-time surgery [\u2013]. Last but not least integrated CAD/CAM workflow may improve the esthetic and functional outcome by optimizing position and contour of the reconstruct [, ].The process of CAD/CAM planning can involve a commercial platform or be done by the clinic itself. Commercial solutions require communication between medical engineers and clinician and external logistic pathways. Exact information about average delivery time is not documented but the workflow results in a planning period of several weeks. Rustenmeyer for instance states a planning period of 2\u20134\u00a0weeks []. For in-house logarithms necessary planning time is not stated to date. Any delay in treatment may interfere with vital structures in extended malignancies or in tumor disease with rapid progression. Thus, the preoperative planning interval has to be as possibly short to provide a sufficient application of a CAD/CAM procedure. CAD/CAM procedures are described both primarily and secondarily [, ] but primary reconstruction in time-critical cases is not done routinely [, ]. Mazzoni et al [] state that CAD/CAM procedure and surgical application should be minimized to an interval of 2\u00a0weeks.The aim of the present retrospective observational study is to describe the development of an in-house workflow with reduced planning time and thereby allowing CAD/CAM based jaw reconstruction through microvascular fibula graft even in urgent cases.Retrospective analysis was done for 30 patients, the case of one patient is illustrated as full workflow. All patients gave written consent into the procedure and use of their data. For the case report the patient gave written consent in the publication of his pictures. The need of ethics approval was waived by the Ethics Commission of the State Chamber of Medicine in Rhineland-Pfalz according to Berufsordnung \u00a7 15 and Landeskrankenhausgesetz \u00a7 36 und \u00a7 37.The workflow was applied in 30 cases for primary and secondary reconstruction in the time from January 2014 to January 2016. The gender distribution was 1:2 (female:male). Average age was 50\u00a0years (50\u2009\u00b1\u200917). Eight patients underwent secondary reconstruction after a tumor free interval of 1\u20136\u00a0years, all other patients were primarily reconstructed with tumor resection during the same surgery. All patients were reconstructed with a free microvascular anastomized fibula bone graft. Two patients did not require skin graft, 19 patients had intraoral, six patients extraoral and three patients combined intra-/extraoral skin grafts.All patients received a probe biopsy beforehand the main procedure. The main diagnosis was oral squamous cell carcinoma (\u2009=\u200920; primary carcinoma \u2009=\u200914; recurrent carcinoma \u2009=\u20096), followed by sarcoma (\u2009=\u20094), ameloblastoma (\u2009=\u20092), adenocystic cell carcinoma (\u2009=\u20091) and osteoradionecrosis (\u2009=\u20091). Twelve patients had undergone irradiation or radiochemo therapy before surgery through neoadjuvant treatment or therapy of former malignancies. Twenty patients had anamnesis of nicotine consumption. Seven patients had positive anamnesis for arteriosclerotic disease.Planning could be applied successfully in all cases. Osteotomy and assembly time did not exert 1\u00a0h in all cases. Over all flap survival was 93%. Patient survival was 90% (\u2009=\u20093; none sooner than 3\u00a0month after surgery). Death was caused by cardiac arrest (\u2009=\u20091) three month after surgery or cervical/pulmonal metastatic tumor recurrence (\u2009=\u20092) after 4 and 7\u00a0month. Intraoperative complications regarding graft osteotomy, assembly and fixation in recipient site did not occur.Postoperative complications occurred in seven patients: Two fibula flaps were lost due to venous combustion in irradiated patients. Five patients suffered major complication (extensive wound dehiscence (\u2009=\u20093) requiring secondary surgery, wound infection with loss of skin flap (\u2009=\u20092), loss of transplant (\u2009=\u20092), recurrence of tumor (\u2009=\u20094; 3\u201310 month after histopathological R0-status; recurrence as cervical or pulmonal metastases), extensive bleeding requiring revision surgery (\u2009=\u20091). Major complication arose only in patients with preoperative radiation. Minor complications arose in nine patients including partial (\u2009=\u20096) or total loss of skin paddle (\u2009=\u20091), limited wound dehiscence (self limiting by secondary healing) (\u2009=\u20096) and venous obstruction with revision surgery (\u2009=\u20092).Analysis of workflow showed a planning period for reconstruction down to a minimum of 4\u00a0days (day of last necessary CT scan - mainly leg CT - to day of surgery). Mean time period was 8\u00a0days. Secondary reconstruction was not included in this analysis as the planning period could be chosen freely und thus was not shortened as much as possible.Thirty patients were successfully planned through an in-house CAD/CAM algorithm for reconstruction with a fibula graft. Major complications did not occur in relation to the planning itself and was attributed to pre-radiated patients. Higher risk of complications and flap loss for this patient group are described in literature [, ]. As far this workflow solution was just used for fibula graft but could also be applied to other bone grafts like scapula or iliac crest flaps [, ]. In literature assessment of surgical time regarding CAD/CAM procedures is heterogenic but mostly states time reduction [, , ]. The percepted reduction of surgical time has not been tested here, but leads to a lower risk of general complications [\u2013].Work bench time was completely done by a surgeon. In our opinion the crucial virtual and laboratory work bench steps, like planning of the osteotomy lines, taking tumor resection and skin perforators in account, or the design of surgical templates need a specific background knowledge. This means these steps should be done by specific trained personnel and cannot be delegated. Thus the effect of reduction of surgery time is a result of a transition of manpower into the pre-surgical phase outside of the OR. The saved surgery time is thus only redeployed as already mentioned [, ]. Overall there is still an economization as only one person is needed for planning instead of a whole OR team.An algorithm applicable without outsourcing makes CAD/CAM planning suitable even in urgent cases of primary cancer resections. One early shortcoming however of the current workflow is, that commercial software for surgery planning is available, but restricts rarely allows free data export thereby hindering a CAD/CAM pathway with internal resources. An open software or the development of such a solution with planning possibilities and appliances for template building does lead to further reduction of workbench time and further economization as show through the late developments in our workflow. This procedure may even allow the application in countries with lesser economic power inside the health system.A complete in-house workflow proved more cost effective by reducing material costs, personnel costs and surgery times. A suitable outsourced 3D print costs about 700\u20ac. The self-printed 3D model costs approx. 150\u2013220\u20ac for each planning. In summary compared with literature estimations our in-house workflow (517\u2013863\u20ac) seems to be lower than reported otherwise [, ]. The shortcoming of our economic evaluation is, that not all indirect costs (hard- and software) have been included and no calculation of the reduced intra-operative costs was possible. This question should be more clearly addressed in future studies including important direct and indirect costs.A possible influence on flap survival resulting from the application of planning still needs evaluation. This far only the positive effect of pre-surgical planning on shape and esthetic outcome is described []. Our hypothesis is, that the extensive involvement of the surgeons with each case necessary during the planning phase might have a positive effect on flap survival and complication rate. However the follow up time in this case series is not sufficient for a profound answer.In conclusion it can be stated, that it is possible to apply a CAD/CAM workflow to fibula graft reconstruction within a few days making this technique available for immediate primary reconstruction of malignant tumors."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0182-x", "title": "Three new species of ", "authors": ["Ching-I Peng", "Rosario Rivera Rubite", "Che-Wei Lin", "Mark Hughes", "Yoshiko Kono", "Kuo-Fang Chung"], "publication": "Botanical Studies", "publication_date": "29 June 2017", "abstract": "The flora of Panay Island is under-collected compared with the other islands of the Philippines.\u00a0In a joint expedition to the island, botanists from Taiwan and the Philippines found three unknown ", "full_text": "In continuation of our taxonomic and evolutionary studies of Philippine  (Nakamura et al. ; Rubite et al. , , ; Hughes et al. ; Tandang et al. ; Peng et al. ), we document novelties of  on Panay, the sixth largest island of the Philippine Archipelago. Elmer Merrill was the first to explore  in Panay Island, describing six new species,  Merr.,  Merr.,  Merr.,  Merr.,  Merr., and  Merr. (Merrill ). There were no further reports on the begonias of Panay since then (Rubite and Madulid ). After securing the necessary permits, a joint expedition to Panay was conducted by botanists from the University of the Philippines Manila, West Visayas State University, and Biodiversity Research Center, Academia Sinica. The group visited four provinces of Panay and found three new species of . From the Municipality of Culasi, Province of Antique are two distinct species:  C.I Peng, Rubite, C.W.Lin, & K.F.Chung from Kipot Falls, Barangay Buenavista, and  C.I Peng, Rubite, C.W.Lin, & K.F.Chung from the foothills of Mt. Madia-as, Barangay Flores. The third new species is  Rubite, C.I Peng, C.W.Lin & K.F.Chung from Igang Cave, Barangay Tapulang, Municipality of Maayon, Province of Capiz. All three new species belong to sect. , conforming to the morphological delimitation of the section (Rubite et al. ) and previous phylogenetic placement (Hughes et al. ).Of the 25 conservation priority biodiversity hotspots identified by Myers et al. (), the Philippines has the lowest percentage of remaining primary vegetation, at 3%. Hence the continuing discovery of endemic Philippine species means the remaining fragments of both primary and secondary native vegetation are of increasing value in terms of natural capital for the Philippines.The molecular phylogenetic results show a relict endemic element to the  flora of Panay (Hughes et al. ). Given the complex geological history of the island, which includes fragments of the Palawan microcontinental block (Hall ), further studies on different groups with an endemic element on Panay would be useful, as it is potentially a key area in the geological and biotic evolution of the archipelago.The discovery of three new species endemic to Panay highlights the importance of the island to the Philippines in conservation terms. The next step is to ensure the long-term future of the species described here in both ex situ collections in their native localities. This is an opportunity for botanic gardens and community conservation groups in the Philippines to achieve a high impact at low cost, as the species are relatively easily cultivated, and have small native populations which could potentially be protected in micro-reserves ()."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-017-0013-4", "title": "Implementation of iterative metal artifact reduction in the pre-planning-procedure of three-dimensional physical modeling", "authors": ["Roy P. Marcus", "Jonathan M. Morris", "Jane M. Matsumoto", "Amy E. Alexander", "Ahmed F. Halaweish", "James A Kelly", "Joel G. Fletcher", "Cynthia H. McCollough", "Shuai Leng"], "publication": "3D Printing in Medicine", "publication_date": "31 March 2017", "abstract": "To assess the impact of metal artifact reduction techniques in 3D printing by evaluating image quality and segmentation time in both phantom and patient studies with dental restorations and/or other metal implants. An acrylic denture apparatus (Kilgore Typodent, Kilgore International, Coldwater, MI) was set in a 20\u00a0cm water phantom and scanned on a single-source CT scanner with gantry tilting capacity (SOMATOM Edge, Siemens Healthcare, Forchheim, Germany) under 5 scenerios: (1) Baseline acquisition at 120\u00a0kV with no gantry tilt, no jaw spacer, (2) acquisition at 140\u00a0kV, (3) acquisition with a gantry tilt at 15\u00b0, (4) acquisition with a non-radiopaque jaw spacer and (5) acquisition with a jaw spacer and a gantry tilt at 15\u00b0. All acquisitions were reconstructed both with and without a dedicated iterative metal artifact reduction algorithm (MAR). Patients referred for a head-and-neck exam were included into the study. Acquisitions were performed on the same scanner with 120\u00a0kV and the images were reconstructed with and without iterative MAR. Segmentation was performed on a dedicated workstation (Materialise Interactive Medical Image Control Systems; Materialise NV, Leuven, Belgium) to quantify volume of metal artifact and segmentation time.", "full_text": "The introduction of three-dimensional physical modeling is evolving as an important tool in the medical field, especially in pre-operative planning of complex surgery procedures, medical training or patient education [\u2013]. However printing of the final medical three dimensional model is preceded by a number of laborious steps involving computer assisted segmentation of the structures of interest, as described elsewhere in great details []. Metal artifacts commonly seen in computed tomography (CT) images, induced by dental hardware or orthopedic prosthesis, hamper the diagnostic evaluation of radiological images, especially affecting the palatine and root of the tongue in cranio-maxillo-facial and head-and-neck imaging [\u2013]. Besides the effect on intracorporal anatomical structures, those artifacts also generate extracorporeal artifact-structures, disturbing the fabrication of the 3D-model, adding additional time-consuming segmentation steps to eliminate those structures. In order to overcome these artifacts, acquisitions and post-processing techniques, such as higher x-ray tube voltage, gantry / head tilting, dual-energy or dedicated metal artifact reduction algorithms have been proposed and implemented in diagnostic CT exams [, \u2013]. However, the impact of these techniques on segmentation of data for 3D printing has not been thoroughly investigated.The purpose of this study is to assess the impact of metal artifact reduction techniques in 3D printing by evaluating image quality and segmentation time in both phantom and patient studies with dental restorations and/or other metal implants.In this study we demonstrated the use of various metal artifact reduction techniques in order to enhance the anatomical 3D segmentation hampered by heavy metal artifacts. In addition we showed that using dedicated iterative metal artifact reduction algorithm is the most promising technique to reduce the metal artifact volume and hence reducing segmentation time. Those findings are important, since a majority of head-and-neck CT acquisitions are associated with metal artifacts due to dental restorations. This results in laborious 3D segmentation of the craniomaxillofacial bones and separation of the mandible from the maxilla which is needed when 3D printing clinical cases. Previous studies evaluating metal artifact reduction techniques mainly focused on the diagnostic performance and our observations made in the phantom and patient studies agreed with the findings in the present literature [, \u2013].Bannas et al. showed an incremental value of gantry tilt on the reader\u2019s sensitivity of detecting oral tumors as gantry tilt redistributed the metal artifacts to other locations outside of the slices where the tumor was []. Our phantom data support the visual results, however quantitatively the segmented artifact volume was not different between both gantry states. Artifact segmentation time of the data in non-tilted position was shorter as the artifact did not have to be removed from as many slices. This is mainly due to the concentration of artifact in a smaller and more condensed volume, resulting in a faster and simplified containment. In clinical practice, if the sole purpose of the CT exams is for 3D modeling, patients should be scanned without gantry tilt, as demonstrated by our phantom data. However, if the scan is for both 3D printing and other diagnostic tasks (e.g. tumor detection) where gantry tilt benefits the diagnosis, the patients should be scanned with gantry tilt. In this case, the benefit of accurate tumor detection outweighs the drawback of increased processing time for 3D modeling. Dedicated MAR technique should be used, which has been shown to have a positive impact for the clinical evaluation and 3D-segmentation.The use of a jaw spacer has been recommended for maxillofacial CT-acquisitions in order to separate both jaws []. Our data support the recommendations without having any effects on the segmented artifact volumes. Clinically many of our 3D printed craniomaxillofacial cases are for tumor resection or congenital facial reconstruction and the surgeon often requests the mandible be printed separately from the maxilla for surgical planning. This requires two separate volumes to be created which is hampered by overlapping artifact which is greatly reduced with the jaw spacer. A separate benefit of the spacer was more realistic appearance to the teeth after segmentation. The most time and labor effective artifact reduction method in both phantom and patient model is the dedicated iterative metal artifact reduction []. Our study hence adds to the current literature stating that the use of this technique enhances the 3D-segmentation workflow. As demonstrated in a previous study, the MAR algorithm can occasionally deteriorate the bone contours if the parameter settings were not optimal []. Therefore, caution should be taken using MAR algorithm and bone contour should be carefully checked based on professional expertise during the segmentation process.Our study has a number of limitations. First, our patient sample size is very small, since the number of patients referred for pre-operative 3D-printing was limited. Secondly, the phantom used was made of acrylic, which has lower attenuation than bone and consistent CT number unlike the bone which is more variable. In this study, we lowered the CT number threshold in the segmentation process to accommodate this difference between the material properties. This procedure, however, was not necessary in the human subject studies. As seen in our patient study, substantial improvement of image quality was achieved with real bones. Another limitation is that the model after thresholding and manually alteration was used as the reference model when artifact volume was calculated. Since this process was done separately for images reconstructed with and without MAR, it is possible there are slight differences in the artifact volume based on manual artifact removal. Building a common reference model with other techniques, such as 3D scanning, may provide a more accurate estimation of artifact volume. However, we expect the difference would be small as the radiologist tried to maintain the same anatomy after artifact removal and carefully reviewed the final data before calculating artifact volume.In conclusion, the use of metal artifact reduction techniques, especially iterative metal artifact reduction algorithm, shortens the time of segmentation for 3D printing and provides a more accurate mandible and maxilla in areas affected by metal artifacts."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-017-0014-3", "title": "Anatomic modeling using 3D printing: quality assurance and optimization", "authors": ["Shuai Leng", "Kiaran McGee", "Jonathan Morris", "Amy Alexander", "Joel Kuhlmann", "Thomas Vrieze", "Cynthia H. McCollough", "Jane Matsumoto"], "publication": "3D Printing in Medicine", "publication_date": "26 April 2017", "abstract": "The purpose of this study is to provide a framework for the development of a quality assurance (QA) program for use in medical 3D printing applications. An interdisciplinary QA team was built with expertise from all aspects of 3D printing. A systematic QA approach was established to assess the accuracy and precision of each step during the 3D printing process, including: image data acquisition, segmentation and processing, and 3D printing and cleaning. Validation of printed models was performed by qualitative inspection and quantitative measurement. The latter was achieved by scanning the printed model with a high resolution CT scanner to obtain images of the printed model, which were registered to the original patient images and the distance between them was calculated on a point-by-point basis.", "full_text": "First used in manufacturing, medical applications of 3D printing or additive manufacturing have been rapidly developing. Using a patient\u2019s own medical image data, 3D printing can be used to create individualized, life-size patient-specific models. These models are increasingly being used as aids in surgical planning for complex cases [\u2013]. 3D models can contribute to surgical procedures by providing surgeons with an accurate life size physical reproduction of the anatomy of interest. In addition, models offer unique educational opportunities, including simulation for resident training [, ]. Research applications include patient-specific imaging and therapeutic phantoms used for advancing imaging techniques, reducing radiation dose, and conducting treatment planning and dose verification in radiation therapy [\u2013]. Additional applications include the development of patient specific surgical guides and the reproduction of forensic models.3D printing offers advantages over conventional manufacturing technologies. Individualized single models can be created as needed in a clinical setting with relatively low cost in a fairly short time frame. Depending on the type of 3D printing technology used, models can be printed with varying material types, colors, and mechanical properties with potential for sterilization. 3D printing is more efficient and less costly than standard manufacturing technologies and is uniquely suited to contribute to individualized patient care.While the role of 3D printing in medicine is rapidly expanding and will certainly be a part of medical care going into the future, it is important that quality assurance (QA) programs are in place as part of the development and maintenance of a 3D printing program. To develop a QA program, it is important to identify the steps involved in generating a 3D model. There are essentially three steps involved in 3D printing in medicine: (1) Step one involves the acquisition of 3D volumetric images of the patient. (2) Step two is to separate out the anatomy of interest from surrounding structures and output the segmented virtual models as stereolithography (STL) files. This step also includes the editing of the original segmented objects, such as wrapping and smoothing. (3) Step three is to print the physical models and clean them. While the accuracy of medical imaging is a critical first step in 3D printing, the additional elements of segmentation and processing of imaging data and the technical aspects of the 3D printing process can all affect the accuracy of the final 3D printed model. Each of these production steps should be individually evaluated, analyzed and optimized. Medical confidence in the accurate representation of patient anatomy and pathology is a necessary component of medical 3D printing applications.A successful QA program requires input from all stakeholders involved in the 3D printing process. As such, it is critical to build an interdisciplinary QA team. Typical stakeholders include: 1) Surgeons or physicians who order the model know the clinical need, and are the end users of the model; 2) Interpreting radiologists who possess expertise in acquiring and interpreting the imaging study; 3) Medical physicists who are experts in the imaging technology and ensure exams are performed with optimized scanning and reconstruction techniques; they also have extensive experience and leadership roles for the QA programs in a radiology department; 4) Technologists who perform the imaging and segmentation; 5) Engineers and operators who do variable amounts of segmentation, model design and printer maintenance. An essential component of a successful QA program is effective communication between these team members to ensure creation of a high quality model.A QA program with validation, verification, and documentation to assure accuracy and quality is therefore a key component of 3D printing. Fortunately, radiology departments have significant expertise in the development of QA programs and this experience can be adapted to medical 3D printing. Expansion of QA programs to include evaluation of the unique characteristics of 3D printing technology and segmentation processes forms the basis of a medical 3D printing QA program. The purpose of this paper is to give an overview of the QA program that has been developed at our institution to assess the accuracy and precision of each step of the 3D printing process.We have developed a systematic approach that involves QA for each of the major steps of 3D printing: imaging, segmentation and processing, and printing. In the following subsections, we will discuss the appropriate QA process for each of these three major steps.The past decade has seen remarkable growth in the use of 3D printing in medicine. The growth has been fueled by the development of high resolution imaging studies merging with the rapid development of 3D printing technologies, and the development of new printing materials. These advances have resulted in reductions in the costs associated with creating high resolution medical models. The evolution of this disruptive technology is expected to revolutionize medical practice.While much attention has been focused on the application of 3D printing in medicine, less attention has been given to ensuring that the 3D printed model is a true and accurate representation of the physical object under study. If this technology is to be widely integrated into advanced medical practices, ensuring the physical accuracy and reproducibility of 3D models by means of a comprehensive QA program is essential.This work addresses these concerns by providing the necessary framework for developing and maintaining a QA program for 3D printing. Incorporation of existing imaging quality procedures forms the foundation for the program. It begins with acquisition of high quality imaging data using ACR accredited personnel and machines. Accuracy of segmentation by trained staff and awareness of the impact of processing STL files is the second step in the program. Verification and validation of the 3D printing process including phantom testing and model analysis is the third step. Unique identification of models along with documentation in patient medical record is also an important part of the process. The QA program delineated in this work thus describes an end-to-end testing of the entire 3D printing process.Among the three major steps in 3D printing: imaging, segmentation, and printing; the segmentation step is the most challenging with regards to QA. Significant effort was reported investigating accuracy when printing boney anatomy, with scanning bone in air, water and in situ with subsequent removal of soft tissues to reveal bone dimensions to perform the comparison [\u2013]. The accuracy of segmenting highly depends on the image quality, such as spatial resolution and contrast to noise ratio. For models scanned in air, such as the QA phantom or the printed model, the segmentation is relatively easy given the high contrast between the model and the air background. It becomes much more challenging for patient cases, especially for segmentation of different types of soft tissues with low contrast. Therefore, it is essential to check for accuracy of the segmentation before printing. We recommend checking the overlaps between final STL files and the original source images. Medical knowledge about anatomy and pathology is required to judge the accuracy of segmentation. Work presented here represents early and simple approaches. Further development on objective, quantitative and time-effective QA method in segmentation is desired.Phantoms are critical for the QA process. Two generations of QA phantoms have been developed and used in this study. A similar QA process and testing phantom like the first generation QA phantom can be easily developed by those starting a 3D medical modeling program. In this way, experience and expertise can be gained without creating too complex of a QA program which may absorb limited resources and personnel. However, the 2 generation phantom has more test objects and shapes which can more comprehensively test all facets of the 3D printing process.One of the validation methods presented in this study was to scan a printed model and compare with the initial segmentation. This method works well to evaluate the total shape of the printed model, but challenges exist to evaluate internal structures of models with multiple components. For models with multiple components, materials with different HU values should be carefully selected to represent different components. The HU range of available printing materials could be a limiting factor in this approach []. The alignment of the source STL and that resulting from scanning the printed model could be potentially difficult in certain scenarios. Although automatic alignment tools exist in some software, the accuracy of the alignment should be carefully evaluated and manual adjustment may be needed. Misalignment will result in errors in the evaluation of agreement. Also, appropriate imaging modality and imaging protocols should be used. High resolution, high geometric accuracy, low noise, and artifact-free imaging method should be used to avoid additional errors introduced by the imaging process in alignment and measurement. This method also has challenges for models built with flexible materials. Since these models may deform after being printed, it is not an easy task to ensure that the model maintains its original shape during the scan. For this scenario, it is helpful to build some supporting structures while building the model to support the model in order to have it retain the original shape it has in the patient. This way, the same method can be used. Given these potential limitations, this method should be carefully evaluated for the specific applications before used as a routine QA process.There are several limitations to this work. First, the protocols are based on experience with a single type of 3D printer and with segmentation software from a single vendor. The general framework and concepts of this QA program, though, can be extended to other types of printers with appropriate adjustments made according to the specific printing technology and to type of segmentation software. Secondly, our experience relies heavily on the use of CT imaging data which is used for the majority of our models as CT provide high spatial resolution and high geometric accuracy, both of which are critical for 3D printed models used in medicine. However, general principles outlined in this paper apply to 3D printing using other imaging modalities too. MRI data is increasing used as an adjunct to the CT data as higher resolution MRI imaging sequences are being developed. The use of 3D Ultrasound data is still in early stages of exploration for 3D printing. Finally, the QA program does not provide specific and quantifiable standard for 3D printing. As this technology evolves substantial QA data from multiple institutions needs to be accumulated over time so that appropriate specific and quantifiable QA standard will be developed and adopted by the medical 3D printing community.In conclusion, this work describes the development of a comprehensive QA program for 3D printing in medicine. It is the hope that the methodologies here described will contribute toward the growing body of work needed to establish standards for QA programs for medical 3D printing."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-017-0015-2", "title": "Surgical reconstruction of the ossicular chain with custom 3D printed ossicular prosthesis", "authors": ["Jeffrey D. Hirsch", "Richard L. Vincent", "David J. Eisenman"], "publication": "3D Printing in Medicine", "publication_date": "27 July 2017", "abstract": "Conductive hearing loss due to ossicular abnormalities occurs from many causes, including trauma, infection, cholesteatoma, surgery and congenital anomalies. Surgical reconstruction of the ossicular chain is a well-established procedure for repair of ossicular defects, but is still plagued by high failure rates. Underlying disease and proper sizing of prostheses are two challenges that lead to component failure. Three-dimensional (3D) printing has been used successfully to solve a number of medical prosthesis problems. Custom 3D printing an individualized ossicular prosthesis would be a potential solution for the wide range of anatomic variation encountered in the pathological middle ear, and could decrease the rate of post-operative prosthesis displacement by increasing the likelihood of a proper fit, in addition to decreasing surgical time.", "full_text": "Conductive hearing loss due to ossicular abnormalities has many etiologies including trauma, infection, cholesteatoma, surgery to treat these diseases, and congenital anomalies. Surgical reconstruction of the ossicular chain is a well-established procedure for repair of ossicular defects, but is still plagued by high failure rates, with success rates in closing the air-bone gap to less than 20\u00a0dB ranging generally from 55%\u201375% [\u2013]. Poor hearing results in many cases can be attributed to anatomical factors and persistence or recurrence of an underlying disease process, such as tympanic membrane retraction, middle ear atelectasis, fibrosis or mucosal pathology. However, none of these fully accounts for persisting air-bone gaps following ossiculoplasty [, ]. That these factors do not fully account for the failure rates is also implied by the fact that similar results are obtained with ossicular chain reconstruction following middle ear trauma, a situation in which most of those factors are not an issue [, ]. Some degree of hearing loss can be attributable to the design of current prostheses, which do not capture all of the mechanical advantages of the normal ossicular chain. Nevertheless, it is still likely that improper fit, due to both inaccurate size, angulation and position of the prosthesis, plays a significant role. In one series with long-term follow-up, more than 40% of failures were attributed to prosthesis or surgeon related errors [] Proper intraoperative sizing of a prosthesis is challenging, and can be affected by limited exposure and variability in the anatomic relationships of the ossicular remnants to each other or to the tympanic membrane, as well as by post-operative changes during the healing process. In particular, the medial-lateral distances between ossicular remnants, the anterior-posterior offsets, and the position of and their relationship to the tympanic membrane or neo-tympanic membrane vary widely from patient to patient in the pathologic setting, [] and are not always readily amenable to reconstruction with off the shelf prostheses.Three-dimensional (3D) printing has been used for a wide variety of medical applications [\u2013]. Custom 3D printing an individualized ossicular prosthesis would be a potential solution for the range of anatomic variation encountered in the pathological middle ear. Custom designed prostheses could decrease the rate of post-operative prosthesis displacement, and improve the hearing outcomes, by increasing the likelihood of a proper fit. Custom printed prostheses would minimize the need for intraoperative estimates of size, and would therefore also decrease surgical time, with resultant cost savings. However, it is not known if current technologies are suitable for application to the small anatomic variations found in the middle ear. The small size of the middle ear and its ossicles present challenges both for reliable image acquisition to provide accurate data for prosthesis design, and for printing of prostheses that faithfully reproduce the measured differences.If each of these is answered in the affirmative, then development of a customized, 3D printed ossicular prosthesis should be feasible.3D printed solutions have been shown to be successful adjuncts to surgical technique. Accurately reproducing a patient\u2019s specific pathologic anatomy for preoperative planning is a common thread. Patient specific custom made anatomic models used in preoperative planning have been shown to decrease operative time [] and in one report to also decrease intraoperative blood loss []. Additionally, models allowing for accurate surgical simulation in orthopedics and cardiovascular procedures have enhanced preoperative decision-making, improved precision and increased work efficiency [, ]. Prosthesis fabrication using 3D printed technique is another developing field [, ].The present study demonstrates that 3D\u2013printed ossicular replacement prostheses are unique in size and shape when using CT imaging as a basis for modeling, and that these differences are detectable by Otologic surgeons, who can accurately match the individual prostheses to their parent bones. The important landmarks within the middle ear are readily detectable during image interpretation of the middle ear. The malleus is usually well seen and masking of that bone is straightforward. The stapes is a much smaller bone and has a much smaller mass to attenuate the CT image beam. As a result the crura of the stapes, the thinnest part of the bone, are not well seen. However, the capitulum and neck of the stapes is more dense, and usually more reliably detected during image interpretation. This is important, because the stapes capitulum is where one side of the prosthesis rests. Once masks are made of these two landmarks a model can be designed and fabricated. The printer used to fabricate the prosthesis used SLA technology. The resolving threshold of the printer is on the order of centimicrons in the XY plane and decimicrons in the Z axis. This allows for an accurate representation of the model to be fabricated without significant intrinsic errors from the printer to be introduced to the prosthesis. The true test of accuracy, however, is if the prosthesis model fits in the space for which it was designed. In four separate trials with different surgeons, each surgeon was able to accurately match the correct prosthesis to its intended temporal bone. This further supports that the differences in size and shape for each prosthesis are meaningful and detectable by the surgeon. Additionally, that it is possible to fabricate a custom made middle ear prosthesis using routine CT imaging of the temporal bone, existing modeling software, and a desktop SLA printer.Accurate quantification of the middle ear for fabrication of a custom prosthesis presented unique challenges. The process starts with identification of the important landmarks. As previously mentioned, the important landmarks for prosthesis construction are detectable with routine CT imaging protocols. The landmarks form the basis and starting point for fabrication of the prosthesis. As a result, subtle anatomic variation is inherently captured in the design of the prosthesis. Thus, establishing a method to accurately quantify parameters of the prosthesis also captures the subtle anatomic variation from ear to ear. Once the model was designed, then the next challenge is printing this very small part. Almost immediately apparent was the increased risk of losing the part during post processing due to its size. This was mitigated by utilizing a sinter box. The sinter box is a designed cage around a part that is fabricated with the part during the printing process. This, in essence, prints a larger part making it more difficult to lose. Additionally, it also provides a way to label the part and increases ease of handling. The torus cradle as seen in Fig.  is the sinter box used during printing of the prosthesis. Persisting conductive hearing loss following ossicular chain reconstruction is multifactorial. The single greatest variable in many cases is likely the underlying disease process, which may render the ear unsuitable for reconstruction over the long term [, , ]. Chronic infections and associated chronic Eustachian tube dysfunction can result in stiffness of the ossicular remnants, middle ear fibrosis, middle ear atelectasis, recurrent otitis media and other factors that decrease the chances of a satisfactory hearing result, either due to intrinsic limitations to adequate sound conduction, or from displacement and/or extrusion of the prosthesis. Nevertheless, technical factors such as imprecise sizing and placement also play a significant role []. These data are supported by the observation that outcomes are not significantly better, if at all, for reconstruction following traumatic ossicular discontinuity, [, ] a situation in which chronic infection and Eustachian tube dysfunction are not usually a factor. CT-based, custom 3D printed prostheses should minimize the impact of these variables, and consequently increase success rates.Inability to accurately simulate the CT imaging in vivo is a technical limitation of the study. The cadaver middle ear used in this study was cut-down to size from the full-sized skull to include only a portion of the surrounding bone. As a result, the attenuation of the CT beam is much less for the cadaver ear and should provide a much better signal to noise ratio when compared to a comparable in vivo image data set. This will need to be addressed in future studies, as the important landmarks needed for prosthesis design are subtle imaging features that may be more challenging to detect on in vivo imaging.An additional limitation of this study includes a lack of functional data. The design of the prosthesis should allow it to function similarly to existing, predicate models, and as such it would be presumed to result in adequate functional restoration of hearing. However, the present study does not offer comparative data, cadaveric or in vivo, demonstrating similar or better mechanical properties of the 3D reconstructed ossicular chain as compared to existing technologies. Another limitation of this approach, in general, is that it would only apply to clinical scenarios in which the middle ear anatomy will not otherwise be altered by the surgical procedure. If extensive removal of disease, including ossicular remnants, tympanic membrane, and/or portions of the external auditory canal and mastoid are planned, then pre-operative CT will not be able to predict the post-extirpative anatomy. This approach is only useful for patients undergoing a planned, isolated ossicular chain reconstruction, with no other procedural alterations in the anatomy.Future studies will address these and other issues. Both cadaveric and in vivo functional results of a custom 3D printed prosthesis need to be measured, and compared to existing models. Additionally, the optimal biomaterial choice needs to be determined. Current materials have a high rate of extrusion when supported laterally by the tympanic membrane alone. As such, standard practice is to interpose a cartilage cap over the prosthesis to prevent that untoward outcome. This, however, can potentially dampen sound transmission, and adds another layer of risk for displacement of an element of the reconstruction. The ideal prosthesis would be fully biocompatible, and not require any additional protective layer.A custom 3D printed ossicular prosthesis is a viable solution for conductive hearing loss due to ossicular chain defects. Commercially available CT scanners can detect significant anatomic differences in normal human middle ear ossicles. These differences can be accurately represented with current 3D printing technology, and otologic surgeons can detect these differences in situ. This process overcomes the common technical challenge of properly sizing a prosthesis intraoperatively, as each model is custom made for an exact fit, and may lead to improved results and decreased operative time."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0181-y", "title": "Genome-wide characterization of the ", "authors": ["Peng Li", "Wangzhen Guo"], "publication": "Botanical Studies", "publication_date": "2 June 2017", "abstract": "\n                           ", "full_text": "As the most important cash crop in the world, cotton provides a very important natural fiber for human beings. Therefore, we tried to explore the important genes related to abiotic stress, biotic stress and fiber development from the whole genome of cotton, hoping to improve cotton quality and yield.  protein family in cotton is a very large family. According to previous reports,  family is not only closely related to plant growth and development, but also plays an important role in stress and disease resistance.The world\u2019s first  gene () was discovered in yeast in 1983 (Gallwitz et al. ). Four years later, Salminen and Novick demonstrated that  () is involved in vesicle trafficking (Salminen and Novick ). In the same year, Tavitian and his colleagues cloned the homologous gene similar to the  gene for the first time through cDNA library in rat brain and named s-like in rat rain () (Martinez and Goud ). In , 57 Rab proteins were found and divided into  to  (Hill and Sylvester ).  to  in  correspond to , , , , , ,  and  in animal, indicating that  family has further differentiation in plant (Pereira-Leal and Seabra ; Brighouse et al. ).\n                         protein family members have about 200 amino acids. Their sequences are conservative with high sequence similarity. All  proteins have five typical conserved domains, including four guanine nucleotide binding domains (G1, G3, G4 and G5) and an effector binding domain (G2) (Takai et al. ; Agarwal et al. ). Four domains of G1, G3, G4 and G5 participate in the binding and hydrolysis of nucleotides. Among them, G1 is the binding site of phosphate or Mg, G4 and G5 are key sites involved in GTP-GDP binding and hydrolysis (Stenmark and Olkkonen ). Mutations in the important amino acid sites of these conserved domains will cause  proteins to produce some constitutive inhibiting or activating mutant proteins. Normally, the amino acid sequence at the C end of the  protein is highly variable, but it ends with two conserved cysteine residues (CC) ultimately. These two highly conserved cysteine residues play an important role in membrane localization and protein function (Rutherford and Moore ). Although five G domains of  protein is discrete distribution in amino acid sequence, they are close to each other to form a special catalytic domain in the three-dimensional conformation of proteins, so as to better exercise the function of a protein (Rutherford and Moore ). The main function of the  protein family member is responsible for intracellular protein transport and they are essential regulator of vesicle trafficking way (Novick and Zerial ; Brennwald ). Newly synthesized secretory proteins are usually transported from one compartment of the organelle to another membrane through vesicles (Gurkan et al. ). They are transported to the endoplasmic reticulum at first, then transported to the plasma membrane through the Golgi apparatus, and some are delivered to the lysosome. In general,  proteins were involved in various cellular physiological functions of vesicular transport, such as cell polarity, cytokinesis, cell plate formation and so on (Barr ). Therefore, vesicular transport includes four steps at least. Vesicles budded from the donor membrane, moved to the receptor, anchored in the membrane receptor, and fused with membrane receptors. These processes all need the active involvement of  proteins (Tuvim et al. ; Yang ).Up to now, the identification of  protein gene family in  (Rutherford and Moore ) has been very clear. But the related gene family research has not been reported in cotton. As is known to all,  gene family is a vital family in plant (Hill and Sylvester ). As one of the most important families in cotton, the  gene family plays a key role in the process of fiber development and biotic and abiotic stresses. With the great progress of genome sequence information in four different cotton species (, ,  acc. TM-1 and  acc. 3-79) (Wang et al. ; Li et al. ; Zhang et al. ; Yuan et al. ), we tried to mine the important genes related to fiber development and biotic and abiotic stresses in the whole genomic level for their functional analysis.In this study, combined with the released genome information in four sequenced cotton species, the cotton  gene family members were systematically studied. We analyzed the characteristics on the gene structures, classification, chromosomal locations, and expression patterns of  gene family members. Our studies will lay a foundation for understanding the functional roles of different  members in the polar growth and stress tolerance in cotton.It is reported that  genes played the various roles in all kinds of plants. Through cotton genome sequence information, we tried to analyze the basic biological information, gene structure, genome distribution, and expression characteristics of  gene family members. We hope to find some important genes related to stress response and fiber development in the whole genomic level, and help to utilize them to improve the quality and yield of cotton.\n                         proteins play an important role in plant growth and development, as well as in biotic and abiotic stress responses. In this study, we individually identified 87, 169, 136, 80  in the four sequenced cotton species. These  are divided into eight groups. In each group, their intron numbers and subcellular localization are basically the same. Further, 60 pairs of segmental duplication due to whole genome duplication and two pairs of tandem duplication were detected, respectively. Expression patterns analysis indicated that most  family members play a certain role in different tissues/organs and different growth stages of cotton, implying their potential function in the polar growth and stress tolerance."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0183-9", "title": "Allelopathy and resource competition: the effects of ", "authors": ["Md Nazim Uddin", "Randall William Robinson"], "publication": "Botanical Studies", "publication_date": "29 June 2017", "abstract": "\n                           ", "full_text": "Allelopathic interference by invasive plant species has potential to impact seed germination, seedling growth, development and establishment of neighbouring plant species, as well as of the same species, in both natural and agricultural systems (Bich and Kato-Noguchi ; Dorning and Cipollini ; Lara-N\u00fa\u00f1ez et al. ). Allelopathy has been considered an important attribute to the success of an invasive species in natural ecosystems (Callaway and Ridenour ; Kimura et al. ; Lorenzo et al. ). The sources of allelochemicals released into the rhizosphere include leaching from leaves and other aerial parts, volatilization, root exudation and litter decomposition (Hussain and Reigosa ; Uddin et al. ; Weir et al. ).\n                        , a ubiquitous wetland plant, is considered one of the most invasive species in the world (Uddin et al. ) however, the origin of the species is still unclear (Plut et al. ). A perennial graminaceous plant, to 3\u00a0m tall, it reproduces mainly through rhizomes and, at low frequency, through seeds.  grows in all temperate zones of the world, especially North America, most countries in Europe, some parts of Canada and Australia (Hocking et al. ; Kulmatiski et al. ), being especially common in south-eastern Australia (Kettenring et al. ; Morris et al. ). The worldwide, regional and local distribution and abundance of  has expanded over the last 150\u00a0years and in most areas it forms dense monocultures (Saltonstall and Miller ). Due to the impacts of  invasions, habitats have been diminished or altered significantly for other flora and fauna causing loss of biodiversity and ecosystem functions (Mack et al. ). Several studies have identified chemicals within  organs which have antialgal, antifungal or antibacterial effects (Li and Hu ). Previous allelopathic studies have shown that water extracts, decomposed materials, root exudates and specific identified chemicals of  organs have strong phytotoxic effects on germination, growth, and establishment of other plant species (Kettenring et al. ; Rudrappa et al. , ; Uddin et al. , , , ) and thus, it is assumed that  achieves its competitive advantages over invasion process into wetlands through allelopathy (Bains et al. ; Rudrappa et al. ).While  has clearly shown phytotoxic potential, the effects should be considered in more ecologically realistic ways by differentiating allelopathic interactions from resource competition. The allelopathic effects might be masked by resource competition among target plants (Barto and Cipollini ; Weidenhamer et al. ). A better understanding of dose\u2013response relationships of allelochemicals would help to clarify this issue. Toxin dilution is thought to occur because plants share and compete not only for resources but also for toxin (Hansi et al. ; Hoffman and Lavy ; Suman et al. ). Population density modifies phytotoxic effects through dilution of available toxins among plants (i.e. phytotoxicity decreases as plant density increases) (Thijs et al. ). Thus, the dose of a phytotoxin received by a plant is inversely related to plant density. This toxin dilution study can be performed using density-dependent experiments, and may be a potential tool for exploring the effects of phyto-toxins on plant growth as well as for differentiating the resource competition from allelopathy. As the study of allelopathic interactions may be hindered by the lack of proper experimental methods, it may be more productive to first demonstrate explicit interference, by allelochemicals rather than rely solely on explanations that involve resource competition or other mechanisms (Barto and Cipollini ; Thijs et al. ).Although the use of activated carbon (AC) as a soil amendment, has use in determining distinct differences between allelopathy and resource competition (Inderjit and Callaway ), AC may change the availability of soil nutrients (Wei\u00dfhuhn and Prati ). Weidenhamer () proposed allelopathic effects might be differentiated experimentally using the density-dependent nature of phytotoxic effects, in turn, causing deviations from predicted growth\u2013density relationships. The effects by allelochemicals depends on density of neighbour-target plant species and might be masked by resource competition at high density (Weidenhamer ). Density-dependent models suggest that yield decreases with increasing density due to resource competition acting as a dominating factor. Alternatively, results due to allelopathy show a slow decrease of yield or even increase in yield as density increases, until density reaches a point where resource competition among neighbouring target plant becomes the dominating factor. The yields of exposed plants to pure chemicals (Andersen ), ground tissue of allelopathic plants (Tseng et al. ) and soil mediated allelopathic plants (Weidenhamer et al. ) are consistent with the assumptions of a density-dependent phytotoxicity model. Andersen () found that reduced plant seedlings of soybeans may partially reduce the negative effects of herbicides as well as Weidenhamer et al. () and Tseng et al. () stated that phytotoxicity decreased as plant density increased.Therefore, this study has been designed to determine the occurrence and magnitude of potential allelopathic effects mediated by  root exudates, its litter and extracts of litter with a wide range of doses through a density-dependent approach. This method might be effective in distinguishing the allelopathic interactions of  with neighbouring plant species from resource competition. We hypothesized that phytotoxic effects of allelochemicals depend on the neighbouring plant density, due to phytotoxins dilution among individual plants.Analysis of growth\u2013density relationships is useful tool for understanding the resource competition and allelopathic interference between plants of the suspected invasive species (Weidenhamer et al. ). The issue \u2018separating allelopathy from resource competition\u2019 is controversial in natural ecosystems (Inderjit and del Moral ; Weidenhamer ) but it is important in plant\u2013plant interactions to evaluate the relative contribution and identify the mechanisms involved in their biological invasion processes. The replacement series design (Dekker et al. ) and using activated carbon (Ridenour and Callaway ) have been proposed that could yield insights into the nature of plant\u2013plant interactions, and provide evidence for allelopathy. However, both methods have some complications in allelopathy studies (Goldberg and Werner ; Goldberg and Fleetwood ; Lau et al. ) whereas density-dependent phytotoxicity test is able to demonstrate in differentiating the relative contributions of those mechanisms in plant\u2013plant interactions (Weidenhamer et al. ).In general, the yield (in terms of growth and development) of plants decreases with increasing density; in contrast, density-dependent phytotoxicity studies reveal that plant growth may be positively influenced up to the point where resource competition acts as the dominant factor. Density-dependent phytotoxicity studies imply a positive feedback between population density and phytotoxins present in a system, as the toxin is shared among increased plant biomass with each plant receiving a proportionately smaller amount of toxin. Density-dependent phytotoxicity stands in contrast to resource competition as increased growth of plants at low density is dependent on large part to the amount of resources available. Despite the allelopathic potential of  on associated and model plant species, as shown by the growth of ,  and  observed in this study was masked by the resource competition but the allelopathic effects of  are well supported (Rudrappa et al. ; Uddin et al. , , , ). These studies showed that water extracts of different organs, residue decomposition and root secreted phytotoxins had negative effect on germination, growth, and development of other plant species.In greenhouse experiments\u00a0of this study,  the strongest growth inhibition was observed in high density treatments when compared to low and medium density. This demonstrates resource competition is the dominating factor, consistent with other studies (Inderjit and del Moral ; Uddin et al. ). Our previous studies demonstrated that allelopathy through root exudates of  had relatively low contribution in suppression of  in comparison to other competitive effects. Again the  litter mediated soil experiment showed the highest root suppression of  potentially due to allelochemicals leached from litter mediated soil as the intermediate density of  showed increased growth compared to low and high density. The findings are well supported by the total assumptions of density-dependent phytotoxicity concept proposed by Weidenhamer (). This study states that growth is reduced at low but diminished at high density compared to control; and plant growth is highest at intermediate density, due to a reversal in slope of the predicted growth\u2013density line.In addition, the laboratory experiments showed a clear density-dependent phytotoxic effect, a result well aligned with other studies (Hansi et al. ; Lambertini et al. ) where allelochemicals, herbicides and inorganic compounds such as copper showing phytotoxicity is density-dependent. Our results suggest the relationship between growths, in terms of biomass, root length, plant height and plant density of  exhibits a reversal in slope indicating the presence of phytotoxins in the litter leachate mediated soil used in the bioassay. The significant variation in phenolic content of unburnt versus burnt residues did not reflect the associate effects on plant growth suggesting heat induced transformation of phenolic compounds might be effective in suppression of plant growth, even though these residues may contain small amount of phenolic compounds. This result is well aligned with the study of Zhang et al. () who found that there was no significant difference between unburnt versus burnt residues of  (L.) Kuntze on the growth of wheat ( L.) seedlings. However, our studies found higher inhibitory effects in unburnt than burnt residue extract but it was not a true reflection of causal relationships between total phenolics and growth variables. Seed germination study showed that germination percentage and root\u2013shoot length of  are a function of both concentration and the amount of phytotoxin available per seed. The inhibition increased as the concentration increased at lower density but stimulation was observed with intermediate density in most of the cases. This suggests that lower seed density increases the availability of phytotoxin per seed. Weidenhamer et al. () found that even lower phytotoxin concentration may cause similar or greater inhibitory effects than higher concentrations, when the amount of phytotoxin per seed is greater.In general, allelopathy research is more concerned with using concentrations, the introduction of soil microbes, and an autotoxicity test, involving a wide range of associated plant species in the bioassays, by questioning whether those involved are ecologically relevant. Despite this, it may be difficult to determine with some precision as to those occurring naturally in the field, but this has importance in determining causal relationships and minimizing effects due to unnaturally occurring situations. Therefore, allelopathy studies should consider more complex bioassays involving soil microbial communities, field concentrations of allelochemicals, multiple test species and using native leachate as a control that might represent the ecological phenomena in the field. For example, soil microorganisms might play an important role in influencing the bioavailability of allelochemicals in soil (Bauer et al. ; Ehlers ; Inderjit ), which could be achieved by the addition of microbial inoculum in experimental soil substrate collected from test species grown in the field. So, further research related to incorporation of soil microorganisms might be imperative to advance the allelopathy as one of its invasion mechanisms.An effort has been made to overcome the concern through measuring the concentration of allelochemicals in the  rhizosphere soil (Uddin et al. ), considering the osmotic potential of higher concentrations (Uddin et al. ), adjusting pH, and measuring the quantity of litter biomass produced per unit of soil or covered area. These criteria in our previous studies have been taken into consideration in this current study. Moreover, separation of allelopathic effects from resource competition is a vital point in allelopathy research which has been addressed in this study, indicating phytotoxins secreted by different means from  are responsible for invasion process except root exudations. Despite the results indicating less inhibition of root exudates by  on  transplanted plants (Uddin et al. ) but toxin may arise from other sources such as residue decomposition into soil, inhibiting germination processes and other growth parameters (Uddin et al. ). These results are well aligned with other allelopathy studies of  in which Welbank () found that decaying roots and rhizomes of  markedly inhibit the root and shoot growth of rape seedlings but no significant inhibition by root secretion. On the other hand, plant\u2013plant allelopathic interactions may be explained by species-specific (Hierro and Callaway ; Prati and Bossdorf ) and contextual relationships (Bauer et al. ) that may prove the consistency of whole results of our studies. Finally, it can be said that the possibility of allelopathy as a probable cause of plant growth inhibition in some natural systems is not denied, but it has not yet been proven to be the sole factor of interference in any study. There are other possible explanations of the effect, e.g. volatilisation, mechanical root interaction etc.The overall observation of growth reductions in test plant species at low densities was inconsistent with the standard resource competition hypothesis and provides support for the hypothesis of chemical interference by . Although, the growth response of test species did not follow the consistency in all experiments, in most cases, the results demonstrate the density-dependent phytotoxicity concept. Therefore, these studies may provide an understanding of plant\u2013plant allelopathy interactions and may distinguish the mechanisms involved in plant interference i.e. resource competition and allelopathy. Our findings may be useful to evaluate the response of agricultural plants such as  to weed residues, and may also provide insight evidence of allelopathic potential in  invaded wetlands. In addition, the density-dependent phytotoxicity phenomenon may bring important ecological implications as a methodological approach in allelopathy (Weidenhamer and Romeo )."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0184-8", "title": "Starch accumulation in hulless barley during grain filling", "authors": ["Xu-guang Zheng", "Jun-cang Qi", "Hong-shan Hui", "Li-hao Lin", "Feng Wang"], "publication": "Botanical Studies", "publication_date": "14 July 2017", "abstract": "Starch consists of two types of molecules: amylose and amylopectin. The objective of this study was increase understanding about mechanisms related to starch accumulation in hulless barley (", "full_text": "Starch is the main end product of carbon fixation during photosynthesis. Starch consists of two major components, amylose and amylopectin. Amylopectin is composed of short \u03b1-1,4-linked chains of glucose. About 5% of these chains are linked together by \u03b1-1,6 linkages (Manners ; Bul\u00e9ona et al. ; Preiss and Sivak ). Amylose is also composed of glucose chains; however less than 1% of these chains are linked by \u03b1-1,6 branches (Imberty et al. ). The structure and relative proportion of amylose and amylopectin are the primary determinants of the physical and chemical properties of starch.Several enzymes are involved in starch biosynthesis. ADP-glucose pyrophosphorylase (AGPP) catalyzes the first reaction in starch synthesis, producing the activated glucosyl donor ADP-glucose (James et al. ). Granule-bound starch synthase (GBSS) is involved in amylose synthesis. Soluble starch synthase (SSS) catalyses the elongation of amylopectin chains. Starch branching enzyme (SBE) introduces branch points into the amylopectin chains. The precise mechanism that controls starch biosynthesis in grain is complex and not well understood.Amylopectin is formed by multiple isoforms of SSS (,  and ) and SBE (,  and ). Several multiple isoform enzymes are plant species-specific. Among the species examined so far, each isoform of SSS and SBE plays a distinct role in amylopectin biosynthesis (Nakamura ).Recent studies suggest that cereal endosperms have distinct cytosolic and plastidial forms of AGPP which are encoded by separate large-subunit and small-subunit genes. The AGPP small subunit gene sequences from various eudicots and monocots differ primarily in exon (James et al. ). There are differing opinions about how  regulates starch synthesis. McCue et al. () suggested that  controls starch synthesis in wheat at the transcriptional and post-transcriptional levels. Wang et al. () studied starch biosynthesis in rice and proposed that  controls starch synthesis at the transcriptional level. There are few reports about the role of other enzyme genes on the synthesis and accumulation rate of amylose and amylopectin in grain. Information about the relative expression of genes encoding AGPP, GBSS, SSS and SBE at different development stages could provide insight about the mechanism controlling starch biosynthesis.Hulless barley is mainly grown on the Tibetan Plateau. Hulless barley seeds consist primarily (63.2\u201365.3%) of starch. In fact, the process of grain filling in hulless barley is the process of starch accumulation. Hulless barley includes both waxy and non-waxy cultivars. The starch in waxy cultivars is composed of amylopectin (98.9%), whereas the starch in non-waxy cultivars is composed of both amylose (16.2\u201323.3%) and amylopectin (74.1\u201378.4%). The objectives of this experiment were (i) to compare the physiological and biochemical characteristics of starch biosynthesis during endosperm development in waxy and non-waxy cultivars of hulless barley and (ii) to study the differential expressions of AGPP, GBSS, SSS, and SBE genes at different grain filling stages.Starch biosynthesis in grain has been studied extensively in wheat and rice but not in hulless barely (Li and Sun ; Nakamura and Yuki ). We observed that starch and its components increased with time during grain filling. The patterns were similar to those observed in wheat and rice (Li and Sun ; Jian et al. ). The starch and amylose accumulation rates were always greater in the non-waxy cultivars (i.e., Beiqing 6 and Kunlun 12) than in the waxy cultivar (Ganken 5). The amylopectin content of non-waxy Beiqing 6 was significantly greater than that of waxy Ganken 5 but not greater than that of non-waxy Kunlun 12.The roles of AGPP, SSS, GBSS, and SBE in starch biosynthesis have been investigated in previous studies (Li and Sun ; Doehlert et al. ; Nakamura and Yuki ). However, the contribution of each enzyme to starch synthesis is still disputed. Our results showed that the maximum activities of AGPP in Beiqing 6 was 26.28\u00a0nmol\u00a0grain\u00a0d and in Kunlun 12 was 21.86\u00a0nmol\u00a0grain\u00a0d were greater in Ganken 5 was 21.10\u00a0nmol\u00a0grain\u00a0d. Furthermore, the activities of AGPP, SSS, GBSS, and SBE were significantly positively correlated with the starch accumulation rate during grain filling. This indicated that AGPP, SSS, and GBSS play important roles in starch biosynthesis in hulless barley. Preiss et al. () reported that AGPP was the rate-limiting enzyme in starch biosynthesis and that AGPP activity was related to the synthesis rate and final starch amount. Okita () observed that SSS activity was consistent with starch accumulation rate. We observed peaks in the activities of AGPP, SSS, and GBSS between 20 and 25 DAA in all three cultivars. This was also the time when starch accumulation rates were highest.AGP is composed of two large subunits and two small subunits, each of which is encoded by distinct genes (- and -). The enzyme is known to be largely extraplastidial (i.e., 85\u201395% cytosolic) in cereal endosperm, but plastidial in other cereal tissues and in all tissues of non-cereal plants (Fig.\u00a0a) (Beckles et al. ). AGPP catalyzes the first reaction in starch synthesis, producing the activated glucosyl donor ADP-glucose. Cao () observed that waxy and non-waxy wheat differed significantly both in - expression and in - expression. However, in this study, AGPP activity and the relative expression of AGPP exhibited similar temporal changes in all three cultivars. This may be because AGPP expression was different in wheat than in hulless barley. Additional research needs to be done to confirm this.Studies of waxy mutations in wheat and other cereals have shown that null mutations in genes encoding granule-bound starch synthase I () result in amylose-free starch in endosperm and pollen grains. The waxy locus in cereals is encoded by  \n                        , which catalyzes the elongation of amylose. In addition to its role in amylose biosynthesis,  \n                         is also responsible for the extension of long glucans within the amylopectin fraction (van de Wal et al. ). In the present study, GBSS enzyme activity peaked later than SBE and SSS (Fig.\u00a0b, d, i, j). This suggested that SBE and SSS may control starch synthesis at the transcriptional level, and that  \n                         may control starch synthesis at the post transcriptional level.Biochemical evidence suggests that  \n                         is primarily responsible for the synthesis of the shortest glucan chains with a polymerization degree of 10 glucosyl units or less (Li et al. ). Further extension of the chains is achieved by the activities of the - and - isoforms (Li et al. ). In the present study, SSS genes were exclusively involved in amylopectin biosynthesis in hulless barley endosperm. Furthermore, was the major SSS form. The relative expression of  \n                         decreased steadily across time (Fig.\u00a0d), and there was a corresponding decrease in the amylopectin synthesis rate (Fig.\u00a0f). This may be one reason for the decline in the amylopectin/amylose ratio across time (Fig.\u00a0).Two SBE gene classes ( \n                         and  \n                        ) differ in the length of glucan chains transferred in vitro and in their substrate specificities.  \n                        , compared with  \n                        , transfers shorter chains, exhibits greater affinity towards amylopectin, and exhibits greater rates of branching with amylose (Guan and Preiss ). The results of the present study indicated similar expression profiles for  \n                         and  \n                         in endosperm. Both  \n                         genes were expressed at high levels from 5\u00a0DAA to 10 DAA, but were then down-regulated after 15 DAA (Fig.\u00a0i, j). Declines in the expression of the  \n                         genes may be another reason for the decline in the amylopectin/amylose ratio across time (Fig.\u00a0). Overall, the results suggested that  \n                         plays an important role during early stages of endosperm development in hulless barley.Overall, the results indicated that the activities of AGPP, SSS, GBSS and SBE had some correlation with the rates of starch synthesis during grain filling in both the waxy and non-waxy cultivars. GBSS has an important effect on amylose synthesis, especially during late grain filling. SSS and SBE are associated with amylopectin biosynthesis. It remains unclear how AGPP, SSS, GBSS and SBE activities are coordinated to control starch biosynthesis rates. Further research will aim to identify direct interactions between starch biosynthetic enzymes, as well as the factors that regulate the enzyme activities.Transcriptome analysis has the advantage of being able to quantify changes in gene transcript levels at different developmental stages in wild-type species, their mutants, and cultivars with different genetic backgrounds. Therefore, expression profiling could (i) lay a foundation for identifying genes involved in the regulation of starch metabolism and (ii) provide valuable insight into the mechanism of metabolic regulation of starch biosynthesis under various physiological conditions.The amylopectin/amylose ratio gradually declined in both Beiqing 6 and Kunlun 12 (Fig.\u00a0). Furthermore, the peak in  expression was later than that of  \n                        ,  \n                        ,  \n                         and  \n                        . This may explain the decline in the ratio of amylopectin to amylose during grain filling in Beiqing 6 and Kunlun 12 (Additional file : Figure S1). The grain of Ganken 5 has high amylopectin content but almost no amylose.  expression was less in Ganken 5 than in Beiqing 6 and Kunlun 12. Overall, we conclude that  is mainly responsible for amylose synthesis whereas  and  are mainly responsible for amylopectin synthesis in amyloplasts."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0185-7", "title": "Transcriptomic analyses reveal clathrin-mediated endocytosis involved in symbiotic seed germination of ", "authors": ["Xu Zeng", "\u2020", "Yuanyuan Li", "\u2020", "Hong Ling", "Sisi Liu", "Mengmeng Liu", "Juan Chen", "Shunxing Guo"], "publication": "Botanical Studies", "publication_date": "24 July 2017", "abstract": "\n                           ", "full_text": "In nature, the establishment of orchid seedlings requires compatible fungi to colonize the seed, thus providing nutrients for the formation of a protocorm. The protocorm is a post-embryonic structure from which both shoot and root systems subsequently differentiate. After the differentiation of green leaves, most orchid seedlings acquire autotrophy, while some orchids (known as fully mycoheterotrophic plants) are achlorophyllous and obtain their entire carbon source from their mycorrhizal fungi (Leake ; Dearnaley ).\n                         is a fully mycoheterotrophic orchid that associates with two groups of fungal partners, i.e.,  and , to complete its ontogenesis, including seed germination, tuber formation, flowering and fruiting.  species (e.g., ) act as a symbiont during the stages of seed germination and protocorm development (Kim et al. ). For further development and enlargement of tubers, flowering and fruit setting,  becomes essential for the nutrient supply. It is also noteworthy that  is a well-known Chinese medicinal orchid (Tsai et al. ). Recently, a number of pharmacological experiments indicated that it had strong potential to combat Alzheimer\u2019s disease, Parkinson\u2019s disease and other neurodegenerative diseases (Manavalan et al. ).Plants are constantly exposed to a wide variety of microorganisms with either friendly or unfriendly intentions. Indeed, some plants are engaged in mutualistic or parasitic symbioses with them, or a plant-microorganism relationship, via a molecular dialogue. The plasma membrane (PM) of plants is a critical barrier that senses fungi and eventually allows their entry or the uptake of microbial molecules. Endocytosis is a process allowing extracellular particles or cargoes to enter the cell, whereby the PM invaginates and pinches off. In plants, Dhonukshe et al. () have showed that clathrin-dependent endocytosis constitutes the predominant pathway for constitutive internalization of PM proteins.In legume- symbiosis, the symbiotic relationship and nodule formation depend on mutual recognition based on a signal exchange between the two partners. Some studies revealed that  root hair curling is associated with a strong stimulation of endocytosis at an early stage of symbiotic interaction with . Many plant genes associated with the initiation of endocytosis are induced, including Rab-like, Arf-like, and dynamin-like GTPases, and phosphatidylinositol 3-kinase. Most of these genes prevent root hair curling and infection thread formation (Leborgnecastel et al. ). However, the contribution of endocytosis to the establishment of orchid mycorrhizae has been hampered by a lack of data.Although mycorrhizal fungi play a significant role in the symbiotic germination of orchids, the research of plant-fungi interactions in orchid mycorrhizae is very limited. In this study, we investigated changes in transcriptomic profiles during the symbiotic seed germination of  inoculated with . Meanwhile, endocytosis has been suggested to play a part in symbiosis between host-plant and endophyte (Wang et al. ). Therefore, we provided a detailed analysis of the expression patterns of genes involved in endocytosis.Previous studies used green orchids as the experimental materials and presented some basic knowledge of the early plant-fungus interactions of orchid mycorrhizae. However, there is still no information about seed germination of achlorophyllous orchids. Since  relies entirely on a nutrient supply from fungal partners, this model provides an ideal system to investigate the symbiotic seed germination of orchids with fungal infections. Our results have useful value for elucidating more information about the symbiotic seed germination of orchids with fungal infections.In conclusion, transcriptomic analyses provide a powerful method for investigating putative genes involved in the symbiotic seed germination of orchids. In this study, we performed transcriptome sequencing of mature seeds and early-stage protocorms from  to identify these genes and quantify their expression in seed germination. Subsequently, we analyzed putative differently expressed genes of clathrin, adaptor protein, AP180, adaptin-like protein, clathrin interactor epsin, dynamin-related protein, and HSC70 that were produced in the process of plant endocytosis. Our results indicated that the up-regulation of expression of genes related to clathrin-mediated endocytosis could play an important role in symbiotic germination in  infected by , especially at the early stage of protocorm development. The RNA-Seq data from our study provide an important resource for studying interactions between plant seeds and symbiotic fungi. Further genomic research of orchids and mycorrhizal fungi will provide new insights into the interactions between fungi and plants."},
{"url": "https://threedmedprint.springeropen.com/articles/10.1186/s41205-017-0016-1", "title": "Using computed tomography and 3D printing to construct custom prosthetics attachments and devices", "authors": ["Peter C. Liacouras", "Divya Sahajwalla", "Mark D. Beachler", "Todd Sleeman", "Vincent B. Ho", "John P. Lichtenberger"], "publication": "3D Printing in Medicine", "publication_date": "22 August 2017", "abstract": "The prosthetic devices the military uses to restore function and mobility to our wounded warriors are highly advanced, and in many instances not publically available. There is considerable research aimed at this population of young patients who are extremely active and desire to take part in numerous complex activities. While prosthetists design and manufacture numerous devices with standard materials and limb assemblies, patients often require individualized prosthetic design and/or modifications to enable them to participate fully in complex activities.", "full_text": "An estimated 1.9 million amputees in the United States [] sustained their amputations from trauma and vascular disease. The United States military care system has seen an increase in the number of amputees since the beginning of the conflict overseas. Over 1600 service members have lost limbs since 2001, with over 300 members having lost multiple limbs. Over 40,000 veterans with limb loss also receive care for their amputations within the DOD/VA system [].\u2018The art and science of prostheses\u2019 dates back over 100\u00a0years [], when a split tree trunk with leather straps was the best replacement leg. The methodology and technology to rehabilitate amputees has evolved over the years from muscle transplant to biomechanical manipulation and now to 3D printing technologies. Although 3D printing is a little over three decades old, it is now revolutionizing the inception and delivery of devices in rehabilitation medicine []. Major advances in medicine occur when medical specialties, biomedical engineers, and technologists collaborate, and the field of prosthetics is no different. Several recent manuscripts, abstracts, and case reports describing original contributions in robotic prostheses to improving 3D printing for prosthesis have been published [\u2013], to. There have been many recent review articles about 3D printing and the contributions of several medical specialties ranging from physiatrists to radiologists towards improving prosthetic devices and the rehabilitation process [, \u2013].The United States military has been at the forefront of these new technologies and working to bring about major advances in prosthetics (e.g. extending battery life, water resistance, and improved control schemes). The majority of military amputees differ from the dysvascular amputees of the general population in that they are generally young and extremely active individuals. Military amputees are driven individuals who take part in complex activities such as kayaking, skiing, climbing, swimming, mechanical maintenance, and team sports. The age and active lifestyle of these patients are something the DOD/VA will be dealing with for the next several decades.Currently, at WRNMMC, conventional technologies are used for socket construction. Our lab and the prosthetic service focuses on specialty attachments to reduce limitations of current prosthetic terminal device and allow individuals to once again take part in activities they desire. In this manuscript, we present examples and methods for using computed tomography and additive manufacturing to construct custom prosthetics attachments and devices.In this manuscript, we researched and presented several methodologies to construct personalized examples highlighting the benefits CT reconstructions and the 3D printing process provide in successfully manufacturing prosthetic devices in limited quantities. These are but a few examples of the applications of this technique. Three-dimensional reconstructions from computed tomography create an accurate starting geometry for designing custom prosthetic attachment and devices when barriers to traditional design processes and production methods exist. Once 3D reconstructions were obtained anywhere from thirty minutes (Wine Glass Holder) to three hours (Hockey Skate Adapters) were spent on the digital design process. 3D Printing has created new opportunities for the production of prosthetic devices and allows for the creation of unique, customized devices. Some limiting factors of these developed methodologies include the availability of equipment and materials, software, scanners, CT scan expenses, 3D printers, and experienced staff.Materials for these prosthetic components were chosen based on the technologies and materials available at the institution. Preliminary results show that the titanium alloy used has allowed for large safety factors, greater than 2.5, on prosthetic attachments analyzed. Future endeavors include 3D printing a mold for to manufacture the Wine Glass Holder from silicone rubber. For less complex designs conventional machining could also be utilized if the facility had in-house capabilities or outsourcing could be more economical. For these particular project and other similar projects, the cost of outsourcing these components would have been higher than the cost of in-house production. CT scanning cost will differ by the institution; however, these objects do not require the radiologist reading.Using CT in conjunction with digital design and 3D printing can be utilized to create custom rehabilitation devices. Facility resources and knowledge can be limiting factors. 3D printing has created new opportunities previously unavailable to prosthetics, occupational therapy, and assistive technology departments. This methodology continues to be utilized when conventional techniques are limiting or suboptimal. Interprofessional collaboration, imaging, and digital manufacturing expertise are vital to the successful form, fit, and function of these devices."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0186-6", "title": "Enzymatic and non-enzymatic comparison of two different industrial tomato (", "authors": ["\u00d6zge \u00c7elik", "Alp Ayan", "\u00c7imen Atak"], "publication": "Botanical Studies", "publication_date": "2 August 2017", "abstract": "The aim of this study is to compare the tolerance mechanisms of two industrial tomato varieties (X5671R and 5MX12956) under drought stress. 14\u00a0days-old tomato seedlings were subjected to 7\u00a0days-long drought stress by withholding irrigation. The effects of stress were determined by enzymatic and non-enzymatic parameters. The physiological damages were evaluated via lipid peroxidation ratio, total protein content, relative water content, chlorophyll content and proline accumulation. Enzymatic responses were determined by biochemical analysis and electrophoresis of SOD, APX, POX and CAT enzymes.", "full_text": "Drought stress is described as one of the most harmful natural hazards that limits the growth, productivity and crop yield of plants. The aggressive usage of natural resources and increase in the global climate temperatures are two main factors causing drought. The general effects of drought stress have been studied in different plant species. The main inhibitory features of drought stress were observed on cell proliferation and expansion, leaf size, stem elongation, root proliferation alongside with crop growth and biomass accumulation (Harrison et al. ; Sekmen et al. ; Shanker et al. ; Zdravkovic et al. ). On the other hand, plants involve specific protective adaptation mechanisms against short term and long term drought. Plants can either enhance response mechanisms to avoid the effects of drought or tolerate the adverse effects of the stress. Plants can reduce leaf size and number, increase the area and length of roots to absorb more water, produce thicker cuticula and wax layers, control stomatal closure rates, induce cell turgor maintenance mechanisms such as accumulation of compatible solutes, reduce light absorbance by reflecting the exposed sun light to reduce the water loss by transpiration and the alternative mechanisms to reduce toxic oxidative radicals which were produced by the stress (Harb et al. ; Shanker et al. ). Therefore, they can avoid the short term drought until the life cycle is completed to produce next the generation. Short term drought stress depends on random climate changes and is mostly predictable. Long term drought stress depends on more complex weather features and needs advanced molecular mechanisms to enhance increased transcription rates of functional and regulatory genes involved in drought tolerance (Harb et al. ; Jeanneau et al. ; Shanker et al. ). In next decades, the drought effected area is expected to enlarge around the world following the reducing sustainable water resources and increasing long term water deficiency situations. Comprehensive physiological and molecular studies are necessary for both in vitro and field conditions to evaluate existing crop varieties and improving them.Tomato () which is a member of Solanaceae family, is one of the most important agricultural plant worldwide, because of its industrial products and nutritional content (George et al. ; Zdravkovic et al. ). According to the 2012 world tomato production report of FAO, Turkey was in fourth place by 11,350,000 million tons of production following China (50,000,000), India (17,500,000) and United States (13,206,950) (FAO ). Tomato is a sensitive plant against various environmental abiotic stress factors. It is known fact that heat and drought stresses have the most limiting effects on tomato varieties among all other abiotic stress factors. Especially, drought stress during vegetative and early reproductive periods of tomato life cycle reduces yield dramatically (George et al. ; Wahb-Allah et al. ; Zdravkovic et al. ).As the other abiotic stress factors, drought causes imbalance on equilibrium between reactive oxygen species (ROS) such as hydroxyl radicals (OH), hydrogen peroxide (HO), singlet oxygen (O), superoxide radical (O) and scavenging activities of antioxidant system elements. Reactive oxygen species are products of metabolic processes taking place in different compartments of cell such as chloroplasts, mitochondria and peroxisomes. ROS, which are the group of free radicals derived from O, are consisted of reactive molecules and ions (Gill and Tuteja ). These free radicals which have high reaction potential are harmless at ground levels and have lifetime in unit of micro seconds until they are scavenged in cell. This short lifetime makes ROS effective only in several 100\u00a0nm radius. However, when cellular components undergo to oxidative stress, biomolecules as proteins, unsaturated fatty acids, enzymes and nucleic acids are exposed to damaging effects of ROS. Loss or malfunction of key biomolecules may even trigger death of cells depending on the duration and intensity of stress. The relation between the antioxidant system elements to suppress toxic levels of ROS within the cell, is physiologically balanced. Therefore, plants initiate enzymatic (POX, EC1.11.1.7; APX, EC1.11.1.1; SOD, EC1.15.1.1; CAT, EC1.11.1.6) and non-enzymatic (carotenoids, ascorbate, glutathione, tocopherols) defense mechanisms to obtain oxidative homeostasis (Bartosz ; Mittler ; Sekmen et al. ). As known from previous reports, different plant varieties may differ on metabolic pathways. Biochemical strategies are getting important to enhance tolerance capacities of plants under drought conditions. Induction of antioxidant enzyme activities is known to have the major role in survival against drought. As a result, it is getting important to detect changes on enzymatic and non-enzymatic antioxidant systems to reveal metabolic pathways of drought stress tolerance of different plant varieties. Therefore, target oriented breeding and selection studies can be conducted to improve economically important crop plants such as tomato against several environmental abiotic stress factors as drought.In the present study, antioxidant systems of two economically important industrial tomato varieties (X5671R and 5MX12956) were investigated against drought stress. Physiological and biochemical characteristics of these two varieties were evaluated under intensive drought conditions according to the activities of antioxidants as POX, APX, CAT, SOD and their isozyme profiles. Both varieties were evaluated for their isozyme band diversities to identify their tolerance responses against drought.Drought stress effects various metabolic processes adversely via altering key biomolecules by causing oxidative stress. Different plant species and varieties may involve alternative response strategies against drought stress. Studying on defense mechanisms against drought stress for several plant species might have importance to reveal new breeding strategies (Farooq et al. ; Griffiths and Parry ). The main aspect of this study is to evaluate enzymatic and non-enzymatic responses of two industrial tomato varieties against 7\u00a0days long drought conditions.In conclusion, all the enzyme systems reveals the difference between the drought tolerant/sensitive varieties. The analysis results showed that the antioxidative system responses are different between two tomato varieties. The present study demonstrated specific changes in antioxidative system in two varieties of . The changes in isozyme patterns and enzyme activities were supported the sensitivity of 5MX12956 and tolerance of X5671R tomato varieties against 7\u00a0days long drought stress. The differences between antioxidative mechanism activities of two industrial tomato varieties that were initiated as a series of responses against drought stress has been proven via biochemical and electrophoretic analyses.According to our results, increased POX1 and POX3 isozyme bands seem to have role in drought tolerance. Also, the main difference between the varieties is the absence of APX1 isozyme band in 5MX12956 tomato variety. Although there are differences in band intensities of APX2 and APX3 isozymes between the varieties during the stress treatment, the relation between the tolerance and APX1 isozyme should be evaluated. Another important isozymic difference was also observed between SOD banding patterns. Fe-SOD and Cu/Zn-SOD2 isozymes seem to be more effective in drought tolerance of X5671R tomato variety.In further studies, the exact relations between tolerance isozyme differences need to be defined and their availability should be evaluated as potential biochemical markers for drought tolerance studies in breeding programs of these tomato varieties."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0178-6", "title": "Identification among morphologically similar ", "authors": ["Paweena Traiperm", "Janene Chow", "Possathorn Nopun", "G. Staples", "Sasivimon C. Swangpol"], "publication": "Botanical Studies", "publication_date": "2 June 2017", "abstract": "The genus ", "full_text": "The genus  Lour., one of the larger genera in Convolvulaceae with approximately 135 species, is mainly distributed in tropical Asia (Staples and Traiperm ). During preparation of a nomenclatural review of the genus, we found that some species of  are quite similar in their vegetative morphology leading to confusion in identification when only leaf and stem characters are available. Species delimitation in Convolvulaceae generally, including , is heavily dependent on floral characters; when fully developed flowers are lacking it can be difficult or impossible to identify species precisely. Thus, when specimens with only leaves, or young flower buds, or fruits are all that is available it may not be possible to do more than narrow down the identification to a group of similar species. We have identified several such cases; these seem to represent species-complexes of morphologically similar species, or perhaps single highly polymorphic species. One such group of species includes  (Kerr) Ooststr.,  Craib,  Traiperm & Staples,  Traiperm & Staples and two unnamed  that appear morphologically distinct. Previously we pointed out that some species complexes have similar macro morphological leaf characters (Staples and Traiperm ; Traiperm and Staples ). In order to explore other characters that might be useful for identification, this research investigates foliar micromorphology as a means to identify plants or specimens that lack flowers.Leaf anatomy of some species in  had been investigated by Metcalfe and Chalk (), Sayeedud-Din () and Pant and Bhatnagar (). Leaf epidermis and stem transverse sections of four  species [ (Burm.f.) Bojer,  Deroin,  Dalzell & A. Gibson and  (Hornem.) Sweet] were studied by Tayade and Patil (, ). A key to species was constructed based on the epidermal characters such as cuticular striations, anticlinal cell walls, and basal cells of trichomes (Tayade and Patil ). Recent publication of a new species of  suggested that leaf epidermal characters, such as the pattern of cutin, can be used to distinguish  Staples & Traiperm from the morphologically similar species  Choisy (Staples et al. ). Anatomical characters strongly supported species identification in an investigation of  section  (Pisuttimarn et al. ).Anatomical information has been used for this analysis because it has proven useful and informative in assisting with taxonomic identification and classification in other plants (Thadeo et al. ). Leaf anatomical characters from several families also revealed similarities between plant species when using phenetic analysis [cluster analysis (CA) and principal component analysis (PCA)] (Nikoli\u0107 and Miti\u0107 ; Breitwieser and Ward ; De Faria et al. ; Aghababaeyan et al. ; Moraes et al. ; Thadeo et al. ; Arthan et al. ; Jayarathna et al. ). In Convolvulaceae, particularly the genus , phenetic analysis based on leaf anatomy has not been done before. The aim of this research was to study the leaf anatomy of  and similar-looking morpho-types and to conduct phenetic analyses using leaf anatomical characters to determine their utility for taxon discrimination and identification.The anatomical characters that are informative for identifying these six morpho-types are the epidermal cell walls and trichome types on the leaf surfaces. Moreover, the characters from leaf transverse section were also useful to distinguish each morpho-type such as the shape of adaxial epidermal cells of the leaf blades, the shape of leaf margins and the outline of petioles."},
{"url": "https://as-botanicalstudies.springeropen.com/articles/10.1186/s40529-017-0187-5", "title": "Collecting near mature and immature orchid seeds for ex situ conservation: \u2018in vitro collecting\u2019 as a case study", "authors": ["Jonathan P. Kendon", "Landy Rajaovelona", "Helen Sandford", "Rui Fang", "Jake Bell", "Viswambharan Sarasan"], "publication": "Botanical Studies", "publication_date": "8 August 2017", "abstract": "Lack of phenological information and efficient collecting methods are considered impediments for orchid seed collecting. This leads to opportunistic collecting as part of general seed collecting schedules that may last few weeks especially in remote areas. The study explored the feasibility of collecting near mature and immature seeds to support conservation action plans. Mature, near mature and immature seeds of orchids were collected from the wild in the Central Highlands of Madagascar (CHM). Seed capsules were collected in sterile culture medium in the wild, to prevent deterioration of seeds inside the capsule after collecting, later to be cultured under laboratory conditions.", "full_text": "Orchids are the most complex and enigmatic group of flowering plants, with a global distribution, yet are considered to be the most threatened family. A large proportion of orchids are endemic in areas where habitat loss, illegal collecting and loss of pollinators are prevalent (Merritt et al. ). Due to the threats to orchids worldwide and the adverse effect on recruitment in the wild they have been included in Appendix II of the Conservation on International Trade in Endangered Species of Wild Fauna and Flora (CITES). Orchid seeds appear to be short-lived compared to crop plant seeds, even if considered desiccation tolerant (Pritchard et al. ; Seaton and Pritchard ). Seed dormancy in many plant families seriously impacts outcomes at the time seeds are sown (Merritt et al. ). Lack of seed information and handling practices continuously affect outcomes in restoration programmes. This includes data deficiencies pertaining to phenology of seed development and maturation for the majority of wild species (Broadhurst et al. ; Mortlock ) and this can lead to asynchronous timing of seed collection (Merritt and Dixon ). In orchids, irregular germination in nature as well as under in vitro conditions is a major bottleneck for conservation of rare and threatened taxa (Barsberg et al. ). In particular, terrestrial orchid seeds are even more difficult to work with due to quick seed maturation, dormancy-related germination issues, and desiccation sensitivity. Therefore, terrestrial orchids present significant conservation challenges.The number of ex situ conservation facilities worldwide has grown dramatically over the years (Wyse-Jackson ), but have become specifically integrated to achieve objectives under national and regional conservation priorities (Maunder et al. ). Nevertheless, biodiversity conservation in resource poor areas of the world still faces an uncertain future. One of the key priorities of ex situ conservation is long term storage in seed banks and cryopreservation facilities which require substantial resources for capacity building. Long term storage of orchids requires cryopreservation at ultra-low temperature as a general rule because longevity under conventional seed banking conditions is not completely reliable (Merritt et al. ).There are nearly 1000 orchid taxa in Madagascar, of which 85% are endemic\u2014an exceptional area of biodiversity for orchids (Moat and Smith ; Tyson ). They are found in several phytogeographical regions with unique climatic conditions. This global biodiversity hotspot is a perfect study area to test orchids for collecting methods and assessing viability. We selected the Central Highlands of Madagascar (CHM) as our study area with over 50 species of orchids. The habitats of the study comprised inselbergs, savannah, grasslands, montane rocky scrubland and gallery forests. This area has granitic rocks, marble and quartzite with the majority of orchids being lithophytes and the rest true epiphytes and terrestrials.In the present paper we describe methods for collecting seeds at different maturity levels from the CHM habitats using IVC methods. IVC is the method of initiating in vitro cultures from plants in the wild. The method has been used in the past for a variety of functions ranging from horticulture to conservation (Warren ; Alvarenga et al. ; Henao et al. ; Saldana et al. ; Brenes et al. ; Engelmann ; Sandoval ). Propagation of orchids from seeds has long been achieved through the use of capsules which are either fully matured or green, un-dehisced and near-mature. Collected dry seeds are cleaned and dried before either storage or culturing for germination. Seed viability upon storage under standard seed bank conditions is still not completely reliable for orchids (Merritt et al. ). In many cases, especially in temperate terrestrial orchids, higher germination rates have been achieved using green capsules than mature seeds. This method is dependent on the immediate culturing of seed after harvest from green capsules. Seeds collected from plants in the wild, spread over large swathes of land mass, from remote locations usually perish due to moisture loss and microbial contamination (unpublished results). The critical aspect of working towards developing a successful IVC protocol is the control of contamination (Pence ) along with maintaining the viability of the differentially matured seeds. In many cases the techniques were specifically developed for only a limited number of taxa as described in the present study. We discuss the importance of this new method to improve collecting and germination efficiency of differentially matured orchid seeds. The implications of this approach to cryopreservation and living collection development for reintroduction and assisted colonisation are discussed in detail.Small numbers of capsules were collected from fragmented populations, single plants in some cases, from the CHM following the guidelines stipulated in the collecting permit from Madagascar Conservation authorities. The majority of seed capsules collected by the IVC method yielded seeds in a good state of freshness that stayed sterile inside the capsules. There was some discoloration in a small percentage of small seed capsules. Based on the size, colour and texture of the seed capsules the sterilisation regime was followed as in Table\u00a0, which shows that almost all the seed capsules collected in Madagascar by IVC gave rise to sterile seed cultures. In total 87.5% of the capsules were sterile and the seeds were also clean after 3\u00a0months of culture. Both NaDCC and PPM were essential to suppress microbial growth at various steps of the process from wild collecting to culture of seeds under laboratory conditions.Collecting mature seeds is the generally accepted approach as seeds remain viable for a reasonable period of time for use in culture compared to less mature seeds as reported before (Stoutamire ). As seeds mature the seed coat becomes hydrophobic which limits water and nutrient absorption (Zhang et al. ) and, as reported before, in terrestrial taxa accumulation of abscisic acid (ABA) occurs as seeds mature (Kinderen ; Lee ; Lee et al. ). This must be the case with some of the species to prepare them to face adverse environmental conditions before they can be germinated with the help of mycorrhizal fungi in the wild. The testa of near mature seeds have living cells and are yet to acquire their hydrophobic nature. Accordingly, there were not many barriers to absorb water/nutrients from the media to germinate in vitro. In addition to the physical barrier the high endogenous ABA level in mature seeds may cause poor germination in orchids, especially terrestrial orchids (Kinderen ). In the cases of  (Lee et al. ) and , the endogenous ABA level is low in immature seeds, and increases rapidly as the seeds approach maturity, coinciding with a rapid decrease in seed germination. Collecting immature seeds at 6\u00a0weeks after pollination (WAP) could give high seed permeability and low endogenous ABA, which results in improved seed germination in vitro (Lee ).Mature orchid seeds showing varying germination frequencies may be demonstrating dormancy induced by inhibitory substances as reported before (Nagashima ). However, immature seeds are only available during a short of window of time and need to be sown immediately because of the lack of an effective preservation method (Steele ; Hirano et al. ). It has been predicted that populations occurring in marginal habitats may be more vulnerable to repeated population crashes which can also result in a significant reduction in genetic diversity (Cozzolino et al. ), which may be the case in many orchids of Madagascar where only a few individuals are left in the wild.Expanding IVC along with conventional collecting for seed banking will significantly improve the success of ex situ conservation of Madagascan orchids. Ultimately there is a great need to understand the importance of mycorrhizal fungi in epiphytes and, especially, lithophytes to improve reintroduction/restoration to support integrated conservation for meaningful outcomes in the longer term. At the moment very little is known about the mycorrhizae of lithophytes and epiphytes of Madagascan orchids except the preliminary studies conducted in our laboratory (Yokoya et al. ).During embryo development in orchids the inner integument shrinks and forms a tight layer, which encloses the embryo, termed \u2018carapace\u2019 (Lee et al. ), mainly described in terrestrial taxa. The cuticular stain Nile Red detects this layer which may play a role in the hydrophobic nature of orchid seed and may help the seeds survive in harsh conditions. The carapace is visible in the seeds of  as reported in terrestrial orchids (Zhang et al. ; Lee et al. ), when seeds were sectioned and stained with Nile Red. The carapace is quite prominent in lithophytic orchids, as in terrestrial taxa, and can act as a robust protective cover for the embryo. When seeds were scarified mechanically, germination percentage achieved was close to the viability percentage recorded by TTC staining. This could be the reason why IVC collected seeds performed better, with carapace under-developed, compared to fully mature seeds with well-developed carapace. In lithophytic species this may be a distinctive feature as they endure long periods of harsh conditions in a dry lithophytic habitat but detailed screening is essential to find out the trend within this group of plants. According Lee et al. () the timing of seed collection outweighs the composition of the culture medium and the seed pre-treatments in a terrestrial orchid, where they have used bleach as chemical scarification. When sonication (0.5\u20135\u00a0min) was used, however, 3\u00a0min exposure improved seed germination by more than 30%. This treatment made perforations in the carapace which improved germination, either by reducing the hydrophobic nature of the seed, aiding moisture absorption and/or breaking the dormancy. Although this is a small study in a lithophytic orchid, the results indicate that the carapace is a barrier which protects the seeds in order to overcome adverse environmental conditions and keep seeds viable until germination can take place under favourable conditions with the help of orchid mycorrhizal fungus/fungi.\n                         appeared to be a good coloniser of its habitat in the study area of Itremo. Although the species has both epiphytic and lithophytic habit, plants were found, in most cases, growing in exposed cracks and bare faces of crags and boulders. Seedling recruitment appeared to be high as large groups of seedlings could be found growing in lichen within a few metres of groups of adult plants. This would tally with a reasonable germination percentage of mature seed (Fig.\u00a0). In contrast, , which is also found as either epiphyte or lithophyte but found predominantly growing as a lithophyte in the CHM, existed in extremely small populations of large adult plants with few seedlings evident in their vicinity. Erratic pollination has been recorded in two separate studies in  (Nilsson and Rabakonandrianina ; Nilsson et al. ) where the pollinator activity is geographically limited. It is striking that, in both IVC and mature capsules with high percentage of full seeds, germination percentage was nil. This discovery raises further concerns for this CITES Appendix I species.The low germination in mature seeds of  may be showing the consequences of inbreeding as only isolated plants were encountered in each location. This has been reported before in this species (Nilsson et al. ). The same kind of response was noticed in the case of mature seeds of an unidentified species of another . Full seed percentage was consistent in all the capsules collected, both mature and IVC (Fig.\u00a0), although mature seeds failed to germinate while IVC seeds germinated very well (more than 60% from all seed capsules). We have conducted further studies with small seed samples in another  species and noticed a similar trend. This means there may be an onset of dormancy/development of carapace layer/s as soon as the seeds start maturing. IVC can be used as a complementary tool as part of an integrated conservation action plan.In both  and  a trend exists where IVC seeds performed far better than mature seeds and this cannot be ignored. As Madagascar is home to about 133  species and more than 20 species of  their ex situ conservation action plans can be built around collecting seeds at immature/near mature stages to support asymbiotic models for living collection development and cryopreservation. The number of full seeds and viability of mature seeds all point to the fact that  is a genus which has serious conservation issues. The reported pollination behaviour of  (Nilsson and Rabakonandrianina ; Nilsson et al. ) explains the low full seed percentage, and the fragmentation we have found in the wild shows very little recruitment, all of which could drive the species to serious population decline in the coming decades. Detailed studies are in progress in our laboratory to understand the symbiotic relationship of this species using fungi isolated from seedlings collected from the wild.Harding et al. () reviewed the ex situ conservation of endemic flora from biodiversity hotspots, specifically using Brazil as an example and are of the opinion that ex situ conservation of endemic taxa in gene banks is often mired by information deficiency in areas of molecular genetics, seed storage and related aspects. Changing climate, population fragmentation and a lack of phenology data means reliable methods are required to collect seeds from as many species as possible to conduct trials for germination, cryopreservation and reintroduction programmes. The recent recommendation to cryopreserve orchid seeds rather than attempting to store them in conventional seed banks at \u221220\u00a0\u00b0C (Merritt et al. ) requires urgent attention. Mature good quality seed collections are a pre-requisite to achieve this system of storage. However, there are some orchid taxa which can only be collected opportunistically and in small numbers which need an alternative approach. According to Nagashima () some mature seeds have intrinsic germination problems due to accumulation of inhibitory substances and ensuing dormancy or potentially by the impermeability of the seed to a level that reduces germination following full seed maturation (Miyoshi and Mii ). Several methods such as sonication (Miyoshi and Mii ), scarification by chemicals (Mweetwa et al. ), and chilling (Kauth et al. ) have been reported to improve germination by degrading the physical barrier. According to Rasmussen () the alternative to bypass this bottleneck is by sowing immature seeds without any pre-treatment. For example, seeds of  showed higher germination when they were harvested close to the completion of embryogenesis but not at seed maturation Nagashima (). As seeds are available only during a specific window of time they should be collected at the right time and sown immediately. When an in vitro laboratory is remotely located and access to facilities is limited it is difficult to collect plant material at the right maturity, therefore, IVC has applications to circumvent this bottleneck.Statistical analysis showed that the tribe Vandeae, comprising the genera  and  had significantly higher germination of seeds collected through IVC than conventionally collected mature seeds. This suggests a trend in seed behaviour in this taxon. In other tribes (here including Podochileae, Cymbidieae and Orchideae), there was no significant difference between the germination of mature versus IVC seeds. The majority of epiphytic species from Madagascar can be collected by the standard collecting method as we have noticed in several species of  and . This shows that in these taxa, for seed that is viable, IVC does not lead to deterioration of seed and can complement conventional collecting as part of collecting for conventional seed banking.Due to restricted availability of orchid capsules from the wild the numbers of capsules obtained for each species varied, as seen in Figs.\u00a0,  and . Only mature seeds were available to collect in the case of . In order to justify the improved orchid seed germination by IVC method a data set consisting of full seed ratio and germination rate for collections of 6 species was subject to Kruskal\u2013Wallis one-way analysis of variance. Comparing with mature seed the collections by IVC method promoted the germination rate of overall collection of six species in the present study with high significance (\u00a0<\u00a00.001), No difference of germination rate observed between species (\u00a0=\u00a00.29). The unverified species  sp. showed significantly higher full seed ratio than . (\u00a0<\u00a00.01) (Kruskal\u2013Wallis test results available in Additional file ). Pearson\u2019s product-moment correlation test on the total 37 observations in the overall data set revealed a weak positive linear relationship (\u00a0=\u00a00.35) between full seed ratio and germination rate of those collected seed (\u00a0<\u00a00.05; Additional file ).One of the principal threats in the CHM habitats is fire (Hermans et al. ; Whitman et al. ) and a number of lithophytic orchids may be more prone to threats of extinction from the studied habitats than terrestrial orchids. This group of vulnerable taxa from the CHM should be prioritised and targeted for ex situ collections with IVC as a potential tool to complement conventional seed collecting.There are several impediments to work on species of high conservation value especially in small islands and biodiversity hotspot countries. The main problems are remoteness of the area for international teams who collaborate with local partners to access plants in the wild, few seed samples to collect, and in particular only limited information available on the phenology of seed capsule development and maturity. Seed storage of recalcitrant crop wild relatives, which underpins food security in the coming decades, and threatened plants (Li and Pritchard ) requires high quality materials. Collecting of recalcitrant seeds for large scale storage programmes require tangible sterilisation models to keep the embryonic axis/embryos in good condition until they can be processed in the lab. Our method using NaDCC offers great potential to work on storage, especially cryopreservation. Berjak et al. () demonstrated the efficiency of NaDCC to keep materials in good condition before cryostorage.For orchids our work done in the Central Highlands of Madagascar points to the fact that large genera such as  and  are facing serious threats to their existence due to low seed set, poor quality of seeds, and intrinsic poor recruitment in addition to the well-documented problems of habitat loss and climate change. As terrestrial taxa have specific maturity periods for their peak viability IVC offers great potential for collecting seeds at the right maturity for species where phenological data is available.The method described here has the potential to aid species with recalcitrant seeds, and vegetative materials from critically endangered species. We believe our findings also have the potential to harvest seeds straight from the plant to growth medium without creating a major shock to the physiology of the seed which is essential for both applied research in groups of high conservation and crop value, and for experimental studies."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0351-x", "title": "Long-term survival after multidisciplinary therapy for residual gallbladder cancer with peritoneal dissemination: a case report", "authors": ["Daisuke Kuga", "Tomoki Ebata", "Yukihiro Yokoyama", "Tsuyoshi Igami", "Gen Sugawara", "Takashi Mizuno", "Junpei Yamaguchi", "Masato Nagino"], "publication": "Surgical Case Reports", "publication_date": "14 June 2017", "abstract": "Although surgical resection is the only curative treatment for gallbladder cancer (GBC), concomitant peritoneal dissemination is considered far beyond the scope of resection. We report a long-term survivor with a residual GBC with multiple peritoneal disseminations who underwent an extended resection after effective chemotherapy.", "full_text": "Advanced gallbladder cancer (GBC) often exhibits distant metastasis at the time of initial presentation. Among the various modes of distant metastasis, peritoneal dissemination is an extremely devastating disease, with a median survival time (MST) of only 4.8\u00a0months []. Therefore, patients with this disease are considered as contraindicated for definitive surgery and undergo systemic chemotherapy. Valle et al. [] reported the results of the ABC-02 trial where the MST of patients with unresectable/recurrent biliary tract cancers who received gemcitabine plus cisplatin (GC) therapy was 11.7\u00a0months; no patients survived for more than 3\u00a0years. The BT-22 trail, reported by Okusaka et al. [], demonstrated an identical outcome to GC chemotherapy. Thus, the effect of the first-line treatment with GC remains limited, and long-term survival in patients with disseminated GBC cannot be expected with chemotherapy alone.Accidental GBC is histologically found after cholecystectomy for either acute or chronic cholecystitis, with a reported incidence of 0.2 to 2.1% [\u2013]. In this setting, severe inflammation commonly triggers perforation of the gallbladder, namely, abdominal contamination of the bile that potentially contains floating cancer cells; consequently, seeding metastasis often develops. Treatment strategies in this setting have yet to be standardized and may be a little different from those used in primary disseminated disease.To our knowledge, few studies of GBCs with peritoneal dissemination have been reported []. Here, we report a rare 5-year survivor of a residual GBC with peritoneal dissemination after cholecystectomy.A 59-year-old male had undergone an open cholecystectomy after the clinical diagnosis of Mirizzi syndrome at a local hospital. Because of severe inflammation, the gallbladder was perforated during surgery, ending in a piecemeal resection. Pathologically, the fractioned specimen involved moderately differentiated tubular adenocarcinoma invading the subserosal layer with positive margin of the dissection plane. Lymph nodes and cystic duct stump were not sampled. The patient was referred to our hospital 1\u00a0month after the cholecystectomy for further treatment.Although a postoperative complication of abdominal wall abscess (Clavien-Dindo classification [] in grade IIIa) had occurred, it was treated with percutaneous drainage and antibacterial agents. The patient was discharged on postoperative day 29. He refused GC therapy in the postoperative period due to non-haematological adverse events, including general malaise. Alternatively, tegafur/gimeracil/oteracil (S-1) was administered for 1\u00a0year after surgery. The treatment regimen consisted of 6-week cycles, in which 80\u00a0mg/m of oral S-1 per day was given for 4\u00a0weeks and no chemotherapy was given for the following 2\u00a0weeks. There were no adverse events associated with S-1. The patient has been alive and without disease for 6\u00a0years after the second surgery.Although specific clinical factors are closely associated with the favourable outcomes, the present report suggests that aggressive surgery along with chemotherapy may be a promising option in selected patients with highly disseminated GBC, who otherwise have an extremely dismal prognosis."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0349-4", "title": "Biliary-duodenal anastomosis using magnetic compression following massive resection of small intestine due to strangulated ileus after living donor liver transplantation: a case report", "authors": ["Ryusuke Saito", "Hiroyuki Tahara", "Seiichi Shimizu", "Masahiro Ohira", "Kentaro Ide", "Kohei Ishiyama", "Tsuyoshi Kobayashi", "Hideki Ohdan"], "publication": "Surgical Case Reports", "publication_date": "25 May 2017", "abstract": "Despite the improvements of surgical techniques and postoperative management of patients with liver transplantation, biliary complications are one of the most common and important adverse events. We present a first case of choledochoduodenostomy using magnetic compression following a massive resection of the small intestine due to strangulated ileus after living donor liver transplantation.", "full_text": "Primary sclerosing cholangitis (PSC) is a chronic cholestatic disease characterized by the progressive fibrosing inflammatory destruction of the intrahepatic and extrahepatic ducts, leading to liver failure []. PSC is the sixth most common cause of the liver transplantation in Japanese adults, following neoplastic diseases, primary biliary cirrhosis, hepatitis C virus cirrhosis, hepatitis B virus cirrhosis, and alcoholic cirrhosis []. The recipient\u2019s common bile duct is certainly resected, and hepatocholangiojejunostomy is needed in the case of liver transplantation for PSC.Historically, surgery has been the standard treatment for biliary stricture. However, surgery may be too invasive for elderly patients or patients with a poor general and nutritional condition. Repeat surgeries for biliary stricture are difficult because of the high risk of vascular complications near the anastomosis, postoperative adhesions, and inflammatory changes []. There is also a risk of inflammation of the anastomosis caused by foreign bodies, such as stitches or clips []. However, magnetic compression anastomosis (MCA) is a less invasive and safer procedure than choledochoenterostomy or choledochocholedochostomy for a biliary stricture or obstruction, with a low rate of complications and re-stenosis []. This procedure is also used for biliary stricture after liver transplantation or palliation of obstructive jaundice for malignancies [, ]. Several reports have demonstrated the superiority of internal over external biliary drainage in terms of intestinal barrier, integrity, absorption of nutrition, and liver function [, ].Here, we report a first case of choledochoduodenostomy managed with magnetic compression following a massive resection of the small intestine due to strangulated ileus after living donor liver transplantation (LDLT).The patient was a 54-year-old Chinese woman with a history of PSC and ulcerative colitis (UC). She had received LDLT using a left lobe graft, with resection of the extrahepatic bile duct and Roux-en-Y anastomosis for end-stage liver disease due to PSC. Her UC was well controlled with aminosalicylates and methylprednisolone. Cholangitis had reoccurred, but she had never previously experienced ileus in the postoperative period.Biliary-duodenal anastomosis could be an alternative method for the management of biliary stricture or obstruction. In addition, MCA could be a less invasive method for treatment of biliary stricture that cannot be accessed by conventional surgery. Careful follow-up of these cases should be done to determine the long-term patency and complications in these patients."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0348-5", "title": "Pancreatic ductal adenocarcinoma with inferior vena cava invasion: a report of three resected cases", "authors": ["Takuya Mizumoto", "Tadahiro Goto", "Hirochika Toyama", "Keitaro Sofue", "Sadaki Asari", "Sachio Terai", "Motofumi Tanaka", "Masahiro Kido", "Tetsuo Ajiki", "Takumi Fukumoto", "Yonson Ku"], "publication": "Surgical Case Reports", "publication_date": "23 May 2017", "abstract": "Pancreatic ductal adenocarcinoma (PDAC) often infiltrates to the adjacent major vasculatures; however, direct invasion of PDAC to the inferior vena cava (IVC) is uncommon.", "full_text": "Pancreatic ductal adenocarcinoma (PDAC) has the worst prognosis among all gastrointestinal cancers. Surgical resection is the only possibly curative therapy; however, only 15 to 20% of pancreatic cancer is indicated for surgery with curative intent [, ]. This is due to not only distant metastasis but also local invasion to adjacent organs. PDAC frequently infiltrates the major vasculatures that exist posterior to the pancreas, such as the superior mesenteric artery, portal vein (PV), superior mesenteric vein (SMV), or common hepatic artery. These findings are considered significant in regulating the resectability of the tumors [, ]. However, direct invasion of the inferior vena cava (IVC) is uncommon. According to the National Comprehensive Cancer Network (NCCN) Clinical Practice Guidelines, tumor contact with the IVC is defined as borderline resectable; however, an unresectable status is not defined in accordance with IVC involvement and IVC invasion is not contraindicated for surgery []. Given the lack of reported cases, the radiological features, surgical implications, and oncological impact of PDAC which invade the IVC are unclear.Resected cases of PDAC directly invading the IVC are rare. PD along with wedge resection of the IVC wall for patients with PDAC directly invading the adventitia of the IVC can be performed safely. Further accumulation of cases is needed to elucidate the prognostic impact of IVC invasion."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0354-7", "title": "Seven esophageal perforation cases after aortic replacement/stenting for thoracic aortic dissection or aneurysm", "authors": ["Yoshihisa Yaguchi", "Yoshimasa Kumata", "Masahiro Horikawa", "Takashi Kiyokawa", "Tsuyoshi Inaba", "Ryoji Fukushima"], "publication": "Surgical Case Reports", "publication_date": "19 June 2017", "abstract": "Esophageal perforation after aortic replacement/stenting for aortic dissection or aneurysm is a rare but severe complication. However, its cause, standard treatment, and prognosis are unclear. We analyzed the treatment and outcome retrospectively from seven cases experienced at our hospital.", "full_text": "Esophageal perforation after aortic replacement or stent grafting for aortic dissection or aneurysm is a rare but potentially fatal complication. However, its cause, standard treatment, and prognosis are unclear [\u2013].There are some reports with the key word of aorto-esophageal fistula (AEF) between the thoracic aorta and the esophagus. However, it is unclear whether the definition of AEF includes esophageal perforation after aortic replacement, as artificial grafts do not fistulize. In fact, most AEF-related papers are about cases after thoracic endovascular aortic repair (TEVAR) procedure [\u2013].The development of esophageal perforation indicates the occurrence and persistence of infection leading to mediastinitis or sepsis. However, it is interesting to consider the cause of the infection and to speculate on what happened at the site of the local lesion, that is, whether esophageal perforation is caused by localized infection including artificial graft infection or the infection is caused by esophageal perforation. In any case, perforation becomes the cause of continuous infection, and esophagectomy may be necessary for definitive treatment. However, esophagectomy is highly invasive surgery, and the indication should be carefully considered, particularly after major cardiovascular surgery. It is not also sure whether localized infection including graft/stent infection is improved after esophagectomy.We retrospectively reviewed our seven cases of esophageal perforation after aortic replacement/stenting and analyzed their treatment and outcome.Patient nos. 1, 6, and 7 could be discharged from hospital or moved to another hospital, but patient nos. 6 and 7 died of major bleeding on postoperative days 320 and 645. These two cases experienced chronic regional infection of the artificial graft/stent. The other four esophagectomy cases died in hospital because of sepsis on postoperative days 14, 30, and 41 and major bleeding on postoperative day 54.The one surviving case (no. 1) was a 65-year-old man who underwent reconstruction without severe complications, and was still alive without signs of infection at 424\u00a0days postoperatively.Delayed esophageal perforation secondary to thoracic aortic replacement or thoracic endovascular aortic repair (TEVAR) is a rare but potentially fatal condition [, \u2013].Seto et al. reported that although the exact mechanism of secondary esophageal perforation after stent grafting remains unknown, hypotheses include (1) direct erosion of the stent graft into the esophagus, (2) pressure necrosis caused by the self-expanding endoprosthesis, (3) ischemic esophageal necrosis due to disruption of the arteries that feed the esophagus, (4) infection of the stent-graft prosthesis (artificial graft for aortic replacement was included in our case), (5) pseudoaneurysm development, and (6) endoleakage into the residual aneurysmal sac [].In our cases that was performed aortic replacement procedure (nos. 1\u20136), (3) and (4) were thought to be a possible cause. In addition, no. 3 case had potential for (2), and no. 5 had potential for (2) and (5). No. 7 with Marfan syndrome had highly potential for (5) and (6). There was no equivalent case for (1).We consider the hypothesis that ischemic esophageal necrosis due to disruption of the arteries that feed the esophagus should be focused. The thoracic esophagus is fed by bronchial and esophageal branches of the thoracic aorta. Aortic replacement or stent grafting can potentially damage these feeding arteries of the thoracic esophagus. We consider that the relatively long period from cardiovascular surgery to esophageal perforation supports this hypothesis. Uncontrolled continuous localized infection including artificial graft infection seems certain to aggravate esophageal wall ischemia and disruption in a similar way.Eggebrecht et al. reported that they observed mild erosive lesions in the esophagus that led to perforation on endoscopy []. This suggested that the lesion took some time to progress to perforation, and ischemic change of the esophageal wall occurred gradually. They also mentioned that recognition of this pre-perforation state could have prompted early triage and/or surgical repair before esophageal perforation.In no. 5 case, we had recognized redness and erosive change of the esophagus on endoscopy before the aortic replacement. If we had decided esophagectomy at that point, we might avoid poor prognosis of the case.The prognosis of esophageal perforation cases after aortic replacement/stenting for thoracic aortic dissection or aneurysm is extremely poor especially in the elderly cases. In the elderly cases (over 80\u00a0years old), nos. 3, 4, and 5 died 41, 14, and 30\u00a0days with sepsis and other severe complications after the esophagectomy, respectively (Table\u00a0.). Therefore, the indication for highly invasive esophagectomy should be decided carefully. We surgeons should restrict the esophagectomy to sustainable patients for invasive surgery in consideration of age and complications. We want to suggest elderly cases over 80\u00a0years old should be refrained from the esophagectomy.It is important to control infection including regional infection and progression of cardiovascular disease for successful treatment as the result of a survival case (no. 1).Artificial graft/stent with chronic infections was considered to be removed for long survival. We also consider that it is important to perform cardiovascular surgery with attention to maintaining esophageal blood flow.Vascular-rich tissue filling (muscle flap or omental) to the infection site after esophagectomy may be useful for infection control. Our surviving case underwent intercostal muscle flap filling which could control prosthetic graft infection."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0350-y", "title": "A fascia lata free flap in pelvic exenteration for Fournier gangrene due to advanced rectal cancer: a case report", "authors": ["Hiroshi Sawayama", "Nobutomo Miyanari", "Hidetaka Sugihara", "Shiro Iwagami", "Takao Mizumoto", "Tatsuo Kubota", "Yoshio Haga", "Hideo Baba"], "publication": "Surgical Case Reports", "publication_date": "26 May 2017", "abstract": "Fournier gangrene due to advanced rectal cancer is a rapidly progressive gangrene of the perineum and buttocks. Emergency surgical debridement of necrotic tissue is crucial, and secondary surgery to resect tumors is necessary for wound healing. However, pelvic exenteration damages the pelvic floor, increasing the likelihood of herniation of internal organs into the infectious wound. The management of pelvic exenteration for rectal cancer with Fournier gangrene has not yet been established. We herein describe the use of a fascia lata free flap in pelvic exenteration for rectal cancer with Fournier gangrene.", "full_text": "Prosthetic material is contraindicated for infected or contaminated abdominal wall defects; hence, the repair of such defects is challenging [, ]. Infected incisional hernias have been treated using autologous tissue grafts, and a previous study reported that treatment with a fascia lata patch is safe and effective [].Fournier gangrene is a rapidly progressive condition in which polymicrobial necrotizing fasciitis develops in the perineal, perianal, or genital areas []. Surgical debridement of necrotic tissue is crucial, and tumor excision is necessary for wound healing. The management of pelvic exenteration for advanced rectal cancer with Fournier gangrene has not yet been established.We herein describe the application of a fascia lata free flap to prevent internal organs herniating into the infectious wound following pelvic exenteration for Fournier gangrene due to advanced rectal cancer.A 15-cm incision was made and a fascia lata free flap (15\u2009\u00d7\u20099\u00a0cm) was created from the left femur (Fig.\u00a0a, b). The ventral, lateral, and back sides of the flap were fixed to the edge of the peritoneum along the resected bladder, bilateral external iliac arteries, and ileal conduit, respectively, (Fig.\u00a0c). After the rectum was resected, the pelvic space was left and the perineal wound was not sutured. However, the perineal wound was separated from the abdominal cavity.Histological examination revealed moderately differentiated tubular adenocarcinoma of the rectum with direct invasion of the prostate and vas deferens. Severe lymphatic and vascular invasion were present, and four metastases of the regional lymph nodes were identified. A swollen right inguinal lymph node was dissected and diagnosed as a metastasis from rectal cancer. The tumor was diagnosed as stage IV, pT4b (prostate and vas deferens) N2 M1 (inguinal lymph node metastasis) according to the Japanese Classification of Colorectal Carcinoma. The tumor was resected with negative proximal and distal margins; however, this case had a high risk of local recurrence, and so, postoperative chemotherapy and chemoradiotherapy were planned.The treatment of Fournier gangrene due to advanced rectal cancer is challenging, as tumor resection is required to remove the infectious area. Pelvic exenteration results in a large defect in the pelvic floor, increasing the likelihood of internal organs herniating into the infectious wound. In the present case, this defect was safely and effectively repaired with a fascia lata free flap.Fournier gangrene is characterized by rapidly progressive necrotizing fasciitis in the perineum and external genital organs. The sources and etiological factors of Fournier gangrene are dermatological (25%), anorectal (21%), diabetes mellitus (20%), urological (19%), and alcohol abuse (9%) []. Only a limited number of studies have reported spontaneous perforation of rectal cancer presenting as Fournier gangrene of the perineum and scrotum [\u2013]. Fournier gangrene is a true emergency that requires aggressive treatment with antibiotics and immediate surgical debridement []. Although our patient underwent emergency surgical debridement, the tumor was exposed by the wound, and exudate persisted. We considered that tumor excision was essential to enable adequate wound treatment; this tumor excision resulted in a large defect in the pelvic floor.The standard surgical procedure used to treat abdominal wall defects is mesh repair [, ]. However, mesh repair is contraindicated in infected abdominal wall defects. A number of studies have reported the repair of an abdominal wall defect using a fascia lata patch [, , ]. In the present case, we used a fascia lata free flap to close the pelvic floor defect. The advantages of a free fascia lata flap are that only a short period of time is required to create the flap, the method is technically simple, leg strength is minimally affected, and there are multiple locations to which the flap can be fixed.There may be an additional advantage to using a fascia lata free flap in pelvic exenteration with Fournier gangrene. The small bowel enters the irradiation field during radiation therapy for rectal cancer, which increases the risk of adverse effects []. Using a fascia lata free flap to close, the pelvic floor defect ensures that the small bowel no longer drops into the pelvic space, which may reduce the adverse effects associated with postoperative radiation therapy for pelvic lesions.We describe a case in which the patient underwent pelvic exenteration for Fournier gangrene due to advanced rectal cancer. Surgical repair of the pelvic floor defect with a fascia lata free flap led to a good clinical course and prevented herniation of internal organs into the infectious wound."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0080-5", "title": "Chronic Exertional Compartment Syndrome in the Forearm of a Collegiate Softball Pitcher", "authors": ["Austin Cole", "John L. Hiatt", "Christopher Arnold", "Terry Sites", "Ramon Ylanon"], "publication": "Sports Medicine - Open", "publication_date": "17 March 2017", "abstract": "Chronic exertional compartment syndrome (CECS) is a recognized condition in the lower limb, with many reports in the literature. However, very few instances include CECS of the upper limb. This article presents the case of a collegiate softball pitcher presenting with CECS in her right forearm. To our knowledge, this is the first case report of a softball player with CECS, with only one similar incident in a major league baseball player.", "full_text": "Chronic exertional compartment syndrome (CECS) is classically defined as a condition presenting with recurrent, ephemeral increases in pressures of confined muscle compartments during exercise. Acute compartment syndrome of an extremity may develop from either traumatic intracompartmental swelling or external compression. However, CECS usually only presents with exercise of the affected compartment, and typically resolves with rest. Increased pressure within compartments leads to transient pain, paresthesia, numbness, and hindrance of muscle activity []. CECS is often overlooked as the cause of muscle pain and paresthesia in the extremities, due to its rarity and diverse manifestation. CECS may present with many different symptoms, appear identical to other etiologies, lack physical exam findings, and appear intermittently as well as transiently [, ]. Diagnosis can be delayed as long as 22\u00a0months in some instances [].The pathophysiology of CECS is not completely understood, but it is certainly different from the more familiar acute compartment syndrome. Rather than through abrupt injury, the pressure within the compartment involved rises upon exercise of the extremity and related muscles. The pathophysiology could possibly be a result of a combination between increased muscle size from higher blood flow, increased tension of the surrounding fascia, higher production of metabolic products, and an increase in extracellular water content [, ]. It is thought that expansion of the exercised muscle compartment can be up to 20% in volume [].Typically, CECS is seen in the young adult athlete who maintains a strict schedule of intense exercise. Most cases of CECS in literature are of the lower extremities, with only a few involving the forearm. Furthermore, almost the entirety of the upper extremity presentations is a result of either manual labor, rowing, motocross, weight lifting, or kayaking [\u2013]. This article presents the case of a collegiate softball pitcher presenting with CECS in her right forearm. To our knowledge, this is the first case of a softball player with CECS in literature, with only one comparable case in a major league baseball player [].A 21-year-old female softball pitcher and outfielder presented with a chief complaint of right forearm pain and paresthesia. The patient described a burning, tingling sensation over the lateral portion of right forearm for the previous 4\u00a0weeks. Initially, symptoms presented only with extensive throwing and upper extremity exercise. However, by this visit it had progressed to presenting in the weight room and even occasionally at rest. She had tried activity modification through decreasing both duration and intensity workouts, along with over the counter anti-inflammatories. However, these did not relieve her discomfort. Physical exam showed full range of motion at the elbow and wrist with flexion, extension, pronation, supination, and radial and ulnar deviation. There was no sign of pigment changes, warmth, or erythema. Palpation revealed hypersensitivity over the lateral right forearm, tenderness over lateral epicondyle, and slight tenderness over medial epicondyle. Tinel\u2019s sign was negative over the cubital tunnel, Adson\u2019s sign was negative, and there was no epitrochanteric lymphadenopathy. A broad differential diagnoses at this point included peripheral nerve entrapment, peripheral neuropathy, motor neuron pathologies, and muscular disorders. Subsequently, the plan was to begin non-steroidal anti-inflammatory drugs (NSAIDs) and to order a nerve conduction study (NCS) and electromyography (EMG).The NCS and EMG tests were normal, and the NSAIDs were not effective over a period of 4\u00a0weeks. Due to consistent and reproducible symptoms with lack of resolve, further work-up was pursued. With a speculation of previous injury, an anatomic abnormality, or compartment syndrome, an MRI of the right forearm was ordered. The result was also normal. With no conclusive evidence of compartment syndrome or soft tissue-related etiology of the forearm, the differential diagnosis shifted toward vascular and neurological etiologies in the cervical region. Thus, a cervical MRI was ordered. With the exception of a slightly increased T1 image finding in the right vertebral artery, there were no significant findings leading to a presumptive diagnosis of an underlying injury in the cervical region. To follow up on the right vertebral artery signal, a CTA of the neck was ordered, which was also found normal.After weeks of studies, the patient still maintained symptoms. At a second clinical evaluation, the patient endorsed some tenderness at rest. The patient was instructed to exercise, during which she performed burpees, push-ups, and medicine ball tosses until pain and tightness in the forearm were felt (at 10\u00a0min). She then continued 5\u00a0min longer, totaling 15\u00a0min of exercise. Upon evaluation, the surgeon found discoloration of the right forearm, tenderness to touch over both epicondyles, and pain in the flexor and extensor compartments. The patient also presented with stiffness and a feeling of heavy pressure within the right forearm. Consequently, a bilateral pre-exercise and post-exercise MRI was ordered to work up possible exercise-induced compartment syndrome.The differential diagnosis at this point included CECS or an unusual presentation of nerve entrapment. The patient was informed and was selected to proceed with intramuscular pressure measurement over a further work-up of nerve entrapment. For definitive diagnosis of CECS, compartment pressure increase upon exercise was needed.Pressure was checked three times: pre-exercise, 3\u00a0min post-exercise, and 5\u00a0min post-exercise. An injection of 1% lidocaine was used to achieve local pain reduction. A catheter was then introduced into the dorsal and volar mid-forearm areas of both the right and the left upper extremities. The transducer was attached. Pre-exercise pressure values were then obtained bilaterally.Following a full patient discussion and consultation, with the understanding that CECS of the forearm is an unusual presentation and difficult diagnosis, it was felt that these findings (along with the presentation and extensive work-up throughout the previous 6\u00a0months) were significant enough to proceed with right forearm fascial releases. An initial curvilinear incision was made across the antecubital fossa and extended downward on the right volar forearm. A fasciotomy was then completed, including release of the bicipital aponeurosis. Finally, careful palpation for any other tight bands was performed. Attention was then turned to the dorsal forearm. A longitudinal incision was made over the mid-dorsal forearm, and a fascial release was performed proximally and distally. The mobile wad was released as well. Copious irrigation was then performed. Tourniquet time totaled 17\u00a0min. Simple nylon closure of the incisions was performed, followed by a sterile compressive dressing. There were no noted complications before, during, or after surgery. The right forearm was then placed in a posterior splint until the dorsal wound healed.This presentation of CECS in the forearm of a female collegiate softball pitcher is exceptionally unique. CECS has appeared in the upper extremity in perhaps only a few dozen instances in literature, usually in certain populations (soldiers, motocross, kayaking) [, \u2013, \u2013]. In addition to the rarity, this case illustrates the difficulty of diagnosing CECS of the forearm. From the initial presentation and plan to acquire EMGs and a NCS to the normal neurological, vascular, and muscular work-ups, the diagnosis of CECS can be recognized as very challenging. In this section, we discuss three modalities we believe aid most in diagnosing CECS in the forearm, methods that have proven less effective, and the paradoxical course this particular case took.The first of the three useful modalities is simply clinical presentation. Although this is not a true  by definition, patient presentation through signs, symptoms, and physical examination proved just as vital to the diagnosis as any true modality would. Specifically, one should look for forearm pain and paresthesia over either the dorsal or the volar aspect of the forearm during exercise. Our patient presented with history of pain upon exercise in both forearm regions, described it as a \u201cburning, tingling sensation\u201d. Erythema, numbness, and hindrance of motion due to discomfort also accompanied the exertional pain. All these are characteristic of other cases of CECS [\u2013, \u2013, ]. Notably, the symptoms never radiated to the hand over the six-month course. Hypersensitivity to palpation over the lateral forearm, tenderness to palpation over lateral epicondyle, and slight tenderness over the medial epicondyle can be further indications of CECS. Another element to consider in future diagnoses is a possible vague, seemingly benign physical exam. The patient maintained full range of motion at the elbow and wrist with flexion, extension, pronation, supination, and radial and ulnar deviation when at rest. Furthermore, pigment changes, warmth, and erythema was only seen upon exercise. It is therefore very important to reproduce symptoms by specific exercise. However, unusual for CECS, our patient did have some history and one clinical exam where tenderness presented at rest. A lack of Tinel\u2019s sign and Adson\u2019s sign, as in this instance, might also aid in diagnosing CECS. Finally, the lack of anti-inflammatory relief could suggest a possible CECS case.A second modality that proved to be extremely helpful in diagnosing CECS was the pre- and post-exercise MRIs. This style of imaging allows assessment of signal intensity changes over time to quantify fluid accumulation within soft tissues. Furthermore, MRIs are safe and prevent exposure to radiation. However, we believe it is important to ensure the history and clinical evaluations steer toward CECS (specifically reproducing symptoms with exercise) before ordering such costly studies. In addition to this case, pre- and post-exercise MRIs have been proven to aid in the CECS work-up elsewhere in literature [\u2013].The third and final modality we recommend is pre- and post-exercise intracompartmental pressure measurements. Because MRIs and clinical presentation are non-invasive techniques, we believe that the addition of compartment pressure evaluation is obligatory before a definitive diagnosis is made and a fasciotomy pursued. Although there has been evidence of variance in normal compartment values, any significant rise in pressure post-exercise is highly indicative of CECS [, , , , , , ]. In fact, many other studies have concluded that the best method for diagnosing CECS is evaluation of intracompartmental pressures (ICPs) before and after exercise [, , , , , ]. However, in agreement with Rorabeck et al., we believe ICPs are only a supplemental component to the history and physical examination. In conclusion, we recommend a work-up of forearm CECS using all three modalities.First and foremost, it is important to understand what the literature proposes as  for ICP values. The standard resting pressure in normal compartments is between 0\u201315\u00a0mmHg in the lower leg. Borderline pressures are from 16\u201324\u00a0mmHg, while values above 25\u00a0mmHg are consistent with the diagnosis of CECS. Even though these values are based on the much more common presentation of CECS in the lower limb, they are used in most studies of CECS in the upper extremity [, , \u2013, ]. Due to the rarity of upper extremity cases, Ardolino et al. attempted to experimentally determine normal, asymptomatic, forearm pressures before and after exercise []. With a 95% CI, normal, asymptomatic forearm extensor pressures in 41 volunteers were found to be between 0\u201325.3\u00a0mmHg, and normal forearm flexor pressures between 0\u201321.4\u00a0mmHg []. Interestingly, there was no significant difference between pre- and post-exercise values. Ardolino et al. also found resting and exercise numerical values significantly higher than what most forearm studies (using ICP values of the leg) had considered normal. [\u2013, ]. In fact, the 95% CI, 0\u201325.3\u00a0mmHg dorsal ICP resting range surprisingly includes the theoretical diagnostic value of CECS (20\u00a0mmHg) used in so many other studies. In conclusion, we believe the accuracy of normal and resting ICPs that many studies provide for the forearm is questionable, with a wide variance [, , , ].With normal values proving inconsistent in literature, we now look at what studies recommend for diagnosing CECS with ICP values. A hypothetical diagnostic criterion for CECS in the forearm has been proposed by Pedowitz et al.; a resting ICP of 15\u00a0mmHg or greater, a 1\u20133-min post-exercise ICP of 30\u00a0mmHg or greater, and a 5-min post-exercise ICP of 20\u00a0mmHg or greater. Of note, this has only been tested with compartments of the leg [, , , ].Because of conflicting studies, variance in results, and contrasting data with this patient, we believe that utilizing a particular numerical value for ICP is not helpful. Instead, the important factor for CECS seems to be the rise in pressure from resting to 5\u00a0min after exercise, perhaps as a percent, and delayed normalization. All reports in literature support that as a consistent phenomenon [, , , , ]. Perhaps these two elements should be investigated more thoroughly in future studies.Unlike the three above modalities we believe are useful in diagnosing CECS of the forearm, other choices have often proven inadequate. As in this case, observation with cessation of exercise for 1\u00a0month, a physical therapy plan, anti-inflammatory medicines, an EMS, a NCS, a non-exercise MRI of the forearm, a cervical MRI, and a cervical CT angiogram have all demonstrated to only rule out a few differential diagnoses or have proven inconclusive. Even though the first step taken by the initial physician in March was to rule out any concomitant nerve compression through EMG/NCS studies, these exams may prove misleading or inaccurate at times [, \u2013]. Other investigations not done in this case have proven futile as well; including X-rays, blood tests, and Doppler ultrasound scans [, ].A final interesting component of this case is the paradoxical presentations throughout the 6\u00a0months. The September 14 visit, after the exercise MRI, delivered unexpected findings. First, unlike the earlier examinations, there was no tenderness to palpation or pain with rest. The patient stated that now only with exercise she experienced discomfort, swelling, erythema, and numbness. However, this constituted only minimal exercise; manifesting after just 30\u00a0s, and even through handwriting. Second, during this visit, the pain was specifically over the dorsal compartment, with less in the volar region. This seems contrary to the MRI findings just 3\u00a0days prior, where abnormal edema was primarily found within the flexor carpi radialis, flexor digitorum superficialis, and flexor carpi ulnaris muscles. It is possible that the cessation of softball and strenuous activity over the two-month summer period in July and August alleviated some of the symptoms. However, it still is important to understand that CECS may present differently at subsequent visits.Due to the extensive clinical examinations and diagnostic work-ups, the successful diagnosis of CECS was finally made in this case. If the three modalities of clinical presentation, exercise MRIs, and ICP measurements had already been established as a step by step protocol, the diagnosis of CECS would have been accomplished much sooner. In review of this case, we believe these three modalities are most important and should be considered in future cases of CECS work-up. Furthermore, we recommend a case-control study examining percent rise from pre- to post-exercise compartment pressures and the time delay in recovery. Finally, the surgical intervention of CECS in the forearm seems to be necessary, since therapy, one-month cessation of exercise, and anti-inflammatories did not prove curative. This case presented over a six-month period without any resolution using these techniques (Table\u00a0). Surgical fasciotomy is encouraged highly in literature for CECS and is proved to be the definitive solution once CECS was diagnosed [, , , , , , , \u2013]. To our knowledge, this is the first report of a case of chronic exertional compartment syndrome in the forearm of a collegiate softball pitcher. The rarity of this condition normally places it low on the differential diagnosis, however, we hope that the conclusions of this case will aid in making swift and accurate diagnoses of upper extremity CECS in future patients."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-016-0069-5", "title": "Physical and Physiological Profiles of Brazilian Jiu-Jitsu Athletes: a Systematic Review", "authors": ["Leonardo Vidal Andreato", "Francisco Javier D\u00edaz Lara", "Alexandro Andrade", "Braulio Henrique Magnani Branco"], "publication": "Sports Medicine - Open", "publication_date": "13 February 2017", "abstract": "Brazilian jiu-jitsu is a grappling combat sport that has intermittency as its core element; in other words, actions of high, moderate and low intensity are interspersed during matches, requiring a high level of conditioning to support optimal levels of performance for the total match time. The athletes perform from four to six matches during a day of competition, and this number may increase if the open-class competition, which is held parallel to the competition by weight class, is considered. This systematic review examined the physical and physiological profiles of Brazilian jiu-jitsu athletes.", "full_text": "In the last decade in particular, there has been a significant rise in the popularity of Brazilian jiu-jitsu. Part of this is due to the success of Brazilian jiu-jitsu athletes in mixed martial arts events []. In national and international competitions of the International Brazilian Jiu-Jitsu Federation, there are nine weight categories for males (<57.5, 64, 70, 76, 82.3, 88.3, 94.3, 100.5 and <100.5\u00a0kg) and eight weight categories for females (<48.5, 53.5, 58.5, 64, 69, 74, 79.3 and <79.3\u00a0kg). Brazilian jiu-jitsu competitions are also divided according to athletes\u2019 age as follows: juvenile (15\u201317\u00a0years of age), adult (>18\u00a0years of age) and master (>30\u00a0years of age) []. The duration of matches takes these variables into account and can vary from 5\u00a0min for white belts to 10\u00a0min for black belts [].Athletes start fighting from a standing position, but most of the combat takes place in groundwork []. The aim of the sport is to make your opponent give up the combat by means of choke, joint locks (wrist, elbow, knee and ankle locks) or pressure techniques, but when there is no submission the matches are decided by the scoring of specific techniques (takedown, guard pass, mount, back mount, back control, knee on belly and sweep), and in the event of a draw by the referee\u2019s decision [].The main characteristic of Brazilian jiu-jitsu is intermittency []. An athlete has to perform on average four to six matches to become champion in the main competitions of the modality [, ]. Various capacities and physical skills are required during a jiu-jitsu match, and thus, the athletes need to be in excellent physical condition to support the demands of the training and consequently the matches []. In this sense, as examples, we can cite aerobic power, which collaborates to maintain a high intensity throughout the match, delay fatigue and achieve a better/faster recovery between matches []; muscle strength, which is used for both attack and defence; muscle power, used in the application of throwing techniques or in some specific movements of groundwork actions (sweeps and guard pass); muscular endurance for maintaining grip on the opponent\u2019s gi (specific apparel for training) when there is a gripping dispute, to dominate the opponent and apply techniques and maintain positions; the reaction time used to dodge and/or anticipate the opponent\u2019s attacks or take advantage of opportune moments for the application of attacks; and flexibility, which collaborates in specific situations of attack or defence [, ]. Also, due to the fact that athletes are divided according to body mass, the fighters are required to present a low percentage of body fat, with greater muscle development, predominantly a mesomorphic profile, which is associated with competitive success, since athletes often reduce their body mass to compete [, ].Given this dynamic of the matches, the athletes are required to possess a high level of fitness. In this regard, for an organization and training prescription with greater specificity, it is essential to know the physical and physiological profiles of the sport\u2019s athletes. Other combat sports such as judo [], wrestling [], amateur boxing [], taekwondo [] and karate [] have had this profile very well described in the literature.However, to date, there have been no in-depth review papers that synthesize the physical and physiological characteristics of Brazilian jiu-jitsu athletes. A review of Brazilian jiu-jitsu athletes\u2019 characteristics could improve the knowledge of coaches, and strength and conditioning of trainers, concerning the physical and physiological profiles needed to reach a high level of performance in this combat sport. Thus, the aim of the present study is to provide a comprehensive review that will help scientists, coaches and athletes to better understand the physical and physiological profile requirements of Brazilian jiu-jitsu. Finally, it is important to point out that the current systematic review centred on analysing the studies involving the gi or kimono in Brazilian jiu-jitsu athletes.In general, Brazilian jiu-jitsu athletes had low body fat, without differences between novices and experts or between elite and non-elite athletes. The mesomorphic component was predominant. Aerobic power was similar to that of other grappling combat sports and did not seem to be influenced by the Brazilian jiu-jitsu athlete\u2019s competitive level. Further research is needed to quantify anaerobic power, especially in upper limbs. The values of isometric handgrip strength are not high. However, specific tests for grip strength endurance using the gi can discriminate athletes with different experience and competitive levels in Brazilian jiu-jitsu. More studies are necessary to describe the maximal strength profile of Brazilian jiu-jitsu athletes. However, until now, maximal dynamic strength has been associated with sporting excellence or success in Brazilian jiu-jitsu athletes in upper limbs. Decisive actions and therefore athletic performance during Brazilian jiu-jitsu matches are mainly dependent on muscular power in both upper and lower limbs; however, more studies are necessary to describe the power strength profile of Brazilian jiu-jitsu athletes. With regard to flexibility, experience and competitive level seem to influence an athlete\u2019s flexibility responses, as experienced athletes had greater flexibility than beginners and elite athletes showed more flexibility than non-elite athletes. Lastly, more research is required to find out whether reaction time can be a determining factor in athletic success in Brazilian jiu-jitsu.Thus, based on the aspects described above, it is remarkable to note that there are a few studies mapping the performance of Brazilian jiu-jitsu athletes, especially involving variables such as aerobic and anaerobic power and aerobic and anaerobic capacity. Because of the intermittent characteristic of Brazilian jiu-jitsu, it is important to carry out more research to report the anaerobic power and capacity performance of such athletes. Moreover, longitudinal studies describing the responses of Brazilian jiu-jitsu athletes to physical training and competitive performance are incipient. Thus, new studies with this purpose are indispensable. Researches describing the female sex are also indispensable, given that only two studies have investigated this population, in which only body composition was measured. In addition, further research is needed to analyse whether there are differences between sex, belt ranks, competitive level and sport experience time, and among the different weight categories for different variables."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0076-1", "title": "Total Energy Expenditure, Energy Intake, and Body Composition in Endurance Athletes Across the Training Season: A Systematic Review", "authors": ["Juliane Heydenreich", "Bengt Kayser", "Yves Schutz", "Katarina Melzer"], "publication": "Sports Medicine - Open", "publication_date": "4 February 2017", "abstract": "Endurance athletes perform periodized training in order to prepare for main competitions and maximize performance. However, the coupling between alterations of total energy expenditure (TEE), energy intake, and body composition during different seasonal training phases is unclear. So far, no systematic review has assessed fluctuations in TEE, energy intake, and/or body composition in endurance athletes across the training season.", "full_text": "Total energy expenditure (TEE) is composed of the energy costs of the processes essential for life (basal metabolic rate (BMR), 60\u201380% of TEE), of the energy expended in order to digest, absorb, and convert food (diet-induced thermogenesis, ~10%), and the energy expended during physical activities (activity energy expenditure, ~15\u201330%) [, ]. Elite endurance athletes are characterized by high fluctuations of TEE, mainly due to the variability of the energy expended during sporting activities. Among elite senior endurance athletes, training loads from 500\u00a0h/year [, ] up to 1000\u00a0h/year [\u2013] have been reported, depending on the specific muscular loading characteristic of the sport. During heavy sustained exercise (e.g., during the Tour de France), TEE can be as high as fivefold the BMR over several weeks []. On the other hand, during recovery days, pre-competition tapers, or during the off-season, the energy expended in activities is far less. Therefore, TEE is expected to be much lower and may even reach levels comparable to that of sedentary behavior.An appropriate energy intake supports optimal body function, determines the capacity for intake of macronutrients and micronutrients, and assists in manipulating body composition in athletes []. It is a challenge for each endurance athlete to appropriately match energy intake and TEE in order to achieve energy balance and thus, weight stability, both on a micro level (i.e., over 1\u00a0day or several days) and through the training and competitive season. Furthermore, endurance athletes in general strive for a low body mass and/or body fat level for various advantages in their sports, specifically during the competition season []. This allows runners and cyclists to reach greater economy of movement and better thermoregulatory capacity from a favorable ratio of weight to surface area and less insulation from subcutaneous fat tissue. Elite endurance athletes are therefore characterized by low body mass and body fat content. For example, in elite Kenyan endurance runners, the body fat percentage was 7.1% [], which is only marginally above the recommended 5% minimum for males []. In the same athletes, body mass index (BMI) was 18.3\u00a0kg/m [], which is generally classified as being underweight []. However, these athletes were in peak physical conditions as the investigations were undertaken and a low body fat percentage and body weight might be an advantage for competition. Achieving a negative energy balance and a concomitant loss of body and fat masses in preparation for competition can be accomplished in phases with high daily TEE solely by the reduction of energy intake, since any further training load increases could cause overtraining []. Therefore, the nutritional goals and requirements of endurance athletes are not static over the training year. Since endurance athletes undertake a periodized training program and follow periodized body composition goals, the nutritional support also needs to be periodized [].Although the concept of training periodization in elite endurance sports has been established for a long time, the coupling of periodized training with nutrition and body composition has gained scientific awareness only recently []. Stellingwerff\u2019s group was one of the first to publish periodized nutrition guidelines for middle-distance athletes [], they then expanded these recommendations for a multitude of power sports []. Nowadays, there are guidelines for carbohydrate, protein, and fat intake during training and competition phases, not exclusively focusing on endurance sports [\u2013]. Meanwhile, for endurance athletes, sport-specific dietary intake recommendations were developed only for a few endurance disciplines (e.g., swimmers [\u2013], distance runners [], marathon/triathlon/road cycling []). But it remains unclear whether endurance athletes are actually following these nutrient guidelines across all seasonal training phases.The validity of either body composition, energy intake, or TEE-determination in athletes strongly depends on the methods used. The measurement of body composition in general is prone to error. It has been shown that acute food or fluid ingestion [], subject positioning [], previous physical activity [], and hydration status [] have an impact on reliability of body composition measurement. Since endurance athletes often train several times per day, it might be difficult to assure best conditions for body composition assessment. According to a recent methodology review performed by Nana et al., only few of the studies, where body composition of athletes was measured with dual X-ray absorptiometry (DXA), provided details about their subject and device standardization []. However, other methods like skinfold measurements require highly experienced investigators [] and strongly depend on the number of measurement sites and the formula used to calculate the percentage of body fat []. Therefore, it is important to report standardization protocols in order to evaluate the quality of data assessment. One main issue in assessing energy intake in athletes is the magnitude of under-reporting, which can amount to 10\u201345% of TEE []. It was shown that the magnitude of under-reporting increases as energy requirements increase []. Since endurance athletes are often characterized by high TEE, we must assume that these athletes are very prone to a high percentage of under-reporting. For determination of TEE objective methods such as doubly labelled water (DLW) or heart frequency measurements are available. However, in many studies subjective methods such as activity records and activity questionnaires are used in order to assess the activity level and TEE of subjects. These methods  TEE or activity level and their validity strongly depends on the breadth of the activity dimensions analyzed.There exist some longitudinal studies that have assessed fluctuations in body composition, dietary intake, and/or TEE of endurance athletes across the training seasons [\u2013], but no systematic reviews have been performed. Therefore, the purpose of this study was to (1) systematically analyze TEE, energy intake, and body composition in highly trained athletes of various endurance disciplines and of both sexes with focusing on objective assessment methods and (2) analyze fluctuations in these parameters across the training season. We hypothesized that endurance athletes show large fluctuations of TEE during different seasonal training phases due to differing exercise loads, and concomitant alterations in energy intake and body composition.The review protocol was developed according to the Meta-analysis of Observational Studies in Epidemiology Guidelines for meta-analyses and systematic reviews of observational studies [].In this systematic review, we examined fluctuations in TEE, energy intake, and/or body composition in endurance athletes across the training season. We found that some, but not all, of the investigated outcomes depended on the time point of data assessment during seasonal training. TEE was highest during the competition phase and higher than energy intake in all seasonal training phases. Alterations in TEE did not lead to adaptations of energy intake in females, whereas in males, a higher absolute energy intake during the competition phase was observed. The finding that male endurance athletes demonstrated the highest fat mass values during the competition phase and the lowest FFM during the transition phase seems to be an anomaly from the pooling of data.Our systematic search initially yielded many studies where TEE, energy intake, or body composition in endurance athletes were investigated. Only a few (2%) reported the time point of data collection with regard to the training season and could thus be included in this review. This is unfortunate since our analysis clearly illustrates how training volume and related TEE vary importantly with seasonal training phases. Specifically and expectedly, both absolute and relative TEEs were significantly higher during the competition phase compared to the preparation phase. Interestingly, these differences were only partly in agreement with alterations in energy intake and/or body composition of endurance athletes.During the transition phase, limited data for TEE and energy intake of endurance athletes was available. Only for body composition, it was possible to compare with other seasonal training phases, although the number of study estimates and therefore, explanatory power, was weak. Future research on elite athletes should focus on the effects of a sudden stop or reduction in TEE on body composition (e.g., because of injury). There exist only a few studies (with conflicting results) where this question has been examined. Ormsbee and Arciero investigated the effects of 5\u00a0weeks of detraining on body composition and RMR in eight male and female swimmers []. RMR decreased, whereas fat mass and body weight increased with detraining. In contrast, LaForgia et al. showed that after 3\u00a0weeks of detraining, no differences in RMR and percentage of fat mass occurred in male endurance athletes []. Unfortunately, energy intake was not reported in either of these studies. Thus, it remains unclear when, whether, and to what extent the body adapts (through changes in energy intake and/or body composition) for the decrease in TEE caused by detraining.Our analysis highlights an important apparent negative energy balance in endurance athletes, both in the preparation and competition phases, when separately examining the energy balance in articles where both energy intake and TEE were assessed (\u2009=\u200911). Negative energy balance was reported during the preparation phase in male [, ] and female [] cross-country skiers, male [] and female [] runners, and female lightweight rowers [] and swimmers [], and amounted to a mean of 304\u00a0kcal/day (4.7% of TEE) for males and 1145\u00a0kcal/day (27.8%) for females. During the competition phase, a negative energy balance was reported in male cyclists and triathletes [], male [] and female [, ] runners, and male cyclists [, ], averaging 2177\u00a0kcal/day (32.5%) for male and 1252\u00a0kcal/day (47.9%) for female endurance athletes. The most obvious explanation for these energy deficits is likely the classical issue of under-reporting energy intake through self-assessment in human studies. A review of nine studies using DLW to validate self-reported energy intake in athletes revealed that under-reporting can amount to 10\u201345% of TEE []. Since under-reporting increases in magnitude as energy requirements increase [], we must assume that under-reporting in the present study estimates was more important during the competition phase. Even when 45% was added to the energy intake of all athletes included in our review, there still remained a negative energy balance of 118\u00a0kcal (2.7% of TEE) in the preparation and 5293\u00a0kcal (53.6%) in the competition phase. Another explanation for the negative energy balance might be the low accuracy and precision of methods used to estimate energy intake in athletes in the articles included in our review. For example, mostly dietary records with a mean observation time of 4.7\u2009\u00b1\u20094.1\u00a0days were used. According to Magkos and Yannakoulia, for athletes, a 3\u20137-day diet-monitoring period would be enough for reasonably accurate and precise estimations of habitual energy and macronutrient consumption []. However, other methods like FFQs and dietary recalls were also used for energy intake estimations. These methods are both memory-dependent and show lower accuracy and precision than prospective methods like dietary records []. However, even when only articles were considered where energy intake was assessed by the use of dietary records, the error remained high (2.5% of TEE during the preparation phase and 54.9% during the competition phase). Finally, the high negative energy balance during the competition phase may also be explained by the fact that, apart from one study, all included studies investigated the TEE during the days with actual competition and not during habitual training days in the competition phase. Thus, it is likely that the TEE during this phase was over-estimated. During the preparation phase, a negative energy balance leading to increased energy store utilization might be desirable by coaches and athletes to reach a sport-specific body composition, but during the competition phase, body composition should not be modified anymore since it is typically already at its optimum. There was one study in which dietary intake was strictly controlled since the subjects were in confinement. Brouns et al. simulated a Tour de France race in a metabolic chamber and calculated the daily energy balance from the energy expended and energy intake as calculated from daily food and fluid consumption []. They found a positive energy balance during active rest days whereas during the exercise days, a significant negative energy balance was observed. The authors concluded that if prolonged intensive cycling increases energy expenditure to levels above a certain threshold (probably around 20\u00a0MJ or 4780\u00a0kcal), athletes are unable to consume enough conventional food to provide adequate energy to compensate for the increased energy expenditure. The authors of a recent review addressing the criticisms regarding the value of self-reported dietary intake data reasoned that these should not be used as a measure of energy intake []. Our analysis supports this statement since, for athletes, relative energy deficits amounted up to 48% of TEE in female athletes and 33% in male athletes during the competition phase. Thus, there is an urgent need for better methods of dietary intake quantification, such as dietary biomarkers and automated image analysis of food and drink consumption []. The classical concept of energy balance, defined as dietary energy intake minus TEE, has been criticized, since according to this definition energy balance is the amount of dietary energy added to or lost from the body\u2019s energy stores after the body\u2019s physiological systems have done their work for the day []. Thus, energy balance is an  from those systems. In contrast, energy availability, defined as the dietary energy intake minus the energy expended during exercise, is an  to the body\u2019s physiological systems, since energy availability is the amount of dietary energy remaining for all other metabolic processes []. Endurance athletes, especially female athletes, show low energy availability (<30\u00a0kcal/kg FFM/day) [] and increased risk for changes of the endocrine system affecting energy and bone metabolism, as well as in the cardiovascular and reproductive systems []. In healthy young adults, energy balance\u2009=\u20090\u00a0kcal/day when energy availability\u2009=\u200945\u00a0kcal/kg FFM/day []. Since the results of the present study indicate a high negative energy balance in endurance athletes, we must assume that the athletes also demonstrate low energy availability. However, due to the limited data, it was not possible to account for other clinical markers (e.g., bone mineral density), menstrual status, or prevalence of eating disorders in the athletes. We recommend that energy balance-related studies in endurance athletes should also assess and report clinical markers, such as bone mineral density and menstrual status, in order to assess the clinical consequences of the mismatch of TEE and energy intake.The aggregate analysis yielded a surprising finding. In male endurance athletes, the absolute and relative fat mass was highest during the competition phase. In contrast, during the transition phase, FFM was lowest, which goes along with our expectations with a decrease in exercise volume and intensity. For the female athletes, we did not find these fluctuations in body composition, except for a higher body fat content during the preparation phase compared to the transition phase. We believe that these findings are due to the paucity of data and to the fact that the number and type of athletes varied between seasonal training phases. Indeed, when separately analyzing the few studies where body mass and composition were assessed during both the preparation and competition phases (\u2009=\u20095), both male and female endurance athletes showed a significantly lower percentage of body fat and higher FFM during the competition phase. Further studies with longitudinal assessments of body composition are required to support these findings. However, in only 5.7% of the studies, where body composition was assessed, satisfactory details about standardization were provided. According to Nana et al., studies involving DXA scans of body composition should report details of the DXA machine and software, subject presentation and positioning protocols, and analysis protocols []. It has been shown that the use of a non-standardized protocol increased the variability for total and fat-free soft tissue mass compared to a standard protocol, which might include a loss in ability to detect an effect of an intervention that might have relevance for sports performance []. The use of non-standardized protocols and the concomitant higher variability might explain some of the unexpected findings of body composition changes in athletes of the present study.In male endurance athletes, absolute energy intake was higher during the competition phase compared to the preparation phase. The relative energy intake was not different, which can be explained by the apparent significant increase of body mass during the competition phase, and is likely an artifact of the aggregation of data from various studies. In female athletes, neither absolute nor relative energy intake was different between seasonal phases. When focusing on longitudinal studies that assessed energy intake during different training seasons in the same cohort, there was a tendency for male athletes to show greater fluctuations in energy intake. In female cross-country skiers, the energy intake was higher during the preparation phase [], whereas in female runners and swimmers, the energy intake was higher during the competition phase []. However, summing up both studies, no significant differences between training season phases were found. In contrast, male endurance athletes showed a significantly higher energy intake during the competition phase, as seen in male runners [], cross-country skiers [], swimmers [], and triathletes []. Although some of the included studies showed greater energy intake in male endurance athletes during the preparation phase (cyclists [, ], swimmers []), the power of these studies was too low to change the results. However, since energy intake varies in male endurance athletes depending on the training season phase, it indeed seems appropriate to adapt dietary recommendations according to the different training season phases, as proposed by Stellingwerff et al. [, ].Our analysis highlights the important seasonal fluctuations in TEE, energy intake, and body composition in male and female endurance athletes across the training season. Therefore, dietary intake recommendations should take into consideration other factors including the actual training load, TEE, and body composition goals of the athlete. The present review supports the statement of the current position stand of the American College of Sports Medicine (ACSM) that energy and nutrient requirements are not static and that periodized dietary recommendations should be developed []. Importantly, our analysis again shows the uselessness of self-reported dietary intake, a well-known limitation to energy balance studies, in endurance athletes. The important underreporting suggested by our analysis again raises the question of whether self-reported energy intake data should be used for the determination of energy intake and illustrates the need for more valid and applicable energy intake assessment methods in free-living humans []. Since we observed a lack of data during the transition phase, future research should focus on the assessment of TEE, energy intake, and body composition on a reduction in training intensity and volume, such as at the end of the competitive season. In addition, future studies dealing with energy balance and nutrient intake in elite endurance athletes should always mention the time point of data assessments (e.g., seasonal training phase)."},
{"url": "https://jrenewables.springeropen.com/articles/10.1186/s40807-015-0015-z", "title": "Dual effect of TiO", "authors": ["F. A. Taher", "Galila M. El-sayed", "N. M. Khattab", "N. Almohamady"], "publication": "Renewables: Wind, Water, and Solar", "publication_date": "4 November 2015", "abstract": "Dye-sensitized solar cell (DSSC) was fabricated using nanosize of the dye sensitizer (Alizarin Yellow, AY) that was prepared by ball milling. \nThe particle size and the composition of nano-Alizarin Yellow (nAY) was investigated using TEM and ", "full_text": "DSSC is an alternative solution for the future energy crisis as a productive source for renewable energy (Kato et al. ; Zhuiykov ; Ludin et al. ). Excitation of dye sensitizer that was doped onto semiconductor or co-semiconductor by sun radiation to generate an electron and leave behind a hole is the initial photon-induced electron reaction in DSSC (Yum et al. ). After transition of the excited electron from semiconductor conduction band to a counter electrode through working electrode, the ground state of the dye is reached by electrolyte oxidation (Choi et al. ; Han and Ho ). The main issue is in returning some electrons back to the dye ground state or electrolyte causing an increase in the electron\u2013hole recombination rate and then deficiency in DSSC efficiency (Lai et al. ; Akpan and Hameed ; Yamaguchi et al. ; Reda ; Kato et al. ; Tian et al. ; Kantonis et al. ; Sharma et al. ; Basheer et al. , ). Since, the efficiency of the DSSC relies on the sensitizer and semiconductor, the idea here is to increase the absorption band of the sensitizer by increasing its surface area or decrease the electron\u2013hole recombination rate using darker co-semiconductor to achieve higher solar conversion efficiency.Actually, Im and his co-worker have used the cocktail effect of TiO and FeO to increase the performance of DSSC. The efficiency of the DSSC has been developed by over 300\u00a0% (Im et al. ). Also, NiO/TiO nanocomposites were prepared and used as modified photoelectrodes in quasi-DSSC with 2.29\u00a0% conversion efficiency as by Mekprasart et al. (). To the best of our knowledge, so far, the effect of CoO as a co-semiconductor was not previously reported therein. In this work, the dye sensitizer was converted to nanosize to investigate its size reduction on the DSSC efficiency. Also, a composite of TiO and CoO was prepared to use as a semiconductor in DSSC. In addition, the effect of terpineol as a solvent was tested via I\u2013V characteristic curves.Five DSSCs were prepared to investigate the effects of their construction on their solar conversion efficiency. The nanosize of AY (less than 100\u00a0nm) has a great effect on the DSSC efficiency that increased by 70\u00a0%. Actually, the presence of CoO as a co-semiconductor in DSSCs electrode increased their efficiency by 165 and 620 times for the cells modified by TiO\u00a0+\u00a0CoO only and TiO\u00a0+\u00a0CoO with nAY, respectively. The presence of solvent (terpineol) increased the efficiency of DSSC by 13-fold. Finally, the predicted mechanism for the conversion of photons to current for the DSSCs was discussed."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0155-4", "title": "Genome-wide identification of grain filling genes regulated by the OsSMF1 transcription factor in rice", "authors": ["Joung Sug Kim", "Songhwa Chae", "Kyong Mi Jun", "Yoon-Mok Pahk", "Tae-Ho Lee", "Pil Joong Chung", "Yeon-Ki Kim", "Baek Hie Nahm"], "publication": "Rice", "publication_date": "26 April 2017", "abstract": "Spatial- and temporal-specific expression patterns are primarily regulated at the transcriptional level by gene promoters. Therefore, it is important to identify the binding motifs of transcription factors to better understand the networks associated with embryogenesis.", "full_text": "Transcription is known to be regulated by the binding of transcription factors to their cognate motifs in the promoter regions of genes. In monocotyledons, several -elements, such as the prolamin box (TGTAAAG), GCN4 motif (TGA(G/C)TCA), and AACA motif (AACAAAA), are highly conserved in the promoters of seed storage protein (SSP)-encoding genes and play central roles in controlling endosperm-specific gene expression during seed maturation (Takaiwa et al. ; Wu et al. ) Maize opaque-2 (O2) is an endosperm-specific transcription factor belonging to the bZIP family that has been shown to bind to the ACGT motif of the maize 22-kDa  promoter and activate transcription (Schmidt et al. ). In addition, a barley bZIP transcriptional activator (BLZ1) binds to the GCN4 motif, which is putatively involved in regulating gene expression in the endosperm (Vicente-Carbajosa et al. ). Further, rice endosperm bZIP, RISBZ2 (also called REB), and opaque-2 heterodimerizing protein 1 (OHP1) specifically bind to the GCCACGT(A/C)AG sequence in the  () gene promoter and the TCCACGTAGA sequence in the 22-kDa  promoter, respectively (Nakase et al. ; Pysh et al. ). Other transcription factors are also involved in regulating the expression of starch synthesis genes. For example, a MYC-like protein (OsBP-5) and OsEBP-89, a member of the ethylene-responsive element-binding protein (EREBP) family, act synergistically as a heterodimer to regulate transcription of the rice  gene (Zhu et al. ). Additionally,  (), which is an EREBP-type transcription factor, negatively regulates starch biosynthesis, and an deficient mutant has been shown to exhibit the enhanced expression of starch synthesis genes in seeds (Fu and Xue ).\n                         (previously called ) is a basic leucine zipper transcription actor that is involved in the regulation of rice eed aturation. It belongs to the maize O2-like protein group and is known to interact with the GCN4 motif (TGA(G/C)TCA) to regulate SSPs (Onodera et al. ). A previous study has shown that  (alternatively named ) is also involved in seed development, including the regulation of starch biosynthesis genes (Wang et al. ). It specifically binds to the ACGT elements in the promoters of both  () and  () (Wang et al. ).  gene expression is restricted to seeds, where it precedes the expression of storage protein-encoding genes (Onodera et al. ). Studies have demonstrated that  is involved in the regulation of starch synthesis, in addition to that of SSP synthesis, in the endosperm (Onodera et al. ; Wang et al. ). Here, we aimed to identify target genes for further functional analysis of OsSMF1.For the large-scale identification of transcription factor target sites, several laboratory techniques, such as chromatin immunoprecipitation (ChIP) and DNA adenine methyltransferase identification (DamID), have been devised (Orian ; Pokholok et al. ; Ren et al. ; van Steensel et al. ; Wyrick et al. ). Recently, chip-based protein-binding microarrays (PBMs) have been developed, allowing for the identification of protein-DNA interactions in vitro. In our previous study, we constructed a quadruple 9-mer protein-binding microarray (Q9-PBM), in which 131,072 quadruple probes were designed to cover all possible combinations of 9-mers of the reverse complementary sequences (Kim et al. ). The specificity of Q9-PBM was confirmed using well-known DNA-binding sequences, including Cbf1 and CBF1/DREB1B, and it was also used to elucidate the unidentified -acting element of the  rice transcription factor (Kim et al. ). The Q9-PBM has mainly been used to assess the interactions of transcription factors with short synthetic DNA sequences and to evaluate their DNA sequence specificities. Here, we used the Q9-PBM to determine the binding motifs of OsSMF1. In a previous study, the RiceArrayNet (RAN) was developed from collective data obtained from 60\u00a0K rice microarrays (Lee et al. ), and it has been widely used (Hamada et al. ; Lorenz et al. ; Movahedi et al. ). We have constructed a new version of the RAN that provides co-expression information on genes, including correlation coefficients, calculated using accumulated data obtained from 300\u00a0K rice microarrays (). To predict the co-expressed genes, we also examined the relationships among genes that are putatively regulated by  by RAN analysis.Q9-PBM analysis revealed that OsSMF1 recognized three binding motifs, namely the GCN4 [TGA(G/C)TCA], ACGT [CCACGT(G/C)], and ATGA (GGATGAC) motifs, with different affinities. We detected 85 genes that were positively regulated by OsSMF1 using the RAN analysis under the tested conditions, with a minimum correlation value of 0.55 and a depth of 1. Among these putative target genes, 18 (21.2%), 21 (24.7%), and 13 (15.3%) contained GCN4, ACGT, and ATGA motifs within their 1-kb promoter regions, respectively. In addition to confirming the known OsSMF1 target genes, we predicted 35 potential target genes that have not been previously described in immature seeds. Using qRT-PCR and the protoplast transactivation assay, we found that in vivo, OsSMF1 activated  and , which contain the GCN4 motif;  and , which contain either the GCN4 or ACGT motifs; and  () and  (), which contain the ATGA motif. The results suggest that  has specific binding affinities for the three motifs and that it functions in a wide variety of seed developmental processes.Determination of the DNA-binding specificities of transcription factors is important for the understanding of transcriptional regulatory codes. Rice seed development is directly related to crop yield and is finely controlled by complex regulatory networks. Gene co-expression network analysis has shown that transcription factors are involved in the complex regulation of rice seed development (Xue et al. ). However, many of the primary target genes of these transcription factors as part of regulatory networks remain to be elucidated. We aimed to identify target genes of the seed-specific transcription factor OsSMF1, which is known to be involved in the regulation of rice SSP gene expression (Kawakatsu et al. ; Yamamoto et al. ) and starch synthesis (Onodera et al. ; Wang et al. ). Q9-PBM analysis, which is a powerful and rapid method for the identification of putative functional -regulatory elements, allows for the accurate quantification of the binding affinities of some transcription factor binding motifs using all possible 9-mer combinations of probes (Kim et al. ). We used this technology to characterize the binding specificities of OsSMF1 and identified three DNA-binding motifs for this transcription factor, including the GCN4 (TGA(G/C)TCA), ACGT (CCACGT(C/G)), and ATGA (GGATGAC) motifs (Figs.\u00a0 and ). It has recently become apparent that a transcription factor may interact with more multiple DNA elements. For example,  MYC2 binds with high affinity to the G-box (CACGTG), T/G (AACGTG), and G-like (CATGTG) motifs (Godoy et al. ). Another rice bZIP protein, REM, binds to both the ACGT and GCN4 motifs (Nakase et al. ). OsSMF1 and REM have a close phylogenetic relationship (52.9%). However, OsSMF1 expression is restricted to the mature seed, whereas REM is expressed in all tissues. Furthermore, the expression of  at 11 DAP was determined to be 6-fold higher than that of  in this study (Additional file : Figure S1). These results suggest that  plays important roles in a wide range of biosynthetic processes during seed maturation. In addition to the known GCN4 binding motif, OsSMF1 also recognizes the ATGA and ACGT motifs. The ACGT motif has been previously shown to be a target of OsSMF1-regulated gene expression, but the essential flanking sequence of this motif has not yet been identified (Wang et al. ). Q9-PBM analysis and EMSA performed in this study revealed that CCACGT(C/G) is an essential sequence that is bound by OsSMF1 (Figs.\u00a0 and ). We found that OsSMF1 bound to TGAGTCA (GCN4 motif), CCACGTC (ACGT motif), and GGATGAC (ATGA motif) with intensities of 6,463, 13,715, and 7,639, respectively. The \n                         value for the binding of OsSMF1 to the ACGT motif was approximately two- and four-fold higher than those for the GCN4 and ATGA motifs, respectively (Fig.\u00a0), demonstrating that OsSMF1 plays prominent roles during seed development by binding to these three motifs in vitro.First, we selected genes containing the GCN4, ACGT, and ATGA motifs in their 1-kb promoter regions from the RAP2 rice database. Second, the RAN analysis, which was based on the expression pattern correlation, was capable of narrowing down the putative target genes of  to a subset of 85 genes (Additional file : Figure S2). To validate these predicted genes, we searched the GCN4 motif for known OsSMF1 binding sequences by combined microarray analysis. Among the six putative genes, four genes (, , and ) have been previously identified as OsSMF1 target genes by transient assay using rice callus protoplasts (Yamamoto et al. ). Among the remaining targets that were detected, two globulin genes ( and ) were characterized as SSPs. A previous study demonstrated that OsSMF1 causes reduced activation of the  promoter compared with that of the  and  promoters (Yamamoto et al. ). However, this group assessed the  region from \u2212340\u00a0bp to +73\u00a0bp, which does not include the ACGT motif positioned at \u2212436\u00a0bp relative to the ATG start codon, in transient assays using rice callus protoplasts. Although patatin-like protein was not classified under the Gene Ontology term \u201cnutrient reservoir activity (GO:0045735)\u201d, it contained the GCN4 motif and was determined to be one of the direct target genes of OsSMF1 (Table\u00a0, Fig.\u00a0). Patatin represents approximately 40% of the soluble protein in potato tubers, but it has not yet been studied in rice.Among the 44 total putative target genes, 9 transcription factors (27.5%), including , were observed, which were assigned to \u201cregulation of nitrogen compound metabolic process (GO:0051171, Table\u00a0).  is specifically expressed in the aleurone and subaleurone layers of the developing endosperm (Onodera et al. ), and the expression of the target genes of this transcriptional activator may be restricted to these subcellular locations. Among the 9 transcription factors, 6 genes, which were preferentially expressed at 11 and 21 DAP when  was also highly expressed (Additional file : Figure S4), were selected and four transcription factors were identified as target genes through in vivo protoplast analysis. The promoters of , , and  contained the ACGT motif, and the  and  genes incorporated the ATGA motif in their promoters. The  transcription factor, which contained both the GCN4 and ACGT motifs on its promoter, showed significantly increased expression by OsSMF1. , which was cloned from maize by homologous cloning using rice , is co-expressed with many starch synthesis genes. Thus,  might be co-expressed with starch biosynthetic genes in the rice endosperm. A previous study showed that  (alternatively named ) null mutants displayed abnormal seed morphology with altered starch accumulation and decreased amounts of total starch, particularly amylase, with binding to the promoters of six starch-synthesizing genes (Wang et al. ). We suggest that  and  might be additional transcription factors that regulate the synthesis of starch in the presence of OsSMF1 during seed development. In addition, both the ACGT and GCN4 motifs in the  promoter provides a high-affinity binding site for  compared to the ACGT motif in the  promoter.  is known to repress the activation of OsSMF1, in addition to promoting the down regulation of  expression (Chen et al. ). Protoplast assay revealed that luciferase activity of the  vector was decreased by 0.5-fold compared with that of the negative control (Fig.\u00a0). Expression of the  transcription factor was increased in the -transformed calli compared with that in wild-type plants (Fig.\u00a0), and OsSMF1 bound to the promoter of  in vivo, as determined by protoplast assay (Fig.\u00a0). Additionally, RPBF and OsSMF1 have been reported to synergistically activate transcription from the promoters of rice SSPs (Kawakatsu et al. ; Yamamoto et al. ). Knock-out transgenic rice, in which the accumulation of OsSMF1 (called RISBZ1) and RPBF was reduced, showed a significant reduction in SSPs (Kawakatsu et al. ). These results suggest that OsSMF1 directly regulates the expression of  and  by binding to the ATGA motif in their promoters to regulate SSPs. However, neither  nor  was expressed by OsSMF1 in the protoplast assay (data not shown). The protoplast in vivo analysis revealed that the binding of OsSMF1 alone to the binding motif was not sufficient to self\u2013activate the OsSMF1 transcription factor (data not shown). This result eliminated the possibility that OsSMF1 autoregulates itself as observed with GUS reporter genes under the control of the OsSMF1 promoter in rice protoplast (Onodera et al. ).The majority of the newly identified target genes have unknown functions in seed development. We showed that two unknown target genes were highly expressed in -transformed calli compared with wild-type calli. Among them,  was identified as an OsSMF1 target gene. Interestingly, we determined that OsSMF1 bound to the promoter of  by protoplast transactivation assay (Fig.\u00a0). OsEnS44 contains a thioredoxin domain, which acts in redox regulation throughout the life cycle of the seed (Wong et al. ). The activation of OsEnS44 by OsSMF1 may be one transcription factor cascade or may act as a transcription factor for the activation of downstream genes that regulate the biosynthesis of the major seed components. Therefore, these results provide novel information regarding the regulatory roles of OsSMF1 in seed maturation. It can be concluded that OsSMF1 is one of the key transcription factors involved in grain filling that functions by regulating the expression of a wide variety of genes during seed maturation.Overall, the Q9-PBM results showed that OsSMF1 bound to TGAGTCA (GCN4 motif), CCACGTC (ACGT motif), and GGATGAC (ATGA motif) with different affinities. First, the transcription factor OsSMF1 regulated the expression of RPBF and OsGZF1, which contained the ATGA motif on their promoters. This result suggested that OsSMF1 regulated SSPs by binding the promoter of these genes. Second, OsSMF1 also regulated starch synthesis by binding to the promoters of  which contained either the ACGT or GCN4 motifs. Finally, we identified  as an OsSMF1 target gene, which contained the GCN4 motif in its promoter region, but its function is unknown in seed development. Further studies on the regulatory mechanisms of OsSMF1, including phenotype observations in transgenic rice, will help advance the understanding of rice seed development."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0162-5", "title": "Silencing of the Rice Gene ", "authors": ["Daniel F. Caddell", "Chang-Jin Park", "Nicholas C. Thomas", "Patrick E. Canlas", "Pamela C. Ronald"], "publication": "Rice", "publication_date": "22 May 2017", "abstract": "The rice immune receptor XA21 confers resistance to ", "full_text": "Cell surface immune receptors, which often perceive conserved microbial signatures present outside host cells, represent a first line of defense against potential pathogens. Well characterized plant cell surface immune receptors include  FLAGELLIN SENSING 2 (FLS2), which recognizes a 22 amino acid epitope derived from bacterial flagellin (flg22) (Boller and Felix ), EF-TU RECEPTOR (EFR) which recognizes an 18 amino acid epitope derived from bacterial elongation factor-Tu (elf18) (Zipfel et al. ), and rice XA21, which recognizes a 21 amino acid tyrosine-sulfated epitope derived from the bacterial protein REQUIRED FOR ACTIVATION OF XA21-MEDIATED IMMUNITY X (RaxX; RaxX21-sY) (Pruitt et al. ). These immune receptors fall into the leucine-rich repeat receptor-like kinase class (LRR-RLK) consisting of an N-terminal signal peptide, predicted extracellular LRR domain, transmembrane domain and an intracellular kinase domain. Upon microbial recognition, the kinase domain is hypothesized to initiate a signaling cascade that leads to resistance against specific pathogens. These immune receptors require the presence of LRR-RLK co-receptors that are part of the SOMATIC EMBRYOGENESIS RECEPTOR KINASE (SERK) family. For example, FLS2-mediated immune response requires the BRI1-ASSOCIATED RECEPTOR KINASE (BAK1) (Chinchilla et al. ) and XA21-mediated immune response requires OsSERK2 (Chen et al. ).Despite the importance of immune receptors to plant survival, few components that mediate the downstream signaling cascade have yet been characterized. One method that has been successful at identifying such proteins is the yeast two-hybrid system. We used this approach to generate an XA21 interactome (Seo et al. ). One of the isolated proteins was named XA21 BINDING PROTEIN 21 (XB21). XB21 encodes an auxilin-like protein that regulates resistance to  and is predicted to function in clatharin-mediated endocytosis (Park et al. manuscript in preparation). To investigate the function of XB21, we performed a yeast two-hybrid screen utilizing XB21 as bait with a rice cDNA library. From this analysis, we identified LEUCINE RICH REPEAT PROTEIN 1 (LRR1). LRR1 shares 55% identity and 68% similarity with the extracellular domain of OsSERK2. To determine if LRR1 is required for XA21-mediated immunity, we assessed its function in rice plants in the presence and absence of XA21.Transgenic rice plants expressing  that were silenced for  (XA21-LRR1Ri) displayed enhanced susceptibility to . XA21-LRR1Ri plants also have decreased expression of  transcripts. These results demonstrate that  is required for XA21 \u2013mediated immune responses and for  transcription. These results also highlight the diverse roles played by LRR containing proteins in regulating plant responses against pathogens.To elucidate the function of XA21 and the XA21 binding protein XB21, we characterized LRR1, which interacts with XB21. LRR1 is a leucine rich repeat containing protein with similarity to the LRR domain of OsSERK2 (Chen et al. ) (Fig.\u00a0), which is a co-receptor for XA21. Silencing of  in the resistant XA21 genetic background conferred enhanced susceptibility to  and reduced the expression of  (Fig.\u00a0b). These results suggest that LRR1 is required for  transcription.Rice LRR1 and its pepper ortholog were previously reported to enhance resistance to bacterial pathogens in  and Chinese cabbage. In this study, we identified a new role of LRR1: transcriptional regulation of the rice immune receptor ."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0159-0", "title": "Dissection of broad-spectrum resistance of the Thai rice variety Jao Hom Nin conferred by two resistance genes against rice blast", "authors": ["Chaivarakun Chaipanya", "Mary Jeanie Telebanco-Yanoria", "Berlaine Quime", "Apinya Longya", "Siripar Korinsak", "Siriporn Korinsak", "Theerayut Toojinda", "Apichart Vanavichit", "Chatchawan Jantasuriyarat", "Bo Zhou"], "publication": "Rice", "publication_date": "11 May 2017", "abstract": "Rice (", "full_text": "Rice is one of the most important staple food crops in the world. The increasing world population leads to increased demand for rice production. In order to feed the growing population, rice varieties that are resistant to known diseases and that can produce high yields should be used (Khush and Jena ). Rice blast, caused by an ascomycete , is one of the most destructive diseases of rice worldwide (Ahn et al. ), which leads to economic losses of more than 70 billion dollars (Scheuermann et al. ). In many Asian countries, the outbreak of rice blast which led to complete losses in rice production has been reported (Sobrizal and Anggiani ; Variar ; Lei et al. ; Hairmansis et al. ; Khan et al. ; Nguyet et al. ).The most powerful tool to control rice blast is to use resistant varieties. These varieties protect rice plants against the pathogens, upon infection, via the induction of hypersensitive response (HR), which occurs via gene-for-gene recognition of a pathogen effector (Avr) and a host-encoded resistance (R) protein (Gururani et al. ). The majority of plant  genes encode proteins that have putative central nucleotide-binding-site (NBS) and carboxy-terminal leucine-rich repeats (LRRs). These NBS-LRR proteins are divided into two major classes: the first class has an N-terminal domain which shares homology with the mammalian toll-interleukin-1-receptor (TIR) domain while the second class encodes an amino-terminal coiled-coil motif (CC-NBS-LRR) (DeYoung and Innes ; Meyers et al. ). To date, there are four types of coding proteins of  genes against rice blast: (1) NBS-LRR protein; (2) CC-NBS-LRR protein; (3) B-lectin binding and intracellular serine\u2013threonine kinase domain protein; and (4) proline-rich protein (Chen et al. ; Zhou et al. ; Ashikawa et al. ; Fukuoka et al. ). The first rice blast resistance gene was cloned by Wang et al.  and more than 20  genes were cloned since then. These blast resistance genes are not randomly positioned on rice chromosomes (Sharma et al. ). Many  genes were cloned from chromosomes 6, 11, and 12. Some are clustered on a gene family (Sharma et al. ). Many allelic genes have been reported, for example  on chromosome 1 (Takahashi et al. ; Fukuoka et al. ),  on chromosome 6 (Qu et al. ; Zhou et al. ; Su et al. ),  on chromosome 9 (Lee et al. ; Takagi et al. ), and  on chromosome 11 (Ashikawa et al. ; Xu et al. ; Yuan et al. ; Zhai et al. ; Hua et al. ). To date, 11 avirulence () genes in rice blast fungus have been cloned. Nine out of 11 genes, namely;  (Orbach et al. ),  (Bohnert et al. ), , ,  (Yoshida et al. ),  (Li et al. ), 9 (Zheng et al. ),  (Zhang et al. ), and  (Wu et al. ), have their corresponding  genes in rice. However, there are no known  genes in rice for  (Kang et al. ) and  (Sweigard et al. ). There are many scenarios that describe the specificity of recognition between resistance proteins and avirulence proteins. For example, one  gene can recognize more than one  gene, an example of which is  that can recognize both  and  (Cesari et al. ), or it could also be that many  genes can recognize the same  gene, as in the case of -D that can be recognized by , and  (Yoshida et al. ). In another case, the presence of two  genes is required for the mediated resistance to rice blast:  and  from Tsuyuake (Ashikawa et al. ) and  and  from RIL260 (Lee et al. ).Genes conferring broad-spectrum resistance are necessary in breeding programs. The presence of even only one  gene can induce resistance to many isolates of  (Qu et al. ). Many broad-spectrum  genes have been validated such as  (Qu et al. ), ,  (Zhou et al. ), \n                         (Jeung et al. ), \n                         (Li et al. ),  (Lee et al. ),  (Hua et al. ),  (Das et al. ), \n                         (Liu et al. ), and  (Su et al. ). Unfortunately, the resistance controlled by most race-specific  genes is prone to collapse often in a few years after introduction to the rice field due to the quick adaptation of rice blast pathogen (Kiyosawa ; Fukuoka et al. ).Many strategies to keep the durability of resistance have been suggested, including multi-varietal planting, pyramiding of major  genes, and using partial resistance genes (Khush and Jena ). The use of partial resistance genes is one way to maintain the resistance of  gene(s) in the field. Partial resistance genes such as  (Zenbayashi-Sawata et al. ),  (Fukuoka et al. ), and  (Fukuoka et al. ) have been reported. The pyramiding of partial resistance genes for enhancing the durability of resistance has been reportedly used in several breeding programs (Yasuda et al. ).Jao Hom Nin (JHN) is a Thai rice variety which shows broad-spectrum resistance against rice blast in Thailand. Only two out of 346 blast isolates collected from all the rice growing areas in Thailand can overcome the resistance of JHN (Sreewongchai ). Moreover, seven out of 124 blast isolates collected worldwide can overcome the resistance of JHN, two of the isolates are from Colombia while five are from China (Sreewongchai ). Two Quantitative Trait Loci (QTLs) which are associated with rice blast resistance were mapped on chromosomes 1 and 11 of JHN (Noenplab et al. ). The major QTL () that controls complete resistance was mapped on chromosome 11 by the SSR markers RM144 and RM224. The minor QTL () which confers partial resistance was mapped on chromosome 1 by the flanking markers RM319 and RM 212 (Noenplab et al. ).  is located on the  locus which is a gene family locus. Interestingly,  is located on the  locus which is composed of  (Lin et al. ),  (Takahashi et al. ),  (Fukuoka et al. ), and  (Ma et al. ). In Thailand, JHN is used as a rice blast resistant donor in breeding programs. Introgressions of  and  from JHN via marker-assisted selection (MAS) into susceptible cultivars such as KDML105 (Noenplab et al. ) and RD6 (Wongsaprom et al. ) were successfully accomplished. In this study, the resistance gene/s in both  and  of JHN, which confer broad-spectrum resistance against rice blast populations from Thailand and the Philippines, were cloned and characterized and their disease spectrum was validated. Revealing the JHN rice blast resistance genes discloses the powerful resistance gene combination for resistance and durability to rice blast fungus populations in Thailand, the Philippines, and worldwide.The most efficient way to control rice blast disease is the utilization of resistant varieties. It is also low-cost and environment-friendly (Manandhar et al. ). The JHN rice variety showed broad-spectrum resistance against Thai and worldwide blast isolates (Sreewongchai ). The resistance of JHN was documented in this study by testing it against 132 Philippine blast isolates and found to be effective to most isolates, indicating that it is also quite promising in resistance against rice blast in the Philippines. The dissection of mechanisms underlying the broad-spectrum resistance of JHN against diverse sets of rice blast isolates worldwide is vital for the utilization of JHN-embedded  genes in varietal improvement against rice blast via marker aided selection approach. Noenplab et al. ()) documented that two QTLs, namely  on chromosome 1 and  on chromosome 11, were responsible for the broad-spectrum resistance of JHN. In this study, we employed a multifaceted approach to further dissect the molecular mechanisms of both  and . The genomic interval spanning  and  were first delimited within respective regions by flanking markers. Sequence analyses revealed that the  and  loci were present in the delimited genomic intervals of  and , respectively. Cluster analysis of the resistance reactions of derived  and  monogenic lines and IRBLs led to the finding of their association with  and , respectively. Finally, gene cloning and sequence analysis revealed that  and  encoded alleles of  and , respectively.The  gene was mapped on chromosome 1 by an analysis of QTL due to its moderate resistance and isolated by extensive characterization of retrotransposon-tagged suppressive mutants (Araki et al. ; Takahashi et al. ). It is located in a gene cluster with a tandem array of four NBS-LRR genes (RGA1 through RGA4) in Nipponbare. It was found that RGA4 rather than RGA1 through RGA3 was responsible for  (Takahashi et al. ). Significant sequence similarity among different homologues suggests that they were likely derived from successive rounds of duplication from a common progenitor (Takahashi et al. ). In addition to , three other functional  genes have been molecularly characterized, namely , , and  corresponding to alleles of RGA2, RGA3, and RGA4 in respective resistant rice varieties (Lin et al. ; Ma et al. ; Fukuoka et al. ). Sequence comparison further revealed that multiple functional polymorphisms cumulatively resulted in the conversion from -mediated race specific resistance to -mediated non-race specific resistance (Fukuoka et al. ). Despite the nature of race-specific resistance,  was reported to be effective in many Southeast Asian countries including Cambodia, Vietnam, and the Philippines (Fukuta et al. ; Nguyen et al. ; Selisana et al. ).A total of six  genes at the  locus were characterized at the molecular level including  from Tsuyuake (Ashikawa et al. ),  from Kusabue (Zhai et al. ),  from K60 (Yuan et al. ),  from Kanto51 (Ashikawa et al. ),  from C101LAC (Hua et al. ), and  from K3 (Zhai et al. ). The functionality of these  alleles is required by the co-existence of two adjacent NBS-LRR genes at the locus. Different  alleles activate immunity to rice blast isolates by recognizing different haplotypes of  allele, demonstrating a classical arms race coevolution model (Kanzaki et al. ). Biochemical and structural biology assays revealed that the delicate binding between a heavy-metal associated domain (HMA) in Pikp-1 and AvrPik-D initiated the host immune response to rice blast pathogen (Maqbool et al. ). In this study, we demonstrated that JHN contained an allele of  in IRBL7-M sharing limited sequence differences in nucleotide but not in protein sequence. Resistance spectrum analysis of IRBLs containing different  alleles against isolates from Bohol clearly indicated that , , and  controlled resistance to the isolates containing  but not other  haplotypes (Selisana et al. ). Interestingly, a pathotype test of 2476 rice blast isolates collected from the north, northeast, and south of Thailand in 2007 against IRBLs revealed that IRBLk-ka () and other -allele containing IRBLs were resistant to most isolates (Mekwatanakarn et al. ). A similar result was obtained in other pathotype tests using 2293 and 1375 Thai rice blast isolates collected in 2011 and 2013, respectively (Mekwatanakarn et al. ; Mekwatanakarn et al. ). Moreover,  was reported to show high frequency of resistance against rice blast isolates in Cambodia (Fukuta et al. ) and Vietnam (Nguyen et al. ).In this study, we demonstrated that the resistance spectrum of JHN is attributable to the additive contribution from both  and , which underpinned the mechanism of broad-spectrum of resistance to rice blast in JHN. It is indeed that JHN has been widely used as an elite resistant donor in conventional and molecular breeding programs against rice blast disease in Thailand. These breeding programs aim to introgress  and  of JHN into commercial Thai rice varieties which are susceptible to rice blast. Examples of these varieties are RD6 (Wongsaprom et al. ), Khoa Dok Mali 105 (Nalampangnoenplab ), IR77955-24-75-284 (Kotchasatit ), Ban Tang (Kaewcheenchai et al. ), Jao Hawm Phitsanulok51 (Noenplab et al. ), and San Par Tong1 (Yajai and Ketsuwan ).In this study, the  genes which control the resistance of  and  in the JHN rice variety were identified. The  in  showed partial resistance while the  gene in  showed complete resistance against Thai and Philippine rice blast isolates. The combination of the two broad spectrum blast resistance genes explains why JHN can still show a high level of resistance for a long period of time in Thailand. The results indicate that  and  in JHN are broad-spectrum resistance genes which are excellent candidate genes to be included in Thailand and the Philippines's rice breeding pipeline to maintain rice blast resistance."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0158-1", "title": "A hairy-leaf gene, ", "authors": ["Norimitsu Hamaoka", "Hideshi Yasui", "Yoshiyuki Yamagata", "Yoko Inoue", "Naruto Furuya", "Takuya Araki", "Osamu Ueno", "Atsushi Yoshimura"], "publication": "Rice", "publication_date": "12 May 2017", "abstract": "High water use efficiency is essential to water-saving cropping. Morphological traits that affect photosynthetic water use efficiency are not well known. We examined whether leaf hairiness improves photosynthetic water use efficiency in rice.", "full_text": "Rice is a major staple food around the world. Nearly half of the rice-growing area is rainfed uplands or lowlands (Kato et al. ), but the productivity of those fields is generally lower than that of irrigated lowlands. Irrigated lowlands produce 75% of all rice grain, and water-limited areas produce 23% (Maclean et al. ). Therefore, effective use of water resources is an important target for sustainable rice cropping.In adapting to new environments, plants gain or lose ecological, morphological, physiological, and anatomical traits (Lande ; Nicotra et al. ). Many higher plant species have leaf hairs, called glandular or non-glandular trichomes (Zeng et al. ). These leaf hairs have important roles such as protection from herbivory by insects and infection by pathogens, reflecting excess radiation, and reducing water loss (Levin ; Symonds et al. ). Their functional role varies with plant species and growth environment. However, the function of leaf hairs in rice remains unknown.Various genes related to leaf morphological traits in rice, including narrow leaf (), rolled leaf (), drooping leaf (), dripping-wet leaf (), and glabrous leaf (), have been identified, and their modes of inheritance have been elucidated in mutant lines (Kinoshita ). DNA markers have facilitated the genetic mapping of quantitative trait loci (QTLs) and the map-based cloning of genes for morphological and physiological leaf traits (Yonemaru et al. ). The rice hairy-leaf genes,  and  (Nagao et al. ; Nagao and Takahashi ), also affect the hairiness of husks. Recently, a dominant gene for pubescence growth and development, , was fine-mapped on chromosome 6 (Zeng et al. ).In general, plant water use efficiency (WUE) is calculated as the amount of dry matter produced divided by the amount of water consumed by the plant during its growth (Condon et al. ), and photosynthetic WUE (WUE) is defined as the amount of carbon fixed in photosynthesis per unit of water transpired (Hamid et al. ; Lawson and Blatt ). Plant WUE and WUE have a close relationship in some crop species (Heitholt ; Peng and Krieg ). Leaf morphological traits commonly affect physiological traits such as CO gas exchange and transpiration (Wright et al. ). Leaf hairs might influence photosynthesis (Johnson ; Pfeiffer et al. ). Although the physiological effects of leaf hairs in rice have been reported (Wada ), it is not well known how leaf hairs relate to photosynthetic traits or water use at the leaf level.Here, we developed a chromosomal segment introgression line (IL) of the wild rice species  in the genetic background of  \u2018IR24\u2019 with a high leaf-hair density. Using genetic, physiological, and morphological analysis, we examined the effects of the leaf hairs on photosynthesis and WUE at the leaf level.We identified the  hairy-leaf gene from wild  is tightly linked to SSR marker  on chromosome 6, and appears to cause the elongation of macro-hairs. The hairy-leaf trait increased leaf surface temperature and WUE by restricting leaf transpiration. These traits may be useful in the development of rice cultivars adapted to water-saving cultivation systems."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0163-4", "title": "Genome-Based Identification of Heterotic Patterns in Rice", "authors": ["Ulrike Beukert", "Zuo Li", "Guozheng Liu", "Yusheng Zhao", "Nadhigade Ramachandra", "Vilson Mirdita", "Fabiano Pita", "Klaus Pillen", "Jochen Christoph Reif"], "publication": "Rice", "publication_date": "19 May 2017", "abstract": "Hybrid rice breeding facilitates to increase grain yield and yield stability. Long-term success of hybrid breeding depends on the recognition of high-yielding complementary heterotic patterns, which is lacking in crops like rice.", "full_text": "Rice belongs to the three leading food crops providing the majority of calories to feed the world (International Rice Research Institute ). The demand for rice is raising steadily due to changes in customer priorities and population growth (Khush ). Thus, rice production has to be increased. Unfortunately, the potential of expanding the cultivated area is limited (Khush ) and, therefore, rice production has to be increased by enhancing yields per area (Khush ). Nevertheless, worldwide selection gain in rice is only 1.0% per year and much lower than the required increase in yield potential of 2.4% (Ray et al. ). Thus, there exits the strong need to develop and implement improved breeding tools (Phillips ; Zhu et al. ). A promising approach to boost yield per area consists in hybrid breeding, which has been successfully applied in rice improvement for some target regions such as China (Khush ; Khush ).The advantages of hybrid versus line breeding are the exploitation of heterosis resulting in higher grain yield (Xu et al. ) and an enhanced yield stability (Longin et al. ). Furthermore, hybrid breeding simplifies to stack major dominant genes and results in substantial return on investments, which is required to refinance future breeding progress (Longin et al. ). Hybrid rice breeding programs were initiated in China in 1964 based on a cytoplasmic male sterility (CMS) system. The first hybrid rice variety was released in 1976 (Barclay ). In addition to CMS, photoperiod sensitive male sterility has been exploited to produce hybrid seeds on a large scale (Chen and Liu ; Li et al. ). Rice grain yield per area increased through hybrid breeding by approximately 40% during the past three decades (Zhu et al. ). As a consequence, hybrid varieties covered more than 50% of China\u2019s total rice growing area in 2003 (Cheng et al. ). Moreover, the success of hybrid rice in China encouraged embarking on a national program for the development of hybrid rice for India in 1989 (Viraktamath ).The optimum exploitation of heterosis requires that the available germplasm is structured into genetically diverse heterotic groups. A heterotic group refers to a collection of genotypes resulting in similar hybrid performance when crossed with individuals belonging to a complementary and genetically distinct germplasm group. A specific combination of two heterotic groups leading to high-yielding hybrids is defined as heterotic pattern (Melchinger and Gumber ). Hybrid breeding based on the concept of heterotic patterns leads to more pronounced variance of the breeding values in contrast to the variance of the dominance deviations, which enhances recurrent selection gain (Reif et al. ).Heterotic patterns have been established in the past either considering hybrid seed production traits such as seed yield of the female lines and high pollination capability of the male lines (Reif et al. ) or they have been developed empirically by testing combining ability among available germplasm (Fischer et al. ). The latter approach is afflicted by the large number of possible hybrid combinations among available elite inbred lines. To solve this challenge, Zhao et al. () developed a three-step approach to search for heterotic patterns: (1) The performance of all possible single-cross combinations is determined through genome-wide or metabolite-based hybrid predictions. (2) The predicted hybrid performances are used to identify superior heterotic patterns based on a simulated annealing algorithm. (3) The optimum size of the heterotic pattern is determined balancing the expected short- and long-term selection.The genetically distant subgroups of  and  have been suggested as a promising heterotic pattern for hybrid rice breeding (Cheng et al. ). Nevertheless, fertility barriers hamper so far the exploitation of this heterotic pattern (Ikekashi and Araki ; Liu et al. ). Several approaches have been used to reduce the sterility occurring when crossing  and  (Guo et al. ; Ouyang et al. ), but none of them has been successfully implemented into hybrid rice breeding programs. The search for heterotic groups within the major germplasm pools is limited to studies based on tropical  rice (Wang et al. ; Xie et al. ). Moreover, previous studies used molecular marker-based genetic distances as proxy of the heterosis or hybrid performance, which is unreliable for unrelated parental inbred lines (Melchinger ; Xu et al. ; Xu et al. ). Thus, other approaches exploiting for example genome-wide hybrid predictions are needed to recognize promising heterotic groups for hybrid rice breeding.Our study is based on genomic and phenotypic data of 1,960 rice hybrids adapted to the Indian sub-continent, which have been evaluated for grain yield in two to four locations. The hybrids were grouped based on grain size, shape and appearance into three market segments: Long grain (LS) segment with grain type of long slender, length of larger than 6\u00a0mm, length/breadth ratio of larger than 3; Medium (MM) grain segment with grain type of medium slender, length of less than 6\u00a0mm, length/breadth ratio of 2.5 to 3.0\u00a0mm; Short (SS) grain segment with grain type of short slender, length of less than 6\u00a0mm, length/breadth ratio of larger than 3. Our main goal was to evaluate the potential and limits of genome-based establishment of heterotic patterns in rice. In particular, the objectives were to (1) investigate the accuracy for genome-wide prediction of hybrid performance in rice, (2) evaluate the benefits of heterotic patterns identified with a simulated annealing algorithm, and (3) assess the optimal size of the heterotic patterns balancing short- and long-term selection gain.More than 50% of the total cultivated rice area in China is grown with hybrid varieties, which was enabled by substantial investments in rice research and breeding (Cheng et al. ). Encouraged by the success of hybrid rice in enhancing the rice production and productivity in China, India initiated a national program for the development and large scale adaption of hybrid rice in 1989 (Viraktamath ). Plant breeders in the government and private sectors have launched hybrid varieties for different states in India. These hybrids have a 10 to 44% higher yield compared to popular high-yielding line varieties (Wanjari et al. ). Thus, a good beginning has been made by ushering in to an era of hybrid rice breeding and production in India (Spielman et al. ). Long-term success of hybrid breeding depends on the establishment of complementary heterotic groups (Reif et al. ; Zhao et al. ). Nevertheless, heterotic groups in rice are not clearly defined (Xie et al. ). This encouraged us to assess the potential and limits of a recently developed three-step approach to search for high-yielding heterotic patterns using data of a commercial hybrid rice breeding program. The hybrid rice breeding program is centered on a CMS hybrid seed production strategy. Lines have been clustered based on their restoration ability into female and male groups. The molecular analyses revealed for two of the examined rice market segments that male and female pools were genetically divergent (Fig.\u00a0). Two major restorer genes  and  are known for the underlying CMS-WA cytoplasm (Zhang et al. ; Zhang et al. ). Thus, a conversion of female into male lines or vice versa can be realized but is sometimes hampered by modifier genes influencing the penetrance of the restorer genes. Despite this, we assumed in our study that the hybridization system is not restricting the grouping of lines.Hybrid breeding and the effective utilization of heterosis crucially depends on the identification of heterotic patterns. In this study, we have evaluated a three-step approach to detect heterotic patterns in rice using data of a commercial hybrid breeding program. Our findings revealed that hybrid rice breeding based on the identified heterotic patterns holds the potential to boost grain yield and represents an important step for the long-term success of hybrid rice breeding."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0161-6", "title": "Association Mapping of Yield and Yield-related Traits Under Reproductive Stage Drought Stress in Rice (", "authors": ["B. P. Mallikarjuna Swamy", "Noraziyah Abd Aziz Shamsudin", "Site Noorzuraini Abd Rahman", "Ramil Mauleon", "Wickneswari Ratnam", "Ma. Teressa Sta. Cruz", "Arvind Kumar"], "publication": "Rice", "publication_date": "18 May 2017", "abstract": "The identification and introgression of major-effect QTLs for grain yield under drought are some of the best and well-proven approaches for improving the drought tolerance of rice varieties. In the present study, we characterized Malaysian rice germplasm for yield and yield-related traits and identified significant trait marker associations by structured association mapping.", "full_text": "Rice is the primary food source for more than half of the world\u2019s population and contributes 30\u201350% of the daily caloric intake (Fairhust and Dobermann, ). Among different rice ecosystems, rainfed upland and rainfed lowland rice occupy 30% of total rice area but contribute only 21% of total rice production. Drought is one of the most severe climate-related risks for rice production in rainfed areas of Asia and Africa (Pandey, ). With limited options for expanding rice area and the existing plateau in the yield potential of irrigated rice, a further increase in rice production has to come from highly vulnerable, less productive drought-prone rainfed lowland and upland rice areas (Khush, ). These areas received much less attention during the Green Revolution and even now most of the varieties grown in these areas are ones that were developed for high-input irrigated conditions. These varieties are highly susceptible to the various abiotic and biotic stresses prevalent in low-input rainfed environments. Thus, there is an urgent need to develop climate-smart rice varieties with multiple abiotic and biotic stress tolerance, and with improved grain quality and high yield potential that are suitable for rainfed areas (Kamoshita et al. ; Lafitte et al. ).Rapid and precise exploitation of the abundant genetic diversity available within rice germplasm is highly critical to ensuring sustainable rice production and global food security in the ever-changing climatic conditions (McCouch et al. ; Voss-Fels and Snowdon ). The recent advances in rice biotechnological tools have been very helpful in unraveling the genetic basis of complex traits within rice germplasm to identify major genes/QTLs for use in rice breeding (Thomson et al. ; Swamy et al. ; Swamy and Kumar, ). For improving drought tolerance, several major-effect grain yield QTLs under drought have been identified and successfully used in marker-assisted breeding (MAB) (Swamy et al. ). But, most of the QTL studies were on biparental or multiparent populations, which are limited by the allelic diversity within the selected parents. In addition, population development is time-consuming and mapping resolution is low (Kumar et al. ; Pascual et al. ). Also, the drought QTLs identified from different studies and meta-analysis of  have clearly shown that only a few major-effect  have been consistently and repeatedly identified, indicating limited exploration of genetic resources to identify novel major-effect  (Swamy et al. ; Kumar et al. ).Marker-assisted pyramiding of  into elite rice varieties has shown that they have synergistic relationships in particular combinations and their effect varies with the genetic background. Introgression of four  into popular mega-variety IR64 and three  in the background of MR219 showed that  have a better effect in different combinations in different genetic backgrounds (Swamy et al. ; Noraziyah et al. ). This further emphasizes the urgent need to identify novel  to improve the drought tolerance of a wide range of drought-susceptible rice varieties.The availability of genome-wide molecular markers, cheaper genotyping services and advances in statistical analysis have made it possible to explore natural populations to identify significant marker and trait associations, also popularly called genome-wide association studies (GWAS) (Korte and Farlow, ). Considering the huge genetic diversity available for multiple traits within rice germplasm, GWAS can be a feasible approach to simultaneously map loci for many traits and the improved mapping resolution helps in precisely identifying the genes/SNPs associated with the traits. There are several successful examples of accurate QTL/gene detection using GWAS in rice and other crop species (Huang et al. ; Han and Huang, ; Wu et al. ; Yang et al. ; Kumar et al. ), but there are only a few association studies for drought-related traits in rice (Vasant ; Courtois et al. ; Vannirajan et al. ; Muthukumar et al. ).The present study was undertaken with the objectives of screening Malaysian rice germplasm for drought tolerance, determining the population structure, doing association mapping of yield and yield-related traits under drought stress and non-stress (NS) conditions, and carrying out reference genome-based analysis of  physical regions.The adverse effects of climate change such as drought and heat are becoming a major threat to sustainable rice production and productivity (IPCC et al., ). In recent years, El Ni\u00f1o-induced drought has become a common phenomenon across many countries of Asia and there is already a clear prediction that El Ni\u00f1o-induced drought will have a significant effect on overall rice stocks in 2016 (Mohanty, ). In order to mitigate drought, there have been increased efforts to breed drought-tolerant rice varieties by both conventional and molecular breeding approaches (Kumar et al. ). Several major-effect QTLs for GY under drought have been identified for both upland and lowland conditions (Kumar et al. ). These QTLs have been successfully used in marker-assisted backcross (MABC) breeding programs to improve the drought tolerance of widely adopted popular but drought-susceptible rice varieties such as IR64, Swarna, MR219 and MRQ74 (Swamy et al. ; Swamy et al. ; Noraziyah et al. ; Noraziyah et al. ). However,  have different synergistic effects in different genetic backgrounds and meta-analysis of the major  has shown that few hotspot QTL regions are repeatedly being identified. To gain an economic yield advantage of at least 500\u00a0kg\u00a0ha, a minimum of two to three  have to be pyramided in elite genetic backgrounds (Swamy and Kumar, ); thus, exploration of genetic resources and the identification of several new  by association mapping is one of the most promising approaches.Association analysis using germplasm collections has been successfully used to identify several major loci for different traits (Han and Huang ; Wu et al. ; Yang et al. ), but only a few association studies exist for drought-related traits in rice (Vasant, 2012; Courtois et al. ; Vannirajan et al. ; Muthukumar et al. ). There is thus large scope to explore germplasm through association mapping to identify loci linked to drought tolerance.Drought is a severe abiotic stress affecting rice production. The identification and introgression of major-effect QTLs are one of the best and proven approaches to improving the drought tolerance of rice varieties. The accuracy and consistency of QTLs have clearly shown that structured association mapping with genome-wide molecular markers is an attractive option to identify major-effect QTLs for GY under drought stress. Some of the new  identified in this study are useful for MAB in combination with other major  The  analysis of QTL regions revealed that several drought-responsive genes were associated with the grain yield under drought."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0157-2", "title": "Rice Bran Metabolome Contains Amino Acids, Vitamins & Cofactors, and Phytochemicals with Medicinal and Nutritional Properties", "authors": ["Iman Zarei", "Dustin G. Brown", "Nora Jean Nealon", "Elizabeth P. Ryan"], "publication": "Rice", "publication_date": "2 June 2017", "abstract": "Rice bran is a functional food that has shown protection against major chronic diseases (e.g. obesity, diabetes, cardiovascular disease and cancer) in animals and humans, and these health effects have been associated with the presence of bioactive phytochemicals. Food metabolomics uses multiple chromatography and mass spectrometry platforms to detect and identify a diverse range of small molecules with high sensitivity and precision, and has not been completed for rice bran.", "full_text": "Rice ( L.) is an essential staple food for more than half of the world\u2019s population (Hu et al. ; Qian et al. ) and is grown in more than 100 countries worldwide (Muthayya et al. ). Rice bran, the outer covering of the rice grain, contains a unique profile of phytochemicals with medicinal and nutritional properties that are beneficial to human health, some of which have been targeted for nutraceutical development for cancer (Henderson et al. ; Verschoyle et al. ), type 2 diabetes (Cheng et al. ; de Munter et al. ; Qureshi et al. ), lipid metabolism regulation (Kuriyan et al. ; Qureshi et al. ; Shibata et al. ; Wang et al. ), immune regulatory processes (Wang et al. ), and obesity (Ham et al. ). Furthermore, we recently showed whole rice bran can protect against enteric pathogens such as  Typhimurium, human rotavirus, and human norovirus (Goodyear et al. ; Kumar et al. ; Lei et al. ; Yang et al. ). Rice bran contains non-saponifiable lipids (i.e. gamma oryzanol), vitamin E (e.g. tocopherols and tocotriols), polyphenols (e.g. ferulic acid caffeic acid and salicylic acid), and phytosterols (e.g. beta-sitosterol) (Henderson et al. ) with reported health properties. Many of these compounds are available in the lipid fraction and also known as rice bran oil (Charoonratana et al. ; Iqbal et al. ; V. Panala, ). Additional compounds from other chemical classes in rice bran merit attention and can be identified via high throughput techniques, such as global, non-targeted metabolomics that can assess a large profile of small molecules present in the whole food. Given the emphasis in previous studies on rice bran lipids (Forster et al. ), this analysis focused on rice bran amino acids, cofactors & vitamins, and secondary metabolites that have medicinal and nutritional properties important to human health.Despite the large body of scientific evidence on rice bran bioactivity, rice bran remains underutilized in human health and nutrition because it is considered an animal feed and is known to undergo hydrolytic rancidity after processing from whole grain rice (da Silva et al. ; Ramezanzadeh et al. , ). Thermal treatments applied to rice bran have helped to stabilize it and prevents rancidity by the inactivation of lipases and peroxidases. One major obstacle to achieving widespread human consumption and acceptance of rice bran is the global perception that rice bran is an animal feed (Ramezanzadeh et al. ).Food metabolomics, or \u201cFoodomics\u201d, provides information on the presence and relative abundance of all compounds in a food matrix. Food metabolome studies have shown compounds across diverse chemical classes such as amino acids, lipids, sugars, peptides, organic acids, phenolic compounds and other secondary metabolites (Wishart, ). Entire metabolite profiles have been completed on several foods including cooked and uncooked rice grain (Heuberger et al. ; Hu et al. ; Kim et al. , ), grape ( L.) (Luca Narduzzi, ), human milk (Andreas et al. ; Wu et al. ), tomato (Moco et al. ), citrus juice (Arbona et al. ), and several other foods and crops (e.g., carrot, beer, wine, and coffee) (Johanningsmeier et al. ) through non-targeted screening methods. Nutritional metabolomics is an experimental approach that uses small molecule profiling to integrate the effects of diet on nutrition, and thus can be used to evaluate the health effects of foods at an individual level (Jones et al. ). Integrating food and nutritional metabolomic approaches can increase our knowledge on the bioactivity of food metabolites, and may increase evidence for metabolic mechanisms by which foods elicit important health effects (Capozzi and Bordoni, ; Herrero et al. ). Accurate food metabolite profiles in regards to food and nutritional metabolomics may also assist in the quantification of dietary intakes and specific food biomarkers.The goal of the food metabolome approach applied herein was to obtain a complete characterization of the rice bran small molecule profile for bioactive components. This study used non-targeted metabolomics to investigate heat-stabilized rice bran from three U.S. rice cultivars for the identification of metabolites with medicinal and nutritional properties. These varieties were chosen for profiling based on human consumption in clinical trials, whereby rice bran intake improved intestinal health parameters by modulating gastrointestinal microbiota and host immunity (Borresen et al. ; Sheflin et al. ; Yang et al. ). The hypothesis was that rice bran contains a distinct stoichiometry of small molecules, covering multiple classes of phytochemicals, including but not limited to amino acids, cofactors & vitamins, and secondary metabolites that have medicinal properties and contribute to the nutritional benefits of rice bran as a whole food. A thorough examination of metabolites across chemical classes revealed a complex network of metabolic pathways that have not been previously examined for rice bran. A detailed analysis of rice bran functional food components allowed for a thorough understanding of how a suite of metabolites in a single food can exhibit therapeutic and preventive medicine properties.The rice bran metabolome analysis herein focused on amino acid, cofactor & vitamin, and secondary metabolite compounds that exhibited medicinal and nutritional properties with an emphasis on chronic and infectious disease control and prevention. The three selected classes of metabolites represented ~46% of total rice bran metabolite profile. Metabolites were described as antioxidative and anti-inflammatory (35 metabolites), antimicrobial (15 metabolites), anti-hypertensive (12 metabolites), cancer chemopreventive (11 compounds). anti-hyperlipidemic (8 metabolites), anti-hyperglycemic (6 compounds), and anti-obesogenic (2 compounds).Antioxidants represented a broad class of compounds available from many different foodstuffs (Carlsen et al. ). Rice bran is a promising candidate for dietary supplementation and nutritional therapy for prevention of chronic and infectious disease via its antioxidant composition. A majority of the rice bran antioxidants (e.g. 4-guanidinobutanoate and taurine from amino acids, tocopherols and tocotrienols from cofactors & vitamins, and ergothioneine and quinate from secondarymetabolites) work through different mechanisms to combat lipid peroxidation, DNA damage, protein modification, and enzyme inactivation caused by free radicals, in particular reactive oxygen species (ROS) (L\u00fc et al. ; Nimse and Pal, ). Oxidative stress caused by free radicals damages host cells and may initiate early stage development of chronic diseases such as cancer, heart disease, Alzheimer's disease, arthritis, cataracts, diabetes, and kidney disease (Morales-Gonz\u00e1lez, ). Antioxidants from rice bran can safely interact with and detoxify free radicals to stop the chain of damaging reactions for disease prevention (Iqbal et al. ; Jun et al. ; Parrado et al. ; Parrado et al. ). For example, quinate, an antioxidant that is naturally synthesized in plants and microorganisms is now described from rice bran via metabolomics (Fig.\u00a0). It was shown that consumption of 3000 milligrams of quinic acid ammonium chelate per day can regulate activation of NF-kB (nuclear factor kappa-light-chain-enhancer of activated B cells) and enhances DNA repair by increasing serum thiol levels (Pero et al. ). Rice bran derived quinate merits further evaluation for similar antioxidant activities.The antimicrobial activity of dietary rice bran can be attributed to at least 15 metabolites across amino acids and secondary metabolites. Understanding the relative contribution of rice bran compounds and the mechanisms of antimicrobial action could be helpful in combating emerging and existing problems associated with resistance to antibiotics. Hence, treatment strategies using natural food molecules from rice bran may prevent progression of infection and associated symptoms as a sustainable, globally available long-term solution (Cowan, ; Kondo et al. ; Srivastava et al. ). For instance, luteolin is a rice bran flavonoid (shown in Table\u00a0) that reduced the growth of a variety of gram-positive bacteria and yeast (Singh et al. ; Srivastava et al. ). We, and others, have previously shown that dietary rice bran has antimicrobial activity in animals and on isolated bacterial strains (Goodyear et al. ; Irfan A Ghazi et al. ; Kim et al. ; Kondo et al. ; Kumar et al. ; Nealon\u00a0et al. ; Yang et al. ; Yang et al. ). Our study revealed two newly identified rice bran amino acids (out of 15) and three newly identified rice bran secondary metabolites with antimicrobial properties; Phenyllactic acid and \u03b1-hydroxyisocaproic acid (leucic acid) from the amino acid metabolic pathway, and 4-hydroxybenzoate, alpha-amyrin, and tartaric acid from the secondary metabolite metabolic pathway. Phenyllactic acid is found in many bacteria as a metabolic byproduct (e.g.  spp.) but not previously identified in any plant sources (Valerio et al. ). Leucic acid has been identified in fermented foods, including certain cheeses, wines, and soy sauce (Mero et al. ). 4-hydroxybenzoate has been previously found in pistachio hulls (Barreca et al. ). Alpha-amyrin is found in Carissa carandas (karanda fruit) (Akansha Singh, ). Additionally, tartaric acid has been found in Hibiscus sabdariffa flower (Da-Costa-Rocha et al. ). Our metabolomics analysis results suggest that nutritional therapy through rice bran's multi-faceted antimicrobial actions merits testing in medical clinical applications to mitigate microbial resistance.Rice bran merits attention for being of considerably high nutritional value. These metabolome analyses confirm that it is a rich source of proteins, fats, minerals and micronutrients, such as B vitamins and trace elements. For example, at 12\u2009\u2212\u200915% protein content and with protein digestibility that is comparable to casein, the macro-nutritional value of rice bran which also contains healthy fats and fibers warrants greater pubic health attention (Saunders, ; Wang et al. ). Rice bran is also a rich source of B-complex vitamins, particularly thiamine and nicotinic acid, riboflavin and vitamin B. A single serving of rice bran (28 grams in accordance to USDA) delivers more than half of the daily nutritional requirements for thiamine, niacin and vitamin B6 (based on a 2,000 calorie reference diet) (; United States Department of Agriculture, ). Vitamins cannot be synthesized by the body and must be ingested, as such inadequate intake or subtle deficiencies in vitamins are risk factors for multiple chronic diseases (Fairfield and Fletcher, ). Recent evidence showed intake levels of thiamin, niacin, vitamin B, total folate, and alpha-tocopherol was improved in colorectal cancer survivors consuming rice bran and suggests that foods with multiple bioactive components and nutrients can play a pivotal role in the prevention of chronic diseases such as cancer and cardiovascular disease (Borresen et al. ; Borresen EC, ).A major strength of the non-targeted metabolomics approach herein was the identification of novel compounds from rice bran with medicinal properties (Fig.\u00a0). The limitations of non-targeted metabolomics in dietary exposure biomarker discovery platforms arise from metabolite concentrations that can vary across cultivars, and inconsistencies in extraction methods or instrument detection limits. Additional limitations for results interpretations from this study involve the limited information for bioavailability of rice bran compounds. The biological properties for rice bran will be dependent on host bioavailability and bioaccessiblity following ingestion, and thus this rice bran food metabolome investigation will assist to identify rice bran exposure biomarkers of intake in people. The variation in gut microbiota composition is another major factor that can influence bioavailability of food metabolites as well as the biological activities (Conlon and Bird, ; Krajmalnik-Brown et al. ). This is the first non-targeted whole food metabolome study of rice bran with an investigative focus towards the suites of amino acids, cofactors & vitamins, and secondary metabolites. Additional metabolic pathways and chemical classes of metabolites from this analysis (listed in Additional file : Table S1) merit continued investigation for medicinal properties and nutritional value.This study identified approximately 453 metabolites from the rice bran metabolome, many of which are described herein as cofactors & vitamins, amino acids and secondary metabolites. These metabolic pathways, among others found in rice bran, have shown positive health effects in animals and humans. The wide range of phytochemicals found in rice bran are likely working synergystically to contribute to rice bran\u2019s functional food properties. The ability of rice bran to fight both infectious and chronic diseases may be in part due to synergistic combinations of phytochemicals, and alongside metabolism by the gut microbiota (Borresen et al. ; Sheflin et al. ; Sheflin et al. ). Rice bran biochemical composition merits further investigation for multiple nutritional therapies and medical food applications."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0166-1", "title": "Overexpression of Rice Auxilin-Like Protein, XB21, Induces Necrotic Lesions, up-Regulates Endocytosis-Related Genes, and Confers Enhanced Resistance to ", "authors": ["Chang-Jin Park", "Tong Wei", "Rita Sharma", "Pamela C. Ronald"], "publication": "Rice", "publication_date": "2 June 2017", "abstract": "The rice immune receptor XA21 confers resistance to the bacterial pathogen, ", "full_text": "Protein folding, unfolding, and turnover is central to cell function and is regulated by molecular chaperones, such as heat shock proteins (Hsps) (Saibil ). Dysregulation of protein folding can cause cell death and other protein misfolding diseases. Among the major families of HSPs, heat shock cognate protein 70s (Hsc70s) play important roles in protein folding, disaggregation, and transportation. Hsp40, which acts as a co-chaperone of these processes, binds to Hsc70 and stimulates the Hsc70 ATPase activity to stabilize the interaction with substrates (Kampinga and Craig ). Hsp40 carries a 70 amino acid signature region, called the J domain, thus Hsp40s are also called J proteins.J-proteins are classified into three types based on the domain arrangement (Cheetham and Caplan ). Type I J-proteins contain an N-terminal J-domain, a glycine-rich region, a zinc-finger domain, and a C-terminal domain. Type II J-proteins are like type I J-proteins except that the zinc-finger domain is missing. Type III J-proteins contain the characteristic J-domain but are structurally divergent outside the J-domain (Koutras and Braun ). Type I and II J-proteins are functionally similar in substrates binding, whereas type III J-proteins have distinct roles in stimulating Hsc70 activity due to the flexibility in the structures. One class of type III J-proteins, auxilin and auxilin-like protein (ALP), plays an important role in clathrin-mediated endocytosis. During this process, vesicles are coated by clathrin, a critical component for endocytosis. The clathrin-coated vesicles then transport receptor proteins and their ligands from the cell surface and the trans-Golgi network to the endosomal system (Ungewickell et al. ). The J-domain of auxilin recruits Hsc70 to newly budded clathrin-coated vesicles and the Hsc70 and auxilin cooperatively remove the clathrin coat uncoating the vesicles (Kampinga and Craig ; Lemmon ).Plant genomes contain more J-proteins than animals. For example, the  and rice genomes contains 120 and 125 potential J-proteins respectively, whereas the human genome contains only 41\u00a0J-proteins (Sarkar et al. ; Rajan and D'Silva ). , rice and human encode 92, 83 and 23, type III J-proteins, respectively (Sarkar et al. ; Rajan and D'Silva ; Qiu et al. ), which are the most abundant type of J-proteins. Recent studies of several plant J-proteins suggest that they function in both plant development and the immune response (examples in Sarkar et al. ). However, most of the J-proteins have not been characterized and the mechanism of action is unknown.The rice XA21 immune receptor is representative of a large class of receptor kinases involved in plant innate immunity (Pruitt et al. ; Schwessinger and Ronald ; Song et al. ; Dardick and Ronald ). A tyrosine-sulfated peptide from , called RaxX (required for activation of XA21-mediated immunity X), is recognized by XA21 and triggers XA21-mediated immune responses (Pruitt et al. ). To elucidate the mechanism of XA21-mediated resistance, we previously performed a yeast two-hybrid screening for XA21 interaction partners using a rice cDNA library (Park et al. ;\u00a0Seo et al. ). We previously reported the characterization of four of these XA21 binding proteins including a WRKY transcription factor (Peng et al. ), a ubiquitin ligase (Wang et al. ), an ATPase (Chen et al. ) and a protein phosphatase 2C (Park et al. ).Here we report the characterization of XA21 binding protein 21 (XB21), predicted to encode a type III J-protein. To determine the in vivo biological function of XB21, we carried out biochemical and transgenic analysis. Co-immunoprecipitation indicates that XB21 directly interacts with the XA21 immune receptor in vivo. Overexpression of  in rice confers enhanced resistance to  sometimes accompanied with a cell death phenotype. RNA sequencing (RNAseq) analysis of XA21 transgenic plants overexpressing  indicates that genes related to \u2018vesicle-mediated transport\u2019 (which includes genes controlling clathrin-mediated endocytosis) and \u2018cell death\u2019 are significantly up-regulated. Taken together, these results indicate that XB21 functions in plant immunity and cell death regulation and suggest that XB21 functions as an auxilin.Here, we characterize a novel XA21-binding protein that carries motifs typical of type III J-protein and is involved in cell death and resistance to .XB21 was originally isolated as an XA21 binding protein in yeast. Here we demonstrate an in vivo interaction between XB21 and XA21 using co-immunoprecipitation assays in rice plants. RNAseq analysis of -overexpressing rice plants revealed up-regulation of genes related to diverse biological processes including those regulating \u2018vesicle-mediated transport\u2019 and \u2018cell death\u2019. These results together with the predicted domain structure of XB21 support the hypothesis that XB21 functions as an auxilin."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0164-3", "title": "Evolutionary relationships and expression analysis of EUL domain proteins in rice (", "authors": ["Kristof De Schutter", "Mariya Tsaneva", "Shubhada R. Kulkarni", "Pierre Roug\u00e9", "Klaas Vandepoele", "Els J. M. Van Damme"], "publication": "Rice", "publication_date": "30 May 2017", "abstract": "Lectins, defined as \u2018Proteins that can recognize and bind specific carbohydrate structures\u2019, are widespread among all kingdoms of life and play an important role in various biological processes in the cell. Most plant lectins are involved in stress signaling and/or defense. The family of ", "full_text": "Proteins of non-immune origin with at least one non-catalytic domain that can recognize and reversibly bind to specific carbohydrate structures are referred to as \u2018lectins\u2019. The occurrence of carbohydrate-binding proteins in all kingdoms of life illustrates their importance. Indeed, the specific interaction between lectins and their corresponding carbohydrate partner(s), either occurring as a free ligand or as part of a glycoconjugate, mediates a multitude of biological processes. These interactions can relay cellular signaling and are of utmost importance for defense reactions, stress signaling, growth and development.In plants, lectins are divided into 12 families of structurally and evolutionary related proteins based on the presence of a conserved carbohydrate-recognition domain (Van Damme et al. ). Like animals, plants can experience several forms of stress due to e.g. environmental conditions or biological agents. Because of their sessile lifestyle, plants cannot move away from these stresses, and therefore have developed a sophisticated system to recognize the different stressors and initiate a specific response to this stress. It was shown that carbohydrate-binding proteins play a pivotal role in plant defense (review De Schutter and Van Damme ).In addition to the structural classification, plant lectins can also be subdivided in 2 classes depending on their expression pattern. The first class groups all lectins that are constitutively expressed. These lectins are usually present at high concentrations in specific cells and organs (e.g. seeds and specialized vegetative tissues), suggesting a dual role as a storage protein involved in plant defense. The second class groups all lectins which are present at a low basal level but when plants are exposed to biotic or abiotic stresses, the expression of these lectins is significantly upregulated. At present, the expression of at least 6 different lectin domains from plants has been shown to be stress regulated. Most of these stress related lectins locate to the cytoplasm and the nucleus of plant cells (Lannoo and Van Damme ).The EUL family, grouping all proteins that show homology to the  agglutinin (EEA) (Petryniak et al. ; Fouquaert et al. ), belongs to the group of stress related lectins. In contrast to many other lectin families that occur only in some plant families, the EUL family represents a group of nucleocytoplasmic proteins that are found throughout the plant kingdom, suggesting they fulfill an essential role in plants (Fouquaert et al. ). The genome of  harbours only one  gene, referred to as . The expression of this gene is upregulated after exposure to some plant hormones, drought and salt stress as well as  infection (Li et al. ; Van Hove et al. ). Furthermore, it was shown that changes in expression levels of the lectin are accompanied with altered levels of stress resistance. Overexpression of  enhanced drought resistance (Li et al. ) and plants showed less disease symptoms after  infection compared to wild type plants or plants with lower transcript levels of the lectin gene (Van Hove et al. ).\n                         L., or Asian cultivated rice, is one of the most important food crops and staple foods. More than half of the world population is dependent on rice, it has shaped their diet, culture and economics. Rice breeding and cultivation has given rise to the existence of several subspecies. The sticky, short-grained japonica rice is grown in dry fields at higher altitudes in temperate environments, whereas the non-sticky, long-grained indica rice grows mostly submerged in lowlands in tropical and subtropical environments. Besides these morphological and agronomical differences, both rice varieties show distinct physiological and biochemical features. Amongst other, these features translate in differences in stress resistance (Yang et al. ; Hu et al. ; Liu et al. ). Although the phenotypic differences between the two subspecies are well studied, the molecular mechanisms behind these traits are only poorly characterized. Since plant lectins play an important role in plant defense and stress signaling, we focus on the identification of lectins in rice.In this paper we identified the putative lectin genes encoded in the japonica rice genome. An in-depth molecular characterization and analysis of the transcription profiles of the EUL family was performed under abiotic stress conditions. The strong conservation of the EUL lectin domain and the stress regulated transcription profiles suggest that this family of lectins fulfills an important role in plant stress signaling.With changing environmental conditions, the ability of plants to react to different forms of stress (e.g. cold, drought and flooding) becomes of key importance for a sustainable future. Understanding the mechanisms governing plant stress responses and identification of the individual components involved, will shed light on the molecular mechanisms behind stress signaling which in turn can be applied to improve stress resistance (Jung et al. ). Accumulating evidence shows that protein-carbohydrate interactions are of vital importance for plant immunity, involving lectin-carbohydrate interactions at the cell surface as well as in the cytoplasmic compartment (De Schutter and Van Damme ; Lannoo and Van Damme ).Based on their sequence similarity to reference members for the different plant lectin families, we retrieved 325 putative lectin genes belonging to nine different lectin families from the genome of  subsp. japonica, indicating that the rice genome harbours a large set of putative lectin domains each characterized by a specific carbohydrate-recognition domain. Since experimental data points to the involvement of EULs in the plant stress response (Van Hove et al. ; Al Atalah et al. ; Fouquaert et al. ), the focus of this paper is on an in-depth molecular characterization of this lectin family in rice.Five EUL genes were identified in the genome of japonica rice. Orthologs of these 5 genes were also found in the indica genome (Additional file : Table S7). Since several databases provide evidence that these EUL genes are expressed, we assume that these five genes are functional. In addition, four pseudogenes have been reported (Fouquaert et al. ) for which no evidence of transcription was found. Although all four pseudogenes were retrieved from the japonica genome based on homology with the  lectin (Additional file : Table S7), only two of them were found to contain an EUL domain using interproscan. Similarly, 2 pseudogenes containing EUL domains were identified in the subspecies indica. Since there is no evidence for transcription of these pseudogenes, the sequences were not taken into account for further analysis.Analysis of the genomic structure of the rice EUL genes revealed a high degree of conservation. When plotting the intron-exon structure of the OsEUL genes, to the protein sequence, it is noted that exon boundaries are present at the same positions in the protein sequences. Furthermore, when analyzing the position of the different rice EULs in the genome, it is observed that OsEULD1A and OsEULD2 are linked by a tandem duplication, which can account for the high similarity in genomic structure. Interestingly, a high degree of similarity is also observed between the intron-exon structure of OsEULS3 and ArathEULS3, the only EUL gene in the  genome. Due to the similarity between the structure of ArathEULS3 and OsEULS3/D1A/D2, it is suggested that the S3 type represents the more ancestral structure.A search in the SNPSeek database comprising the data of the 3000 rice genome project, identified 46 SNPs in the rice EUL genes. Analysis of the SNPs that give rise to an amino acid change revealed clear preferences between indica and japonica subspecies for the SNPs in the D-type EUL genes. Further research is needed to elucidate whether the SNPs contribute to differences in stress tolerance of rice.In PFam (PF14200, Ricin-B lectin 2, 103 amino acids) and SSF (SSF30570, Ricin-B like lectins, 151 amino acids) the EUL domains are annotated as ricin-B like lectins based on the presence of the QxW motif typical for ricin-B domains. Compared to the shorter pFam domain, the annotation of the EUL domain by SSF (SSF50370) shows an extended domain of 151 amino acids including a second QxW motif. Since the EUL genes share no significant overall sequence similarity with any protein comprising a ricin-B domain (Fouquaert et al. ), lectins containing the EUL domain were grouped as a separate lectin family.Phylogenetic analysis of the EUL domains showed a close relationship between the EUL domains of the S-type EULs and the second domain of the D-type EULs. The percentage sequence identity between the S-type EUL domain and domain 1 of D-type EULs ranges between 68 and 76% (similarity between 80 and 86%) while the sequence identity between S-type EUL domains and domain 2 of D-type EULs ranges between 71 and 80% (similarity between 82 and 92%) (Additional file : Table S3). This higher sequence similarity between the S-type EULs and the second domain in D-type EULs is in agreement with the higher degree of conservation in the genomic structure of these domains.As pointed out by Fouquaert and Van Damme (), and documented by Agostino et al. (), the carbohydrate-binding domain of the  lectin exhibits a notable promiscuity in the accommodation of diverse carbohydrate structures. The spatial organization of the amino acid triad D-N-Q forming the carbohydrate-binding site of EULs, accounts for the promiscuity in the binding of simple sugars and oligosaccharides to the \u03b2-trefoil domains of  lectins. In this respect, docking experiments performed with mannose, \u03b1-1,2-dimannoside and LacNAc, resulted in the anchorage of the sugars to the carbohydrate-binding site of the \u03b2-trefoil domains through a very similar network of hydrogen bonds and stacking interactions. However, some discrepancies occurred according to the number and length/strength of hydrogen bonds anchoring the sugars to the carbohydrate-binding site. The docking experiments with OsEULS2 confirmed the previously reported substrate specificity for high-mannose N-glycans and lactosamine related structures (Al Atalah et al. ). However, for OsEULD1A a substrate specificity for galactose related sugars or galactose containing glycoproteins was reported whereas mannose did not inhibit the agglutination of rabbit erythrocytes caused by OsEULD1A.Previous promoter analysis performed using the PLACE database (Al Atalah et al. ) identified the presence of several TATA and CAAT boxes and identified three major classes of promoter elements, among which the light responsive elements, the ABA and GA responsive elements and the elements related to other (a)biotic stresses. There is some controversy about the significance of the TATA box for transcription initiation since analyses in  (Molina and Grotewold ) and rice (Civan and \u0160vec ) genomes revealed that only 29% of the Arabidopsis genes and 19% of the genes in  comprise a TATA box in their promoter region. The enrichment analysis identified one TATABOX1 motif in OsEULS2 promoter, but similar motifs were absent in other OsEUL promoter sequences. Identification of cis-regulatory elements revealed a high number of promoter elements putatively related with the abiotic stress responsiveness of the OsEUL genes. To discriminate between false positives and true biologically significant elements, coregulatory genes, evolutionary sequence conservation and information about open chromatin regions were integrated into the analysis. A large set of cis-regulatory elements retrieved from the OsEUL promoter sequences can be classified in the WRKY, basic leucine-zipper (bZIP) and NAC transcription factor families, known to be associated with abiotic stress responses (Banerjee and Roychoudhury ; Fang et al. ; Nijhawan et al. ). These data are in agreement with the GO enrichment analysis that yielded GO terms related to stress signaling and metabolism, response to abiotic stimuli and abiotic stress, cell signaling etc.Motifs associated with WRKY transcription factors are abundantly present within the promoter sequences of the OsEULS2 regulon. WRKY transcription factors have been implicated in different biological processes such as response to wounding, senescence, development, dormancy, cold and drought tolerance, metabolism and hormone signaling pathways. In addition, numerous WRKY genes are involved in response to biotic and abiotic stress (Berri et al. ). The five rice WRKY transcription factors with binding sites identified in the OsEULS2 promoter all show stress regulated expression profiles. The responsiveness of WRKY11, WRKY28 and WRKY62 to abiotic stress was shown in a microarray analysis (Jain et al. ), where WRKY11 was upregulated in drought stress, whereas WRKY28 and WRKY62 were responsive to salt stress. With the exception of WRKY11, these transcription factors were also upregulated under several biotic stresses, such as bacterial, viral and fungal infection (Marcel et al. ; Zhou et al. ; Berri et al. ).Within the promoter of OsEULS3, and its coregulated genes, a PALBOXAPC element was identified (Additional file : Table S4). This element was found to be significantly enriched in promoters of stress related genes that are downregulated by ABA (Yazaki and Kikuchi, ) and agrees with the downregulation of OsEULS3 in rice shoots treated with ABA (Fig.\u00a0).A predominant group of motifs in the OsEULD1B promoter are associated with transcription factors belonging to the bZIP family, these are known as stress inducible transcription factors (Banerjee and Roychoudhury ) involved in drought, osmotic, salt stress and some of them also in cold stress (Liu et al. ; Lu et al. ). Among them TRAB1 and OSBZ8 are reported as ABA related elements. While OSBZ8 expression is reported in the roots of ABA treated seedlings and in developing embryos, TRAB1 shows more general expression in different plant organs as well as in different stages of embryo development and in seeds (Banerjee and Roychoudhury ). Their responsiveness to dehydration and salt is confirmed in microarray data (Nijhawan et al. ) and in the case of OSBZ8 the upregulation is also associated with salt tolerance in indica rice cultivars (RoyChoudhury et al. ). Another group of transcription factors retrieved in the EULD1B promoter analysis belongs to the NAC transcription factor family, involved in different stress and developmental processes (Fang et al. ). Previously the NAC/NAM transcription factor LOC_Os07g37920 was identified as a stress inducible gene under water deficit conditions (Ray et al. ). Similarly Fang et al. () reported upregulation of this transcription factor in response to salt and drought stress. RNA sequencing data showed that the same transcription factor was involved in root development and auxin signaling (Hiltenbrand et al. ).Enhanced expression for OsEUL genes in plants subjected to abiotic stress was confirmed by proteomics data. Already in 1997 Moons et al. reported on the stress induced expression of one of the OsEULs after treatment of rice seedlings with ABA (Moons et al. ). More recent proteomic analyses (Cheng et al. ), qRT-PCR investigations (Al Atalah et al. ) have confirmed the differential transcription of several OsEUL genes upon ABA and salt stress. Expression of OsEULD1B was found to be upregulated upon drought stress (Rabello et al. ). EUL proteins in barley and wheat were also reported as drought related proteins. EULS3 and EULD1B genes are induced and upregulated at protein level in barley crown and roots from drought sensitive cultivars while the drought tolerant cultivar shows a constant protein level for EULD1B (V\u00edt\u00e1mv\u00e1s et al. ). Similarly, two proteins belonging to the EUL family, one of them classified as the EULD2 type are detected at higher protein levels in a drought tolerant wheat cultivar especially in drought treated plants compared to the control plants at 10\u00a0days after opening of the flowers (Jiang et al. ).Transcript levels for ArathEULS3, the EUL homolog from Arabidopsis were also significantly upregulated after ABA treatment (Li et al. ; Van Hove et al. ), MeJA treatment (Van Hove et al. ) and drought (Li et al. ). In addition, Arabidopsis plants overexpressing ArathEULS3 revealed a better tolerance to drought stress (Li et al. ) and  infection (Van Hove et al. ). All these findings support the statement that EUL genes are actively involved in the signal cascades triggered by several abiotic stresses.In contrast to ABA, JA causes a significant downregulation for most EULs in rice with the exception of OsEULS2 (shoots and roots) and OsEULD1B (mainly shoots). The information retrieved from TENOR is confirmed by proteomics studies involving some EULs (Cho et al. ). Moons et al. () also report the downregulation of OsEULD1B in roots after 24\u00a0h after JA treatment which is in agreement with the RNA sequencing data and in the same paper EULD1B is defined as not responsive to salicylic acid (SA) or ethylene.At present, there are only a few reports that show the involvement of OsEUL genes in response to pathogen attack. qRT-PCR experiments and microarray data revealed that transcript levels for some OsEULs are upregulated after infection of rice with  pv. oryzae (OsEULD1B, OsEULD2),  (OsEULD2) or root knot nematodes () (OsEULD2) (Al Atalah et al. ; Kyndt et al. ). In line with this, several WRKY transcription factors with binding sites identified in the OsEUL promoters are regulated by diverse biotic stresses. For example, WRKY71 for whom a binding site was identified in the OsEULS2 promoter was upregulated upon infection with  (Berri et al. ). Other WRKY transcription factors with binding sites in the OsEULS2 promoter were upregulated upon infection with  (Zhou et al. ) and  (Marcel et al. ).The EUL lectin family is unique in that representatives of this family are present throughout the plant kingdom and have been identified in every fully sequenced genome of land plants. However, differences are observed in the number of EUL genes and the type of EULs in different plant species. The analysis of 9 monocot and 8 dicot genomes revealed a higher number of putative EUL genes in monocots (ranging 5 to 8) compared to dicots (ranging 1 to 3). In addition, it is observed that dicot species only possess S-type EULs while S-type as well as D-type lectins are identified in monocots (Additional file : Table S4). One exception is  where only S-type EUL sequences were identified. Judging from 70 EUL domain sequences, representing 33\u00a0S-type and 19 D-type EULs, identified in all genomes under study, the conservation of the EUL sequences is high. A high degree of sequence conservation was observed not only for the D-N-Q triad and the aromatic residues F/L118-W136 but also for the complete EUL domain.In addition, the phylogenetic tree built from all EUL domains under study, revealed distinct clusters. Overall a good correlation was observed between the annotation of the EUL domains for the different species and the rice domain in this cluster. Reconciliation with the species tree provided insight in the three duplication events leading to the different S-type and D-type EULs. Interestingly the EUL domains from the dicot species are grouped in a separate branch and this branch clusters close to the cluster containing the EUL domains from  which, like the dicots, solely encodes the S-type lectins. Analysis of the reconciliated tree revealed that  diverged from the other monocots before the first duplication event that gave rise to the monocot S- and D-type EULs. This can explain why banana only contains S-type EUL lectins of the same (ancestral) type as the dicot species. In other monocot species the S-type EULs were subject to differentiation that gave rise to the S2-type and S3-type EULs.The strong conservation of the -related lectins within the plant kingdom, not only at protein level but also at genomic level suggests an important role for these proteins in the plant cell. The identification of multiple stress responsive elements in the promoter sequences of these genes as well as the transcriptional regulation of these genes in response to stress or hormonal treatments are in agreement with a role in plant stress signaling. However, further research, such as the generation of rice plants overexpressing OsEULs, is needed to experimentally prove the involvement of EULs in stress signaling and plant defense in rice."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0165-2", "title": "\n                     ", "authors": ["Hua Yuan", "\u2020", "Shijun Fan", "\u2020", "Juan Huang", "Shijie Zhan", "Shifu Wang", "Peng Gao", "Weilan Chen", "Bin Tu", "Bingtian Ma", "Yuping Wang", "Peng Qin", "Shigui Li"], "publication": "Rice", "publication_date": "25 May 2017", "abstract": "Both grain size and grain number are significant for rice yield. In the past decade, a number of genes related to grain size and grain number have been documented, however, the regulatory mechanisms underlying them remains ambiguous.", "full_text": "Grain size and grain number are the most important factors determining grain yield in rice. So far, a lot of genes related to grain size and number have been documented. Regarding grain size, multiple signaling pathways that influence grain size have been identified in rice (Li and Li , ; Zheng et al. ; Zuo and Li ), including the ubiquitination-mediated proteasomal degradation pathway, G-protein signaling pathway, the mitogen-activated protein kinase signaling pathway, transcriptional regulatory factors and phytohormone pathways. The ubiquitination-mediated proteasomal degradation pathway includes  (Song et al. ) and  (Shomura et al. ; Weng et al. ), and the G-protein signaling pathway includes  and  is a homolog of , the G protein \u03b3-subunit of Arabidopsis (Li et al. ), which encodes a putative transmembrane protein and functions as a negative regulator for grain size (Fan et al. ; Mao et al. ).  encodes a plant-specific G protein \u03b3 subunit and mutants in it have short grains (Huang et al. ). The mitogen-activated protein kinase (MAPK) signaling pathway includes  and  interacts strongly with , indicating that the  cascade influences grain size in rice (Duan et al. ; Liu et al. ). The transcriptional regulatory factors pathway includes ,  and  and  encode the plant-specific transcription factors  and  (Si et al. ; Wang et al. ), respectively.  encodes a transcription factor Growth-Regulating Factor 4 (), which regulates grain length and width mainly by promoting cell expansion (Che et al. ; Duan et al. ; Hu et al. ; Li et al. ). Another important signaling pathway controlling grain size is phytohormone pathway, which has been proven to play a crucial role in rice development, especially in the course of grain growth. For example, , a putative serine carboxypeptidase, functions as a positive regulator of grain size (Li et al. ), and inhibits the interaction between  and , suggesting that  might regulate grain size through the brassinosteroids (BR) signaling pathway (Xu et al. ).  encodes a putative protein phosphatase with a Kelch-like repeat domain (), and may function as a negative regulator through BR signaling in rice (Hu et al. ; Qi et al. ; Zhang et al. ). Regarding grain number, many QTLs and genes have been cloned, such as  (Ashikari et al. ),  (Yan et al. ),  (Qiao et al. ),  (Wu et al. ) and  (Jin et al. ). However, as complex agronomic traits, the regulatory mechanism of grain size and number remains largely unknown.\n                        , as a -associated receptor kinase (), has been proven to participate in BR signal transduction and regulate plant architecture in rice (Li et al. ; Shimin et al. ), but no mutants have been identified so far, and there was no comprehensive investigation of  function on yield-related traits. In this study, we identified a mutant of  in the  background, named , which exhibited a decreased plant height, grain size, grain number and panicle length. Loss of function of  in the  background exhibited significantly decreased grain size and number, but less so than in the  background. Our results show that the reduced grain length of  was due to decreased cell proliferation.  was insensitive to BR, and  affected the expression of BR-related genes. Pyramiding analysis indicated that  regulate grain length independently of .\n                         plays an essential role in the regulation of grain size, grain number and plant height in both the  and  backgrounds, and is important for rice yield. The function of  on grain size and number is affected by genetic background.  regulates grain size by participating in cell proliferation, and is independent of . The glycine substituted in  is located in the kinase domain, is highly conserved among different species, and is presumably important for the molecular function of . Together, our work is helpful for unveiling the molecular function of , and indicating that  is a potential target to manipulate for increasing rice yield."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0168-z", "title": "Identification of ", "authors": ["Andrea Volante", "Alessandro Tondelli", "Maria Aragona", "Maria Teresa Valente", "Chiara Biselli", "Francesca Desiderio", "Paolo Bagnaresi", "Slavica Matic", "Maria Lodovica Gullino", "Alessandro Infantino", "Davide Spadaro", "Giampiero Val\u00e8"], "publication": "Rice", "publication_date": "8 June 2017", "abstract": "\n                           ", "full_text": "\n                         disease is one of the most serious and oldest problems affecting rice production, first described in 1828 in Japan (Ito and Kimura ) and currently identified in Europe, Asia, Africa, and North America (Ou ; Pra et al. ). In various rice growing countries, significant yield losses caused by the disease can range from 50% to more than 70% (Ou ; Rood ). Increasing  disease incidence has been reported in Italy (Amatulli et al. ) and major growing areas of Asia such as Pakistan, South Korea, Bangladesh, Northern India, and Taiwan (Khan et al. ; Park et al. ; Haq et al. ; Gupta et al. ; Chen et al. ).\n                         is caused by one or more seed-borne  species, mainly  (Wulff et al. ), and the disease may infect rice plants from the pre-emergence stage to the mature stage, with severe infection of rice seeds resulting in poor germination or withering (Iqbal et al. ).  belongs to hemibiotrophs , whose initial infection relies on a living host (biotrophic), and progressive infection involves a consumption and destruction of the host cells (necrotroph; Ma et al. ). Seeds contaminated with the  provide initial  for primary infection. Under favorable environmental conditions, infected plants have the capacity to produce numerous  that subsequently infect proximate healthy panicles through aerial conidial diffusion by wind, producing infected seeds (Ou ; Ora et al. ; Matic et al. ). During primary infection, mature rice plants are tall, frequently stunted, with an angle of leaf insertion wider than in healthy seedlings. Moreover, infected plants eventually die, while panicles on surviving plants do not develop any grains, thus resulting in yield loss (Desjardins et al. ; Mew and Gonzales ; Ou ).The altered plant morphology is due to the ability of  to produce and secrete gibberellic acids (GAs) (Bearder ; Ou ). Although GAs are considered as secondary metabolites (SMs) in  because they are not essential for fungal growth and development, they are thought to contribute to the virulence of , the only  species capable of GAs biosynthesis, by controlling jasmonic acid-responsive gene expression and jasmonic acid-mediated plant immune responses (Wiemann et al. ; Siciliano et al. ). GA production was also associated with fungicide sensitivity of different  isolates (Tateishi et al. ; Tateishi and Suga ).The most common management practices to limit  are based on thermal seed treatment (hot water immersion) or fungicides. The hot water immersion method (Hayasaka et al. ) was demonstrated ineffective on severely infected rice seeds, because thermal effect is not efficiently transmitted to the pericarp layers. Also seed dressing with fungicides has restricted efficiency in destroying the spores of the , owing their resistance to several fungicides (Iqbal et al. ; Park et al. ; Kim et al. ; Lee et al. ). Promising results have only recently been obtained through a combination of antagonistic yeasts and thermotherapy (Matic et al. ). However, the current incidence of  disease is increasing, leading to serious concerns in the main rice-producing areas worldwide (Wahid et al. ; Ma et al. ) and there is a strong request for alternative disease control measures, such as the identification of rice  resistant cultivars (cvs.). However, only a few accessions were reported to effective source of resistance to . An extensive search carried out on more than 400 rice accessions identified only one and 12 cvs. with high and moderate resistance, respectively (Li et al. ). Similarly, in other studies only a few resistant varieties were identified after application of different screening procedures (Lv ; Khokhar and Jaffrey ; Kim et al. ).Knowledge on mapped loci conferring resistance to  is very limited. Two QTLs for  resistance derived from the Chinese  cv. Chunjiang 06 were identified on chromosomes 1 and 10, explaining each one about 13% of phenotypic variation (Yang et al. ). Hur et al. (), using near-isogenic lines (NILs) derived from a cross between the highly resistant  variety Shingwang and the  susceptible variety Ilpum, identified a major QTL, named as , on the long arm of chromosome 1 explaining 65% of the phenotypic variation and not coincident with the QTL identified in Chunjiang 06. More recently, three QTLs were identified on chromosome 1 (Fiyaz et al. ). Two of them ( and ), detected on the short arm of chromosome 1, represent novel QTLs, while the third one () was mapped in coincidence with the  QTL described by Hur et al. ().A large genetic diversity of the pathogen population has been highlighted for strains isolated from Asia, Africa and Europe (Wulff et al. ; Jeon et al. ; Valente et al. ), therefore supporting the necessity of additional loci conferring rice resistance to . Whole-genome association mapping (GWAS) has recently demonstrated to offer better resolution than QTL mapping, thus reducing the QTL interval of confidence and, consequently, the number of candidate genes underlying individual QTLs (Huang et al. , ; Courtois et al. ). Linkage disequilibrium (LD) decay, which determines the expected resolution in the GWAS approach, has been reported to range from 500\u00a0kb in the temperate  rice background to 75\u00a0kb in the  background (Huang et al. ; Mather et al. ), even considering that in germplasm collection of more related temperate  rice accessions values of LD decay of 1250\u00a0kb were also observed (Biscarini et al. ).The main objective of the present study was the screening of a  rice germplasm collection for  resistance after artificial inoculation with a virulent isolate of , in order to map the genetic polymorphisms underlying rice resistance against this disease. Genome-wide association study allowed the identification and localization of two new QTLs conferring rice resistance to  disease in the rice  background.In this work, a panel of rice temperate and tropical  accessions from different origins, mainly from Italy (67), USA (28), Portugal (11) and Spain (10) was assembled in order to identify new loci associated with resistance to the rice  disease through a GWA mapping approach. The analysis of LD decay for each chromosome in this panel showed an average value of 1992 Kb, which is considerably higher than those commonly reported in the literature of about 150\u2013180\u00a0kb for  backgrounds (Mather et al. , Huang et al. , Courtois et al. ). However, higher LD values ranging from 600\u00a0kb up to 2\u00a0Mb were also observed in a number of cases for  and  rice (Xu et al. , Kumar et al. ); moreover, a germplasm collection of more related temperate  rice accessions recorded values of LD decay of 1250\u00a0kb (Biscarini et al. ). Since the panel used in this study represents a sub-group of the panel used in Biscarini et al. (), it is conceivable that the higher values of LD observed here are most likely due to a lower level of diversity among the varieties included in the present collection, suggesting that few historical recombination events occurred in this population. Moreover, SNP density applied in our study can contribute to the higher LD value estimated; in the present work a total of 31,752 SNPs were used to estimate LD (and for GWAS analysis) while Courtois et al. () used 16,664 markers (both SNPs and DArTs) and Mather et al. () used only 522 markers. LD estimates tend to be higher with denser SNP panels (Khatkar et al. , O\u2019Brien et al. ), and LD patterns tend to emerge clearly only at higher SNP densities (Bacciu et al. ). The resulting higher LD detected may eliminate true positives if in one region in LD more than one significant association is present, however considering both, the extent of LD decay observed and the expected average marker density (calculated as 0.09 SNP/Kbp in the whole population), we were confident that this panel represented an excellent resource for investigating  resistance in  rice.Screening of the GWAS panel allowed the identification of accessions with a low disease index (I*); even considering that the disease incidence among temperate and tropical  accessions was not statistically different, ten of the 12 more resistant accessions (i.e. those showing I* values <27) were identified within the tropical  background, raising the possibility that higher frequency of effective  resistance loci is present in tropical with respect to temperate . However, since no screenings for  resistance involving relevant numbers of accessions belonging to the different rice groups (temperate and tropical , , , ) have been carried out so far, these conclusions cannot be adequately supported. The GWAS analysis was therefore carried out using a restricted number of related sub-populations (temperate and tropical ). As previously observed, this approach from one side increases the possibility to detect associations for alleles that are segregating only in one or two populations while are fixed in others, but from the other side the resulting higher LD may eliminate true positives (Famoso et al. ; Zhao et al. ). However, the high frequency of the resistant phenotypes in the tropical  sub-population detected in this work, leveraged power to detect alleles that were segregating within this sub-population.To our knowledge, the present work represents the first report on the utilization of a GWAS approach for the identification of resistant loci effective against the  disease of rice. Two genomic regions were associated to  resistance and delimited to about 0.41\u00a0Mb on chromosome 1 (from position 628,091 to 1,040,823) and 0.59\u00a0Mb on chromosome 4 (from position 31,162,467\u00a0bp to 31,757,436\u00a0bp), and were named as  and , respectively. Different  resistance QTLs have been previously located on rice chromosome 1 (Fiyaz et al. ; Hur et al. ; Yang et al. ). Of these,  and  (Fiyaz et al. ) and  (Yang et al. ) were located on the short arm of chromosome 1. The comparison between our resistance-associated region on chromosome 1 and , identified in a 0.26\u00a0Mb region between RM10153 and RM5336 (from position 3,105,042 to 3,367,533; Fiyaz et al. ), demonstrated that  was located apart from . Similarly, the region associated to , ranging from RM10271 and RM35 (from position 4,657,288 to 8,411,302\u00a0bp; Fiyaz et al. ), and , spanning from RM7180 to RM486 (from position 34,105,454 and 34,956,597\u00a0bp; Yang et al. ) resulted different from . Moreover, previous studies (Ma et al. ) indicated that rice varieties with the  gene, a semi-dwarf gene resulting in defective 20-oxidase GA biosynthetic enzyme and localized from 38,382,382 to 38,385,504, are susceptible to  disease. The detected position for  indicates that this resistance  is not related to the  allele. All these observations therefore indicate that the QTL we have detected on chromosome 1 represents a novel unknown  involved in  resistance. Moreover, no -resistance loci have been previously mapped on chromosome 4, suggesting that, also in this case,  represents a new genomic region associated to  resistance.Alternative alleles for SNPs representing peak markers for the two resistance loci were identified, where the \u201cA\u201d and \u201cC\u201d alleles for  and  respectively, were associated to a lower  disease incidence. Noteworthy, when the 12 most resistant accessions (with I*\u00a0value <27) were analyzed, 11 of them carried the combination of the \u201cA\u201d and \u201cC\u201d alleles (for Adair, the C allele at  has been imputed from neighbor markers, data not shown), suggesting that pyramiding of the two loci should provide effective levels of resistance. Within these accessions, the tropical  sub-group was predominant (10 accessions out of 12); this result, together with the observation that tropical  have a lower level of average disease incidence, may indicate that higher breeding pressure for  resistance was applied in the tropical  than on the temperate sub-group. This observation is further supported by the higher frequency of the resistant allele (\u201cA\u201d) at the  peak marker observed in tropical  (41.5%) compared to temperate  (5.1%). Sequences corresponding to the peak markers for  and  are here provided (Additional file : Figure S5). These sequences can be used to develop SNP-based high-throughput markers to be used in marker-assisted selection for pyramiding the two resistant loci in  susceptible lines. Moreover, effective resistance loci were also identified in several different commodity classes, including round (Greppi), long A (Maioral and Bengal) and long B (Arsenal, Adair, King), an aspect that should facilitate the introgression of  resistance into rice lines addressing different market requirements.Several annotated genes encoding functions compatible with resistance were identified for both resistance loci genetic intervals. These included receptor-like kinases, such as LRK10 and LRK14, known to participate to wheat leaf rust resistance (Feuillet et al. ) for the  region, while for  a NB-LRR gene, receptor kinases and an ABC transporter were identified. A second approach for identification of candidates was based on the integration of mapping position and RNA-Seq data previously obtained in a comparative transcriptome analysis of resistant and susceptible rice cvs., Selenio and Dorella respectively, in response to  (Matic et al. ). DEGs located within or near the two QTLs regions were analyzed according to the criteria indicated in Methods. The analysis did not lead to identification of candidates for , as only one locus encoding for a protein of unknown function fitting the criteria was located on this QTL region. For the 31750955, a SHR5-receptor kinases was listed among the candidate genes using the combined DEGs and map position approaches. Finally, also a gene encoding for a sulphate transporter, transcribed at higher rates in Selenio during infection (Matic et al. ), was located in the  region. It is well known that sulphur increases resistance in different plant-fungal pathogen interactions, inducing the production of a number of sulphur compounds implicated in defense responses like glucosinolates, phytoalexins, HS, cysteine and glutathione (Walters and Bingham ). Thus, another possible  function might be related to S uptake and related production of S-resistance compounds.Overall, the  search for candidate genes, in the two QTL regions (qBK1_628091 and qBK4_31750955) identified in this work, highlighted several genes with functions associated to disease resistance that could represent candidates for  resistance. It should however be considered that these genes were identified on the Nipponbare genome and that, currently, the reaction of this rice cv. to  infection is not known. Additional investigations involving targeted resequencing of the two QTL regions in resistant and susceptible accessions here identified and the comparison of these regions with the available Nipponbare sequence are therefore required. To address the final identification of the genes responsible for  resistance we are developing high-resolution mapping populations for  and  through crossing accessions bearing only one of the two loci with highly susceptible accessions. These materials will allow a fine mapping of the two loci and a more detailed and precise assessment of the candidate genes here reported until the identification of the genes underlying the QTL involved in resistance.Screening of a  rice germplasm collection carried out with a virulent  isolate allowed the identification of accessions bearing relevant levels of resistance. The subsequent GWAS approach under stringent conditions identified two previously un-identified  resistance loci on the short arm of chromosome 1 and on the long arm of chromosome 4. Since high levels of phenotypic resistance to  was associated to the cumulated presence of the peak markers resistance alleles at the two loci, it is expected that they can have an additive effect that could be exploited also in resistance breeding. Candidate genes with a putative role in  resistance were identified in the two genomic regions highlighting several gene functions that could be involved in resistance opening the way for the functional characterization of the resistance loci."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0170-5", "title": "Comparative Expression Analysis of Rice and ", "authors": ["Yun-Shil Gho", "\u2020", "Sun-A Park", "\u2020", "Sung-Ruyl Kim", "\u2020", "Anil Kumar Nalini Chandran", "Gynheung An", "Ki-Hong Jung"], "publication": "Rice", "publication_date": "24 June 2017", "abstract": "Peroxiredoxins (PRXs) have recently been identified as plant antioxidants. Completion of various genome sequencing projects has provided genome-wide information about PRX genes in major plant species. Two of these -- ", "full_text": "Peroxiredoxins (PRXs) catalyze the decomposition of peroxides and function in diverse compartments to protect cells against damage from reactive oxygen species (ROS) (Dietz ). The PRX family contains thiol-dependent peroxidases that are evolutionarily widespread in bacteria, fungi, animals, cyanobacteria, and plants (Umate ; Dietz et al. ; Bhatt and Tripathi ). Completion of several genome sequencing projects has provided genome-wide information about PRX genes in major plant species. According to their primary structure, they have been classified into four functional subgroups: PrxQ, 1-CysPrx, 2-CysPrx, and Type-II Prx (Umate ). In , PrxQ helps determine tolerance to abiotic stresses such as cold and salt (Jing et al. ), 1-CysPrx is involved in delaying seed germination under abiotic stress (Haslekas et al. ), PrxIIE represses protein nitration and is active in pathogen defenses (Romero-Puertas et al. ), while PrxIIF regulates root growth in the presence of cadmium and salicylhydroxamic acid (Haslekas et al. ). Two model systems,  (rice) and  (herein ), have 10 and 11 PRX members, respectively, in their genomes (Umate ). Based on their subcellular localization, the PRXs in  comprise four subgroups: four PRXs (2-CysPrxA, 2-CysPrxB, PrxQ, and PrxIIE), localized in the chloroplast; a PrxIIF, in the mitochondrion; a 1-CysPrx, in the nucleo-cytoplasm; and three (PrxIIB, PrxIIC, and PrxIID), in the cytosol (Dietz ). In , 2-CysPrx is involved in growth and photosynthesis (Baier and Dietz ; Pulido et al. ). For rice, however, no genetic studies have revealed functional roles for any PRXs except rice 1 Cys-peroxiredoxin (R1C-Prx/Os1-CysPrxA), which acts as a dormancy regulator and an antioxidant, based on a study with transgenic plants of tobacco, an heterologous system, that constitutively express  (GenBank Accession Number C19186; Lee et al. ).Expression profiles under diverse developmental stages and growth conditions are a simple and powerful tool for obtaining information about genes for which functions have not been characterized (Chandran et al. , ). We recently suggested putative functions for most members of several gene families based on the integration of experimental and meta-expression data (Jung et al. ; Jin et al. ; Nguyen et al. ; Nguyen et al. ; Nguyen et al. ). Applying a similar approach with the rice PRX family could help elucidate the functions of those members.The 2-C methyl-D-erythritol 4-phosphate (MEP) pathway is a unique and essential process for bacteria, algae, and plants (Proteau ). The final product, isopentenyl pyrophosphate, is used for the synthesis of diverse secondary metabolites such as isoprenoids, carotenoids, chlorophylls, and tocopherols, as well as for hormones such as gibberellins and abscisic acid (Jung et al. ). We recently identified a T-DNA insertional mutant in a gene encoding the second enzyme in this pathway, 1-deoxy-d-xylulose 5-phosphate reductoisomerase (DXR; EC 1.1.1.267). Under greenhouse conditions, this mutant exhibits an albino phenotype at the early seedling stage (Jung et al. ). Co-expression of genes in the MEP pathways with those in the pathways for chlorophyll and carotenoid biosynthesis implies a potential relationship among those pathways during photosynthesis (Jung et al. ). In addition, the photosystems are major sources for generating ROS during photosynthesis (Pospisil ). Because the four PRXs in rice are localized to the chloroplast (Dietz et al. ), we are very interested in learning how they are involved in light responses or photosynthesis.Here, we performed a comparative expression analysis, within the context of a phylogenic tree, using PRX family genes from rice and . Our results suggested probable functional orthologs between specific pairs. Tandemly duplicated  and  showed expression that was distinctly preferential in both developing seeds and germinating seeds. We then used transgenic plants expressing the GUS reporter gene under the control of their promoters to confirm the pattern of differential expression by these two tandem duplicates. Finally, we identified unique -acting regulatory elements (CREs) for root-, embryo-, or endosperm-preferential expression in rice. In the rice mutant  (), which is defective in the light response, downregulation of two  genes indicated the involvement of those proteins in -mediated light signaling.We carried out comparative expression analysis of rice and Arabidopsis PRX family genes which suggests conserved or diversified roles between the two species, leading the identification of tandemly duplicated rice PRX genes in the 1-CysPrx subgroup, Os1-CysPrxA and Os1-CysPrxB, differentially expressed in seeds. Os1-CysPrxB showed embryo- or root-preferential expression, while Os1-CysPrxA showed endosperm-preferential expression. Analysis of the cis-acting regulatory elements (CREs) revealed unique CREs responsible for embryo and root or endospermpreferential expression. In addition, the presence of leaf/shoot-preferential PRXs in rice suggests their evolutional requirement to survive in the growth environment with a higher light intensity when compared with that of Arabidopsis. Downregulation of two PRXs in the dxr mutant causing an albino phenotype implies that those genes have roles in processing ROS produced during photosynthesis. Predicted protein-protein network associated with four PRXs suggests useful regulatory model for further study."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0167-0", "title": "High Resolution Mapping of QTLs for Heat Tolerance in Rice Using a 5K SNP Array", "authors": ["Shanmugavadivel  PS", "Amitha Mithra  SV", "Chandra Prakash", "Ramkumar  MK", "Ratan Tiwari", "Trilochan Mohapatra", "Nagendra Kumar Singh"], "publication": "Rice", "publication_date": "5 June 2017", "abstract": "Heat stress is one of the major abiotic threats to rice production, next to drought and salinity stress. Incidence of heat stress at reproductive phase of the crop results in abnormal pollination leading to floret sterility, low seed set and poor grain quality. Identification of QTLs and causal genes for heat stress tolerance at flowering will facilitate breeding for improved heat tolerance in rice. In the present study, we used 272\u00a0F", "full_text": "Rice is a major staple food crop for nearly half of the world population. The global population is projected to grow from seven to nine billion by 2050 and to reach ten billion before 2100 (United Nations ). To ensure food security to the added population, rice production has to increase by 0.6 to 0.9% annually until 2050 (Carriger and Vallee ). However, rise in global average temperature to the tune of 0.5\u00a0\u00b0C in the twentieth century and future projections in the range of 1.4\u20135.8\u00a0\u00b0C by the end of this century (IPCC ), will be detrimental to crop yield (Lobell et al. ). Declining farmland resources coupled with global warming have forced rice cultivation to marginal environments and beyond the normal rice season. This in turn has exposed the rice crop to higher day temperature (>33\u00a0\u00b0C) adversely impacting seed set (Nakagawa et al. ; Prasad et al. ; Jagadish et al. ).Heat stress alters the initiation and duration of developmental phases, especially the duration from floral/panicle initiation to anthesis/panicle exertion in plants (Sato et al. ). Heat stress during flowering and anthesis can lead to failure in fertilization due to pollen or ovule sterility (Matsui et al. ). Early reproductive processes viz., micro- and megasporogenesis, pollen and stigma viability, anthesis, pollination, pollen tube growth, fertilization, and early embryo development are all highly susceptible to heat stress (Giorno et al. ). Flowering stage is crucial for crop productivity as heat stress during this phase causes reduced pollen fertility and low seed set in rice (Jagadish et al. ). Anthesis processes, including anther dehiscence, pollination, and pollen germination are most sensitive to high temperature stress in rice. The main cause of spikelet sterility induced by high temperature is anther indehiscence (Matsui et al. ). High temperature inhibits swelling of pollen grains, which is a driving force for anther dehiscence in rice. Successful anther dehiscence depends on several parameters, including rupturing of septa, expansion of locule walls, pollen swelling, and rupturing of stomium (Liu et al. ).Enhanced heat tolerance in rice is required at flowering stage to avoid spikelet sterility. Since 1985, germplasm screening for high temperature stress tolerance has been carried out by different research groups worldwide (Sarwar and Avesi ; Matsui and Omasa ; Jagadish et al. , ). Heat tolerance at flowering stage in rice is attributed to multiple genes with cumulative effects on trait expression, otherwise called as quantitative trait loci (QTL; Cao et al. ; Xiao et al. ; Ye et al. ). The discovery of genes/QTLs for enhanced tolerance to high temperature stress has practical implications in agriculture. Mapping of QTLs for heat tolerance in rice was first reported by Cao et al. () based on percent spikelet fertility using doubled haploid population derived from IR64/Azucena cross. Thereafter many research groups have mapped QTLs for heat stress tolerance using F, back cross inbred lines (BIL) and recombinant inbred lines (RIL) populations, evaluated at the time of heading in controlled environment conditions (Chang-Lan et al. ; Chen et al. ; Zhang et al. , ; Jagadish et al. ; Xiao et al. ; Ye et al. , ; Cheng et al. ; and Poli et al. ). Some of these studies created high temperature condition for phenotyping by late planting in open field (Xiao et al. ; Tazib et al. ; and Zhao et al. ). Almost all of these studies employed RFLP or SSR markers, except Ye et al. (, ), who used 300 SNP markers for the QTL mapping. IR64 has been used as one of the parental lines in generating mapping populations for mapping heat stress tolerance QTLs in some studies (Cao et al. ; Ye et al. , ), while Nagina22 and its derived mutant lines have been used as parents in generating mapping population by other researchers (Buu et al. ; Poli et al. ). There is a report on using IR64/Nagina22 derived F population for mapping heat tolerance QTLs (Ye et al. ).Mapping of QTLs for heat stress tolerance using stress tolerance indices, which compare the performance of genotypes under control and stress condition, have not been reported earlier, but it has been utilized for mapping salt stress tolerance (Fernandez ; Pandit et al. ; Tiwari et al. ). The relative performance of genotypes under stress and control conditions can be used as an indicator to identify and map QTLs, which can be further used in breeding crop varieties for stress tolerance, rather than mapping QTLs based on phenotypic performance in stress environment alone (Raman et al. ). This has practical relevance since genotypes with low yield potential under control condition quite often show higher tolerance to stress than high yielding genotypes. Genomic regions governing salinity stress tolerance was successfully mapped in rice using stress indices (Pandit et al. , Kumar et al. ; Tiwari et al. ). The present study focused on identification of QTLs for heat stress tolerance at flowering stage in a RIL mapping population derived from Nagina22/IR64 cross using controlled phenotyping facility for imposing heat stress, using stress tolerance indices for normalization of intrinsic differences in yield potential and high density SNP mapping. High density linkage map is expected to result in finding QTLs flanked by closely linked markers that can be readily used in breeding programmes for marker assisted selection.QTLs for heat tolerance have been mapped on different chromosomes of rice by different research groups during the last decade (Cao et al. ; Chen et al. ; Zhang et al. , ; Jagadish et al. ; Xiao et al. ; Ye et al. , ) (Additional file : Table S3). In the current study, using a reasonably large RIL population, high density SNP map and phenotyping under controlled facility, we identified four heat tolerant QTLs in rice, of which three were novel namely  and . Among these,  was the major QTL for percent spikelet sterility. Further, we also identified a known major effect QTL, .1/.1 for both the indices of yield. Zhang et al. () have reported this QTL in a RIL mapping population derived from a cross Zhongyouzao8 x Toyonishiki between SSR markers, RM405 and RM274 flanking a 23\u00a0Mb interval. In their study, this QTL explained 10.7% phenotypic variation for spikelet fertility under heat stress while it was for SSI/STI for yield in our study. Further, the QTL interval was narrowed down to a 331 kbp region comprising of 54 genes in our study. This was because earlier studies have used either SSR markers (maximum 264) or less than 300 SNP markers for mapping QTLs for heat tolerance whereas we have used more than 800 SNPs and 272 RILs to achieve a much higher resolution (Buu et al. ; Ye et al. ,\u00a0). Our ability to identify QTLs in such narrow intervals could be attributed to the use of 5K SNP array comprising of SNPs from abiotic stress responsive genes (Kumar et al. ). Some important candidate genes located in the high effect and minor QTLs identified in the present study are discussed below for their probable role in enhanced spikelet fertility and yield under heat stress.There were 65 genes in the QTL region, , including transporters, transcription factors such as  (), -finger domain containing TF (), , and  zinc finger, transcriptional regulators, glycosyltransferase microtubule associated protein, and annexin (Additional file : Table S1). Tapetum, the innermost cell layer of the anther wall, plays a crucial role in anther development, microspore/pollen formation, and pollen wall formation. During late pollen development, tapetal degeneration triggered by an apoptosis-like process is essential for viable pollen formation (Li et al. ).  () present in the .1 QTL region encodes a PHD-finger protein that controls programmed tapetal development and degradation to ensure functional pollen formation in rice (Li et al. , ).  is expressed specifically in tapetal cells and microspores during anther development in stages 8 and 9 and initiates a typical apoptosis-like cell death, thereby ensuring proper pollen grain development (Li et al. ). Loss of function of  displayed uncontrolled tapetal cell proliferation and swelling, delayed DNA fragmentation, and pollen wall development, causing complete male sterility (Li et al. ). Timely initiation of tapetal programmed cell death is essential for the regulated release of wall materials from the tapetum to the developing microspore including carbohydrate, lipid molecules, and other nutrients. This gene might be responsible for maintaining higher fertility in N22 under heat stress by timely initiation of PCD in N22 tapetal cell to ensure more fertile pollen grains than in the susceptible parent IR64. This gene otherwise named as a \u20131/ is responsible for thermo-sensitive genic male sterility in -1, one of the oldest and often-used TGMS line in  two-line hybrid rice breeding programs in China (Qi et al. ). Also,  () gene encodes for a protein homologous to the PHD-finger class of transcription factor and has been demonstrated to be involved in tapetal development and pollen wall biosynthesis (Yang et al. ).Glycosyltransferase attaches a single or multiple sugars to different bio-molecules and highly expresses in mature pollen grains and is involved in mature pollen grain formation in rice (Moon et al. ).  () of rice present in  is involved in pollen wall formation, especially, the exine and intine construction and pollen maturation. The  mutant failed to produce mature pollen grains since its pollen had disrupted intine structure owing to low levels of starch and protein (Moon et al. ). Similarly,  1 () gene of  encodes GT31 family glycosyltransferase in Arabidopsis and might be involved in galactosylation of arabinogalactan proteins (AGPs). The mutant of  exhibit defective and irregular exine pattern and suggests that primexine localized AGPs could play a role in sporopollenin adhesion and patterning in early microspore wall development (Li et al. ). HSFs are main players in imparting heat stress response by activating transcription of downstream genes including HSPs (Guo et al. ).  has a role in root development in Arabidopsis and involved in early stage of heat shock (Lohmann et al. ; Begum et al. ). A similar  has been identified in this major QTL on chromosome 9, which is yet to be characterized in rice.Microtubule-associated proteins play a crucial role in the regulation of microtubule dynamics, and important for plant cell and organ development (Liu et al. ). The 65-kD microtubule-associated protein (MAP65) family member in Arabidopsis (\u20131) is ubiquitously expressed during the cell cycle and in all plant organs and tissues with the exception of anthers and petals (Smertenko et al. ). However, Microtubule-associated protein \u20131a (LOC_Os09g27700) of rice is expressed in anther and pistil. This might be indicative of its role in reproductive organ development in rice and hence is a good candidate for further studies in rice. Annexin functions to counteract oxidative stress, maintain cell redox homeostasis, and enhance drought tolerance (Szalonek ). Down-regulation of  annexin5 () in transgenic -RNAi lines caused sterile pollen grains. Ann5 is involved in pollen grain development, germination and pollen tube growth through the promotion of endo-membrane trafficking modulated by calcium (Zhu et al. ).  in wheat is highly expressed in floral bracts, pistil, anthers and immature endosperm and it correlates with anther development. But it fails to be induced by low temperature in thermosensitive genic male sterile lines, suggesting that specific down-regulation of  is associated with cold induced male sterility in wheat (Xu et al. ). The relative expression levels of  in the stamen strongly correlated with male fertility in recovery lines (Xu et al. ). One such annexin 10 (/ LOC_Os09g27990) is present in the QTL interval \n                     A common response of organisms to drought, salinity, and temperature stresses is the accumulation of sugars and compatible solutes including trehalose. The increased trehalose accumulation correlates with elevated capacity for photosynthesis under both stress and non-stress conditions in rice (Garg et al. ). Trehalose-6-phosphate synthase (TPS) plays an important role in trehalose metabolism and signalling. Overexpression of the trehalose-6-phosphate synthase gene  enhances the tolerance of rice seedling to cold, high salinity and drought stress without other significant phenotypic changes (Li et al. ). Similarly, the over-expression of trehalose-6-phosphate phosphatase in maize ears increases both kernel set and harvest index in drought stress condition. Increase in yield to the tune of 9% to 49% under non-drought or mild-drought conditions, and 31% -123% under more severe drought conditions, relative to yields from non-transgenic controls was observed (Nuccio et al. ). Similarly, trehalose concentration increased upon 4\u2005h of heat stress at 40\u00a0\u00b0C and 4\u00a0days after cold stress at 4\u00a0\u00b0C in  (Kaplan et al. ). Over-expression of  and  under stress associated  promoter provided protection against drought, salt, freezing, and heat stress (Miranda et al. ). Hence, trehalose synthase and trehalose phosphate synthase are a probable candidate genes underlying QTL ./. for yield under heat stress.Our analysis for non-synonymous SNPs between the parents in the candidate genes like , \u20131/, ,\u20131a (LOC_Os09g27700), , trehalose synthase, trehalose phosphate synthase, , pectin methyl esterase and tesmin could not find allelic variants. This could be due to low coverage of the sequence data in either or both of the parents or lack of variation present in coding regions in the above genes. Further, the InDel polymorphism for these genes is not known. Alternatively, there could be variations in promoter, intron-exon junctions and UTR regions, which are not yet known. Hence, future effort is required to deep sequence these regions in the parents to identify polymorphisms, if any, in candidate genes. Nevertheless, the listed SNPs between N22 and IR64 can be utilised for fine mapping and functional validation of the QTLs.The interaction network analysis showed evidence for the involvement of the major QTL, .1/.1, in digenic interactions, strengthening the role of this region in imparting heat tolerance. Though most of the SNPs involved in epistasis were in genes encoding uncharacterized expressed proteins, two SNPs were from known proteins coding genes namely, WD domain, G-beta repeat domain containing gene (LOC_Os05g44320) and dehydrogenase gene (LOC_Os04g52280). The WD40 protein is reported to play a role in diverse protein-protein interactions or protein-DNA interactions by acting as scaffolding molecule and promoting protein activity and thus functioning as a positive regulator of plant responses to various abiotic stresses such as salinity, osmotic and dehydration stress in plants (Mishra et al. ; Kong et al. ). Further, it is involved in various biological process, viz., signal transduction, gene transcriptional regulation, protein modifications, cytoskeleton assembly, vesicular trafficking, DNA damage and repair, cell death and cell cycle progression (Zhang and Zhang ).Present study using Nagina22/IR64 RIL mapping population and a 5K SNP genotyping chip, identified a major novel QTL .1 for reproductive stage heat tolerance in a 394 kbp region of rice chromosome 9. The study also confirmed the presence of a known major QTL for heat tolerance on chromosome 5 (), which was narrowed down from 23\u00a0Mb in the original study to a much smaller interval of 331 kbp. This QTL was also involved in digenic interaction. Though the polymorphism survey in the candidate genes using the available data did not produce any trait linked variation, the SNPs identified could be useful in fine mapping. Further sequencing and functional validation is required for the identification of actual genes in these QTL regions responsible for the heat tolerance. Nonetheless, the two major QTLs identified here can be employed directly for crop improvement by marker assisted selection (MAS) after development of suitable scorable markers for breeding of high yielding heat tolerant rice varieties."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0174-1", "title": "Updating the elite rice variety Kongyu 131 by improving the ", "authors": ["Xiaomin Feng", "Chen Wang", "Jianzong Nan", "Xiaohui Zhang", "Rongsheng Wang", "Guoqiang Jiang", "Qingbo Yuan", "Shaoyang Lin"], "publication": "Rice", "publication_date": "20 July 2017", "abstract": "Kongyu 131 is an elite ", "full_text": "Rice is one of the most important cereal crops worldwide, and more than half of the global population depends on rice as a staple food. The continuously increasing population, decreasing arable land, global climate change and expanding demands for biofuels have challenged global food security (Takeda and Matsuoka ). During the past 60\u00a0years, the green revolution marked by the application of semi-dwarf genes in rice (Sasaki et al. ; Spielmeyer et al. ) and the application of hybrids in southeastern Asia have greatly improved rice yield. However, research has shown that the top three rice producers, India, China and Indonesia, experienced yield stagnation in more than 37%, 78% and 81% of their respective rice growing areas from 1961 to 2008 (Ray et al. ). Therefore, it is challenging, but there is also great potential, to increase rice yield using molecular breeding methods in the coming decades.The grain yield of rice is a complex quantitative trait determined by three components: effective tillers per plant, grain number per panicle and grain weight (Sakamoto and Matsuoka ; Xing and Zhang ). Grain number per panicle is further composed of two subcomponents: spikelet number, which is mainly determined by the number of primary and second branches, and the seed setting rate of the spikelets (Xing and Zhang ). Since the rice genome was sequenced (Goff et al. ; International Rice Genome Sequencing Project ; Yu et al. ), significant advancements have been made in the functional genomics of this crop. In recent years, a number of genes and quantitative trait loci (QTLs) for yield-related traits were identified and characterized (Chen et al. ). Major QTLs for yield traits, such as  (Ashikari et al. ),  (Xue et al. ),  (Fan et al. ),  (Weng et al. ), and  (Wang et al. ), have been characterized. For instance,  encodes cytokinin oxidase/dehydrogenase (OsCKX2), an enzyme that degrades cytokinin. When the expression of  is reduced, cytokinin accumulates in inflorescence meristems and increases the number of reproductive organs, which enhances the number of grains and leads to increased grain yield (Ashikari et al. ). These genes, together with QTLs and their superior allelic variations, provide the basis for rice molecular breeding.The rapid accumulation of rice genome resequencing data not only assisted in the identification of functional genes or QTLs but also provided numerous polymorphic genome sequences for molecular marker development (Huang et al. , , ; Xu et al. ). High-resolution markers, including restriction site-associated DNA (RAD) and single nucleotide polymorphisms (SNPs), together with the analysis platforms, improve the efficiency of genotyping and reduce the cost (Miller et al. ; Simko ; Tung et al. ). Based on sexual hybridization, traditional variety breeding depends on genetic recombination and phenotypic selection but has the disadvantages of low selection efficiency of complex traits, vulnerability to environmental influence and a long breeding cycle. Marker-assisted selection (MAS) involves the transformation of phenotypic to genotype selection, which can improve the efficiency and accuracy of the selection of target traits, and has been widely used for crop genetic improvement (Chen et al. ; Jena and Mackill ; Xu and Crouch ).Breeders have improved the resistance to rice blast, bacterial blight and brown planthopper with the aid of MAS (Jiang et al. ; Liu et al. ; Pradhan et al. ; Wang et al. ). Although MAS has been applied in the breeding of many crops, it has limitations. First, MAS selects the target traits through the molecular markers linked to the genes for the target traits, and the recombination between the molecular markers and target genes can reduce the selection accuracy. To date, almost all research on resistance breeding through MAS used markers linked to resistance genes, but reports on the selection of target genes through genic or functional markers (Andersen and Lubberstedt ) are rare. Second, introducing target genes through MAS can result in linkage drags. Third, introducing beneficial allelic genes cannot efficiently improve the target traits due to different genomic backgrounds. Solving these problems can make molecular breeding more accurate and efficient. In addition, there are many reports on improvements in disease and insect resistance, but reports on complex traits, such as yield improvement, are rare.Kongyu 131 ranked top among the elite rice varieties of the third accumulative temperature belt in Heilongjiang Province because of its superior traits such as early maturity, superior quality, cold tolerance and wide adaptability. In 2005, the planting area of Kongyu 131 reached 770,000\u00a0ha, accounting for more than half of the total rice planting area. We improved the resistance of Kongyu 131 to rice blast by improving its blast resistance gene locus (Zhang et al. ). We also found that Kongyu 131 has the disadvantage of relatively few grains per panicle compared with other varieties; therefore, the objective of this study was to improve the grain number per panicle and subsequent yield of Kongyu 131 by substituting the  locus in the genome of Kongyu 131 with a favorable allele from GKBR.As a preeminent rice variety, even though Kongyu 131 has potential for increased production, it is difficult to improve yield while maintaining other desirable traits using MAS. In this study, the genome of Kongyu 131 was rebuilt through crossing, backcrossing and self-crossing. During this process, non-target chromosome fragments were excluded by background selection, and linkage drag was minimized by recombinant selection from large populations. Subsequently, a new Kongyu 131 genome carrying the favorable allele of  was built. We termed this method \u201cVariety Update\u201d, which can be used to improve a less desirable trait of an elite variety by replacing only a minute chromosome fragment (Lin et al. ).To improve the yield of the elite rice variety Kongyu 131 and reserve its other desirable agronomic traits, we attempted a new approach, i.e., the rebuilding of the Kongyu 131 genome. During the process of genome reconstruction, the favorable allele  from the donor parent was introgressed into the genome of Kongyu 131. The yield per plant of the new Kongyu131 increased by 8.3% and 11.9% in two locations, and this new variety can be deployed in Heilongjiang Province instead of Kongyu131."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0171-4", "title": "\n                     ", "authors": ["Yong Zhou", "\u2020", "Yajun Tao", "\u2020", "Jinyan Zhu", "\u2020", "Jun Miao", "Jun Liu", "Yanhua Liu", "Chuandeng Yi", "Zefeng Yang", "Zhiyun Gong", "Guohua Liang"], "publication": "Rice", "publication_date": "20 July 2017", "abstract": "Rice plays an extremely important role in food safety because it feeds more than half of the world\u2019s population. Rice grain yield depends on biomass and the harvest index. An important strategy to break through the rice grain yield ceiling is to increase the biological yield. Therefore, genes associated with organ size are important targets for rice breeding.", "full_text": "The world\u2019s population is estimated to grow to around 8.5 billion by 2030 (). To feed the growing population, it is estimated that agricultural production needs to increase by 60% (Yamaguchi and Hwang ). Rice ( L.) is a staple food of more than 3.5 billion people, mainly in Asia (Seck et al. ). A significant improvement in rice yield per unit ground area would significantly reduce the global food shortage.Rice grain yield is defined as the product of yield sink capacity and filling efficiency (Kato and Takeda ). To achieve new breakthroughs in yield, breeding efforts have focused on expanding the yield sink capacity, mainly by increasing the number of grains per panicle and grain size. Strategies including high fertilizer inputs and optimized cultivation methods have been used to increase grain number and enhance grain filling to maximize rice production. New varieties, especially the so-called \u2018super rice\u2019 cultivars that produce large numbers of grains per panicle with a large yield potential have been bred and cultivated. There have also been breakthroughs in elucidating the molecular mechanisms underlying rice yield traits. Using molecular genetic approaches, researchers have identified several genes that control the size of rice panicle and grain. For example, mutants of , , , , , , , , and  genes were found to produce abnormal inflorescences and smaller panicles (Chu et al. ; Ikeda et al. ; Komatsu et al. ; Komatsu et al. ; Kurakawa et al. ; Li et al. ; Li et al. ; Qiao et al. ; Suzaki et al. ; Zhao et al. ). Several quantitative trait loci (QTL) controlling grain number have been identified. Among them, , , , , and  negatively regulate grain number per panicle (Ashikari et al. ; Gu et al. ; Jiao et al. ; Jin et al. ; Luo et al. ), while  and  are related to increased grain number (Dong et al. ; Fujita et al. ).Rice grain size is defined by grain length, width, length-width ratio, and grain weight, and is another important factor in determining rice yield. Generally, dwarf mutants of the genes involved in gibberellin (GA) and brassinosteroid (BR) biosynthesis and signaling, such as , , , , , , , and , produce smaller grains (Ashikari et al. ; Hong et al. ; Itoh et al. ; Mori et al. ; Tanabe et al. ; Yamamuro et al. ). Several QTL related to grain size have been isolated. For example, , , ,  and  control grain size (Fan et al. ; Li et al. ; Qi et al. ; Song et al. ; Weng et al. ),  and  regulate grain shape (Wang et al. ; Wang et al. ; Wang et al. ; Zhou et al. ), and  and  control grain filling (Ishimaru et al. ; Wang et al. ).In this study, we characterized a rice mutant,  (), which showed reduced grain number per panicle and smaller grains compared with those of wild type (WT). The  gene, isolated via a map-based cloning approach, was found to be a novel allele of  (), which encodes a cytochrome P450 protein.  regulates the expression levels of genes involved in BR synthesis and BR response. Overexpression of  in a high-yielding cultivar background significantly enhanced grain weight and increased grain yield, suggesting that  is key target gene with possible applications in yield breeding.Rice is a very important food crop because it feeds more than half of the world\u2019s population. Grain yield improvement is the main aim for rice breeders. Rice yield potential is determined by biomass and the harvest index. The harvest index of the cultivated rice varieties in China has almost reached its theoretical limit. Improving biomass production is an effective strategy to break through the yield ceiling. Rice biomass depends on plant organ size, which is controlled by genetic factors and environmental conditions. Although numerous genetic determinants of organ growth have been characterized, our understanding of how organ size is regulated is incomplete.To reveal more genes related to organ size in rice, we characterized a rice mutant, , with reduced panicle and grain size. Map-based cloning demonstrated that reduced transcription of  caused by an SNP in its promoter region resulted in the mutant phenotype. Down-regulation of  by RNAi also resulted in smaller panicles and grains. This finding suggested that  plays a positive role in regulating panicle and grain size in rice.\n                         encodes a cytochrome P450 superfamily protein CYP724B1, and is allelic to the previously reported  which plays roles in BR synthesis (Tanabe et al. ). In plants, BRs are essential steroid hormones that regulate diverse processes during plant development, such as stem elongation, vascular differentiation, male fertility, senescence, and responses to various biotic and abiotic stresses (Yang et al. ). As described in a previous study, both the  and  mutants exhibited shortened internodes and produced extremely small round grains. However, the  mutant did not show this severe phenotype, with only a 5.5% and 9.4% reduction in grain length and grain weight, respectively, compared with those of WT. Molecular detection indicated that an SNP variation in the promoter region slightly decreased the expression level of , leading to the mutant phenotype. These data suggested that  is a weak mutant of the  gene. In rice, a BR-deficient mutant displayed erect leaves, reduced plant height, and decreased tiller number and grain size (Yang et al. ). In addition to these common traits, the  mutant has some distinct traits. For instance,  has a very short main axis with most of the primary branches clustered at the base of the main axis, and few secondary branches. Together, these results suggested a novel and important role for  in regulating inflorescence development.Because  is involved in regulating panicle and grain size, we were interested in whether this gene could be used to improve rice yields. To investigate the function of , we generated several overexpressing lines in Nipponbare background. Strong expression of  in the transgenic lines was detected by qRT-PCR (Fig. ). At the heading and mature stages, rice plants harboring the overexpression construct outgrew Nipponbare plants (Fig. ), indicating that  controls vegetative growth. When grown in paddies, the biomass of overexpressing lines was greater than that of Nipponbare (Fig. ; Table ). Multiple sink-related traits, including grain size and grain number per panicle, were enhanced in the overexpressing lines. As anticipated, the grain yield per plant was increased by 16.8% and 15.7%, compared with that of Nipponbare. Next, we created overexpressing lines using WYJ7, a high-yielding variety, as the recipient. These overexpressing lines also showed significant biomass and grain yield improvements (Fig. , ). Taken together, these results indicated that  has multiple beneficial effects on grain yield components, and is valuable for high-yield rice breeding.Recently, Wu et al. () found that the overexpression line (OE-) of  under the control of the maize ubiquitin promoter showed increased grain length and 1000-grain weight. However, there was no significant increase in the grain yield per plant of OE- plants as compared with WT, because the transgenic plants showed profound changes in plant architecture, such as larger leaf angles and narrower leaves. Transgenic plants expressing  under the control of panicle-specific promoters from  and  produced larger seeds and increased grain yields without changes in other agronomic traits, such as grain number per panicle. In this study, the overexpressing lines in both the Nipponbare and WYJ7 backgrounds produced larger seeds, more grains, and increased yields, compared with those of WT. We noticed that the  transcripts in OE- lines accumulated more than 450-fold than in WT. In this study, however, there is only 30.0- to 93.6-fold increase in  expression in the overexpression transgenic lines (Figs.  and ). As a result, the phenotypic variations in our data are not as big as Wu\u2019 results. We speculate that the yield-increasing effect of  may be determined by its expression pattern, and may also be affected to some extent by genetic backgrounds.The rice  mutant showed reduced organ size, fewer grains per panicle, and smaller grain. Map-based cloning investigated that the  gene, encoding a cytochrome P450 protein, is allelic to  (). Morphological and cellular analyses suggested that  positively regulates grain size by promoting cell elongation. Elevated expression level of  significantly increased 1000-grain weight and grain number per panicle, and subsequently enhanced grain yields in both the Nipponbare and Wuyunjing7 (a high-yielding cultivar) backgrounds. These results suggest that  is key target site to increase yields in rice breeding programs."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0175-0", "title": "Panicle blast 1 (Pb1) resistance is dependent on at least four QTLs in the rice genome", "authors": ["Haruhiko Inoue", "Mitsuru Nakamura", "Tatsumi Mizubayashi", "Akira Takahashi", "Shoji Sugano", "Shuuichi Fukuoka", "Nagao Hayashi"], "publication": "Rice", "publication_date": "1 August 2017", "abstract": "Rice blast is the most serious disease afflicting rice and there is an urgent need for the use of disease resistance (R) genes in blast tolerance breeding programs. ", "full_text": "Rice blast is a pathogenicity that affects rice plants worldwide. In order to minimize the damage of rice blast, resistance (R)-gene cultivars are commonly used. The use of R genes to develop resistant cultivars has become an urgent goal in rice breeding programs. Several molecular approaches to study rice blast resistance genes have been developed during the last decade. So far, hundreds of blast  genes have been mapped in the rice genome. A large group of  genes is clustered in several genomic regions reviewed in (Liu et al. ). Most of the genes encode nucleotide-binding site (NBS) leucine-rich repeat (LRR) proteins that interact with pathogen effectors and trigger defense reactions following a gene-for-gene model of recognition (Bryan et al. ; Okuyama et al. ; Cesari et al. ). One of the plant defense mechanisms is the recognition of Microbe-Associated Molecular Patterns (MAMPs) by receptors, leading to generally weak defense responses called MAMP-Triggered Immunity (MTI). Some pathogens overcome the resistance by secreting effector proteins that interfere with host resistance and promote pathogenicity. Plants have evolved to recognize against the effectors through Effector-Triggered Immunity (ETI). True resistance genes are specific and provide strong resistance to the rice blast, but the effect of R genes decline after 2 or 3\u00a0years.In contrast to MTI or ETI, the quantitative resistance genes are durable to the blast, but are less understood (Cook et al. ; Fukuoka et al. ). Recently, intensive genetic analyses of quantitative \u201cleaf\u201d blast resistance genes including , , ,  have been carried out (Nguyen et al. ; Zenbayashi-Sawata et al. ; Terashima et al. ; Fukuoka et al. ; Xu et al. ). In contrast to these leaf blast resistance genes, the panicle blast () resistance gene has been used as the durable form that confers rice plants partial resistance to the rice blast with no fungal race specificity (Fujii et al. ).  gene has been introduced into rice cultivars in Korea (Lee et al. ). The cultivars have remained resistant to panicle blast for over 35\u00a0years. Cloning of  has identified that the resistance is weak on the early stage and gradually becomes strong for the reproductive stage of the plant, and is dependent on the  expression (Hayashi et al. ). After heading stage, the resistance of  cultivar is very strong to rice blast.The resistance mechanism of  is attributed to its interaction with WRKY45 (Inoue et al. ), which plays a crucial role in the salicylic acid (SA) pathway in rice immunity (Shimono et al. ). WRKY45 is regulated by a ubiquitin-proteasome system in the rice nucleus (Matsushita et al. ). The Pb1 coiled-coil domain interacts with the nuclear-localized WRKY45 (Inoue et al. ), resulting in a Pb1-WKRY45 complex is a weaker target than WRKY45 for protein degradation. WRKY45 overexpressed transgenic rice plants have strong resistance not only to the leaf rice blast but also to the panicle blast (SHIMONO et al. ). The genes in the downstream pathway of WRKY45, including , ,  (Nakayama et al. ), and diterpenoid phytoalexin (DP), a bHLH transcription factor (Yamamura et al. ), play a crucial role in disease resistance. Cytokinins (CKs) play a role in mediating the signal of  infection to trigger the induction of DP biosynthetic genes in benzothiadiazole- (BTH) primed plants (AKAGI et al. ).Kanto 209 (K209) cultivar is one of the cultivars harboring ; however, the resistance of the cultivar is weak in comparison with other  cultivars. In the K209 cultivar of rice, the expression of WRKY45 is lower than that of Koshihikari Aichi SBL (KASBL). The lower expression of the gene might be due to the genotype of the cultivar. To identify the mechanism of this rice blast resistance, we crossed the K209 and KASBL cultivars. Genomic DNA of their Fprogeny was subjected to SNP-based Golden Gate assay and we found at least four quantitative trail loci (QTL) to the panicle rice blast resistance. Among the four QTLs, we focused on QTL7, which was located on chromosome 7 and showed -dependent defense against panicle blast in field tests, and performed a map-based cloning.In this report, we have identified the QTL loci of  based on the resistance to rice panicle blast with map-based screening. We found that  was negatively dependent at least on three QTLs, 7, 9 and 11, and positively on one, QTL 8. This understanding of the molecular mechanism of blast resistance paves the way for creating a line in future containing just the positively acting QTL in order to make a more effective -mediated resistance."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0169-y", "title": "Phenology, sterility and inheritance of two environment genic male sterile (EGMS) lines for hybrid rice", "authors": ["R. El-Namaky", "P.A.J. van Oort"], "publication": "Rice", "publication_date": "29 June 2017", "abstract": "There is still limited quantitative understanding of how environmental factors affect sterility of Environment-conditioned genic male sterility (EGMS) lines. A model was developed for this purpose and tested based on experimental data from Ndiaye (Senegal) in 2013-2015. For the two EGMS lines tested here, it was not clear if one or more recessive gene(s) were causing male sterility. This was tested by studying sterility segregation of the F2 populations. ", "full_text": "Rice hybrids often have higher yields than high-yielding inbred varieties, often between 15% and 20% higher (Virmani et al. ). Where local seed markets are well functioning, hybrids can play an important contribution to farmers\u2019 livelihoods, local and regional food security. In 2010, AfricaRice initiated breeding for hybrid rice (El-Namaky and Demont ). To produce hybrids, a line with male sterility is crossed with a local popular variety (the male pollen donor). Resulting F seed (hybrids) benefits from the positive effects of heterosis and benefits from genes from the local popular variety, which ideally makes the F seed higher yielding yet still well adapted to the local environment. Environment-sensitive genic male sterility (EGMS), also called Photoperiod-thermo-sensitive genic male sterile (PTGMS), has been extensively used for preventing self-pollination in the production of hybrid seeds in various crops (Virmani et al. , Xu et al. ). Compared with three-line sterile lines in a hybrid rice system, EGMS can maintain sterile line production without using restorer lines. Furthermore, a two-line hybrid rice system by application of EGMS has many advantages, including a wider range of germplasm resources used as breeding parents, higher yields, and simpler procedures for breeding and hybrid seed production (Virmani et al. ; Zhou et al. ). With the discovery of the photoperiod sensitive genic male sterility (PGMS) line Nongken 58S in rice (Shi ), there has been great progress in two-line hybrid rice breeding in China.Inheritance of male sterility is important in breeding programs for locally well adapted hybrids. In the first round, an EGMS parent is crossed with a local popular variety to produce the F seeds. The F generations were obtained from self-pollination of F hybrids. From this segregated population, breeders will be able to select a subset of plants which performed best, i.e. combining the most desirable genes from the original EGMS line and the local popular variety. This subset is used as the second generation EGMS. In comparison with the first generation EGMS and F hybrids, the second generation will have more desirable genes inherited from the local popular variety as well as the genes responsible for male sterility and needed to produce hybrids. This process can be repeated, leading to hybrids ever better performing in the test environment. Clearly, for this type of breeding it is desirable that only one recessive gene causes male sterility, in which case 25% of the F population will express the EGMS trait. If two recessive genes are involved, then a much smaller fraction of the F population will have the EGMS trait (1/16\u00a0=\u00a06.25%), thus making breeding much more cumbersome. For this reason, it is important to understand inheritance of the EGMS trait.The first objective of this paper is to develop a model for predicting flowering date and sterility of two EGMS lines as a function of sowing date, location data and weather data. The second objective is to study the F population, to investigate how many gene(s) are involved in causing male sterility.We first describe our data. Next the method of studying inheritance of the EGMS trait. The third main section of the Methods describes a new phenology and sterility model and the methods used for testing and comparing models.A newly developed model could accurately simulate phenology and sterility of two EGMS lines grown in Senegal as a function sowing date, weather variables and a limited number of genetic parameters. Daylength from panicle initiation to flowering was the main explanatory variable, additionally also a significant effect of minimum temperatures during the same period was found. The model can be useful for identifying safe sowing windows for production of hybrids and multiplication of the male sterile parent. A statistical analysis of inheritance revealed that only one recessive gene is causing the male sterility. In Senegal, Dry season (February \u2013 July) is the suitable period for F hybrid seed productions and Wet season (August \u2013 December) is the suitable period for multiplying the EGMS lines."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0173-2", "title": "Mesocotyl Elongation is Essential for Seedling Emergence Under Deep-Seeding Condition in Rice", "authors": ["Hyun-Sook Lee", "Kazuhiro Sasaki", "Ju-Won Kang", "Tadashi Sato", "Won-Yong Song", "Sang-Nag Ahn"], "publication": "Rice", "publication_date": "14 July 2017", "abstract": "Direct-seeding cultivation by deep-seeding of seeds (drill seeding) is becoming popular due to the scarcity of land and labor. However, poor emergence and inadequate seedling establishment can lead to yield loss in direct-seeding cultivation by deep-sowing. In rice, mesocotyl and coleoptile are primarily responsible for seedling emergence from deeper levels of soil.", "full_text": "In Asia, two rice planting methods are used: transplanting and direct seeding. In direct seeding, seeds are sown directly in wet/puddled soil, unpuddled soil, or standing water (Kumar and Ladha ). In Asia, approximately 21% of the total rice area is used for direct seeding, and this is expected to increase owing to the scarcity of land, water, and labor (Pandey and Velasco ). Rice varieties suitable for direct seeding should include high germination ability, seedling vigor, fast root growth, early tillering ability, and lodging resistance (Lee et al. ; Tang ). Faster and uniform germination and seedling emergence resulted in more vigorous seedling growth and increasing yield in direct-seeding (Farooq et al. ). In direct seeding, deep seeding (drill seeding) is known to reduce damages from wildlife and improve lodging tolerance. However, when seeds are sown deep, the seedlings need to elongate their organs to ensure that the plumule reaches the soil surface. Seedling emergence is an important criterion for determining the actual yield during direct seeding cultivation of rice. The coleoptile (the protective sheath that covers the emerging shoot) and mesocotyl (the structure between the scutellar and coleoptilar nodes in an embryo) of rice are primarily responsible for the emergence of seedlings from deeper soil layers (Turner et al. ; Dilday et al. ). Seeding depth is also an important factor for dry direct-seeding rice cultivation. Seedling growth for temperate  cultivars was adversely affected when seeds were sown deeper than the optimum seeding depth of 2\u20133\u00a0cm (Lee et al. ). However, shallow seeding can increase the incidences of damage by birds, and plants might suffer lodging after the heading stage. Thus, obtaining information on the optimum seeding depth for direct seeding for diverse germplasm is necessary (Lee et al. ).Previous studies reported that mesocotyl and coleoptile elongation are governed by many genetic and environmental factors (Takahasi ; Takahasi ). Their elongation shows significant variation across genotypes. Mesocotyl elongation is greater in  rice than in  rice (Suge ; Takahashi ; Lee et al. ), whereas the coleoptile is longer in  cultivars than in  cultivars under submerged condition (Takahashi ). Among  cultivars, upland rice generally produced shorter coleoptiles and longer mesocotyls than lowland type (Chang and Vergara ).Quantitative trait loci (QTLs) associated with mesocotyl and coleoptile elongation in rice have been reported. QTLs for mesocotyl elongation were detected using various segregating populations from interspecific or intrasubspecific crosses (Katsuta-Seki et al. ; Cai and Morishima ; Cao et al. ; Huang et al. ; Lee et al. ). Katsuta-Seki et al. () reported 3 QTLs for mesocotyl elongation on chromosomes 3, 6, and 11 by using F3 populations from the cross between 2  cultivars grown in glass tubes. Cai and Morishima () detected 11 QTLs for mesocotyl length by using 125 recombinant inbred lines (RILs) derived from a cross between an  cultivar and a strain of wild rice. Further, 5 QTLs for mesocotyl length were detectedusing an RIL population from the cross between 2  cultivars by using the filter paper method (Huang et al. ). Cao et al. () detected 8 QTLs on chromosomes 1, 3, 6, 7, 8, and 12 under moderate and low temperate conditions in a doubled haploid population derived from a cross between a  cultivar and an  cultivar. In a previous study, we detected 5 QTLs for mesocotyl length on chromosomes 1, 3, 7, 9, and 12 by using agar media and the same backcross inbred line (BIL) populations used in this study (Lee et al. ).QTLs for improving seedling vigor or seedling establishment, including traits for seedling emergence, seed germination, or shoot length, and coleoptile length have been detected (Redo\u00f1a and Mackill ; Zhou et al. ; Xie et al. ). Redo\u00f1a and Mackill () detected 2 QTLs for coleoptile length and 5 QTLs for mesocotyl length by using an F2:F3 population derived from a cross between  cultivar Labelle and  cultivar Black Gora. Further, 5 QTLs for seed vigor traits such as coleoptile length and radicle length and 3 QTLs for seed germination under low and normal temperature conditions were identified using populations derived from a cross between two  cultivars (Xie et al. ). In addition, 9 QTLs for seedling vigor traits, including coleoptile emergence, were detected under two field conditions\u2014drained and flooded soil\u2014and 2 QTLs for coleoptile emergence were identified on chromosomes 1 and 3 under flooded soil condition (Zhou et al. ).In this study we aimed to show that mesocotyl elongation is essential to seedling emergence under deep-seeding using chromosome segment substitution lines (CSSLs). Although several studies identified QTLs for mesocotyl or coleoptile elongation, the locations and effects of the detected QTLs varied according to the test methods (Redo\u00f1a and Mackill ; Zhou et al. ; Huang et al. ; Xie et al. ). Furthermore, no study tried to characterize and detect QTLs for mesocotyl under deep-seeding condition in rice. Also, study to analyze the association between mesocotyl elongation and seedling emergence under various soil depth conditions is rare in crop including rice (Chung ; Alibu et al. ; Lu et al. ).This study attempted to (1) identify QTLs associated with mesocotyl and coleoptile elongation under the deep-seeding soil condition, and (2) to evaluate the effect of mesocotyl elongation on seedling emergence using deep-seeded CSSLs.In rice, seeding depth for direct seeding is an important factor for ensuring seedling establishment. Lee et al. () recommended a 2\u20133-cm sowing depth for  rice. When the seeding depth is >3\u00a0cm, seedling emergence was markedly delayed (Kawatei et al. ; Murai et al. ; Luo et al. ; Chung ; Alibu et al. ). Moreover, when seeds were sown deeper than 5\u00a0cm, the first leaf developed under the soil surface, and seedling establishment decreased remarkably (Chung ). Similar result was obtained in this study: seedling emergence speed and percentage at 7\u00a0cm and 10\u00a0cm soil depths were lower than those at the seeding depth of 3\u00a0cm in all the genotypes (Fig. ). Although 3-cm seeding depth might be optimal for direct seeding based on these results, many studies have used seeding depths of less than 3\u00a0cm, which caused seed desiccation and floating and seedling lodging (Lee et al. , Kumar and Ladha ). Genotype differences for seedling emergence and coleoptile length were more pronounced at 5\u00a0cm than at 3\u00a0cm seeding depth (Figs.  and ). The two parents and the 3 CSSLs showed >90% seedling emergence within 7\u00a0days when the seeding depth was 3\u00a0cm, whereas Kasalath and CSSL-5 showed >90% emergence and the other three lines reached a plateau at 80% emergence at 5\u00a0cm seeding depth (Fig. ). These results indicate that soil depth of 3\u20135\u00a0cm is appropriate for direct seeding of cultivars that show good mesocotyl elongation without affecting seedling establishment. Based on these findings, we selected 5\u00a0cm soil depth for screening seedling emergence traits under the deep-seeding condition.Mesocotyl and coleoptile lengths are directly related with seedling emergence in deep seeding and enhanced mesocotyl or coleoptile elongation is associated with better seedling emergence and establishment (Turner et al. ; Murai et al. ; Luo et al. ; Chung ; Alibu et al. ). Murai et al. () reported that a relationship exists between seedling emergence ability and lengths of leaf, mesocotyl, coleoptile, and leaf internode using dwarf lines under 7\u00a0cm soil depth. Chung () reported positive correlations between the rate of seedling emergence and mesocotyl or coleoptile length at 5\u00a0cm seeding depth by using 116 Korean weedy rice accessions. Alibu et al. () suggested that upland rice accessions with long mesocotyl showed better seedling emergence than those with long coleoptile under the dry direct-seeding condition. Lu et al. () showed that long mesocotyl rice accessions had higher emergence rate than that of the short one at 2 and 5-cm sowing depth. In addition to the seeding depth, in dry seeding, the lengths of mesocotyl and coleoptile can be affected by moisture content. Under submergence, coleoptile growth was stimulated, although no increase in mesocotyl length was observed (Takahashi ; Alibu et al. ). This suggests that mesocotyl and coleoptile elongation is important for seedling emergence depending on the environment. In our study, a positive correlation between mesocotyl length and seedling emergence was observed (Figs.  and ). Seedlings of Kasalath and CSSL-5 that have long mesocotyls emerged considerably faster than those of the other genotypes that have short mesocotyl at the 7 and 10\u00a0cm seeding depths (Figs.  and ). At 7\u00a0cm seeding depth, seedlings of Kasalath and CSSL-5 emerged over 70% in 7-8\u00a0days after sowing, whereas those of the other genotypes and Nipponbare showed 70% emergence at 10\u201311\u00a0days after sowing. This delayed seedling emergence of up to 3\u20134\u00a0days might be unfavorable for seedling establishment and competition with weeds.Evaluation of RDRS showed genotypic variation of mesocotyl and coleoptile length (Fig. ).  accessions tend to have shorter mesocotyls than , whereas  showed the longest coleoptiles among four groups. In this study, -I accessions had the longest mesocotyls among the four groups, followed by the tropical  group (Fig. ). These results are consistent with the previous findings (Suge ; Takahashi , Sato ).QTL analysis was conducted for mesocotyl and coleoptile length of seedlings sown at 5\u00a0cm soil depth. Measuring mesocotyl and coleoptile traits in the soil is difficult mainly due to the environmental influence and experimental errors. Several studies attempted to map QTLs for mesocotyl and coleoptile length by using the glass tube method (Katsuta-Seki et al. ), slant-board test (Redo\u00f1a and Mackill ), filter paper with distilled water (Huang et al. ), agar medium (Lee et al. ), and filter paper on agar medium (Xie et al. ). However, QTL analysis for mesocotyl or coleoptile elongation at the 5\u00a0cm seeding depth by using CSSLs in rice has not yet been performed. We detected 5 QTLs under field condition in two replicates. Interestingly, QTLs for mesocotyl elongation were commonly mapped to chromosomes 1 and 3 for different mapping populations and under different experimental conditions. The QTL  was located on the long arm of chromosome 3, and this QTL interval overlapped with regions of QTLs reported in previous studies (Katsuta-Seki et al. ; Redo\u00f1a and Mackill ; Cai and Morishima ; Cao et al. ; Huang et al. ). Using genome-wide association study (GWAS) association mapping, Wu et al. () and Lu et al. () identified some candidate genes controlling mesocotyl elongation on chromosome 1 and 3. Coleoptile QTLs were detected on chromosomes 3 and 6 by Redo\u00f1a and Mackill () and on chromosomes 5, 8, and 11 by Xie et al. (). On chromosomes 1 and 3, two QTLs for coleoptile length were identified under flooded soil condition (Zhou et al. ). Interestingly,  and  that were detected in the present study did not overlap with the coleoptile length QTLs that was reported previously; this could be attributed to the different genetic materials and test methods used.Among 54 CSSLs, two lines, CSSL-5 and CSSL-33 showed significantly longer mesocotyl than Nipponbare. CSSL-5 showed the highest emergence ratio (70%) followed by CSSL-33 (17%) (Fig. ). CSSL-5 and CSSL-33 shared  suggesting that  is a major QTL. In contrast,  is minor and affected by testing methods based on the finding that the  QTL was detected in the agar condition (Lee et al. ) but not in the soil condition in this study. However, CSSL-5 seedlings were shorter in mesocotyl length and lower in emergence rate than Kasalath. The results from this study indicate that the  QTL with a high additive effect has the potential to enhance early seedling emergence in combination with the  and other QTLs not detected. Also, information on the interaction among these QTLs will be necessary to understand the genetic control of mesocotyl length. These findings will contribute to our understanding of the genetic control of mesocotyl and coleoptile elongation, and seedling emergence and provides information on QTL regions associated with early seedling emergence for rice breeding programs.We showed that the variation in mesocotyl elongation is associated with seedling emergence under the deep-seeding condition. Genetic analysis using BIL and CSSL plants confirmed that two major QTLs,  and  for mesocotyl elongation have the potential to enhance early seedling emergence at 5\u00a0cm seeding depth. This is the first report to show the effect of two QTLs for mesocotyl elongation detected both in agar media and deep-seeding soil on seedling emergence using CSSLs using deep-seeding soil condition. (Lee et al. ). High-resolution mapping and cloning of these QTLs should be performed to understand the genetic control of mesocotyl elongation to ensure higher seedling emergence and to develop deep-seeding-tolerant rice cultivars for direct seeding by using marker-assisted selection."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0172-3", "title": "Sugary Endosperm is Modulated by ", "authors": ["Yunjoo Lee", "Min-Seon Choi", "Gileung Lee", "Su Jang", "Mi-Ra Yoon", "Backki Kim", "Rihua Piao", "Mi-Ok Woo", "Joong Hyoun Chin", "Hee-Jong Koh"], "publication": "Rice", "publication_date": "20 July 2017", "abstract": "Starch biosynthesis is one of the most important pathways that determine both grain quality and yield in rice (", "full_text": "Rice is the staple food for more than 3 billion people globally. The endosperm is an edible part of the rice grain, and has been one of the major targets for grain geneticists and breeders to enhance grain yield and quality. Endosperm development directly regulates grain formation at the grain filling stage. Mature rice endosperm contains starch, storage proteins, lipids, and other substances. Studies on starch have been an essential focus in rice research. Starch is the primary component that makes cereal crops economically and commercially important. Starch research is also becoming increasingly relevant for industrial and manufacturing applications.Rice starch is composed of amylose (linear \u03b1-1,4-polyglucans) and amylopectin (\u03b1-1,6-branched polyglucans). Amylopectin has a distinct fine structure called multiple cluster structure, and accounts for approximately 65\u201385% of storage starch (Nakamura ). Starch is synthesized by four enzyme classes, with multiple subunits in each class: ADP-glucose pyrophosphorylase (AGPase); starch synthase (SS); starch branching enzyme (BE); and starch debranching enzyme (DBE). Other enzymes, such as phosphorylase (Pho) and disproportionating enzyme, are thought to be involved in starch biosynthesis. BE and DBE have important roles in determining amylopectin structure. BE forms the \u03b1-1,6-glycosidic bonds of amylopectin, whereas DBE trims improper branches generated by BE (Fujita ).BE isoforms are classified into two groups, BEI (RBE1) and BEII. Cereals have two BEII isozymes, BEIIa (RBE4) and BEIIb (RBE3). These isoforms are classified according to the transferred amylopectin chain length. For example, BEII transfers shorter chains than BEI, and BEIIb transfers shorter chains than BEIIa, during extended incubations (Mizuno et al. ). The expression patterns of BE isoforms also differ. BEI and BEIIa transcripts have been localized in the endosperm and other tissues, whereas BEIIb is expressed only in the endosperm and reproductive tissues. In rice, BEIIa is expressed earlier than either BEIIb or BEI (Mizuno et al. ; Ohdan et al. ). Previous reports designated BEIIb-deficient mutants in maize and rice as  () mutants, in which the abundance of short amylopectin chains was reduced (Kim et al. ; Nishi et al. ). Other transgenic research about  gene was reported that the manipulation of BEIIb activity can generate various starch type rice, containing chalky and sugary endosperm (Tanaka et al. ). The RNA interference results demonstrated that reduced expression of BEIIa (SBEIIa) caused increase of amylose content in wheat endosperm (Regina et al. ). However, the specific functional role of BEIIa has not been elucidated in rice because the seed phenotypes of BEIIa-deficient mutants and wild-type plants are not significantly different (Fujita ).DBEs directly hydrolyze \u03b1-1,6-glycosidic linkages of \u03b1-polyglucans. DBEs are classified into two types in higher plants, Isoamylase (ISA1, ISA2, and ISA3) and Pullulanase (PUL). According to Fujita (), Isoamylase1 (ISA1)-deficient mutants () were called as  mutants in rice () and maize (). These mutants have a defect in the amylopectin cluster structure, which results in the accumulation of a polymeric water-soluble polysaccharide (WSP) termed phytoglycogen, and a reduction in the starch content (James et al. ). There are various  mutant types, EM series, reported by Japanese group (Nakamura et al. ; Nakamura et al. ; Wong et al. ). The  locus in rice is located on chromosome 8 (Fujita et al. ; Yano et al. ). In transgenic  rice expressing the wheat  gene, phytoglycogen synthesis is substantially replaced by starch biosynthesis in the endosperm (Kubo et al. ). This result implies that ISA1 is essential for amylopectin crystallinity and biosynthesis in both rice and wheat. In maize, double mutant defective in both ISA2 and SSIII generated water-soluble glucans in the mutant endosperm, although single mutants of either ISA2 or SSIII could synthesize normal amylopectin (Lin et al. ). By contrast, the contribution of PUL for amylopectin trimming was much smaller than that of ISA1, and PUL function partially overlaps with that of ISA1 (Fujita et al. ).Since rice grains of  mutants primarily contain water-soluble carbohydrates instead of starch even after maturity, they have the potential value in breeding programs for diversified qualities and commercial uses. However, they have not been used in practice due to the severely wrinkled grains and subsequent problems in milling. We developed a mild-type sugary mutant in rice,  (), which displayed an intermediate phenotype between the  mutant and wild type. Grains of the  mutant have better quality for subsequent processing and higher yield than . In addition, palatability, protein and amylose content which are crucial for breeding were increased in  mutant (Yoon et al. ;  was renamed as  to avoid confusion with Nakagami et al. ()). Therefore,  mutants could be valuable for practical applications and nutritional aspects. This study performed map-based cloning to identify the genes responsible for the  phenotype. Our results not only identified a potential resource for utilizing the sugary endosperm mutant for commercial benefit, but also may provide a new insight into starch biosynthesis.Genetic mapping of the  rice mutant was used to identify and isolate two recessive genes,  and . The  mutant has a mild sugary phenotype, which preferentially accumulates desirable sugar compositions of non-starch polysaccharide rather than starch, and is more commercially viable than the  mutant because it does not display excessive wrinkling, which interferes with milling (Koh and Heu ; Yoon et al. ). Segregation ratios of the F population showed that the  phenotype was controlled by complementary interactions between  and . We demonstrated that  and  were associated with the genetic modifications that were responsible for the sugary endosperm phenotype. Although a single mutation in  did not affect endosperm phenotype, the mutation in  moderately recovered the sugary endosperm from the severe wrinkling caused by . Therefore,  mutant seed maintains a sugary phenotype, but the seed quality is superior (less wrinkled) than that of .There have been several reported mutants and transgenic rice related to sugary-type endosperm. Among them, severe sugary-type endosperm mutants, such as EM914 (Nakamura et al. ; Wong et al. ), #1\u20131 (Tanaka et al. ), and  suppression and  over-expression transgenic lines (Utsumi et al. ), were similar to the  mutant used in this study. Of them, phenotype of EM914 was governed by mutated  while #1\u20131 and  over-expression line had different genes than . It is interesting that a mild sugary mutant, which was reported as a variation of  mutant by Nakamura et al. (), had the similar phenotype to  mutant although only  locus was involved in the  mutant. The reason for the phenotypic similarity between  mutant by Nakamura et al. () and  mutant in this study remains to be comparatively studied. Recently, a rice novel endosperm mutant, named as , was reported (Nakagami et al. ), in which the results on the activity of BE in the  mutant was unlike the  mutant, indicating that the  mutant was different from the  mutant.Nakamura () reported that inhibition of BEIIa activity caused low levels of short amylopectin chains with degree of polymerization (DP) \u226410 in rice leaf sheath, in which BEIIb is not expressed; however, the BEIIa-deficient mutant does not exhibit a significant change in amylopectin chain length profile in rice endosperm. Because a direct function of the  could not have been elucidated yet, we suggest that the amino acid substitutions in  and  changed the protein complex or enzyme interaction involved in starch biosynthesis, and might be responsible for the  phenotype affecting amylopectin structure. Future studies will perform enzymatic analyses to test this hypothesis.Previous studies investigated possible interactions between ISA and other enzymes. The debranching enzyme PUL was related to the sugary phenotype (Fujita et al. ). Amylose content, seed morphology, and starch granules of  mutant lines were essentially the same as those of wild-type plants. By contrast, double  and  mutant lines contained higher levels of WSPs and had shorter amylopectin chains with DP \u22647 in the endosperm compared with the  parents, indicating that PUL can partly compensate for starch biosynthesis. The absence of ISA activity primarily affected the sugary endosperm phenotype regardless of the presence of PUL activity. This result was very similar using the  mutant; however, no differences in  sequences were identified in the Hwacheong wild type and the  mutant.\n                         () encodes a CBM48 domain-containing protein (Peng et al. ). FLO6 may act as a starch-binding protein interacting with ISA1, and may be a bridge between ISA1 and starch during starch biosynthesis. ISA1 may have interacting factors that mediate starch binding, and interacting enzymes that have not yet been elucidated. Previous research evaluated protein-protein interactions of starch biosynthetic enzymes. Crofts et al. () reviewed that co-immunoprecipitation analysis revealed the following associations in rice: BEIIa-BEIIb, BEIIa-BEI, BEIIa-Pho1, BEIIa-SSI, and BEIIa-SSIIIa. The BEIIa-SSI interaction was also identified in wheat and maize (Tetlow et al. ). These results suggested that some isozymes involved in starch biosynthesis in rice formed active protein complexes. These combined results suggest a possible mechanism of BEIIa function in rice endosperm.Phenotypic variation is a critical consideration for phenotypic analysis of sugary endosperm in  populations because of environmental effects. Satoh et al. () evaluated the effect of growth temperature on the frequency of various grain phenotypes and the extent of starch accumulation in the wild type and mutant, and reported that starch accumulation in the  () mutant endosperm was affected by temperature. Similarly, the seed phenotypes of  and  mutants differed slightly between plants grown in the field and those grown in the green house (data not shown). To reduce this phenotypic variation, all seeds from a whole main panicle of F and F plants grown in the field were used for genetic mapping. The mutants and wild-type seeds were grown together under the same conditions and prepared for phenotypic analysis at the same time. Future studies should assess the effects of environmental factors, especially temperature, on phenotypic variation.In this study, we propose that mutated  plays a role in restoring the severely wrinkled sugary phenotype caused by  in rice endosperm, although  mutation alone did not result in a significant phenotypic change. The observed complementary interaction between  and  provides novel insight into the roles of starch biosynthesis enzymes and their interactions. Our result can facilitate the breeding of functional rice cultivars with special nutritional qualities, and might be applicable to endosperm modification in other cereal crops.Grains of  mutant have less wrinkled feature, which make it more commercially feasible by easy polishing in processing. Here, we report the cloning of a new sugary gene in rice, which controls the thickness of sugary endosperm grains. Genetic analysis revealed that phenotype of  mutant was controlled by a complementary interaction of two recessive genes,  and . Complementation test demonstrated that sugary endosperm was modulated by . These results may facilitate the breeding of sugary endosperm rice for commercial use and will be helpful to enlarge our understanding on starch biosynthesis in rice endosperm."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0179-9", "title": "Dissecting combining ability effect in a rice NCII-III population provides insights into heterosis in ", "authors": ["Hao Zhou", "Duo Xia", "Jing Zeng", "Gonghao Jiang", "Yuqing He"], "publication": "Rice", "publication_date": "29 August 2017", "abstract": "Combining ability is a measure for selecting elite parents that make the highest contributions to hybrid performance. However, the genetic bases of combining ability and how they contributed to heterosis is seldomly known.", "full_text": "The phenomenon heterosis has been applied to rice breeding for improving grain yield since the early 1970s in China (Cheng et al. ; Darwin ; Li et al. ). To breed an ideal cross with the highest grain yield and great stress resistance is the ultimate goal of hybrid rice breeders. As parents with excellent agronomic traits do not always pass those traits on to their progenies, breeders often test the potential of a selfing line by cross it to several other lines. The potential for creating high heterosis progenies of an inbred line is called combining ability, a concept proposed by Sprague and Tatum and has been widely used in cross breeding for elite parent selection (Comstock and Robinson ; Griffing ; Sprague and Tatum ). Parent lines with high general combining ability (GCA) in grain yield and resistance to pests and diseases are more likely to form hybrids with satisfactory performance. On the other hand, some parent lines of general GCA also is able to form excellent hybrids and this is caused by another special effect called special combining ability (SCA). The both effects of GCA and SCA are able to create hybrids with high heterosis but the genetic bases of these effects are largely unknown.The first attempt to unveil the genetic basis of combining ability was done by Griffing (). He proposed the methods of using diallel-cross to dissect the genetic variance into additive variance and non-additive variance, and estimated the GCA and SCA effects. His study provide theory basis for estimating genetic variance and combining ability effect in various mating design, including complete or incomplete diallel (Griffing ), North Carolina design (Comstock et al. ), and top crossing (Hill et al. ). Later study conducted transcriptome analysis and molecular markers to reveal the relationship between combining ability and genetic diversity (Ajmone et al. ; Bernardo ; Frisch et al. ; Stupar et al. ). Those studies revealed that the genetic distance between two parents are positively correlated with combining ability and hybrid performance.Recently, with the development of molecular marker, linkage analysis has been used to dissect the GCA effects into quantitative trait locus (QTL) (Lv et al. ; Qi ; Qu ). Liu et al. () utilized a NCII design, performed linkage analyses to GCA, and confirmed that  and  (Koo et al. ; Xue et al. ) are major genes for GCA of heading date, plant height and spikelet per panicle in rice. Though a few QTLs contributed to combining ability has been indentified, the genetic bases of combining ability are still not clear and how they contributed to hybrid performance was totally unknown. In this study, we developed a both NCII and NCIII population (see Methods) derived from an  cross, performed linkage analysis to both GCA and SCA effects, and explored how QTLs of combining ability contributed to hybrid performance of hybrid rice.\n                         cross were reported to has higher heterosis than  cross and  cross (Yuan ). Although  generally has better yield performance,  carries many beneficial alleles that are uncommon in  gene pools (for example, ,  and ) (Fujita et al. ; Huang et al. ; Jiao et al. ). Positive partial dominance and overdominance effects have served as the major causes of heterosis in  F hybrids (Huang et al. ). Matting design in classic genetics helps to dissect the contributions of different genetic effects and QTL mapping in molecular genetics helps to identify important loci for hybrid performance (George ; Zeng ). We combined both methods of classic and molecular genetics to detect alleles of additive and non-additive effects contribute to inter-subspecies hybrid performance. This new method is better than simply perform QTL mapping to hybrid performance, as we identified more loci with higher significances and more clear effects (Additional files , , and : Tables S1-S3). For example,  is the major locus with additive effects (a\u00a0=\u00a0\u22127.54) on plant height of hybrids and  is the major locus with overdominace effect on grain yield of hybrids (Sasaki et al. ; Yan et al. ). The same locus may have different effect on different traits as  is also a major locus with additive effect (a\u00a0=\u00a0\u22129.42) on heading date of hybrids. The  (Yang et al. ) and  are two loci conferring the wide-compatibility between  and . The two QTLs will accelerate the development of intersubspecific hybrids with high heterosis.Prediction of hybrid performance is important in hybrid rice breeding. Heterosis and combining ability are two main indexes for hybrid performance. In this study, we dissect the hybrid performance of ZS97\u00a0\u00d7\u00a0WYG and the NCII population in to GCA and SCA effects (Table ). Among 11 agronomic traits, PH, HD, KGW and PL were mainly controlled by GCA effects, and major QTLs were detected for these traits (, ,  and  (Che et al. ; Huang et al. ; Sasaki et al. ; Yan et al. )) (Figs.  and ). On the contrary, GF, YD and SS were mainly controlled by SCA effects, and three QTLs (,  and ) contributed to these traits (Fig. ). With the knowledge of these information, we will be able to predict the hybrid performance of an  cross.We dissected the effects of GCA and SCA in an  cross and identified lots of known and unknown QTLs for them. Among these QTLs, ,  and  largely contributed to the hybrid grain yield of  cross. These results provide insights into the genetic bases of combining ability and heterosis and will provide valuable information for the improvements of  hybrid breeding."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0176-z", "title": "Identification of resistant germplasm containing novel resistance genes at or tightly linked to the ", "authors": ["Gui Xiao", "Frances Nikki Borja", "Ramil Mauleon", "Jonas Padilla", "Mary Jeanie Telebanco-Yanoria", "Jianxia Yang", "Guodong Lu", "Maribel Dionisio-Sese", "Bo Zhou"], "publication": "Rice", "publication_date": "4 August 2017", "abstract": "The rice ", "full_text": "Rice blast, a devastating rice disease caused by the fungal pathogen  is affecting rice production across all rice-growing areas worldwide (Ashkani et al. ). Introgression of resistance () genes into rice cultivars remains the most economical and effective approach for rice blast disease management (Ashkani et al. ; Tanweer et al. ). The bottleneck of this approach is that, after an individual  gene is isolated and deployed in the varieties, it can be overcome in a short time (usually in 2\u20133\u00a0years) by the emergence of a compatible pathogen because of the high level of avirulence () effector variability in the pathogen (Skamnioti and Gurr ; Valent and Khang, ). Therefore, it is essential to enrich and diversify the -gene pool by extensive and continuous exploration of novel  genes or alleles in diverse germplasm for the choice of effective  genes in a rice resistance breeding program.To date, more than 100 rice blast  genes and over 350 resistance quantitative trait loci (QTLs) have been genetically identified (Tanweer et al. ). Of the 100  genes, 25 were molecularly characterized (Liu et al. ; Fukuoka et al. ; Su et al. ; Ma et al. ; Chen et al. ). Most of them encode proteins having nucleotide binding site (NBS) and leucine-rich repeat (LRR) domains. It is evident that many NBS-LRR-type  genes are organized as alleles located at the same loci (Leung et al. ). For example, at least eight  alleles were molecularly characterized at the  locus, which is located on the distal end of the long arm of chromosome 11 (Chen et al. ; Campbell et al. ; Ashikawa et al. ; Yuan et al. ; Zhai et al. ; Hua et al. ; Ashikawa et al. ; Zhai et al. ). It is worth noting that many -gene alleles are extremely sequence-related to each other. For example, , one of two NBS-LRR genes at the  locus in Kusabue, differs from its allele  in Kanto51 by only four nucleotides confined in the region encoding the NBS domain (Zhai et al. ; Ashikawa et al. ). Other NBS-LRR genes,  and , are even identical to each other (Zhai et al. ; Ashikawa et al. ). A similar scenario was also observed at the  locus (Takahashi et al. ).Several approaches were employed for the identification of novel  genes or alleles of known  loci, such as map-based cloning, allele mining and genome-wide association study (GWAS). Recently, 97 loci associated with rice blast resistance were identified using the GWAS approach (Kang et al. ). By combining the RNA interference (RNAi) approach, the candidate gene in LABR_64 corresponding to resistance to all five isolates was validated and confirmed to be an allele of . In addition to the traditional gene-linked markers (Wang et al. ; Fjellstrom et al. ; Hayashi et al. ; Thakur et al. ), gene-specific or diagnostic markers were recently reported for the identification of novel blast  genes or alleles in diverse germplasm, such as  (Ramkumar et al. ),  (Costanzo and Jia, ) and  (Chen et al. ).The  locus located on the short arm of chromosome 6 proximal to the centromere was reported to harbor at least eight functional alleles from different donor varieties (Su et al. ; Qu et al. ; Zhou et al. ; Deng et al. ; Jeung et al. ; Wang et al. ; Jiang et al. ). Analyses of genetic diversity of the  homologues in cultivar and wild rice species revealed that the  homologues were subjected to strong diversifying selection (Zhou et al. ; Dai et al. ; Liu et al. ). Molecular characterization of , ,  and  revealed that a limited number of sequence variations disproportionately confined within the LRR regions of the encoded  proteins mainly determined the distinct recognition specificities of these alleles to different sets of rice blast isolates (Su et al. ; Zhou et al. ; Qu et al. ). Moreover, these alleles are each embedded within a cluster containing multiple sequence-related paralogues at the locus in the respective donor varieties (Su et al. ; Zhou et al. ; Qu et al. ). The feature of the complex organization of highly sequence-related homologues at the  locus makes it difficult to develop gene-specific molecular markers for the diagnosis and identification of known and novel alleles from diverse germplasm. Intriguingly, contrasting to the absence in susceptible rice varieties, the  alleles are exclusively present in the resistant haplotypes (Su et al. ; Zhou et al. ), prompting us an assumption that the  sequence could be targeted for developing markers for allele mining at the  locus. In this study, we aim to develop a resistant haplotype specific marker at the  locus and apply it for the identification of novel resistant germplasm containing new  alleles. The development of introgression lines via marker-assisted backcrossing (MABC) enabling the analysis of resistance spectra of these novel introgression lines against a wide collection of rice blast isolates in the Philippines will be also described.We identified three resistance germplasm containing novel  genes at or tightly linked to the  locus which conferring broad-spectrum resistance against rice blast. The marker Pi2/9-RH which developed from the conserved 5\u2032 portion of the  sequence could be widely used as a diagnostic marker for the quick identification of resistance donors containing functional  alleles or unknown linked  genes. The development of three new introgression lines containing the  introgression segment may play an important role in disease assessment and rice blast resistance breeding."},
{"url": "https://thericejournal.springeropen.com/articles/10.1186/s12284-017-0181-2", "title": "Large-scale deployment of a rice 6\u00a0K SNP array for genetics and breeding applications", "authors": ["Michael J. Thomson", "\u2020", "Namrata Singh", "\u2020", "Maria S. Dwiyanti", "\u2020", "Diane R. Wang", "Mark H. Wright", "Francisco Agosto Perez", "Genevieve DeClerck", "Joong Hyoun Chin", "Geraldine A. Malitic-Layaoen", "Venice Margarette Juanillas", "Christine J. Dilla-Ermita", "Ramil Mauleon", "Tobias Kretzschmar", "Susan R. McCouch"], "publication": "Rice", "publication_date": "30 August 2017", "abstract": "Fixed arrays of single nucleotide polymorphism (SNP) markers have advantages over reduced representation sequencing in their ease of data analysis, consistently higher call rates, and rapid turnaround times. A 6\u00a0K SNP array represents a cost-benefit \u201csweet spot\u201d for routine genetics and breeding applications in rice. Selection of informative SNPs across species and subpopulations during chip design is essential to obtain useful polymorphism rates for target germplasm groups. This paper summarizes results from large-scale deployment of an Illumina 6\u00a0K SNP array for rice.", "full_text": "Future challenges to sustainably produce food for 9.5 billion people by 2050 using less land and fewer inputs will require higher yields in intensive systems under increasingly variable environments. Modern plant breeding and genetic research programs aim to utilize the latest technologies to accelerate the annual rate of genetic gain to keep up with rice demand. High-throughput molecular marker techniques have enabled routine, low-cost genotyping for both targeted and genome-wide approaches. Targeted methods, where a few markers (<100) are used to genotype a large number of samples, provide an efficient strategy for forward selection of major genes in breeding programs. Genome-wide methods, including fixed arrays and next generation sequencing, provide marker densities appropriate for genome-wide association studies, QTL mapping, diversity analysis, DNA fingerprinting, impact assessment studies, and breeding applications such as genomic selection (Thomson ; Varshney et al. ).Single nucleotide polymorphisms (SNPs) are the markers of choice for most high throughput genotyping applications because they are ubiquitous in eukaryotic genomes, cost-effective to assay using automated platforms, and because allele calling, data analysis and data-basing are straightforward due to their biallelic nature. A number of medium- or high-resolution SNP arrays in rice have been deployed, primarily for genome-wide association studies, including a 44\u00a0K SNP chip (Zhao et al. ), 50\u00a0K SNP chips (Chen et al. b; Singh et al. ), and the 700\u00a0K igh-ensity ice rray (HDRA, McCouch et al. ). These arrays provide automated platforms to dissect phenotype-genotype associations, while at the same time offering valuable datasets that can be used to validate high-quality SNP markers that are informative within and between key germplasm groups. The subsequent development of lower resolution detection platforms, including KASP, TaqMan, and Fluidigm that target individual SNPs, and the low-density SNP arrays, have made use of the wealth of information published from the higher-density arrays to extract informative SNPs and invariant SNP flanking sequences that convert well to other assays (McCouch et al. ; Tung et al. ; Chen et al. a).Historically, sets of 384, 768, or 1536 SNP markers were used for diversity analysis, QTL mapping, marker-assisted backcrossing, specialized genetic stock development, and pedigree confirmation among breeding lines in rice (Nagasaki et al. ; Zhao et al. ; Chen et al. ; Thomson et al. ; Ye et al. ; Rahman et al. ; Shah et al. ). Despite their utility across a variety of applications, the limited numbers of SNP markers in each assay required the development of multiple SNP sets in order to provide a high enough resolution of polymorphic markers for use with specific germplasm groups.Combining SNP sets into larger arrays helps increase the number of potential users per array, which lowers cost while providing increased resolution across a diversity of germplasm. Previously, an Illumina Infinium 6\u00a0K array in rice (RICE6K) was developed in Wuhan, China to provide polymorphic SNPs within and between the  and  subgroups for applications in background selection, mapping population genotyping, variety identification and purity tests, and bulk segregant analysis (Yu et al. ). With rapid genotyping turn-around times, ease of allele calling and data analysis provided by this and other 6\u00a0K SNP chips, breeders and geneticists can interact more directly and rapidly with the data and incorporate genotyping results in their programs without dependence upon bioinformatics specialists.The primary alternative to fixed SNP arrays is reduced representation next-generation sequencing, such as restriction-site associated DNA (RAD) sequencing or genotyping by sequencing (GBS). These methods provide large numbers of genome-wide SNP markers at a low cost (Baird et al. ; Elshire et al. ; Peterson et al. ). While RAD-Seq and GBS have been very successful for certain applications, several limitations have become apparent as adoption has widened. In addition to reliance on complex protocols for library preparation, the requirement to multiplex hundreds of samples to minimize cost, and long delays in obtaining sequencing output, a challenge for many groups has been the costly bioinformatics infrastructure needed to support downstream analytical pipelines for accurate allele-calling. Although imputation techniques enable researchers to fill in gaps in data sets, GBS approaches typically suffer from large amounts of missing data, making it challenging to accurately call heterozygotes. More recently, core facilities are faced with the challenge of dealing with licensing of these technologies due to the KeyGene patent for Sequence-Based Genotyping (Truong et al. ; US Patent 8,815,512).At the other end of the spectrum, targeted simplex SNP approaches, such as TaqMan and KASP-based genotyping, offer an alternative to fixed arrays for applications requiring a few, high-value markers across very large populations (Eathington et al. ; He et al. ; Semagn et al. ). These assays can be cost-effective for large sample sizes (1000\u00a0s-10,000\u00a0s) and are ideal when trait-predictive SNP markers are available for selection of large effect genes in breeding programs; however, their cost advantage is lost for applications involving small numbers of lines or requiring more than 100\u2013200 genome-wide SNP markers. Thus, while no single genotyping system is ideal for all applications, the wide range of available genotyping platforms now offer solutions that can provide an optimal balance to meet the needs of different users, taking into account cost per sample, marker resolution, turnaround time, allele call rates, and data analysis requirements (Thomson ).To replace multiple rice 384-SNP sets and provide a high-quality set of informative SNP markers for genetics and breeding applications, an Illumina Infinium 6\u00a0K SNP chip for rice, referred to as the Cornell_6K_Array_Infinium_Rice (C6AIR), was designed for use at both Cornell University and the International Rice Research Institute (IRRI). The design of the C6AIR includes 1571 SNP markers from legacy BeadXpress SNP sets and 4429 SNPs selected from whole genome re-sequencing data to be polymorphic within and between the target germplasm groups and mapping parents. This paper describes the efficacy of the C6AIR for QTL mapping, genetic diversity analysis, SNP fingerprinting of breeding lines, tracking of introgressions, and checking for recovery of recurrent parent background during marker-assisted backcrossing. Subsequently, an improved second-generation array was developed by removing SNPs that performed poorly on the 6\u00a0K array and increasing the number of bead types to just over 7000. This new 7\u00a0K rice array, referred to as the C7AIR, provides continuity with data sets from the C6AIR while increasing the number of high quality SNP loci for future use in genetics and breeding applications.The C6AIR has proven to be an effective genotyping system for rice diversity analysis, QTL mapping, tracking introgressions, genetic stock development, and fingerprinting studies at both Cornell University and at the Genotyping Services Lab at IRRI over the last 4\u00a0years. Over 40,000 samples were run successfully, providing genotyping data for a large number of genetics, breeding and impact assessment projects of importance to people throughout the world. Arrays such as the C6AIR provide a relatively small but sufficient number of SNPs so that data can be handled without massive bioinformatics pipelines, while at the same time providing high enough resolution for most genetics and breeding applications. The SNPs on the C6AIR consistently have high call rates, including for heterozygotes, facilitating data management and integration of genotyping data across runs and populations. Because the C6AIR can readily distinguish the five major subgroups of  as well as , it is especially useful for fingerprinting studies and, as summarized in this paper, finds a broad range of applications in rice breeding programs. The improved second generation C7AIR offers all of the benefits of the 6\u00a0K array with additional high quality SNP markers. These resources provide the rice community with rapid, cost-effective tools for low-density, genome-wide SNP genotyping across a wide range of rice germplasm, and with the potential to dramatically increase resolution via imputation when integrated with other publicly available high-density rice genome datasets."},
{"url": "https://security-informatics.springeropen.com/articles/10.1186/s13388-016-0028-1", "title": "Analytics for characterising and measuring the naturalness of online personae", "authors": ["Jason R. C. Nurse", "Arnau Erola", "Thomas Gibson-Robinson", "Michael Goldsmith", "Sadie Creese"], "publication": "Security Informatics", "publication_date": "8 September 2016", "abstract": "Currently 40\u00a0% of the world\u2019s population, around 3 billion users, are online using cyberspace for everything from work to pleasure. While there are numerous benefits accompanying this medium, the Internet is not without its perils. In this case study article, we focus specifically on the challenge of fake (or unnatural) online identities, such as those used to defraud people and organisations, with the aim of exploring an approach to detect them.", "full_text": "Today more than ever, people across the world are exploiting the Internet for work and pleasure, and are utilising an increasing variety of devices and services to do so\u00a0[]. Exploitation of cyberspace results in both conscious information-sharing and publication (of both personal and corporate variety) and, inevitably, the creation of persistent data that many users may be unaware of (perhaps as metadata or as old data thought to be removed or put out of reach). While there are undoubtedly many benefits to our interaction in cyberspace, the quantity of threats, risks and general peril are constantly growing\u00a0[], with data breaches, hacks and identity-fraud almost commonplace.In this case study-based article, we concentrate on the problem of fake online identities, and their increasing use to manipulate, deceive and defraud people and organisations\u00a0[\u2013]. Reflecting on the literature, there has been considerable work in the space of detecting fake accounts and bots. Cao et al., for instance, propose a technique using social-graph properties to rank users on a site according to their likelihood of being fake\u00a0[]. They later extend this consideration to explore the use of clustering in identifying groups of malicious accounts (under the assumption that they have very similar properties and actions, e.g., posting and uploading behaviour)\u00a0[].In Viswanath et al. [], an unsupervised machine-learning approach for detecting anomalous user behaviour in social networks is introduced. Through experimentation on Facebook profiles, the authors demonstrate the use of their clustering technique (based mainly on \u2018like\u2019 rates and activities) in identifying fake and compromised user accounts. In the spam and bot-detection domain, Fong et al.\u00a0[] and many others (e.g., \u00a0[, ]) also attempt to tackle the problem of fake profiles, typically using a mixture of techniques, which often apply a priori knowledge (e.g., bots ephemeral nature or posting habits) to detect fake accounts. We have also engaged in research in this domain by using machine learning to explore which factors may be the most important in making automated text (produced by bots) convincing\u00a0[].The novelty of our work as compared to the existing literature is the in-depth analysis of a complete online persona; this includes all of its facets and how it is used across multiple sites. We posit that through a detailed characterisation of how real personae portray themselves and act online, an approach can be crafted to detect fake or anomalous identities, particularly, those somewhat carefully maintained and used for malevolent purposes. As a basis for this approach, we draw on a comprehensive model of identity developed in our previous research\u00a0[, ]. This allows us to characterise identities, from the attributes present and the inferences that can be made from them (e.g., inferring a person\u2019s name from their email-address), to the overall existence of an identity across several sites. The ability to comprehensively model an identity can be an extremely effective tool in understanding what is natural behaviour online, and consequently, what may be an unnatural and potentially harmful persona.In what follows, we present our approach and the case study used to examine it, before then critically reflecting on our findings regarding the approach\u2019s utility. Specifically, we first consider naturalness as a concept, what it means for a persona to be natural and how naturalness may be usefully characterised. Next we detail the analytics (i.e., intervention) that we propose for measuring the naturalness of an unknown online persona. We then present the results from a case study experiment conducted to explore our approach. Finally, we reflect on these results and outline ways to evolve the analytics, before concluding the article.We begin our work in this section by introducing the proposed approach to characterise and measure naturalness. This is then followed by a definition of our case study and presentation of results.In this research, our aim was to develop a method that could characterise the natural online presence of a type of individual and analytics to measure whether personae of unknown origin might be considered as natural. In general, from our case study analysis, we found that our approach to conceptualise an individual\u2019s presence could be regarded as successful. In terms of developing effective analytics for naturalness however, improvements in the approach are required. Ultimately, this meant that we were not able to use the measure as is it stands to distinguish natural from unnatural (or differently-typed) personae. Consequently, this has affected the planned subsequent use of the approach to detect fake online persona. Below, we reflect on some of the main reasons why this might not have been possible.As the number of organisations and individuals online increases, cyberspace becomes an even more attractive area for malevolent parties, armed with various schemes and tricks meant to deceive others. In this paper, we have presented and explored an approach that is ultimately targeted at enabling us to better distinguish between real and fake (or malicious) online identities. This approach focuses on allowing an enhanced understanding of online personae, while also facilitating the characterisation of a natural online presence and the measurement of conformity to such a presence.Reflecting on the case study-based assessment of the approach that was conducted, there were several areas where our approach performed well, but also many others where further improvement is required before it could be applied to judge fake personae. These areas will be the focus of our future work, and include: further assessments of the criteria through which naturalness is defined, and refined analytics and combinatorics to measure a persona\u2019s naturalness. Lastly, we are in the process of exploring the full application of clustering approaches using complete online personae (i.e., data from multiple sites) as a means to identify naturally occurring personae types in large datasets. This could be used to complement our existing approach and provide more insight into the initial dataset from which naturalness (via naturalness templates for instance) would be defined."},
{"url": "https://slejournal.springeropen.com/articles/10.1186/s40561-016-0025-3", "title": "Non-intrusive assessment of learners\u2019 prior knowledge in dialogue-based intelligent tutoring systems", "authors": ["Vasile Rus", "Dan \u015etef\u0103nescu"], "publication": "Smart Learning Environments", "publication_date": "3 February 2016", "abstract": "This article describes a study whose goal was to assess students\u2019 prior knowledge level with respect to a target domain based solely on characteristics of the natural language interaction between students and conversational Intelligent Tutoring Systems (ITSs). We report results on data collected from two conversational ITSs: a micro-adaptive-only ITS and a fully-adaptive (micro- and macro-adaptive) ITS. These two ITSs are in fact different versions of the state-of-the-art conversational ITS DeepTutor (", "full_text": "Assessment is a key element in education in general and in Intelligent Tutoring Systems (ITSs; ()) in particular because fully adaptive tutoring presupposes accurate assessment (; ). Indeed, a necessary step towards instruction adaptation is assessing students\u2019 knowledge state such that appropriate instructional tasks (macro-adaptation) are selected and appropriate scaffolding is offered while students are working on a task (micro-adaptation or within-task adaptation).We focus in this article on assessing students\u2019 prior knowledge in dialogue-based ITSs based on characteristics of the tutorial dialogue interaction between students and such systems. Assessing students\u2019 other states, e.g. affective state, that are important for learning and therefore important to further adapt instruction to each individual learner is beyond the scope of this work.When students start interacting with an ITS, their prior knowledge with respect to the target domain is typically assessed using a multiple choice pre-test although other forms of assessment such as open answer problem solving are sometimes used. The pre-test serves two purposes: enabling macro-adaptation in ITSs, i.e. the selection of appropriate instructional tasks for a student based on student\u2019s knowledge state before the tutoring session starts, and, when paired with a post-test, establishing a baseline from which the student progress is gauged by computing learning gains (post- minus pre-test score). This widely used pre-test/post-test experimental framework is often necessary in order to infer whether the treatment was effective relative to the control.While the role of a pre-test is important for assessing students\u2019 prior knowledge, there are several challenges with having a pre-test. First, a pre-test (as well as the paired post-test) takes up a non-trivial amount of time. This is particularly true for experiments consisting of only one session in which case the pre-test and post-test may take up to half the time of the full experiment. For instance, a 2-h experiment could be broken down into three parts: 30 min for pre-test, 1 h of actual interaction with an ITS, and 30 min for post-test. Altogether, in this particular case the pre-test and post-test take 1 h which is half the time of the whole experiment.More worryingly is the fact that in such experiments the pre-test may have a tiring effect on students. By the time students reach the post-test many of them will be so tired they will underperform even if they learned something during the actual training, thus, jeopardizing the whole experiment. For instance, in one of our experiments about 30 % of the subjects simply randomly picked one of the choices for the multiple-choice questions in the post-test without even reading the question. We observed this behavior by analyzing the time students took to pick their choice after they were shown a question on screen. About a third of the students took on average less than 5 s per question which is not even enough to read the text of the question. By comparison, the same students took on average 36 s to respond to similar questions in the pre-test. By eliminating the pre-test in the above illustrative experiment, we can reduce the overall experimental time to 1 h and 30 min, thus reducing tiring effects. By eliminating both the pre-test and post-test, we can further reduce the total experiment time.Additionally, many times there is a disconnect between the pre- and post-test questions and the actual learning tasks and process. To overcome this challenge,  argue for a shift towards emphasizing performance-based assessment which is about evaluating students\u2019 skills and knowledge while applying them in authentic contexts. For instance, reading instructions in a role-playing game allows assessing students\u2019 reading comprehension skills (). Using explicit tests in such contexts would interfere with the main task and are therefore not recommended. They advocate for the use of stealth assessment while students engage in a particular activity. Like in stealth assessment, we advocate here for non-intrusive assessment during problem solving in dialogue-based ITSs. To this end, the goal of our work presented here was to investigate to what degree we can automatically infer students\u2019 knowledge level directly from their performance while engaging in problem solving with the help of an ITS.Eliminating the need for learners to go through a standard pre-test and a post-test saves time for more training, eliminates tiring effects and testing anxieties, and ultimately provides a more accurate picture of students\u2019 capabilities as the assessment is conducted in context, i.e. while they solve problems in our case. In particular, we investigate how well we can predict students\u2019 prior knowledge, as measured by a standard multiple-choice pre-test, based on characteristics of the tutorial dialogue interaction with the hope that if the predictions are close enough we can do without the pre-test in the future. We are also interested in finding out the minimum tutorial dialogue interaction that would yield an accurate estimate of students\u2019 prior knowledge.We would like to emphasize that we are not arguing for a complete elimination of explicit assessments such as multiple-choice tests which have their own advantages for learning such as testing effects (the memory retrieval processes activated during testing benefit long-term memory of the target material; ()). Rather, we propose to investigate to what extent we can measure students\u2019 knowledge level from interaction characteristics such that, when needed, we can employ this kind of non-intrusive assessment.We conducted our research on data collected from an experiment with high-school students using the state-of-the-art conversational computer tutor DeepTutor (). As mentioned, our goal was to find interaction features that are good predictors of students\u2019 pre-test scores and to create prediction models that would be as useful as the multiple choice pre-tests in measuring students\u2019 prior knowledge. The best model we found can predict students\u2019 prior knowledge, as measured by a summative pre-test, with =0.949 and adjusted -square =0.833. We also determined the minimum dialogue length which is necessary to be able to make the best predictions.The remainder of the article is organized as follows: Section \u201c\u201d briefly discusses previous relevant work while Section \u201c\u201d presents a brief overview of the computer tutor that provided the context for our experimental analysis. The following section decribes the approach. The data is presented in the next section which is followed by the \u201c\u201d section offering details about the various prediction models and the results we obtained from these models. The article ends with a section on conclusions and further work.The most directly relevant previous work to ours is by  who studied the problem of inferring students\u2019 prior knowledge based on prior knowledge activation (PKA) paragraphs elicited from students. PKAs were generated by students as part of a meta-cognitive training program. Lintean and colleagues employed a myriad of methods to predict students\u2019 prior knowledge including comparing the student PKA paragraphs to expert-generated paragraphs or to a taxonomy of concepts related to the target domain, which in their case was biology. Students\u2019 prior knowledge level or mental model were modeled as a set of three categories: low mental model, medium mental model, and high mental model. There are significant differences between our work and theirs. First, we deal with dialogues as opposed to explicitly elicited prior knowledge paragraphs. Second, we do not have access to a taxonomy of concepts against which we can compare students\u2019 contributions. Third, we model students\u2019 prior knowledge using scores obtained on a multiple-choice pre-test.Predicting students\u2019 learning and satisfaction is another area of research directly relevant to ours. Among these, we mention the work of ) who used three types of features to predict learning and user satisfaction: system specific, tutoring specific, and user-affect-related. They employed the whole training session as unit of analysis, which is different from our own analysis because we use instructional task, i.e. a Physics problem in our case, as the unit of analysis. Our unit of analysis serves better our purpose of finding out the minimum number of leading instructional tasks to accurately assess students\u2019 knowledge level. Furthermore, their work was in the context of a spoken dialogue system while in our case we focus on a chat-based/typed-text-based conversational ITS. Another difference between our work and theirs is their focusing on user satisfaction and learning while we focus on identifying students\u2019 knowledge level.\n                        ) worked on predicting the quality of student answers (as error-ridden, vague, partially-correct or correct) to human tutor questions, based on dictionary-based dialogue features previously shown to be good detectors of cognitive processes (cf. ()). To extract these features, they used LIWC (Linguistic Inquiry and Word Count; ()), a text analysis software program that calculates the degree to which people use various categories of words across a wide array of texts genres. They reported that pronouns (e.g. I, they, those) and discrepant terms (e.g. should, could, would) are good predictors of the conceptual quality of student responses.\n                        ) worked on predicting the project performance of students and student groups based on stepwise regression analysis on dialogue features in Online Q&A discussions. To extract dialogue features they made use of LIWC and speech acts, which are semantic categories such as  or  that indicate speakers\u2019 intentions (). Yoo and Kim found that the degree of information provided by students and how early they start to discuss before the deadline, are two important factors explaining project grades. A similar research was conducted by Romero and colleagues () who also included (social) network related features. Their statistical analysis showed that the best predictors related to students\u2019 dialogue are the number of contributions (messages), number of words, and the average score of the messages.In our work presented here, we use some of the features described by the above researchers, such as session length or dialogue turn length, and other novel features such as information content.The work described in this article has been conducted in the context of the state-of-the-art intelligent tutoring system DeepTutor (). To better understand this context, we offer in this section an overview of intelligent tutoring systems in general and of DeepTutor in particular.Our approach to predict students\u2019 knowledge level in the context of dialogue-based ITSs relies on the fact that each tutorial dialogue between the system and a student has its own characteristics which are strongly influenced by students\u2019 background and the nature of instructional tasks. Indeed, students\u2019 knowledge level is reflected in the tutorial dialogue between the system and the student, e.g. as the learner becomes more competent the level of help from the ITS should drop. The level of help can be quantified as the number of hints, for instance. Furthermore, the dialogue characteristics are also influenced by the nature of the training tasks. If similar tasks (addressing same concepts in similar or related contexts) are used throughout a whole tutorial session, one might expect that by the time a student reaches the last problems in the session he would master them, thus, requiring less help from the tutor by the end of the session. On the other hand, if the problems are increasingly challenging or simply unrelated to each other then the students would be continuously challenged throughout the whole session; in such a scenario the number of hints a student receives should not drop throughout a session.We are exploring the relationship between students\u2019 prior knowledge and dialogue features in two different setups with two different task selection strategies which allows us to explore the impact of different task selection policies on the dialogue characteristics and therefore on our models for predicting students\u2019 prior knowledge. Indeed, we work with data collected from training sessions with two versions of DeepTutor: micro-adaptive-only and fully-adaptive (macro- and micro-adaptive). In the micro-adaptive-only condition, students are working on tasks that were so selected to address typical challenges for all students, i.e. following a one-size-fits-all approach. In this micro-adaptive-only condition, students received scaffolding while working on a task (within-task adaptivity) based on their individual performance on that particular task. For instance, if a student articulated a misconception during the solving of a problem, the system would correct it.In the macro-adaptive condition, students were assigned to four groups corresponding to four knowledge levels (low knowledge, medium-low knowledge, medium-high knowledge, and high-knowledge) and appropriate instructional tasks were assigned to each group using an Items-Response Theory style analysis (). That is, high-knowledge students received more challenging problems appropriate for their level of expertise while low knowledge students received less challenging problems. The consequence of this more-adaptive task selection policy is reflected in the dialogue characteristics as, for instance, the percentage of hints (explained later) is expected to be similar for both high-knowledge and low-knowledge students as the tasks are similarly challenging relative to the knowledge level of the students. Within a task, the fully-adaptive ITS offered identical micro-adaptivity to the micro-adaptive-only ITS. It should be noted that in the micro-adaptive-only case, the problems were selected (two each) from the set of problems used for the four knowledge groups in the fully-adaptive condition.Our goal was to understand how various characteristics associated with dialogue units corresponding to instructional tasks in a session relate to students\u2019 prior knowledge as measured by the pre-test, which is deemed as an accurate estimate of students\u2019 prior knowledge level. Our first step towards this goal was to do a feature analysis which is described next.We explored in this article models to predict students\u2019 prior knowledge based on features characterizing the dialogue-based interaction between a computer-based tutor and a learner. This work was part of our greater goal to move towards non-intrusive assessment methods that would allow learners to focus on the major task, e.g. solving problems or playing a game, and improve their learning experience by eliminating test axieties and tiring effects.Our results are quite promising with respect to moving towards a world in which learners focus on instruction with no explicit testing. Indeed, our linear regression models based on a number of interaction features yielded in the best cases an = 0.949 and adjusted -square = 0.833. This best result was obtained when developing prediction models using the data from the fully-adaptive ITS. This is expected because in the fully-adaptive case the models were more specialized, i.e. we derived prediction models for each of the four student knowledge levels: low knowledge, medium-low knowledge, medium-high knowledge, and high-knowledge. It should be noted that the best results for the prediction model derived from the micro-adaptive-only ITS data were very good too: = 0.878 and -square = 0.693. Furthemore, scaffolding features seemed to be the most predictive as a group, as somehow anticipated in a tutorial context, followed by content-generation features.Our findings have two important implications for the future development of ITSs that would integrate non-intrusive assessment methods such as the ones proposed in this article. , the best models derived from the micro-adaptive-only sessions provide a better estimate of the accuracy ITS developers should expect for predicting learners\u2019 prior knowledge level in future ITSs and should be the model to be integrated first in such future ITSs, despite the fact that these models are less accurate, although pretty accurate for that matter, than the more specialized models derived from the fully-adaptive ITS data. The reason is obvious: in order to use the fully-adaptive models, the ITS needs to make a guess or have some a priori measurement of the learners\u2019 knowledge, so that it can decide which fully-adaptive model to use for a more precise measurement of learners\u2019 knowledge levels based on their performance on the tasks in the tutorial session. However, giving learners a pre-test in order to infer their knowledge level first defies in a way the whole purpose of our intended goal: inferring learners\u2019 prior knowledge level from characteristics of the tutor-learner interaction only, without an explicit pre-test. In the case when a learner\u2019s knowledge level is known a priori, e.g. from a recent classroom test, and is available as input to the ITS then the ITS could simply trigger the more specialized and more accurate prediction model corresponding to the specific learner\u2019s knowledge level without the need to use the micro-adaptive-only prediction model.\n                        , the fully-adaptive models\u2019 high accuracy can be interpreted as validating the set of selected instructional tasks, i.e. Physics problems in our case, in the tutorial session. Task selection is a critical step in a computer tutor because it has major implications for the effectiveness of the system. If the tasks are too easy, then the learner is bored leading to her disengagement while if the tasks were too difficult the learner would be frustrated and, again, disengaged, to the point that in some cases she might even quit using the tutoring system. Indeed, the tasks should be at the right level of difficulty, not too easy and not too difficult but just right, in order to stimulate the learner and keep her engaged in the learning process throughout the whole tutorial session. That is, the role of the intelligent tutoring system is to keep the learner in the zone of proximal development () through an appropriate set of tasks with respect to the learner\u2019s current knowledge state. In this sense, having components that could monitor the quality of the selected task would thus be very beneficial. It should be noted that because the task selection step is an upstream step in the tutorial process any bad decision regarding task selection would propagate to later, downstream tutoring stages. To illustrate our point, imagine an ITS with a perfect micro-adaptive module which would provide ideal scaffolding to each learner working on a particular Physics problem. Even if the scaffolding within a task were optimal, learners would not learn much if the Physics problem were way below their knowledge level. Not only that but, as mentioned earlier, the learner would feel bored and in the worst case scenario she might decide to quit using the tutoring system. Our recommendation is that future developers of ITSs should implement both types of models: the micro-adaptive-only models are needed to get a sense of learners\u2019 knowledge level without an explicit pre-test while the fully-adaptive models are needed to monitor and validate learners\u2019 knowledge level and the quality of the instructional tasks throughout the entire tutorial session.We plan to further explore the topic of assessing students\u2019 prior knowledge from dialogues by investigating affect-related features as well as by using other prediction mechanisms such as classifiers to predicting categorical knowledge levels. Furthermore, we plan to study how similar models can predict post-test scores. We are aware that students\u2019 knowledge levels evolve during training, assuming they learn, and therefore there are limitations to our methodology. We do plan to explore in the future ways to infer students\u2019 knowledge levels throughout a session, e.g. by having a human expert read the transcripts of a tutoring session."},
{"url": "https://robomechjournal.springeropen.com/articles/10.1186/s40648-017-0083-5", "title": "Research on effective teleoperation of construction machinery fusing manual and automatic operation", "authors": ["Takanobu Tanimoto", "Kei Shinohara", "Hiroshi Yoshinada"], "publication": "ROBOMECH Journal", "publication_date": "25 May 2017", "abstract": "It is known that the work efficiency of teleoperated construction machinery is lower than that of directly operated machinery. Assistance via automatic control is expected to improve the work efficiency. However, this assistance might break the feeling of control and prohibit control adjustments by the operator.", "full_text": "The development of unmanned construction using teleoperation has been accelerated for disaster sites or mines. However, it is known that the work efficiency of teleoperated construction machinery is lower than that of directly operated machinery []. We need to improve the work efficiency. Unmanned construction work for practical use may be realized by teleoperation and autonomous operation. However, both approaches have some known weaknesses.Autonomous operation plans and controls the construction machine with measured around information. Yamamoto proposed autonomous excavating operation using a hydraulic excavator []. The complete calculation of the behaviors of stone and sand is difficult, as these behaviors may change owing to the machine control, the weight, the ratio of water content, the ratio of composition, etc. It is not easy to measure the ratio of water content and the ratio of composition. Even if this information can be measured, the large number of particles of sand will make real-time calculation difficult. Therefore, current autonomous operation cannot control a construction machine in all situations. Of course, expert operators can manipulate the sand as they imagined as long as they are on board the machine.A teleoperation system controls a construction machine from a remote place using images captured by a camera attached to the construction machine. During teleoperation, operators lack a visual field, the acceleration of the machine, the sound of an engine, and a sense of perspective. In particular, a two-dimensional (2D) image makes it difficult to obtain a sense of perspective. A lack of perspective increases the working time because it is difficult for operators to adjust the position []. Past studies supplied a sense of perspective with three-dimensional images [], superimposed computer graphics (CG) [], etc. However, three-dimensional images tire the operators because the working distance of a construction machine is different from the screen distance. Superimposed CG provides distance information to the operators. The tip of a construction machine was superimposed on the ground with CG. However, the CG information is in a different format from the normal perspective. Therefore, operators must think and convert the CG information into the perspective information in their mind. In contrast, autonomous operation can control the position accurately and quickly by measuring the distance to the target.Therefore, we need both autonomous operation and teleoperation for unmanned construction. However, if the unmanned construction system switches between the autonomous and manual modes, the work cycle is separated into parts. The work efficiency is not improved, and the operator is frustrated when switching modes. In contrast, a semiautomatic system assists with manual operation according to a designed plan combining automatic and manual operation. Researchers have previously proposed assistance systems [, ]. Shimano proposed assistance that limited control in the case of an out-of-design plan []. Kubo adjusted a manipulability measure when controlling the manipulator of a leg-type robot []. However, these researchers evaluated the accuracy or performance of the system, but they did not evaluate the feeling of control. In situations that are difficult to calculate for the assistance system, misdirected assistance causes a sense of discomfort during control and a low work efficiency.Ideally, the assistance would not cause discomfort and result in a high work efficiency. One strategy for achieving this is for the operator to remain unaware of the assistance. If operators remain unaware, they obtain a feeling of control that is the same as manual control. Igarashi proposed assistance without human awareness []. Igarashi et al. modified the dynamical parameters of a robot to approach an internal model to operate a robot with complete control by an operator []. However, they failed because of discomfort felt by the operators when changing the actual robot dynamics. Therefore, a limit on the rate of change in the dynamics was proposed, and the assistance worked without human awareness and improved performance [].In an experiment, operators controlled a mobile robot by steering to trace reference lines. However, in our construction machine, operators control on the basis of a work plan without a detailed reference trajectory. Operators have common actions, e.g., excavation and manipulation of a hoist arm. Operators change these actions according to the work target, habitual actions, and work progress. In particular, in teleoperation, it is difficult for operators to control the tip of a construction machine in three-dimensional space without perspective.Therefore, in this paper, a semiautomatic system that fuses control by manual and automatic operation is proposed, which achieves a high work efficiency and a feeling of control that is the same as manual control, by the teleoperation of a construction machine. The target of this work is the hoist swing that is typically used during the operation of a construction machine.The first experiment involves a 2D CG situation. It is difficult for a skillful operator to control the construction machine. However, operators could obtain information for control as long as the 2D CG situation. In this experiment, awareness of assistance was evaluated automatically and manually along with two assistance methods by a sense of agency [].The second experiment involves teleoperation of miniature excavator. Teleoperation does not provide the operators with a sense of perspective, which is different from the 2D CG situation. The assistance parameter of the 2D CG situation was reflected in the teleoperation. The work accuracy and the sense of agency were evaluated without a sense of perspective.In the first experiment, the assistance methods were evaluated with a 2D CG simulation. The test operators could not control a real excavator without a license. It was difficult to obtain many expert operators. Moreover, amateur operators could not control the excavator well in the real world. However, it is easy to control a 2D CG simulation. Operators became familiar with the controls immediately and could obtain all of the control information without removing the perspective. Therefore, the assistance methods were evaluated using a simulation, and the results were reflected in the teleoperation experiment.The teleoperation of a construction machine has a low work efficiency due to the lack of perspective. If an automatic system can provide assistance with the operator\u2019s control, operators may feel uncomfortable. Therefore, the purpose of this study was to develop a semiautomatic system with a high work efficiency and achieve a feeling of control that is the same as manual control using teleoperation of a construction machine.In this paper, the target work was the hoist swing, and an experiment was conducted assuming an ideal trajectory. A hoist swing is the typical and repeated work of a construction machine and was generally a fixed trajectory. The previous assistance method becomes too strong near the target point and disturbs the operator\u2019s feeling of control. Therefore, the semiautomatic system supported the ideal trajectory with little assistance and maintained the operator\u2019s feeling of control to be the same as manual control.The first experiment was carried out in a 2D CG environment that does not require perspective, in which the target is easy to control. The second experiment was carried out using the teleoperation of a miniature excavator that is difficult to control in a 3D environment without a sense of perspective. In these experiments, the awareness of assistance was evaluated for automatic, manual, trajectory, and goal assistance by the sense of agency. Operators often felt assistance during the manual trials and often did not notice the assistance during the automatic trials. Considering the results, the operators sense is ambiguous, and it can be considered that there is possibility that assistance can be used without awareness. When operators controlled the semiautomatic system with assistance to ensure the ideal trajectory, the sense of agency was better than that for goal assistance. Moreover, trajectory assistance decreased the error in the depth direction, which was higher for manual control. The results show that trajectory assistance possibly improves the accuracy and maintains a feeling of control that is the same as manual control during teleoperation. In the future, the feeling of control when operators modify the target position and a trajectory that is different from the automatic target will be evaluated."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0081-4", "title": "A Placebo-Controlled Trial of Riboflavin for Enhancement of Ultramarathon Recovery", "authors": ["Martin D. Hoffman", "Taylor R. Valentino", "Kristin J. Stuempfle", "Brandon V. Hassid"], "publication": "Sports Medicine - Open", "publication_date": "28 March 2017", "abstract": "Riboflavin is known to protect tissue from oxidative damage but, to our knowledge, has not been explored as a means to control exercise-related muscle soreness. This study investigated whether acute ingestion of riboflavin reduces muscle pain and soreness during and after completion of a 161-km ultramarathon and improves functional recovery after the event.", "full_text": "Post-exercise muscle pain and soreness have been well documented in ultramarathon running [\u2013]; however, the precise etiology and pathophysiology remain elusive. Presumably, the process begins with mechanical damage to the muscle and connective tissue; is followed by inflammation, swelling, and the production of free radicals; and culminates in the pain and soreness felt during and after exercise [\u2013]. Markers of inflammation and oxidative stress are known to be high following an ultramarathon [, , \u2013].Numerous pre- and post-exercise interventions using nutritional supplements and dietary strategies have been investigated for prevention or treatment of exercise-related muscle pain and soreness [, ]. Since the inflammatory response and free radical production with oxidative stress are likely involved in the mechanism leading to exercise-related muscle soreness, it seems plausible that supplementation with substances having anti-inflammatory or antioxidant properties may be an effective means of controlling such soreness. While research has provided some general support for the effectiveness of such substances in reducing exercise-related muscle soreness [, ], previous studies specific to ultramarathon running have demonstrated no effect. For instance, 6\u00a0weeks of vitamin E and C supplementation was found to have no effect on muscle damage, inflammatory markers, or muscle function recovery after a 50-km trail run [, ]. Furthermore, 3\u00a0weeks of supplementation with quercetin, another substance with known antioxidant properties, was not found to alter antioxidant capacity or oxidative damage, inflammation, muscle damage, or post-race muscle soreness from a 161-km ultramarathon [, ].A common nutritional supplement that we believe has not been investigated for its effect on exercise-related muscle soreness is vitamin B2 (riboflavin). As for other flavonoids, riboflavin is known to exhibit antioxidant properties and protect tissue from oxidative damage [\u2013]. Riboflavin is also important in cell repair and production and for protecting mitochondrial and other enzymes as a mitochondrial enzyme cofactor or cofactor precursor []. It is one of eight water-soluble B vitamins found in many foods and must be regularly supplied in the diet as it is not stored by the body []. Human trials have largely focused on the efficacy of riboflavin supplementation for migraine prophylaxis with favorable findings [, ]. Whether or not it might provide a protective role or enhanced recovery from exercise-induced skeletal muscle damage is unknown.The purpose of this study was to investigate whether acute ingestion of riboflavin is effective at reducing muscle pain and soreness during and after completion of a 161-km ultramarathon, and in improving functional recovery after the event. The study was performed in association with the 161-km Western States Endurance Run (WSER) since this race is known to induce considerable muscle damage and pain [, , \u2013, \u2013]. Based upon the evidence that riboflavin can protect tissue from oxidative damage and is important in cell repair, we hypothesized that riboflavin would be effective at reducing muscle pain and soreness and improving muscle recovery after this extreme level of exercise.Of the 353 race entries, 44 runners enrolled in the study (22 in each group) and started the race. Of this group, 37 (84.1%, 20 in riboflavin group, 17 in placebo group) finished the race and 32 (18 in riboflavin group, 14 in placebo group) completed data collection. Of the 32 completing data collection, 8 had enrolled in advance of race registration and completed both pre-race 400-m run trials (5 in riboflavin group, 3 in placebo group). Overall race finish rate was 79.3% (280 of 353 starters), which was similar (\u2009=\u2009.55) to that for the study participants.As shown in Table\u00a0, pain medication use during the race was not different between groups but was at 50% among the riboflavin group and 36% among the placebo group. Considering the 9 subjects in the riboflavin group who reported using pain medication during the race, 7 used a NSAID and 2 used acetaminophen at a mean (range) of 29% (12\u201362%) of the maximum recommended 24-h dose. Of the 6 subjects in the placebo group who reported using pain medication during the race, 1 used a NSAID and 4 used either acetaminophen, paracetamol, or paracetamol with codeine at a mean (range) of 39% (10\u201375%) of the maximum recommended 24-h dose. Comparison of muscle pain and soreness ratings between subjects who used pain medication during the race with those not using pain medication during the race revealed no suggestion of a group or interaction effect, whether considering all time points (\u2009=\u2009.43 and \u2009=\u2009.61 for group and interaction effects, respectively) or just the ratings during the race and at the finish (\u2009=\u2009.98 and \u2009=\u2009.82 for group and interaction effects, respectively).Subjects who received riboflavin correctly suspected they had received the treatment 50% of the time, whereas 23% of the placebo group incorrectly thought they were in the treatment group. These rates of suspicion about being in the treatment group were not statistically different (\u2009=\u2009.16). There were 3 subjects in the riboflavin group and 1 subject in the placebo group who noted a yellow urine color.This work is a preliminary examination of riboflavin for potential benefits of reducing muscle pain and soreness during and after strenuous exercise and at enhancing recovery from strenuous exercise. The findings suggest that the vitamin may have some benefits. Indeed, riboflavin supplementation immediately before and midway during prolonged exercise appeared to be linked with reduced muscle pain and soreness during and at the completion of the exercise, and there was some evidence for enhanced functional performance during the initial several days after the exercise. While the findings require cautious interpretation, they are adequately interesting to warrant further investigation.Given that post-race plasma CK concentrations were similar between groups, there is no evidence from this work that riboflavin acts by reducing muscle cell rupture. Rather, it would seem that it must act by altering the physiological response to exercise in other manners. While the underlying mechanism of action cannot be established from this study, it seems conceivable that the antioxidant properties of riboflavin [\u2013] could explain reduced muscle pain and soreness during exercise, although the lack of reduced muscle pain and soreness during recovery does not seem consistent with this mechanism. On the other hand, the mitochondrial protective function of riboflavin [] might be a plausible explanation for the riboflavin group demonstrating enhanced functional recovery without improvement in muscle pain and soreness during the recovery period.Participants of the WSER are generally well-trained and experienced ultramarathon runners given that a recent qualifying ultramarathon is required to gain entry into the race. In this regard, they were conditioned for this type of activity and were adapted for controlling and responding to significant muscle injury from prolonged running. It is possible that an effect of riboflavin could be even greater in a group of subjects who are more na\u00efve to strenuous exercise.We chose to provide two 100-mg doses of riboflavin which were received immediately prior to exercise and around 11\u201316\u00a0h later during the approximately 20\u201330\u00a0h it took to complete the race. This dose and schedule were chosen because we felt it would be feasible to achieve subject cooperation and that any effectiveness should still be evident even if this was not the optimal dose or dosing schedule. The doses we provided were well above the recommended dietary allowance for riboflavin of 1.3\u00a0mg/day for adult men and 1.1\u00a0mg/day for adult women []. While the body absorbs little riboflavin from single doses beyond 27\u00a0mg [], the vitamin appears safe at much higher doses [] and riboflavin supplements are typically available in 100-mg capsules with recommendations to take 1\u20132 per day. Plasma concentrations of riboflavin and flavocoenzymes have been shown to peak around 2 and 3.5\u00a0h, respectively, after a single 60-mg dose of riboflavin, but the plasma concentrations remain elevated for hours []. Considering this information, it is possible that a lower dose at more frequent intervals might be more effective and would be recommended for future studies of this nature if feasible in the study environment.The present findings indicate that muscle pain and soreness ratings of our subjects had returned to pre-race levels by 5\u00a0days after the race. For the subsample of 8 subjects who performed the pre-race 400-m runs, their times at 10\u00a0days after the race were statistically similar to pre-race times, although mean times were still ~5% slower at 10\u00a0days after the race. Our prior work at the WSER had also demonstrated that muscle pain and soreness ratings had statistically returned to baseline by post-race day 5, but 400-m run times were not examined in that study beyond post-race day 5 at which time pre-race performance had not been fully recovered []. In the present work, we extended the post-race time period of examination to 10\u00a0days and found that this appears to be close to the timeframe of 400-m run recovery. This is not intended to suggest that athletes are fully recovered from a 161-km ultramarathon within 10\u00a0days or shortly thereafter but rather that our subjective measure of resting muscle pain and soreness and our objective measure of 400-m speed had nearly recovered during this relatively short time period.The WSER serves as an excellent environment to induce muscle pain and damage as evident from prior work at the race [, , \u2013, \u2013]. This is confirmed with the present work in which muscle pain and soreness ratings at the end of the race averaged ~7\u20138 on the 10-point scale (sore to very sore) and median post-race plasma CK concentrations were ~7000\u201313,000 U/L. However, subject recruitment is a challenge in performing research of this nature in a competition setting such as the WSER. This is reflected in the number of subjects we were able to recruit. In particular, it is unfortunate that the number of subjects completing the pre-race 400-m runs was so low, which limits the robustness of our interpretation of the findings relative to functional recovery. While the treatment and placebo groups appeared to be well matched and blinding appeared to be adequate in this study, we cannot be certain that the groups were well matched for baseline performance at the 400-m run.We acknowledge some other limitations with this study resulting largely from constraints related to the study being performed at a competition. Perhaps most importantly is that a sizable percentage of the subjects (50 and 36% in the riboflavin and placebo groups, respectively) used pain medication during the race and some used pain medication in the 10-day post-race period. The use of NSAIDs during this event has been common, ranging from 32 to 57% among those participating in our prior research [, , ]. Interestingly, earlier work has demonstrated that NSAID use during the race was not effective at controlling post-race muscle soreness [] though the effect of NSAIDs on muscle pain and soreness during the race has not been systematically examined. Among the present subjects using pain medication during the race, the usual dosage was relatively low compared with the maximal recommended dose during 24\u00a0h. Not surprisingly, our separate comparison of muscle pain and soreness ratings between subjects who used pain medication during the race and those not using pain medication during the race revealed no suggestion of an effect of the pain medication on this variable. Thus, given these considerations, it seems unlikely that the use of pain medication during the race confounded the present finding of lower muscle pain and soreness during and at the completion of the race among the riboflavin group compared with the placebo group. Another potential study limitation is that, because we did not assess dietary practices of the subjects, it is conceivable that one group had a greater intake of anti-oxidants than the other. Additionally, the 400-m runs were unsupervised and self-timed, but this was the most feasible approach and we believe this study population was capable of maximally exerting themselves during unsupervised trials and correctly recording the times. Finally, we also recognize that some might consider it ideal to have measured pre-race plasma CK concentrations and to examine the pre-race to post-race change in plasma CK concentration rather than just the post-race value. But, since these runners would have reduced training prior to the race, we would expect pre-race plasma CK concentrations to have been very low relative to the post-race values, as previously demonstrated [\u2013], so not using the pre-race to post-race change would not have altered our interpretation of the findings for this variable.From this work, we conclude that there is some evidence that riboflavin supplementation immediately before and midway through prolonged running may reduce muscle pain and soreness during and at the completion of the exercise and that there is some suggestion that riboflavin might enhance functional recovery after the exercise. We acknowledge that this study is a preliminary examination of riboflavin for this purpose and involved a small number of subjects in which the dose and dosing schedule might not have been optimal. As such, the findings appear intriguing and warrant additional investigation of riboflavin as a means to reduce muscle pain during exercise and to enhance post-exercise recovery."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0079-y", "title": "The Effect of Fluid Intake Following Dehydration on Subsequent Athletic and Cognitive Performance: a Systematic Review and Meta-analysis", "authors": ["Danielle McCartney", "Ben Desbrow", "Christopher Irwin"], "publication": "Sports Medicine - Open", "publication_date": "18 March 2017", "abstract": "The deleterious effects of dehydration on athletic and cognitive performance have been well documented. As such, dehydrated individuals are advised to consume fluid in volumes equivalent to 1.25 to 1.5\u00a0L\u00a0kg", "full_text": "The deleterious effects of dehydration (fluid loss) on athletic and cognitive performance have been extensively researched. Recent meta-analyses detected meaningful decrements in aerobic [] and anaerobic [] exercise performance and muscular strength and endurance [] when subjects commenced activity in an already dehydrated state. Experimental investigations have also demonstrated motor-skill impairments on sport-specific exercise tests (e.g. cricket [], basketball [, ], golf [], field hockey [] and surfing []) following fluid loss. Whilst evidence indicating a detrimental effect of dehydration on cognitive function is less consistent [], decline in memory, perceptual discrimination and mood state has been observed in some studies []. Dehydration is commonly observed amongst athletes [\u2013] and manual workers (e.g. military, fire fighters and labourers) [], who rely upon physical and mental proficiencies to compete or train at elite levels and remain productive in the workforce. This evidence has provided the rationale for fluid replacement recommendations.The American College of Sports Medicine (ACSM) Guidelines on Exercise and Fluid Replacement [] and the Position of the Academy of Nutrition and Dietetics on Nutrition and Athletic Performance [] recommend dehydrated individuals consume 1.25 to 1.50\u00a0L of fluid per kilogram of body mass (BM) lost to replenish body water content, if the fluid deficit is large and recovery time is limited (i.e. <12\u00a0h). Whilst the importance of returning to euhydration over a period of a day(s) is not in dispute, many individuals are required to undertake repeated bouts of activity, where limited time between tasks exists or the demands of a subsequent activity (i.e. type, duration and intensity) and/or the environment (e.g. conflict zone) may influence the appropriateness of the aforementioned guidelines. Within this context, consuming fluid has the potential to enhance or inhibit performance. Thus, determining rehydration strategies that counteract the detrimental effects of fluid loss, whilst optimizing performance on subsequent tasks, is important.Ingesting large volumes of fluid may cause gastrointestinal (GI) discomfort, impeding performance. Particularly if the amount of time available to consume fluid is limited or fluids with higher calorie loads (e.g. milk-based beverages) and hence slower rates of gastric emptying are ingested [, ]. The nature of the subsequent activity, e.g. the mechanical \u2018bouncing\u2019 action caused by high intensity running, may also impact GI symptomology []. Conversely, drinking large fluid volumes promotes rapid initial gastric emptying [], facilitating fluid absorption, and may convey greater benefit than drinking smaller volumes. To date, the majority of investigations examining the effect of ingested fluid volume on subsequent performance have employed a prolonged (i.e. overnight) rehydration period [\u2013], reducing the probability of GI disturbance influencing subsequent performance. Thus, the importance of ingested fluid volume and its impact on subsequent exercise performance outcomes remains unclear. The aim of the present systematic review and meta-analysis was to examine the impact of consuming fluid following a period of dehydrating sweat loss on subsequent athletic and cognitive performance. Understanding how to maximize the benefits of fluid intake under these circumstances will inform the development of future fluid replacement guidelines.The following research protocol was devised in accordance with specifications outlined in the  []. The methodology of this review is registered at the International Prospective Register for Systematic Reviews, identification code CRD42016036560.Individuals prone to dehydration (e.g. athletes and manual workers) may have limited opportunity to adequately rehydrate prior to performing physically or cognitively demanding activities. The present systematic review and meta-analysis examines evidence for the effects of fluid intake on subsequent athletic and cognitive performance following dehydrating sweat loss. A beneficial effect for fluid intake was strongest when athletic performance involved continuous exercise tasks. Further, the magnitude of improvement appeared greater when the continuous exercise was performed at elevated environmental temperatures and over longer exercise durations. Whilst the volume of fluid consumed (relative to BM lost) did not appear to influence the size of the treatment effect, fluid intake at levels complying with current recommendations for completely replacing lost fluid (1.25\u20131.50\u00a0L\u00a0kg BM lost) [, ] are yet to be thoroughly investigated. Evidence for a beneficial effect of fluid intake on intermittent, resistance and sport-specific exercise performance and cognitive function or mood is less apparent and requires further elucidation.The weighted mean effect suggests that fluid ingestion following a period of dehydration significantly improves continuous exercise performance, compared to control conditions (no fluid or negligible fluid intake). Individual estimates all indicated a beneficial effect from fluid intake; however, the magnitude of the improvement was heterogeneous (\n                        \u2009=\u200980.5%) which may reflect differences in the methodologies employed between studies. Simple meta-regression determined that 82% of variation between trials can be attributed to differences in the ambient environmental temperature at which subsequent exercise was performed, with fluid intake demonstrating greater efficacy under heat stress conditions. The decline in aerobic performance that occurs with hypohydration has largely been attributed to circulatory strain, whereby reductions in blood volume limit oxygen transport to the exercising muscle [, ]. Under elevated environmental temperatures, blood flow is also redirected to the skin facilitating evaporative cooling, augmenting circulatory strain and further impairing exercise performance []. These physiological perturbations are typically characterized by increased heart rate and core temperature [, , ]. Hence, thermoregulatory parameters were monitored in many of the studies reviewed (11/15) [, , , , , , , , , , , , ]. The majority of reviewed studies reported that consumption of fluid was associated with significant reductions in core or rectal temperature (7/11) [, , , , , , ] and heart rate (6/10) [, , , , , , , ] at various time points during continuous exercise performance (i.e. for at least one fluid intervention). Thus, fluid intake may offset circulatory strain typically observed when exercise is undertaken in warm environments. The multiple meta-regression analysis also suggests that differences in the duration of the continuous exercise performed may account for a proportion of the heterogeneity observed between experimental trials, with exercise performed over longer durations yielding greater benefit from fluid intake than short duration exercise. However, as the majority of performance tests included in the analysis were relatively short in duration (4\u201330\u00a0min), we cannot be certain that this relationship would hold true over longer exercise durations (i.e. 2\u20138\u00a0h).Results of the meta-regression failed to indicate a statistically significant relationship between the volume of fluid consumed and continuous exercise performance improvements. However, the majority of trials tested a quantity of fluid that was within a narrow fluid intake range (i.e. 1.0\u20131.05\u00a0L\u00a0kg BM lost, \u2009=\u200913 out of 18). Hence, the performance effects associated with ingesting a comparably small volume of fluid (e.g. \u22640.75\u00a0L\u00a0kg BM lost) or an amount consistent with recommended guidelines (e.g. 1.25\u20131.50\u00a0L\u00a0kg BM lost) remains uncertain. Three experimental investigations have examined the dose-response effect of ingested fluid volume on continuous exercise performance following a period of dehydration with the results demonstrating inconsistent findings [, , ]. Unfortunately, the investigation with the greatest contrast in fluid volumes (i.e. 0.75 vs. 1.50\u00a0L\u00a0kg BM lost []) did not employ a \u2018no fluid\u2019 control and was unable to be included in the meta-analysis. Findings from previous studies suggest that fluid intake during exercise exceeding that dictated by thirst may not provide additional performance benefits []. However, only three of the publications reviewed measured subjective thirst within the investigation (and these studies did not test different fluid volumes, i.e. only one intervention vs. control). Therefore, it is not clear whether the equivocal effect of fluid intake volume can be attributed to thirst sensation. Based on current evidence, prescribing fluid volumes required to optimize performance on a subsequent continuous exercise task requires clarification.If relatively small and large fluid intakes elicit comparable treatment effects, individuals who have limited time to rehydrate prior to performing aerobic activities may opt to consume smaller fluid boluses, delaying complete rehydration until circumstances permit (e.g. overnight). This strategy may reduce the probability of the drinker experiencing volume-induced GI discomfort during subsequent activity, which may occur when larger fluid volumes are ingested []. Only one of the 42 publications reviewed monitored GI symptomology []. In this study, subjective ratings of GI discomfort following different fluid intakes (~0.5 vs. 1.0\u00a0L\u00a0kg BM lost) were described as mild to moderate and moderate to high on each trial, respectively. This suggests that larger fluid volumes are likely to induce some degree of participant discomfort which may compromise performance. However, research examining continuous exercise performance following two volumes of fluid intake (i.e. 0.75 vs. 1.50\u00a0L\u00a0kg BM lost) demonstrated significantly faster (~3.0%) running performance associated with the larger bolus []. Importantly, this study employed a prolonged (i.e. overnight) rehydration period reducing the probability of severe GI disturbance and allowing ingested fluid to equilibrate throughout the body. Further research examining exercise performance (and GI symptoms) when large fluid volumes are ingested over short rehydration periods is warranted.The effect of fluid intake on intermittent, resistance, sport-specific and balance exercise types remains unclear. It appears that fluid ingestion following a period of dehydration may improve performance on subsequent intermittent, resistance and sport-specific exercise tasks. However, methodological differences make comparison of results across trials challenging.In regards to intermittent exercise, 4 of 10 trials (\u2009=\u200995 male subjects) observed a significant positive effect of fluid intake on performance, whilst no trial reported a significant performance decrement. Similarly to the results from continuous exercise, beneficial effects of fluid intake are apparent when intermittent exercise tasks have been completed in warm environments [, ]. The impact of task duration may also influence the likelihood of observing performance effects, with longer duration tasks more regularly demonstrating a performance enhancement associated with fluid ingestion [, ]. For instance, Maxwell et al. [] observed that fluid intake only benefited performance on a second repeated sprint bout completed in the latter stages of testing.Concerning resistance exercise, 6 of 9 trials (\u2009=\u200983 subjects, 93% male) observed a significant positive effect of fluid intake on performance. One trial reported that fluid intake was detrimental to performance []. However, results from this study need to be interpreted with caution as the strength performance task was performed following an endurance task that varied in duration. Evidence indicating a beneficial effect of fluid intake on resistance exercise performance appears stronger when tests of muscular endurance, rather than tests of muscular strength, are employed []. Findings from this systematic review demonstrate significantly improved performance on 3 out of the 4 submaximal intensity resistance exercise tasks [, , ]. In contrast, performance on only 4 out of 15 maximal intensity tests demonstrated improvement with fluid intake [, , ]. Current evidence is inadequate to determine the influence of other variables (e.g. participant population, mode of dehydration) on the effect of fluid intake. Further research examining the effects of fluid intake on resistance exercise performance using standardized procedures is required.The 6 trials (\u2009=\u200964 subjects, 84% male) that evaluated the effect of fluid intake on sport-specific exercise performance exhibited considerable heterogeneity, with tests of cricket [], soccer [, , ], squash [] and racehorse riding [] performance all being employed. Whilst the majority of sports-specific research has demonstrated no impact of fluid consumption on subsequent performance, the paucity of data and lack of replication studies makes it difficult to determine an overall effect of fluid intake on sport-specific exercise performance.The present systematic review identified 14 trials (\u2009=\u2009174 subjects, 90% male) examining the effect of fluid intake following a period of dehydration on cognitive function and mood state. Evidence indicating a beneficial effect of fluid intake on cognitive performance was only observed in some studies [, ], and there was no clear indication of greater treatment efficacy on a particular cognitive domain. However, some limitations to the current evidence exist. In 4 trials, the cognitive assessment was conducted \u22645\u00a0min after concluding the dehydration protocol [, , ]; further 4 trials did not provide the necessary information to calculate the amount of time between the conclusion of the dehydration protocol and commencement of the cognitive tests [, , ]. Prior research indicates that acute exercise has a small positive effect on cognitive performance (typically dissipating within ~15\u00a0min of exercise cessation) [], whilst elevated core temperature via heat stress may provide additional cognitive burden []. Therefore, the residual effects of physiological stressors used to induce dehydration in these trials may obscure any influence of fluid intake on cognitive performance. Investigations examining the effect of hydration on cognitive performance should also employ neuropsychological tests that have previously demonstrated sensitivity to nutritional interventions [, , ]. Yet, only two studies included in the present review selected cognitive tests on this basis [, ]. The majority did not provide any rationale for their chosen method of assessment [, , , , , ], increasing the likelihood of false-negative reports. Fluid consumption positively influenced mood state (measured as reduced anger, fatigue, depression, tension and confusion) in 4 out of the 6 trials where it was measured [, , ]. Whilst this may suggest that self-reported mood state questionnaires are more sensitive to the effects of fluid intake than objective tests of cognitive function, subjective mood ratings were only influenced by fluid intake during trials where significant cognitive effects were also observed, i.e. effects on mood and cognition were not independent of one another. Collectively, it appears that the influence of fluid intake on mood and cognitive performance is still poorly understood and requires further research employing tasks with demonstrated sensitivity.This review does contain a number of limitations. Firstly, only studies with accessible full text articles written in English were included. Second, three of the studies reviewed [, , ] examined rehydration in combination with another placebo treatment (studies were excluded if fluids were co-administered with another experimental treatment). Thus, participants\u2019 perceptions regarding the expected treatment may have influenced these results. Third, as oral fluid replacement cannot be blinded, it is conceivable that the placebo effect may account for a small amount of benefit observed with rehydration. However, it was necessary to exclude research studies that blinded participants to hydration status using intravenous methods because the infusion does not accurately mimic the physiological effects of oral rehydration. Fourth, the present review elected to compare against a \u201cno fluid\u201d or \u201cnegligible fluid\u201d control condition, because a euhydrated control may be confounded by the effects of the dehydration protocol itself (i.e. hyperthermia or fatigue). However, using this comparison, we cannot determine whether fluid intake fully or partially restored performance to euhydrated levels. Similarly, fluid ingestion may have minimal or no effect on athletic or cognitive performance if the outcome measured is not sensitive to the effects of modest fluid losses. Fifth, where fluid was administered at the time of dehydration (i.e.  fluid intake), rather than following dehydration (i.e.  fluid intake), different physiologic responses to the dehydration protocol may occur on control and intervention trials, e.g. decreased core temperature leading to reduced central fatigue. This could have implications for subsequent athletic performance, and consequently, the magnitude of the overall treatment effect. Sixth, fluid intake \u2264200\u00a0mL was considered \u2018negligible\u2019 and included within the definition of control conditions. However, one study has reported that ingesting 100\u00a0mL of fluid (25\u00a0mL boluses at 5-min intervals during exercise) increased TTE following exercise-induced dehydration []. Thus, trials administering \u2264200\u00a0mL fluid to dehydrated control subjects may underestimate the true magnitude of the treatment effect.Collectively, the results of the present review suggest that individuals who have limited opportunity to adequately rehydrate prior to performing continuous exercise in a heated environment should consume fluid, even if the body water deficit is modest (1.3% reduction in BM) and fluid intake is inadequate for complete rehydration (0.5\u00a0L\u00a0kg BM lost). The influence of fluid intake for those individuals performing intermittent, resistance and sport-specific exercise or undertaking cognitively demanding tasks is not as well understood, and this review serves to highlight areas for future research."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0082-3", "title": "Exercise Training but not Curcumin Supplementation Decreases Immune Cell Infiltration in the Pancreatic Islets of a Genetically Susceptible Model of Type 1 Diabetes", "authors": ["Leandro Kansuke Oharomari", "Camila de Moraes", "Anderson Marliere Navarro"], "publication": "Sports Medicine - Open", "publication_date": "4 April 2017", "abstract": "The main mechanism involved in the pathogenesis of autoimmunity is an uncontrolled inflammatory response against self-antigens. Therefore, anti-inflammatory factors, such as the intake of bioactive compounds and a physically active lifestyle, may decrease or cease the development of autoimmune diseases. Type 1 diabetes (T1D) is an autoimmune disease characterized by pancreatic \u03b2 cell destruction. The non-obese diabetic (NOD) mouse is a model of spontaneous T1D and is the model most similar to human disease.", "full_text": "Intense inflammatory response is a main feature of autoimmune diseases. Type 1 diabetes (T1D), for instance, occurs due to a chronic inflammatory response with enough severity to destroy most of the pancreatic \u03b2 cells []. The incidence of some autoimmune diseases, including T1D, has been increasing worldwide over the past decades [\u2013], and environmental factors are the most accepted explanation for this phenomenon. For example, vitamin D status [] and the hygiene hypothesis [] are being investigated. Currently, physical inactivity and processed food intake during childhood appear to be linked to the development of chronic [\u2013] and inflammatory disease [] in early stages of life. Furthermore, the Overload Hypothesis suggests an association between a modern lifestyle and beta cell overload, which could make these cells more vulnerable to an autoimmune response [].Intervention using functional foods with anti-inflammatory properties could decrease or halt the autoimmune response, leading to a delay or even prevention of autoimmunity. Dietary patterns characterized by high bioactive compounds, such as a Mediterranean diet, promote an \u201canti-inflammatory environment\u201d in humans []. Among several compounds, curcumin, a polyphenol found in the rhizome of , is one of the most studied bioactive compounds due to its anti-inflammatory properties [\u2013]. Additionally, studies have shown that physical exercise also promotes anti-inflammatory responses through several mechanisms, such as increasing antioxidant activity [], releasing myokines and hormones [], modulating immune cell metabolism [], and decreasing inflammation signaling [].Currently, the non-obese diabetic (NOD) mouse, a model of spontaneous T1D, is the model that is most similar to human T1D []. Over the last 30\u00a0years, it has been used in research to improve the science of autoimmunity []. As in humans, T1D occurs in NOD mice due to immune cell infiltration into pancreatic islets (insulitis) and subsequent destruction of \u03b2 cells. Additionally, it has been demonstrated by ample literature that TNF-\u03b1, a pro-inflammatory cytokine, has an important role in the pathogenesis of T1D in NOD mice [\u2013].Some studies have tested bioactive compounds in NOD mice. Supplementation with cocoa flavonoids or green tea catechin reduced diabetes incidence in this animal model [, ]. However, no studies have tested the potential of exercise training to prevent T1D in NOD mice. Therefore, this study aimed to investigate the effects of exercise training and curcumin supplementation on T1D progression in NOD mice.To the best of our knowledge, this study is the first to show that exercise training has the potential to protect pancreatic \u03b2 cells against an immune response in vivo. However, curcumin supplementation failed to attenuate insulitis in NOD mice.Curcumin is a polyphenolic compound that exhibits low bioavailability []. The route of administration and the dose of curcumin used in this study were established based on previous studies that aimed to simulate a rich polyphenol diet [\u2013]. Although chronic oral curcumin (500\u00a0ppm in diet) reached detectable concentrations in plasma (0.035\u00a0\u03bcg/ml) and in brain tissue (0.469\u00a0\u03bcg/ml) in a dose-dependent study [], our study indicates that this concentration was not enough to prevent an immune response against pancreatic \u03b2 cells. However, intraperitoneal curcumin supplementation (25\u00a0mg/kg body weight) inhibited leucocyte infiltration in accelerated murine models of T1D []. These data suggest that the concentration of curcumin required to modulate immune function could not be reached using dietary strategies.The low incidence of T1D observed was expected because animals were fed a gluten-free diet and were not in a germ-free environment [, ]. However, NOD mice present ~60% immune cell infiltration in islets even in the absence of T1D [], which is consistent with our findings. Exercise training lowered this rate to 30%.The effect of exercise training in diminishing body weight gain is well documented and was observed in the present study, showing the importance of physical exercise for caloric balance. Lower body weight gain could be one mechanism that explains the reduction of insulitis in the T and TC groups. Since obesity induces chronic inflammation [] and the Overload Hypothesis proposes that environment factors, such as obesity, increase T1D risk [], this line of thought is strengthened.Another possible mechanism to explain the effects of exercise training is through dendritic cells modulation. It is well known that dendritic cells modulate both innate and adaptive immune responses, and the role of these cells on the development of T1D was recently demonstrated []. Several studies have shown that exercise training decreases the number of dendritic cells or diminishes their response [\u2013]. Thus, exercise could mitigate autoimmunity by shaping dendritic cell activation.Regarding cytokine signaling, IL-6 has been recognized as one of the myokines produced during exercise training []. In 2010, exercise training was documented as an anti-inflammatory approach, which is able to prevent type 2 diabetes, cardiovascular diseases, cancer, and dementia []. Although no significant differences were seen in cytokine levels, the CUR, T, and TC groups exhibited an IL-6 concentration more than twofold higher than the C group (\u2009=\u20090.07). In an ex vivo study, the pancreas of trained animals had fewer apoptosis biomarkers than sedentary animals. When an IL-6 blocker was added to the trained animals\u2019 pancreas, the apoptosis biomarkers rose to sedentary levels. The authors concluded that the benefit of exercise training on pancreatic \u03b2 cell survival is through the IL-6 pathway []. However, the results of the present study cannot reinforce the role of IL-6 in preventing insulitis because the CUR group had the same insulitis markers as the C group, which suggests that the effect of exercise training is due to other mechanisms.In conclusion, moderate intensity exercise training has the potential to protect pancreatic \u03b2 cells against an immune response in NOD mice. The limitation of the present study is a lack of mechanisms that establish a causal effect, as well as the lack of an evaluation of other inflammatory markers, and both pancreatic and blood oxidative stress, epinephrine levels, and the characterization of infiltrating immune cells. Those measurements are an important area of future research. Therefore, additional and prospective studies are needed to uncover the mechanisms that explain the link between exercise training and autoimmunity."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0083-2", "title": "Neck Muscle EMG-Force Relationship and Its Reliability During Isometric Contractions", "authors": ["Riccardo Lo Martire", "Kristofer Gladh", "Anton Westman", "Bj\u00f6rn O. \u00c4ng"], "publication": "Sports Medicine - Open", "publication_date": "14 April 2017", "abstract": "Susceptible to injury, the neck is subject to scientific investigations, frequently aiming to elucidate possible injury mechanisms via surface electromyography (EMG) by indirectly estimating cervical loads. Accurate estimation requires that the EMG-force relationship is known and that its measurement error is quantified. Hence, this study examined the relationship between EMG and isometric force amplitude of the anterior neck (AN), the upper posterior neck (UPN), and the lower posterior neck (LPN) and then assessed the relationships\u2019 test-retest reliability across force-percentiles within and between days.", "full_text": "The human neck allows head movements and load-bearing while protecting vital neural structures and stabilizing the visual and vestibular systems []. Its flexibility [], combined with the heavy head weight relative to the neck muscles\u2019 force capacity [, ], however, makes the area susceptible to pain and injury. Neck pain is the fourth leading cause of disability globally [], with an average one-year prevalence of 37% in the general population []. During athletics, where rapidly changing conditions result in high demands on the spine, the neck is also prevalently injured, sometimes gravely [\u2013]. Consequently, the area is subject to scientific investigations, not least within the sport sciences [\u2013].From a functional perspective, the neck can be viewed as a three-part stabilizing system in which support is provided by passive spinal structures, while stability is regulated according to demands via the neck muscles in a feedback-driven neural control scheme []. To coordinate swift variations in spinal posture and loads, a highly optimized system which stabilizes the spine in both static and dynamic situations is required []. Two neural control strategies have been identified for maintaining cervical spine stability in an upright neutral posture: reciprocal muscle activation, which is direction-specific to postural perturbations, and co-contraction of agonistic and antagonistic muscles []. In the sagittal plane, these strategies presumably function primarily via four main neck force-generating muscles, which include the sternocleidomastoids, the semispinalis capitis, and the splenius capitis []. These muscles are all near their maximum force-generating capacity in a neutral spine posture [], wherein perturbations are commonly countered under quasi-isometric conditions [], and are frequently targeted in studies of the neck [\u2013]. It is therefore relevant to investigate them in a neutral spine posture during isometric contractions.Surface electromyography (EMG) is a technique frequently used to examine forces and activation patterns during direct and indirect perturbations to the head [\u2013]. EMG allows indirect estimation of internal loads via Newton\u2019s laws of motion when the EMG-force relationship is characterized, which is essential to injury mechanism elucidation, as direct measurements of internal loads are both infeasible and ethically undesirable. It is generally accepted that the EMG-force relationship is positive; however, reported relationship shapes vary and need to be established separately for individual muscles due to the many factors that influence EMG measurements [, ]. To our knowledge, five studies have, to various extents, examined the EMG-force relationship of the neck during isometric contractions in a neutral spine posture [\u2013]. Reported results have incongruities, but suggest a rectilinear relationship for measurements over sternocleidomastoids and semispinalis capitis, and a curvilinear relationship over splenius capitis. However, because none of these studies provided models, information from the EMG-force relationship cannot be extrapolated. In addition, methodological limitations such as small sample sizes, insufficient statistical analyses, contraction intensities below 50% of maximal force, or ramp contractions, which incorporate incremental speed of contraction intensity as a confounding variable, impede the results\u2019 dependability. Hence, further investigation of the neck muscle EMG-force relationship is necessary to facilitate interpretation of studies based on EMG measurements and to provide a basis for accurate biomechanical models. To adjust for imprecision in force estimation, it is also essential to assess the EMG-force relationship\u2019s reliability. This study therefore examined the neck muscle EMG-force relationship and quantified this relationship\u2019s test-retest reliability.This study examined neck muscles\u2019 EMG-force relationships and assessed the modeled relationships\u2019 test-retest reliability. The main findings were that relationships were rectilinear for the anterior neck (AN) and the upper shoulders (US), and curvilinear for the upper (UPN) and lower posterior neck (LPN). Group-level relationships were similar between trials, and relationship reliability for individual participants was acceptable over most contraction intensities, albeit less reliable between than within days.Consistent with previous findings [, ], AN displayed a near one-to-one rectilinear relationship between EMG and force, and the model provided a good data fit which accounted for 98% of the total variance. This model can therefore aid interpretation of EMG studies when inferences are to be linked to force on a group level and have an accuracy within a 4% mean error margin. In contrast, both UPN and LPN displayed curvilinear relationships and force values were considerably higher than EMG values over most contraction intensities. Assuming a one-to-one relationship for these muscles would therefore lead to underestimated force values if inferences were to be drawn from EMG data. The final models for UPN and LPN had an excellent fit to the data, with 97\u201398% of the total variance accounted for, and an accuracy level within an average 4\u20135% error margin. Our UPN results are in agreement with the findings of three prior investigations [, , ], whereas a fourth study reported a rectilinear relationship []. The standardized weights used to derive the latter study\u2019s results, however, corresponded to a limited range of low intensities in our study, for which, in isolation, the relationship could also be interpreted as rectilinear. In sum, our results for UPN are supported by findings in the literature and therefore likely reflect the actual EMG-force relationship for the electrode placement area. For LPN, one previous study reported a curvilinear relationship [], whereas three prior studies reported rectilinear relationships [, , ], contrariwise to our results. This incongruity is probably a result of the limited contraction range investigated in two of these studies [, ]. Indeed, upon visual inspection of the LPN EMG-force relationship curves in the 0\u201350% range (Fig.\u00a0), it is apparent that a linear curve could be fitted to the data and likely provide an accurate predictive measure, but the measures could not be extrapolated beyond that range, whereas our curve provides predictive values over nearly the whole force range. In the third study [], the few subjects (five) combined with the limited number of investigated contraction intensities (three) is unlikely to have provided the proper resolution to elucidate the curvilinearity of the relationship. In the study in agreement with our results, contractions were measured over 3\u201375% range for 10 females. The larger contraction range examined likely provides the explanation for the agreement. In sum, our results agree with those presented in the literature, with the difference that our measurements over nearly the full contraction range combined with our sample size of 18 allowed us to elucidate the EMG-force relationship over a larger interval and to a more specific degree. Visual inspection of LPN data suggested that piecewise linear models may have a better fit for some individuals. However, a greatly increased complexity rendered the implementation of such models impractical.Overall EMG-force relationships displayed a maximum 6% between-trial discrepancy, with the rectilinear model being more similar between trials than curvilinear models across most of the contraction range (Fig.\u00a0). No clear systematic biases accounting for these discrepancies were observed, and we therefore assume that they are the result of random variation which needs to be considered when inferences are to be drawn from EMG to force on group-level test-retest investigations of similar sample sizes. No analogous data exists for EMG-force relationship reliability on an individual level, and guidelines for relevant point estimate evaluation have therefore not yet been established. However, universal guidelines have previously defined ICC estimates of 0.00\u20130.39, 0.40\u20130.59, 0.60\u20130.74, and 0.75\u20131.00 as poor, fair, good, and excellent, respectively [], and SEM estimates for acceptable reliability range from 7.5\u201320% of the measured variable, with 15% being the most common cutoff []. Interpreting ICC and SEM point estimates in light of these guidelines, within-day and between-day reliability for individual AN EMG-force relationships was excellent over 9 and 12% MVC, respectively. For UPN and LPN, within-day reliability was excellent over 20% MVC, whereas between-day reliability was fair over 58 and 33% MVC, respectively. Hence, reliability was unacceptably low at low contraction intensities, and consistently higher within rather than between days, which supports that individual EMG-force relationships are stable over most contraction intensities within the same day, but can vary considerably for between-day measurements. Universal standards should be interpreted with caution, as they may have considerable limitations when generalized. ICC estimates are context-specific, as magnitude of ICC is inversely related to sample homogeneity [, ]. In this study, the mean between-subject coefficient of variation following 20% MVC ranged from 17\u201327% across sampling areas and was 10\u201318% lower from first- to second-day trials. The lower second-day sample variance therefore provides a partial explanation for the large reduction in ICC estimates between days that was not consistently reflected in the SEM estimates. In contrast to ICC, which suggested that relationship agreement was unacceptably low between trials over certain force intensities, SEM showed that absolute differences remained within an acceptable level. A less conservative interpretation based on absolute between-trial differences alone therefore suggests acceptable reliability for UPN between days over 58% MVC and for LPN past 10% MVC both within and between days. AN and LPN can therefore likely be reliably measured both within and between days for all but the lowest contraction intensities, whereas UPN is limited to within-day measurements over 20% MVC and between-day measurements over 28% MVC.This study has some limitations. The measurements obtained should be construed as location-specific rather than muscle-specific due to the various factors influencing EMG measurements []. One major factor was EMG activity contributions due to co-contraction of adjacent muscles [], which was of particular concern for the UPN measurements, owing to its proximity to the antagonist AN. In contrast, force measurements were limited in that only the net force effect was obtained, while antagonistic activity added to internal loads; however, including antagonistic activity as a covariate in regression models did not improve them, thus indicating their influence to be inconsistent. The complexity of the neck muscle anatomy [] rendered precise electrode placement challenging, and likely decreased signal quality in some measurements. LPN signal amplitude was lower relative to other sampling areas and is attributable to the tendinous structures below the electrodes. The high signal stability and good regression model fit, however, support that valuable information can be extracted from this location, and it is questionable whether upwards relocation of electrodes would improve signal quality, as prior research has also reported low signal amplitudes on posterior muscles at the C4-level [].The EMG-force relationship models provided permit inferences to be drawn from isometric neck muscle EMG measurements to force on a group level, and enable cervical loads to be estimated during activities in EMG-driven biomechanical models. In total, three force estimation models were derived: one for neck flexion and two for neck extension. Of the two latter models, the higher accuracy of the LPN model makes it the better choice for estimating neck extension forces. In addition, the between-trial differences in mean EMG-force relationships suggest that the models are stable over time. They do not, however, allow prediction of individual values, with maximum prediction errors ranging between approximately 30\u201360% force across sampling areas. Further, individual-level relationships showed insufficient reliability over the lower parts of the contraction range, which prevents meaningful analyses from being made for individuals over those intensities. Figure\u00a0 allows force to be estimated visually, while the regression coefficients provided in Table\u00a0 enable precise force calculation. In contrast to many laboratory studies, factors such as temperature, sound level, time of measurement, surrounding crowd, and exact electrode placement were allowed to deviate between trials to render results more representative of field measurements. Combined with the recruitment criteria, the sample\u2019s demography, and neck strength being similar to normative values [], our results may be generalized to relatively fit and healthy adults between 20\u201350 years.Of the studied models, a rectilinear model was best suited for the anterior neck while curvilinear models were best suited for the upper and lower posterior neck. All models had small between-trial variation in the mean and we therefore conclude they can be reliably used to estimate force from EMG on a group level. Models did not have sufficient accuracy for force prediction from single EMG values. However, EMG-force relationships for individual participants typically had acceptable reliability over most of the contraction range both within and between days, suggesting that individual-level relationships remain stable over time. Provided models enable inferences to be drawn from EMG to force on a group level, and reliability estimates facilitate adjustment for measurement imprecision."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0087-y", "title": "Biomedical Risk Factors of Achilles Tendinopathy in Physically Active People: a Systematic Review", "authors": ["Maria Kozlovskaia", "Nicole Vlahovich", "Kevin J. Ashton", "David C. Hughes"], "publication": "Sports Medicine - Open", "publication_date": "18 May 2017", "abstract": "Achilles tendinopathy is the most prevalent tendon disorder in people engaged in running and jumping sports. Aetiology of Achilles tendinopathy is complex and requires comprehensive research of contributing risk factors. There is relatively little research focussing on potential biomedical risk factors for Achilles tendinopathy. The purpose of this systematic review is to identify studies and summarise current knowledge of biomedical risk factors of Achilles tendinopathy in physically active people.", "full_text": "Achilles tendinopathy is one of the most prevalent overuse tendon injuries associated with physical activities such as running and jumping []. It is the most common Achilles tendon disorder, with the highest incidence among runners, track and field athletes and volleyball, tennis and soccer players []. The term \u2018tendinopathy\u2019 is an umbrella term for the description of tendon conditions encompassing pain, swelling and impaired performance [, ]. Achilles tendinopathy can be acute or chronic. Diagnosis is usually made via a combination of clinical history, physical examination with or without medical imaging. The Victorian Institute of Sport Assessment-Achilles questionnaire (VISA-A questionnaire) may also be used to grade the severity of tendinopathy. In the acute phase, the cardinal symptoms are morning pain and stiffness and pain at the beginning and end of exercise sessions, with relief in between. The tendon is diffusely swollen, and there may be palpable crepitus. Tenderness is maximal 2\u20136\u00a0cm above the insertion. In chronic tendinopathy, the tendon remains painful with exercise but the tendon is nodular and thickened rather than swollen and oedematous [, ].Originally, the pain of Achilles tendinopathy was attributed to an inflammatory process. While inflammatory cells have been observed, particularly in the early stages of Achilles tendinopathy, the response does not seem to be a traditional inflammatory response [, ]. Several models have been proposed to explain the aetiology of Achilles tendinopathy [\u2013]. The continuum model of tendon pathology suggested by Cook and Purdam consists of three stages: reactive tendinopathy, tendon dysrepair and degenerative tendinopathy []. The first stage of the pathology results from acute overload of the Achilles tendon and can be characterised as a non-inflammatory proliferative response in the cell and matrix. The second stage is described as attempted tendon healing, through increased production of collagen and proteoglycans. Degenerative tendinopathy is the third stage and characterised by potentially irreversible changes in cell and matrix condition such as tenocyte apoptosis and matrix disorder []. Conservative, non-invasive management is the initial treatment of choice for Achilles tendinopathy, particularly in the early phases []. Recalcitrance to conservative management is not uncommon however, and some clinicians argue that conservative management of chronic Achilles tendinopathy is time consuming and unsatisfactory []. Surgery is sometimes recommended after exhaustion of conservative treatments, but response rates are variable [].Intrinsic and extrinsic aetiological factors interact in the genesis of Achilles tendinopathy. Intrinsic risk factors include demographic factors (sex, age, weight and height) and genetic polymorphisms; and local anatomical factors include leg length discrepancy, malalignment and decreased flexibility. Extrinsic factors comprise therapeutic agents (corticosteroids, antibiotics), environmental conditions, and physical activity-related factors, including training patterns, technique and equipment [, ]. The majority of the studies published over the last two decades are dedicated to the investigation of anatomical features and biomechanical faults as possible causes of Achilles tendinopathy. Several reviews have identified certain anatomic characteristics and biomechanical and training load errors as risk factors for Achilles tendinopathy [\u2013].The contribution of biomedical risk factors to the development of Achilles tendinopathy remains unclear. These may include medical comorbidities and physiological, biochemical and genetic factors. Imaging and histopathological analysis of the degenerative and recovery processes in tendon have provided a better understanding of the processes associated with tendon structure and metabolism. In terms of biochemical and metabolic processes, tendon tissue is relatively inert, with new collagen synthesis being a slow process []. Looking at the genetic coding of proteins comprising tendon structure may lead to a better understanding of the underlying molecular processes. Several genetic association studies have demonstrated that genetic polymorphisms influence collagen tissue structure, its turnover, degradation processes and therefore susceptibility to tendon injuries. Medical comorbidities and treatment of medical conditions also affect tendon structure and function [, ].There is relatively little research focussing on potential biomedical risk factors for Achilles tendinopathy. The purpose of this systematic review is to identify studies and summarise current knowledge of biomedical risk factors of Achilles tendinopathy in physically active people. This information will contribute to the understanding of Achilles tendinopathy and may inform future prevention strategies.The selected studies focused on a diverse range of biomedical risk factors for Achilles tendinopathy such as demographic factors, chronic medical conditions, lifestyle habits and genetic factors, including those contributing to collagen structure and tendon homeostasis and those involved in apoptosis and inflammation. The majority of the included studies were case-control studies (Fig.\u00a0). The main limitation of the included studies was their level of evidence. According to the recommended OCEBM hierarchy, cohort and case-control studies approach levels of evidence 3 and 4, with systematic reviews and randomised controlled trials providing levels 1 and 2 evidence [].This limitation must be taken into consideration when reviewing the results of these studies. Here, we discuss the different approaches undertaken and highlight the main findings obtained in the search for the key biomedical risk factors for Achilles tendinopathy.It is clear from previous research that biomechanical issues and training load errors are risk factors for Achilles tendinopathy []. This systematic review suggests that biomedical risk factors are an important consideration in the future study of propensity to the development of Achilles tendinopathy. Increased BMI and adverse lipid profile were associated with tendinopathy and may be important biomarkers of tendon pathology. It is evident that certain genetic markers contribute to the risk profile of Achilles tendinopathy; however, the demonstrated associations are currently somewhat ambiguous, and predictive power has not been demonstrated. Further investigation is required in this area. In addition, there appears to be diversity in the genes, dependent on geographical differences that are significantly associated with Achilles tendinopathy. This suggests that genetic risk factors for tendinopathy might be modified by geographic factors. It is evident that the risk of Achilles tendinopathy conferred by biomedical factors is complex and may be a result of the interplay between various genetic, biochemical and systemic factors which may be exacerbated by physical load. Further elucidation of biomedical risk factors will aid in the understanding of tendon pathology and patient risk, thereby informing prevention and management strategies for Achilles tendinopathy."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0089-9", "title": "Efficacy and Tolerability of Peritendinous Hyaluronic Acid in Patients with Supraspinatus Tendinopathy: a Multicenter, Randomized, Controlled Trial", "authors": ["C\u00e9sar Flores", "Ram\u00f3n Balius", "Guillermo \u00c1lvarez", "Miguel A. Buil", "Luisa Varela", "Carlos Cano", "Joaqu\u00edn Casariego"], "publication": "Sports Medicine - Open", "publication_date": "5 June 2017", "abstract": "Physical therapy and peritendinous hyaluronic acid (HA) injections have both shown promising results in the treatment of shoulder tendinopathies. However, the superiority of treatment combining physical therapy and HA is unclear.", "full_text": "Tendinopathy is a common injury in athletic populations, secondary to overuse [, ]. However, it may also occur in the overall population as a result of repetitive or excessive loading, and abnormal or unusual movements []. Also, metabolic- and hormone-related clinical conditions such as diabetes, menopause, and adiposity have been identified as systemic risk factors for tendinopathies [\u2013]. Although the incidence of tendinopathies in the overall population has not been established, different authors have reported a rising trend associated with increasing interest in sport activities in high-income societies [, , ].The first-line pharmacological treatment for tendinopathies is most often based on non-steroidal anti-inflammatory drugs (NSAIDs), which are useful for pain control but may cause patients to ignore warning symptoms, resulting in further damage to the affected tendon [, ]. Intratendinous injections of corticosteroids are also common in the standard management of tendinopathies and appear to successfully reduce inflammation and pain at short-term; however, the risk-benefit ratio of their use in the treatment of tendinopathies is currently controversial [, ]. Owing to the limitations of pharmacological treatments in modifying the structure of the tendon, the management of tendinopathies usually includes other non-pharmacological interventions with proven benefits in tendon recovery such as relative rest, cold, ultrasound, and physical therapy, specifically eccentric style exercises and stretching to prevent stiffness [, ]. Other non-pharmacological interventions proposed include shock wave therapy, iontophoresis, sclerotherapy, and nitric oxide patches; however, results are not very consistent and multicenter trials are needed to confirm their efficacy. Among all of the physical therapeutic interventions, eccentric strengthening has shown remarkably good therapeutic outcomes in the treatment of various tendinopathies such as those affecting the patellar, Achilles, supraspinatus, and wrist tendons [, \u2013].Among the recent advances in the management of tendon disorders, the peritendinous administration of hyaluronic acid (HA) has shown promising results in clinical trials including patients with tennis elbow [], patellar tendinopathy [], Achilles tendinopathy [, ], and various disorders involving tendons in the rotator cuff [\u2013]. HA is the primary component of synovial fluid and provides the joint with lubrication and shock absorption []. Although the mechanisms of action in the treatment of tendinopathies are not well established, peritendinous injection of HA has shown to reduce tendon adhesion, provide mechanical protection, and upregulate the vascular endothelial growth factor and type IV collagen, resulting in the acceleration of tendon healing [\u2013]. In the particular case of tendinopathies involving the rotator cuff, the use of HA injections led to a significant improvement of shoulder function in all published trials [, , , , ]. However, the superiority of HA over other interventions in scales assessing pain is controversial, and few studies have evaluated the efficacy of HA in the treatment of lesions affecting a particular tendon. In this regard, some authors have highlighted the need for further randomized trials to better establish which grade of lesions may benefit most from peritendinous injection of HA [].In this parallel-group, randomized, controlled trial, we investigated the efficacy and safety of peritendinous injection of HA in patients with persistent supraspinatus tendinopathy. To this end, we compared the therapeutic outcome of treatment with HA as an adjuvant to physical therapy with that of physical therapy as sole therapeutic intervention.In this multicenter, parallel-group, randomized trial including patients with supraspinatus tendinopathy, we found that both physical therapy alone and physical therapy combined with subacromial HA injections had good tolerability and resulted in significant pain reduction and successful functional recovery. When compared to physical therapy only, patients treated with physical therapy and HA returned significantly earlier to work and needed fewer rehabilitation sessions.The study groups were well balanced regarding the baseline demographic and clinic characteristics, including the prevalence of the affected shoulder, which was mostly the right one, as expected for an overuse injury. NSAID consumption was greater in the HA group; however, no significant differences arose between the two groups. Likewise, we did not observe significant differences in the baseline scores of the assessed scales, except the TSK, which was significantly higher in the HA group. The TSK has been little used in the assessment of HA treatment efficacy and measures the perceived threat of movement and expected pain in injured patients. In this regard, the fear of an injected treatment (not existing in the control group, as no placebo treatment was administered) could explain the discrepancy between the different TSK score and the homogeneity in other baseline measures of pain, such as the VAS score or NSAID consumption. Overall, the baseline scores of the TSK, VAS, and ADL scales revealed a moderate severity of the tendinopathy in our study patients.After 90\u00a0days of follow-up, patients in both groups experienced a significant improvement in the VAS, ADL, and TSK scores, irrespective of the intervention received. This result is consistent with previous studies proving that treatment based solely on eccentric strengthening is sufficient for mid-term improvement in pain and function []. When compared with physical therapy only, we found that the addition of treatment with subacromial HA significantly reduced the number of rehabilitation sessions and the treatment days needed for recovery. The reduction in the TSK score during the follow-up period was also greater and occurred earlier in the HA group, although the baseline differences between scores in each group might have influenced the differences observed during follow-up. In line with the better therapeutic outcome observed in the HA group, both patients and physicians rated significantly higher the efficacy of the combined treatment of physical therapy and subacromial HA than physical therapy only. In the case of physician perception, combined treatment was rated significantly higher than physical therapy from the first visit (day 7).In light of the differences observed between study groups in the indirect measures of efficacy, we would also expect more remarkable differences between groups in treatment outcomes related to pain and the ability to perform ADL. However, NSAID consumption and the assessment of the VAS and ADL scores did not reveal significant differences between patients treated with physical therapy only and those treated with physical therapy and subacromial HA injections. Of note, as a clinical study performed on a medical device, the doses of concomitant medicines were not recorded, and therefore, no dose adjustment was performed in the comparative analysis. Considering the proven efficacy of physical therapy in the treatment of tendinopathies of the rotator cuff, it seems reasonable that a larger sample size would provide clearer differences between groups. Nevertheless, the superiority of HA treatment in pain improvement is unclear, and results in previous studies assessing the efficacy of HA injections in patients with tendinopathies affecting the rotator cuff are inconsistent. Merolla et al. found that HA monotherapy significantly reduced pain in more visits than physical therapy only [], whereas Sengul et al. reported similar pain scores in patients receiving both interventions []. Similar discrepancies have been found when investigating the efficacy of a combined treatment using HA injections and physical therapy. Thus, Meloni et al. concluded that rehabilitation exercises and HA injections were significantly more efficacious in pain reduction than the same exercises accompanied with placebo injections containing sodium chloride []. On the other hand, Ozgen et al. compared the efficacy of HA injections with that of physical therapy and found no differences in pain improvement in most of the follow-up visits []. Of note, the baseline VAS scores reported by Ozgen et al. in the study sample were remarkably small (below 1), which may have contributed to narrow the differences between groups. On the other hand, the ADL score has been barely assessed in trials investigating the efficacy of injected HA in patients with tendinopathies of the rotator cuff. In our study, all participants received clear instructions not to undertake physical labor with the affected arm until recovery. Such indication, not included in pivotal trials assessing the reliability of the ASES score [, ], allowed us to limit the bias related to the different jobs and activities performed by each patient; however, it was likely to influence the overall ADL score, as patients may have limited their day-to-day activities, regardless of their functional recovery and pain.In line with safety data previously reported in the literature [, , ], both interventions were safe and well tolerated, with a 12% rate of adverse events rate\u2014all of them mild\u2014, mostly reported by patients in the control group. Furthermore, both patients and physicians rated tolerability over 2.5 points on a 4-point Likert scale during the entire follow-up. Perceived tolerability progressively increased throughout the follow-up period, and no significant differences were observed between the study groups.The main limitation of our study is the lack of control groups. Unlike other studies, aimed at investigating the efficacy of HA injections alone, our objective was to compare the combined treatment with the gold-standard for the management of tendinopathies. The presence of a placebo group with non-active peritendinous injections might have limited possible biases, particularly in patient-reported efficacy. Accordingly, the inclusion of a sham physical therapy would have yielded more extreme differences between groups. However, including more than two intervention groups would have dramatically reduced the size of each group. In fact, the final sample size (84 patients) was larger than that in most studies assessing the efficacy of HA injections in tendinopathies of the rotator cuff [], but it still limited the observation of significant differences in some scales. Along the same lines, the open-label HA administration may introduce a bias in both the physician and patient efficacy assessment. Finally, due to the sample size limitations, all centers were considered as a whole and no cluster analyses were performed to explore the center bias.Despite the limitations of our study, the results obtained in most of the assessed scales are consistent and support the use of subacromial HA injections as adjuvant treatment to physical therapy in the management of supraspinatus tendinopathy. In our experience, treatment based only on physical therapy was sufficient to reduce pain and improve function. In addition, the combination of physical therapy with subacromial HA reduced the number of rehabilitation sessions and recovery time. These findings suggest that combined treatment with physical therapy and HA may not only increase patients\u2019 quality of life but it might also benefit the healthcare system and society by reducing rehabilitation costs and time off work."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0090-3", "title": "Relationship Between the Relative Age Effect and Lengths of Professional Careers in Male Japanese Baseball Players: a Retrospective Analysis", "authors": ["Hiroki Nakata"], "publication": "Sports Medicine - Open", "publication_date": "2 June 2017", "abstract": "The mechanisms underlying the relative age effect in sport events have been investigated for more than two decades. The present study focused on the relationship between the relative age effect and lengths of professional careers among professional male Japanese baseball players.", "full_text": "The relative age effect is regarded as a contributing factor to sporting success. For example, the Federation Internationale de Football Association (FIFA) uses a system for youth soccer with January 1 as the cut-off date to establish its age groups. Within the same age category, a difference of almost one full year may exist between the oldest and youngest participants. Therefore, relatively older children within a particular age group are more likely to achieve sporting success. This phenomenon has been called the relative age effect. Relatively older children have advantages in growth, biological maturity, and cognitive development []. In addition, relatively older children (athletes) have a greater opportunity to participate in competitions and, consequently, may enhance their psychological, technical, and tactical abilities, thereby supporting greater athletic development []. The relative age effect has been confirmed in many types of sports, including baseball [, ], soccer [\u2013], tennis [], cricket [], basketball [, ], NASCAR [], sumo wrestling [], rugby [], judo [], ice hockey [\u2013], and winter sports [\u2013].Moreover, several studies have examined the relative age effect from a historical perspective [, , \u2013]. It generally takes several years or decades for a sport to gain popularity in a given country. Thus, historical analyses are needed in order to clarify the beginning of the relative age effect in a country and compare differences in the skew of this effect among generations.The present study focused on how long the relative age effect continues into adulthood because most studies have focused on junior players, while, to the best of our knowledge, only a few studies have examined this topic. We previously reported that the relative age effect persisted among players older than 22\u00a0years of age when, theoretically, no physical advantage is expected for older players []. The relative age effect has been demonstrated in professional athletes who graduated university (college) at 22\u00a0years old; however, this relationship was weaker than that among those who graduated high school at 18\u00a0years old. Steingr\u00f6ver and colleagues [] recently investigated whether relative age influenced career lengths in the National Basketball Association (NBA), National Hockey League (NHL), and National Football League (NFL). They showed that the number of matches played was significantly larger in relatively younger players than in relatively older players in the NHL. No significant differences were observed in career lengths in the NBA or NFL between relatively younger and older players.The present study examined the relationship between the relative age effect and lengths of professional careers among professional male Japanese baseball players. Steingr\u00f6ver and colleagues [] reported significant differences in career lengths between relatively younger and older players in the NHL; however, this relative age effect needs to be confirmed in other countries if universal factors are truly related to this effect. In other words, even if a significant relative age effect is observed in a country, the popularity and system of a sport differ among sports and countries. In Japan, a unique annual-age grouping has been applied since 1886, which is between April 1 and March 31 of the following year. Therefore, April 1 is the beginning of the \u201cnew year\u201d (i.e., cut-off date), and this specific calendar follows an education system including elementary, junior high, and senior high schools and university (college), government, and companies. Sports calendars also follow this system. Thus, players born in April, May, and June are expected to have a relative age advantage. Grondin and Koren [] reported that the relative age effect for baseball was more important in Japan than in the USA because large numbers of Japanese players were born during Q1 (April\u2013June). Based on these backgrounds, a relative age effect was hypothesized to exist on the lengths of professional careers among Japanese professional male players.The present study investigated the relative age effect on the lengths of professional careers among male Japanese professional baseball players. The results revealed that the number of players with professional careers of more than 19\u00a0years was markedly smaller in Q4 than in Q1, Q2, and Q3. In addition, relative age was found to be a very important factor for the development of expertise among Japanese male baseball players and may involve long-term disadvantages after becoming professional players. In other words, even if relatively younger players became professional players, their talent may not be sufficient to continue for a long career such as more than 19\u00a0years, or they may be more likely to drop out of a professional career. This result was in contrast to previous findings showing the absence of a relative age effect in the NBA and NFL, and the favoring of relatively younger players in the NHL [].It is difficult to explain why relatively older baseball players have longer professional careers. One explanation is disadvantages in childhood. Relatively older players may have greater opportunities for selection and experience in childhood because they are naturally heavier, taller, stronger, and faster; have greater endurance; and are more coordinated than younger players during childhood [], all of which translate into performance advantages in most sports []. This may lead to more long-term advantages for relatively older players in adulthood because of the development of self-confidence in childhood; however, these advantages are expected to become less apparent towards adulthood when physical maturity evens out. A second explanation is that this phenomenon is specific to Japan and Japanese professional male baseball players because many activities related to sports and academics are based on a unique cut-off date (April 1), which is not the case in other countries. Furthermore, as described in the \u201c\u201d section, the relative age effect for baseball is more important in Japan than in the USA []. These possibilities may interact. Further studies are needed in order to elucidate the mechanisms responsible for this phenomenon in more detail.As a limitation of the present study, the lengths of professional careers may be associated with many factors other than the relative age effect. Baker and colleagues [] reported that career lengths in Major League Baseball (MLB) were longer for infielders than for outfielders and catchers. Koz and colleagues [] also showed a significant negative relationship between the draft round and games played in the NHL, NBA, and NFL and fielding players in MLB. However, the present study did not focus on the playing position or draft round. These factors need to be examined in more detail in future studies. In addition, in the present study, the significant relationship between the relative age effect and lengths of professional careers was only observed in Japanese male baseball players. Thus, this relationship needs to be examined in other sports including soccer, volleyball, Ekiden (a long-distance relay running race on roads), basketball, and sumo wrestling because these sports include a significant relative age effect among Japanese male athletes [].The results of the present study provide additional information for elucidating the mechanisms underlying the relative age effect in professional sports. Our results suggest that the relative age effect in professional sports may be related to the lengths of professional careers."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0091-2", "title": "The Validity and Responsiveness of Isometric Lower Body Multi-Joint Tests of Muscular Strength: a Systematic Review", "authors": ["David Drake", "Rodney Kennedy", "Eric Wallace"], "publication": "Sports Medicine - Open", "publication_date": "19 June 2017", "abstract": "Researchers and practitioners working in sports medicine and science require valid tests to determine the effectiveness of interventions and enhance understanding of mechanisms underpinning adaptation. Such decision\u00a0making is influenced by the supportive evidence describing the validity of tests within current research.\u00a0The objective of this study is to review the validity of lower body isometric multi-joint tests ability to assess muscular strength and determine the current level of supporting evidence.", "full_text": "Testing of specific strength capabilities is a critical aspect of understanding the strategies that best enhance muscular strength [, ]. In assessing strength, Tillin et al. [] recommended multi-joint rather than single-joint testing due to the specific neural and mechanical conditions in athletic performance tasks such as sprinting and jumping. Additionally, Gentil et al. [] found no increased motor unit recruitment in single- versus multi-joint muscle actions. Isoinertial tests such as repetition maximum back-squats are frequently used to test lower body multi-joint strength [\u2013]. Whilst isoinertial tests are common, associated methodological considerations challenge their validity to assess changes in muscular strength []. These considerations include approaches to squat depth, technical skill required to complete the range of movement under high external load as well as the number of trials required to build up to a maximal test load [\u2013]. Previous research has reported that isoinertial tests lack practicality due to limitations in using isoinertial repetition maximum tests with certain populations, such as novice, elderly or functionally limited participants [, , ].It is suggested that isometric multi-joint tests (IMJT) provide a time efficient assessment of muscular strength [, ] that allows less interruption to training compared to isoinertial repetition maximum testing. Given the potential practical merit, it is important to understand the overall validity of IMJTs. The predominance of research to date investigating IMJTs has assessed their specific correspondence to dynamic performance tasks, such as jump height [], change of direction [] and sprint performance []. However, this work does not enhance understanding of IMJTs validity as a measure of muscular strength. Therefore, research attention is required to evaluate the evidence directly investigating validity properties of IMJTs as an assessment tool to evaluate muscular strength.Validity of a test refers to the degree to which it measures what it intends. Many different properties of validity can be assessed to examine the efficacy of a test. These properties include face and content validity, based on a judgement that the test is likely to measure the intended construct and that the test adequately represents the construct of interest []. Criterion validity is defined as the extent a test adequately reflects scores on a \u2018gold standard\u2019 test measuring the same construct []. Construct validity is the level to which the test measures the intended construct and the inference that can therefore be made from the scores. Construct convergent/discriminant validity relate to the extent to which test scores correlate/or not with scores on another test of the same construct. Construct validity for known groups is the degree test scores differ between groups known to be different on the variable of interest []. Hypothesis testing is the level to which measured values reflect the pre-defined hypotheses in terms of expected magnitude and direction of correlations or differences []. Reproducibility refers to the extent repeated measures (test-retest) provide similar results and is comprised of both reliability and agreement parameters []. Agreement concerns the intra individual difference between measurement (absolute measurement error) and reliability refers to the level of variance between two or more measurements that is due to the \u2018true\u2019 difference []. Responsiveness (longitudinal validity) is defined as the ability of a test to detect change over a time [] and should be described in relation to the smallest detectable change. The smallest detectable change can be measured from test-retest studies provided the length of time between tests is appropriate and the variable being measured remains stable between tests []. It is important that the defined components of validity are investigated to understand the efficacy of tests and for researchers to make appropriate decisions on their use.To determine the appropriateness of IMJTs to evaluate muscular strength and responsiveness to resistance exercise interventions, knowledge of validity properties is essential. Noting previous recommendations that validity is accumulated from multiple studies and cannot be demonstrated \u2018once and for all\u2019 by any single study [], there is a clear requirement for a synthesis of evidence for IMJT. As such, the aims of this review were to determine the current evidence for IMJTs to assess muscular strength. We hypothesized that IMJT would have strong evidence in terms of reliability demonstrating intraclass correlation coefficient (ICC)\u2009\u2265\u20090.70. Additionally, criterion validity, construct validity and responsiveness to resistance exercise interventions would have moderate evidence supporting the validity of IMJT.The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed []. The updated Consensus-based Standards for the Selection of health Measurement Instruments (COSMIN) checklist [] was applied as a method to critique the methodological quality and rating of measurement properties of individual research articles.Assessment of physiological mechanisms and adaptations associated with resistance exercise is critical to improve understanding and efficacy of interventions []. The practical benefits of isometric multi-joint tests to assess resistance exercise interventions have been previously discussed [, , , ]. The aims of this review were to determine the level of evidence for IMJTs in their assessment of muscular strength.Strong evidence was found for reliability of combined IMJTs including strong evidence for the isometric squat test and moderate evidence for the isometric mid-thigh pull independently. ICC values for reliability of peak force in the seven studies included in the best evidence synthesis ranging from \u22650.80 to 0.99, well above the acceptable threshold for ICC >0.7 as discussed in Baumgartner and Chung []. Researchers and practitioners can be confident with the reliability of IMJT measures of muscular strength. All best evidence synthesis inclusions within this review used a repeated trials design conducted on the same day. No studies to the authors knowledge used a day-to-day variability in measurements design (stability reliability) as defined by Baumgarter []. Atkinson and Nevill [] have previously cautioned that exercise performance tests are affected by systematic bias. As such, reliability investigations may benefit from greater than one day between tests to get a true measurement on day to day variability. This review therefore highlights that stability reliability warrants further investigation, whereby studies using experimental designs account for day-to-day variability in measurements.Our results demonstrate strong evidence to support construct validity (hypothesis testing) for combined isometric multi-joint tests, with moderate evidence for the isometric squat test and isometric mid-thigh pull, respectively, as independent tests. Primarily, the experimental design of three studies in the best evidence synthesis were correlational [, , ] with one study implementing an acute responsiveness design. No studies within the best evidence synthesis employed a study design assessing IMJTs discriminant validity, to investigate the difference between known groups. This is an additional aspect of validity that requires further exploration to fully understand the efficacy and application of IMJTs.It was hypothesized that criterion validity would be supported with moderate level evidence based on knowledge of existing literature examining the relationship of isometric tests to dynamic performance tests. Additionally, Juneja et al. [] suggests isometric strength testing has a strong potential to predict dynamic performance in strength based activities. Contrary to our hypothesis and previous suggestions [], no appropriate evidence supporting the criterion validity of IMJTs was found. With nine studies in the overall review rated as poor methodological quality for criterion validity, none were accepted into the best evidence synthesis. Within this review, the isoinertial repetition maximum was defined as the gold standard comparison. As such, findings are not equally comparable with previous work by Juneja et al. [] who evaluated criterion validity with various dynamic performance tests. This review highlights a paucity of evidence to support criterion validity of IMJTs relating to isoinertial repetition maximum performance. The current lack of evidence for criterion validity of IMJTs is due to eight studies within this review rated poor in methodological quality based on insufficient participant numbers to satisfy a higher rating. However, seven of these studies were rated positive for measurement properties with correlations 0.70 with the gold standard test. This demonstrates a likelihood of strong evidence for criterion validity where future research investigating this critical component of validity satisfies key methodological criteria, such as appropriate participant numbers.Moderate evidence was found supporting responsiveness for combined IMJTs in keeping with our hypothesis. Surprisingly this evidence was found only in studies using the isometric squat test [, ]. The principal reasons for studies not being included in the best evidence synthesis were due to insufficient participant numbers or absence of longitudinal interventions to assess responsiveness. Whilst multiple studies in this review [, , , , ] use acute response designs and receive poor methodological quality rating, they may have some generalizable merit for sports medicine and science readers. Given the resistance exercise intervention used by Alegre et al. [] was classified as a muscular power intervention, only one study within the best evidence synthesis has examined responsiveness to a muscular strength intervention using high-intensity loading. Therefore, the use of moderate and high intensity loading schemes within resistance exercise interventions warrant further investigation to assess responsiveness of IMJTs.Common methodological protocols for IMJTs are apparent amongst the best evidence inclusions (see Table.\u00a0). Typically, studies use a 5-s test duration, 2\u20133 trials per testing session and between trials recovery time of 3 to 5\u00a0min. The instruction given to participants is consistently to push or pull as \u2018hard and as fast as possible\u2019 dependent on the type of the test, in all but one included study []. Methodological approaches to familiarization of participants, instruction around pre-tension and the processing of the trials was found to be variable within current literature. Joint angle at which the isometric test occurs is another methodological variation amongst studies, although inclusions within the best evidence synthesis in this review utilize a knee angle approximately 120\u00b0 of flexion with one study using a 90\u00b0 knee angle [].IMJTs have been utilized as a measurement tool within 59 studies analyzed within this review. Researchers and sports practitioners based on strong evidence supporting their efficacy can confidently utilize isometric multi-joint tests with respect to reliability and construct validity. The findings of this review are generalizable to male, female, trained and untrained participants. IMJTs have demonstrated moderate responsiveness to resistance exercise. Future work to investigate this component of validity would further the understanding of current evidence. Despite the plethora of investigations examining critical aspects of validity, caution is urged in the application of IMJTs in relation to measurement error and criterion validity. Variability in test protocols must be carefully considered when interpreting IMJTs outcomes; therefore, authors are encouraged to provide comprehensive details on their respective testing protocols."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0092-1", "title": "Efficacy of wearing compression garments during post-exercise period after two repeated bouts of strenuous exercise: a randomized crossover design in healthy, active males", "authors": ["Kazushige Goto", "Sahiro Mizuno", "Ayaka Mori"], "publication": "Sports Medicine - Open", "publication_date": "3 July 2017", "abstract": "The efficacy of wearing [a] compression garment (CG) between repeated bouts of exercise within a same day has not been fully understood. The present study determined the\u00a0effect of wearing a\u00a0CG after strenuous exercise sessions (consisting of sprint exercise, resistance exercise, drop jump) twice a day on exercise performance, muscle damage, and inflammatory responses.", "full_text": "Since athletes commonly perform intensive physical training or competitions on consecutive days, facilitation of recovery process is important to maximize competitive success and to prevent excessive fatigue []. Several strategies are currently employed on sports fields to aid the recovery process, including massage [], active recovery [], water immersion [], contrast bathing [], and hyperbaric oxygen supply []. In addition, the use of compression garments (CG) during post-exercise period has been recently increasing attention as a novel option to promote muscular strength recovery and to attenuate exercise-induced muscle damage [\u2013].Although some evidences exist for beneficial effects of wearing a CG during exercise [\u2013], majority of previous studies failed to support the performance-enhancing effect of the use of a CG during exercise [, \u2013]. In a latest review [], CG did not reveal positive effects on running performance, maximal and submaximal oxygen uptake, or the performance of strength-related tasks after running. In addition, MacRae et al. [] suggested that the use of CG may have had some help for certain aspects of jump performance in some situations []. However, only limited evidence [] showed the beneficial effects of CG on the performance of other exercise types (e.g., pedaling exercise, running). Alternatively, improvement of recovery by wearing a CG during post-exercise period is more apparent [, , , \u2013]. Kraemer et al. [] demonstrated that wearing a whole-body CG for 24\u00a0h after resistance training caused rapid recovery of power output for the bench press throw and attenuated muscle soreness with lower creatine kinase (CK) concentration on the following morning. Jakeman et al. [, ] reported that the recovery of jump performance following 100 plyometric drop jumps was significantly improved when the subjects wore a CG during post-exercise period. Furthermore, we have previously reported that recovery of muscular strength for upper limb muscles was significantly improved during early phase (3\u20138\u00a0h) of post-resistance exercise period by wearing whole-body CG. However, for the lower limb muscle, a significantly faster recovery of muscular strength occurred at 24\u00a0h after exercise []. In a previous study using endurance exercise, wearing CG for 24\u00a0h after 30\u00a0min of downhill running promoted significantly recovery of counter movement jump height []. Potential factor for promoted recovery by CG during post-exercise period is suggested to be reductions of venous blood pooling and subsequent swelling in muscles []. In addition, Born et al. [] pointed out that the use of CG during post-exercise may assist performance recovery.Athletes are often required to conduct strenuous exercise or competition twice a day, separated with several hours (12\u00a0h <) of rest. However, the influence of wearing CG between the repeated bouts of exercise within a same day has not been fully understood. Duffield et al. [] reported that combined treatment of cold water immersion (15\u00a0min at 10\u00a0\u00b0C) and wearing CG (during 3\u00a0h) after the tennis specific drill and match play sessions promoted recovery of counter movement jump (CMJ) height at the beginning of subsequent match play. However, due to the recovery enhancing effect by cold water immersion [\u2013], the impact of the use of CG itself has not been identified.Therefore, the purpose of the present study was to determine effect of the CG during post-exercise period after two repeated bouts of exercise (including repeated sprint exercise, resistance exercise, drop jump) on exercise performance (e.g., power output during bench press exercise, jump height, rebound jump index), muscle damage, and inflammatory responses. We hypothesized that the use of CG during post-exercise period after the two repeated bouts of exercise would promote recovery of muscle function.Blood lactate concentrations were markedly increased after Ex1 and Ex2 (main effect for time: \u2009=\u20090.00, \u2009=\u200973.5, partial \n                        \u2009=\u20090.88). However, there responses were similar between the CG and CON trials, and no significant difference between the trials was not observed at any time points (interaction: \u2009=\u20090.38, \u2009=\u20090.93, partial \n                        \u2009=\u20090.09). Similarly, exercise increased significantly blood glucose concentration (main effect for time: \u2009=\u20090.00, \u2009=\u200919.2, partial \n                        \u2009=\u20090.66), with no significant difference between the two trials (interaction: \u2009=\u20090.18, \u2009=\u20091.61, partial \n                        \u2009=\u20090.14, main effect for trial: \u2009=\u20090.40, \u2009=\u20090.76, partial \n                        \u2009=\u20090.07).The score of subjective muscle soreness at 24\u00a0h during post-exercise period was significantly lower in the CG trial than in the CON trial for pectoral major muscle [CG: 33\u2009\u00b1\u200921\u00a0mm, CON: 48\u2009\u00b1\u200925\u00a0mm, \u2009=\u20090.04, d\u2009=\u20090.65], while the value was inversely lower in the CON trial than in the CG trial for hamstring [CG: 43\u2009\u00b1\u200924\u00a0mm, CON: 34\u2009\u00b1\u200926\u00a0mm, \u2009=\u20090.047, d\u2009=\u20090.36]. There was no significant difference in scores of subjective muscle soreness for biceps branch, triceps brachii, or quadriceps femoris. Exercise significantly increased score of subjective fatigue (main effect for time: \u2009=\u20090.00, \u2009=\u200964.26, partial \n                        \u2009=\u20090.87). However, time course of change in subjective fatigue was not significantly different between the two trials during post-exercise period (interaction: \u2009=\u20090.52, \u2009=\u20090.85, partial \n                        \u2009=\u20090.08, main effect for trial: \u2009=\u20090.75, \u2009=\u20090.11, partial \n                        \u2009=\u20090.01).In the present study, we have determined influence of wearing CG during post-exercise period on changes in exercise performance and exercise-induced muscle damage markers in response to two repeated bouts of training sessions separated with 4\u00a0h of rest period. Consequently, time-course changes in exercise performances for lower limb muscles or muscle damage markers in blood were similar between CG and CON trials. For the upper body muscles, no significant interaction (trial\u2009\u00d7\u2009time) or main effect for trial was found for MPO during bench press exercise. However, the use of CG revealed faster recovery of the MPO 4\u00a0h after the first bout (Ex1) and second bout (Ex2) of exercise sessions (\u2009>\u20090.05 vs. baseline value), whereas in the CON trial, the MPO remained significantly lower throughout 24\u00a0h of post-exercise period.The height of CMJ and performance variables for RJ did not differ significantly between CG and CON trials over 24\u00a0h of post-exercise period. Moreover, no significant difference in repeated sprint ability was observed between the two trials at any time points. These results differ from the findings from earlier studies in which wearing CG during post-exercise period promoted recovery of CMJ height [, ], MVC [, ], maximal isokinetic strength for lower limb muscles [], and maximal power output during 5\u00a0min of pedaling []. In the present study, all subjects started wearing the CG from immediately after completing first bout of exercise (Ex1), and we have tested whether the recovery of muscle function was improved even during the early phase (4\u00a0h) of post-exercise period. In a previous study by Jakemen et al. [], the subjects wore the CG for 12\u00a0h after 100 drop jumps. Consequently, recovery of performances for squat jump and CMJ was significantly improved by wearing CG at 24\u00a0h, but not at 1\u00a0h during post-exercise period. We have previously observed that wearing CG during post-exercise period facilitated significantly recovery of MVC for lower limb muscles at 24\u00a0h after resistance exercise. However, improved recovery of MVC was not observed at 1, 3, 5, and 8\u00a0h after the exercise []. Therefore, 4\u00a0h of wearing CG after the first exercise session may be insufficient to assist recovery of muscle function for lower limb muscles. Moreover, repeated sprint ability did not differ significantly between the two trials during Ex2 (at 4\u00a0h after completing Ex1) or at 24\u00a0h during post-exercise period. Because exercise-induced muscle damage impairs repeated sprint ability and sprint running performance [], the use of CG during post-exercise period was expected to attenuate impairment of repeated sprint ability. The absence of improved repeated spring ability in the CG trial was inconsistent with a report [] that showed increased performance for repeated sprint performance (10\u2009\u00d7\u200940\u00a0m run) by wearing CG during 24\u00a0h of post-exercise period in rugby players. However, in the present study, repeated sprint ability during Ex2 and at 24\u00a0h during post-exercise period did not differ significantly from the value during Ex1 (baseline value) in either trial, suggesting that the exercise-induced decrement of power output was not evident during post-exercise period.The MPO during bench press exercise significantly decreased immediately after Ex1 and Ex2 in both trials. However, in the CG trial, the MPO was recovered to baseline value following wearing CG for 4\u00a0h after both Ex1 and Ex2, while the values at the same time points remained significantly lower from baseline value in the CON trial. Influence of wearing CG on recovery of muscle function for upper body muscles has not been fully elucidated, but bench press throw power was significantly higher at 24\u00a0h after resistance exercise when the subjects wore the whole-body CG during post-exercise period. In contrast, promoted recovery of muscle function was not observed for lower limb muscles []. We have also shown that recovery of 1RM for chest press was significantly improved at 3, 5, and 8\u00a0h after the resistance exercise by wearing whole-body CG during post-exercise period []. Although somewhat inconsistent results exist [], it is likely that wearing whole-body CG elicits recovery for upper body muscle function rather than for lower body muscles. However, in the present study, the recovery enhancing effect for upper body muscle function by wearing CG was smaller compared with two previous studies using resistance exercise protocols [, ]. The different outcome may be explained by difference in number of resistance exercise employed (four to six exercises in the previous studies vs. three exercises in the present study).According to a recent systematic review and meta-analysis by Hill et al. [], the use of CG after damaging exercise had a moderate effect in reducing the severity of muscle soreness and CK elevation and promoting recovery of muscle strength and power. In fact, 66% of the subjects analyzed (205 subjects in total from different studies) experienced reduced elevation of CK concentration. Similarly, Kraemer et al. [] revealed that serum CK concentration at 24\u00a0h after resistance exercise was significantly lower after wearing whole-body CG during post-exercise period than the value after wearing non-compression garment. The score of muscle soreness for pectoral major muscle was significantly lower in the CG trial at 24\u00a0h during post-exercise period. Although mechanism for reduced muscle damage markers by wearing CG is still speculative, applied pressure by the garments generates an external pressure gradient that attenuates changes in osmotic pressure and reduces the space available for swelling and hematoma to occur []. A reduction of osmotic pressure with attenuating swelling may provide impaired inflammatory action and experience of soreness. In contrast, there were no significant differences between the two trials at 24\u00a0h during post-exercise period for serum CK, Mb, leptin, and plasma IL-6 concentrations. This result is not surprising, because the finding corresponds to previous reports presenting no influence of CG during post-exercise period on muscle damage markers in blood (e.g., CK, Mb, IL-6, C-reactive protein) [, , ].Some limitations in the present study need to be considered carefully. In the present study, a psychological effect cannot be excluded because it is difficult to use CG in completely blinded conditions. However, we did not display detailed information of the prescribed CG, including expected outcomes and hypothesis. Furthermore, blood variables in the present study reflect physiological responses for both upper and lower body muscles, and we cannot clarify the differences in muscle damage and inflammatory responses between upper body and lower body muscles. Finally, we were not able to measure the pressure levels applied for the present subjects, although we have previously determined the pressure levels of the same CG among different subjects []. Therefore, it is possible that inter-individual differences of pressure levels and/or insufficient levels of pressure may have masked efficacy of the CG.From practical viewpoints, the present findings may provide information regarding post-exercise treatment to promote recovery of maximal power output during training schedule with strenuous training sessions twice a day. The facilitation of recovery of muscular power output will be important to improve quality of subsequent training session, and wearing the CG during post-exercise period may have had some positive effects on recovery. Further researches are required to determine the efficacy of combined effects of CG and other traditional treatments (e.g., cold water immersion) on recovery of exercise performance in competitive athletes.In conclusion, wearing whole-body CG during post-exercise period after two bouts of exercise sessions separated with 4\u00a0h of rest period did not promote recovery of muscle function for lower limb muscles or did not affect exercise-induced muscle damage markers in blood among physically active males. However, it was likely that the use of CG during post-exercise period may have had some favorable effect on recovery of power output and severity of muscle soreness for upper body muscles."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0098-8", "title": "Illicit Drug Use Among Gym-Goers: a Cross-sectional Study of Gym-Goers in Sweden", "authors": ["Yasmina Molero", "Ann-Sofie Bakshi", "Johanna Gripenberg"], "publication": "Sports Medicine - Open", "publication_date": "29 August 2017", "abstract": "The use of anabolic-androgenic steroids has increased among gym-goers, and it has been proposed that this may be part of a polysubstance use pattern that includes the use of illicit drugs. Still, epidemiological data on illicit drug use among gym-goers of both genders are meager. The aim of the present study was thus to examine the use of illicit drugs and its correlates in a large sample of men and women who engaged in weight training at gyms across Sweden.", "full_text": "Research shows that 65% of citizens in the European Union exercise at least once a week. Among this group, 30% exercise at sport clubs such as gyms and fitness centers []. In the USA, approximately 21% of adults reported exercising regularly [], and more than 55 million memberships were purchased at health clubs and fitness centers in 2015 []. Exercise is a health-promoting activity associated with several benefits, including reduced risk of coronary heart disease, type 2 diabetes, breast and colon cancer, as well as premature mortality []. At the same time, there is growing evidence indicating that use of anabolic-androgenic steroids and non-medical use of prescription drugs have increased among gym-goers [\u2013]. It has been proposed that this may be part of a polysubstance use pattern that involves other illicit drugs as well (e.g., cannabis and stimulants) []. Polysubstance use is associated with poorer mental health, sexual risk behavior, negative social consequences, and increased risk of infectious disease [, ]. Furthermore, concurrent use of substances may have synergistic negative effects on brain function []. It has been suggested that doping prevention efforts should target gym-goers []. Given the association between doping substances and illicit drugs [, \u2013], prevention efforts could therefore also address the use of illicit drugs []. Still, the research base on illicit drug use among gym-goers is limited, and developing knowledge is imperative in orientating preventive efforts.Several studies have examined the use of illicit drugs among sportspeople through questionnaires or toxicological testing [\u2013]. Results from these studies indicate elevated rates of illicit drug use, with cannabis and stimulants being the most commonly used drugs. However, these samples have been restricted to elite athletes or adolescent populations. Epidemiological studies focusing on illicit drug use among adult gym-goers are few in number. One study on 311 gym-going gay men in New York showed that 6\u201335% (depending on the type of drug) reported having used a drug during the past 6\u00a0months []. Another study on 1592 gay men who attended gyms in London showed that 56% reported having used an illicit drug during the past year []. It was proposed that many of these men do not frequent gyms as a health-promoting activity, but rather to achieve an idealized muscular, physically strong body. This was suggested to be indicative of a gay subculture that focuses on physical prowess and risk behaviors, including illicit drug use []. However, results from these two studies may be difficult to generalize to other gym-attending populations and across gender.The present study is part of a larger on-going project entitled 100% Pure Hard Training (100% PHT) []. In the 100% PHT project, the prevalence of doping substances (i.e., anabolic-androgenic steroids and growth hormones) and other illicit drugs is measured among gym-goers who engage in weight training (i.e., work with free weights or machines), and the effect of a doping prevention programme (i.e., 100% PHT) is examined. In the present study, the use of illicit drugs, benzodiazepines, and doping substances among gym-goers was assessed using a cross-sectional design. This assessment was carried out prior to implementation of the prevention programme.The overall aim of the study is to examine the use of illicit drugs in a large sample of men and women who engage in weight training at gyms across Sweden. Specifically, we conducted a cross-sectional examination of the (a) frequency and type of illicit drugs used, (b) frequency of non-medical use of benzodiazepines, (c) age and sex differences in illicit drug use, (d) associations between use of illicit drugs and weight training frequency, and (e) associations between use of illicit drugs and use of doping substances.Our study examined illicit drug use in a large sample of male and female adult gym-goers. Illicit drug use estimates in our study were slightly elevated in comparison to estimates in population-based studies in Sweden [, ] and could simply reflect illicit drug use in the general population. This may seem contradictory, however, as training at gyms is typically considered a health-promoting behavior and prevalence rates could thus be expected to be lower among gym-goers. Our findings show that a substantial proportion of young adult males who lift weights have used several illicit drugs. This suggests that illicit drug use among sportspeople, possibly for ergogenic or analgesic purposes, is a public health problem not limited to elite athletes [, , ]. A proportion of younger recreational sportspeople may be at risk of developing substance abuse problems (including doping substances), yet there are few arenas on which young individuals can be reached other than nightlife settings and universities [\u2013]. Previous research shows that young people who party frequent gyms to socialize, to offset the effects of substance use, or to purchase illicit drugs []. Gyms could thus provide an additional innovative setting for intervention and prevention efforts targeting doping and illicit drug use, because such establishments already deal with health promotion and do not allow on-site alcohol consumption."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0097-9", "title": "Evidence of Concussion Signs in National Rugby League Match Play: a Video Review and Validation Study", "authors": ["Andrew J. Gardner", "David R. Howell", "Christopher R. Levi", "Grant L. Iverson"], "publication": "Sports Medicine - Open", "publication_date": "22 August 2017", "abstract": "Many professional sports have introduced sideline video review to help recognise concussions. The reliability and validity of identifying clinical and observable signs of concussion using video analysis has not been extensively explored. This study examined the reliability and validity of clinical signs of concussion using video analysis in the National Rugby League (NRL).", "full_text": "Participation in many full contact and collision sports, such as rugby league, carries with it a risk of concussion []. In-game concussion diagnosis, however, remains a highly challenging task for the athletic trainer and sports medicine physician. On-field or sideline clinical assessments can be challenging due to the heterogeneous presentation of an athlete following a head impact, the non-specific nature of many of the clinical signs and symptoms of concussion [], the absence of a reliable concussion biomarker [], and the regularity with which some concussion signs emerge and evolve over time []. Recognising a potential concussion and removing an athlete from play is understood to be an important intervention for reducing the risk of a worse clinical outcome following injury []. However, it is acknowledged that in some instances, concussions may be missed from the sideline []. This may occur for a variety of reasons, but commonly the transient early physical signs may resolve before the player can be removed from play and assessed [].Some prior studies suggest that worse outcomes following concussion are associated with on-field signs and symptoms, such as loss of consciousness [], amnesia [, ], mental status change for more than 5\u00a0min [], and dizziness []. It is important to appreciate that the literature on the association between on-field signs and symptoms is mixed. For example, loss of consciousness has been associated with worse clinical outcomes in some [, , , , , ], but not in most studies [, , , , , , , , , , , \u2013, , ]. The vast majority of studies examining loss of consciousness base this finding on a questionnaire or interview completed with the athlete, not video review of the injury event for confirmation. Similarly, post-traumatic amnesia has been associated with worse clinical outcomes in some [, , ], but not in most studies [\u2013, , , , , , , , , , ]. Dizziness has been observed as an on-field symptom associated with a protracted recovery of greater than 21\u00a0days (6.34 time more likely) [], but assessing dizziness is subjective and may or may not manifest as an objective sign (e.g., gait ataxia). Thus, video review may allow for the quantification of objective concussion signs, but not subjective symptoms.In the sport of rugby league, the concussion incidence rates have been reported to vary widely depending on the level of competition []. In one study of three National Rugby League (NRL) clubs, a concussion incidence rate of 14.8 concussions per 1000 player match hours was reported [], while a rate of 28.3 concussion per 1000 player match hours were reported from one NRL club over a 15-year (1998\u20132012) period [].The use of video footage on the sideline for reviewing a concussion has been introduced in a number of professional sports as a method to improve the recognition of a possible concussion that may be missed by on-field medical personnel []. Video studies have now been conducted in a variety of sports such as rugby league [\u2013], rugby union [], hockey [], and Australian rules football [, , ]. In addition to the introduction of sideline video review, the governing body of the sport in Australia (the National Rugby League) implemented a new the \u201cconcussion interchange rule\u201d (CIR). The CIR requires the mandatory removal of any player suspected of having sustained a concussion. The CIR allows a player to be removed from play for 15\u00a0min to be assessed by the club medical officer, including completing the Sports Concussion Assessment Tool third edition (SCAT-3). Following the assessment, if the player was not diagnosed with concussion, they are permitted to return to play without using an interchange. The incidence of use of the CIR was 24.0 (95% CI 20.7\u201327.9) uses of the CIR per 1000 NRL player match hours [], and 44.9 (95% CI 38.5\u201352.3) uses of the CIR per 1000 National Youth Competition player match hours [].The primary aim of this study was to determine the rate of six objective concussion signs that occurred during the 2014 NRL season, as well as the sensitivity and the specificity of these signs to classify whether or not a diagnosed concussion resulted from the event. The secondary objective of this study was to analyse the intra- and inter-rater reliability of these signs using the video recordings of each incident that activated the CIR.The players who were medically diagnosed with a concussion were significantly more likely (1.52 times) to exhibit unresponsiveness and a vacant stare (2.35 times) than players who were removed from play under the CIR, not diagnosed with a concussion, and were returned to play. Clutching the head and gait ataxia were not significantly different signs between those who were and were not medically diagnosed with a concussion (see Table ).A sign that was always present in cases of diagnosed concussion was slow to get up (sensitivity\u00a0=\u00a0100%), although it had low specificity (50%). A possible seizure was observed only four times during the season and on three occasions those athletes were medically diagnosed as having a concussion (3/60 diagnosed concussions; sensitivity\u00a0=\u00a05%, specificity\u00a0=\u00a0100%). A blank or vacant stare had fairly high sensitivity and specificity (75% and 84%, respectively; see Table ).To expand previous video analysis work in collision sports, this study explored the rate of six observable signs of concussion, as well as their sensitivity and specificity during match play of a National Rugby League season. The results of the current study revealed moderate to perfect intra-rater reliability or better for all video signs. For inter-rater reliability, clutching or shaking head, slow to get up, and unresponsiveness had moderate inter-rater reliability, whereas post-impact seizure had weak inter-rater reliability, and gait ataxia and blank or vacant stare had poor inter-rater reliability. These findings are relatively consistent with those reported from a video review study of concussion signs during Australia Football League (AFL) match play, which found that intra-rater reliability was generally better than inter-rater reliability, and blank or vacant stare was the sign with the lowest agreement [].Blank or vacant stare was the only sign that had reasonably high sensitivity (75%) and specificity (84%); all other signs did not have both high sensitivity and specificity for a concussion diagnosis. Some of the signs had high sensitivity but low specificity, suggesting that those signs may be a useful marker in flagging a potential concussion, but not helpful in confirming the diagnosis. Therefore, medical personnel working in tandem on the sideline and with individuals reviewing video may be an appropriate strategy to achieve a sensitive and specific approach to concussion diagnosis. In a video review of concussion signs during match play in the Australian Football League (AFL), slow to get up had the highest sensitivity (87%), but low specificity (19%). All other video concussion signs examined in this prior study had high specificity but low sensitivity, such as blank and vacant look [specificity 100%, sensitivity 9%], motor incoordination [specificity 95%, sensitivity 29%], impact seizure [specificity 93%, sensitivity 7%], and rag doll appearance [specificity 91%, sensitivity 16%]). The highest PPV was found with the \u201cblank and vacant look\u201d (100%), and \u201cmotor incoordination\u201d (81%) signs []. However, in the current study, we found that the highest PPV was for seizure (75%), while slow to get up (100%), blank or vacant stare (95%), and gait ataxia (91%) were the signs with the highest NPV.The sign with the highest relative risk of diagnosed concussion was a blank or vacant stare (2.35 times more likely; 90% CI\u00a0=\u00a01.40\u20134.30), while unresponsiveness was 1.52 times more likely (90% CI\u00a0=\u00a01.06\u20132.10). Gait ataxia (1.46 times more likely; 90% CI\u00a0=\u00a00.94\u20132.38) and clutching or shaking head (0.77 times more likely; 90% CI\u00a0=\u00a00.55\u20131.12) were not statistically different. Blank or vacant stare appears to be a conceptually different sign to the other five. The detection of a blank or vacant stare tends to be through a secondary screening process, in that it is often only observed if, and when, a player is injured and the camera zooms in closely. It is also arguably the most subjective of the six signs and therefore vulnerable to greater variation between raters when classifying this sign. This notion is supported by the weaker inter-rater reliability of this sign not only in the current study but also in previous video review studies [, , ]. In our study, the low inter-rater reliability was due to one rater seeing the sign more often than the other, recording it as present, and then both raters ultimately agreeing that it was present. Observing a blank or vacant stare was rare in a recently published NHL video review study []. We do not know why the sign was so uncommon in that study compared to ours. It might be more difficult to see the sign through a hockey helmet, and it might also be more difficult to see it based on camera angles when players are moving off the ice. Among the other signs, inter-rater reliability was the strongest for clutch or shake head, slow to get up, and unresponsiveness, each with a moderate agreement classification. These signs may be more easily seen during field-side evaluations; their intra-rater reliability was strong to perfect. In contrast, the minimal agreement between raters for gait ataxia or blank/vacant stare warrants caution with their use.There were several limitations of the current study. In some instances, the available video footage was not sufficiently clear to code all signs (i.e., the view from the available camera angle was obscured, or a close up of the incident was not available). This \u2018missing data\u2019 was excluded from the analyses, which might have slightly improved the support for the utility of some of the visible signs. This was particularly true for the blank or vacant stare sign, where missing data on this sign was common. Despite being the most sensitive and specific sign for concussion diagnosis, blank or vacant stare had the worst inter-rater reliability (0.22, with 71 absolute disagreements between raters). For these reasons, if video review within professional sports are implemented, then access to high quality reviews with the capability of multi-angle and slow motion replays to allow for close ups would be optimal [] and would reduce the likelihood of missing data. Although the video reviewer was blinded to the sideline assessment results and the medical diagnosis of concussion for this study, they were only partially blinded to the use of the CIR. Given that the process for enacting a use of this rule requires the trainer to provide a signal to the sideline, and the official on the sideline identifies the interchange with a green card, the video reviewer was able to identify many instances where the CIR was used. Another study limitation was that there were no concussions that were subsequently (beyond match day) diagnosed by the club medical and reported to the researchers, which is inconsistent with other video review studies that have reported a numbers of cases of post-game diagnosis of concussion []. A further limitation of the current study pertains to the generalizability of the current findings to other levels of rugby league. The current study was a post-game review of an adult, male, professional league, and as such the results may not necessarily be generalizable to in-match real-time analysis, or other levels of match play. Our use of an operational definition of exclusion to categorise the observed signs into \u2018plausible concussion signs\u2019 versus signs that were more likely attributable to other factors, was also a limitation in terms of its subjectivity and reproducibility for future work in video review of concussion signs. Finally, only one reviewer completed the coding of the entire game, for every game in the season; the inter-rater reliability of that type of coding is unknown.A conservative approach to sideline concussion management would be to remove a player from play based on any evidence of possible concussion signs. This approach is encouraged as the best management strategy and in the best interest of the welfare of the player []. However, there is limited information available on the reliability and validity of identifying the objective signs of concussion when using video analysis, and indeed not all instances of observed concussion signs occur as a result of the player having sustained a concussion. The signs of concussion appear to be quite sensitive to concussion when reviewing known (i.e., medically diagnosed concussions) or suspected (i.e., players who have used the CIR) injury. When reviewing an entire season of match play, specific objective concussion signs such as slowness in getting up and clutching/shaking the head occurred commonly during professional rugby league match play, but did not typically reflect concussion occurrence. For these reasons, video injury surveillance can be difficult to interpret, but may provide a useful adjunct to the clinical assessment of potential concussion. With improved access to video replays, clear definitions and education regarding the observable signs, and improved communication between video observers and sideline medical personnel, the detection of concussion may improve."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0095-y", "title": "Characteristics of the Foot Static Alignment and the Plantar Pressure Associated with Fifth Metatarsal Stress Fracture History in Male Soccer Players: a Case-Control Study", "authors": ["Sho Matsuda", "Toru Fukubayashi", "Norikazu Hirose"], "publication": "Sports Medicine - Open", "publication_date": "7 August 2017", "abstract": "There is a large amount of information regarding risk factors for fifth metatarsal stress fractures; however, there are few studies involving large numbers of subjects.", "full_text": "A fifth metatarsal stress fracture (MT-5 fracture) is a common injury in soccer players. In fact, a previous investigation of a European soccer league found that 78% of stress fractures occurring in professional soccer players involved the fifth metatarsal bone. The incidence was 0.037\u20130.04/1000 exposure hours [, ] and 0.10\u20130.12/1000 athlete exposures in Japan [].The MT-5 fracture is well known and requires a long period to recover from [\u2013]. Moreover, MT-5 fractures may not achieve union because of poor blood flow around the injured region (the proximal diaphysis) []. For instance, surgical treatment requires a shorter period (15.2\u00a0\u00b1\u00a010.5\u00a0weeks) before regaining the ability to play than conservative treatment (26.3\u00a0\u00b1\u00a011.0\u00a0weeks) []. Therefore, occasional surgical treatment is recommended for athletes [, , , \u2013]. However, even if the MT-5 fracture is treated surgically, it takes at least 3\u20138\u00a0months before the individual can play sports again. Therefore, this injury has a negative impact on performance, and prevention of its occurrence is important.This study aimed to identify the possible risk factors for recurrence of MT-5 fractures. To clarify this issue, we compared the static foot alignment and distribution of foot pressure during leg calf raise exercises of players with a history of MT-5 fractures with those of healthy players. The calf raise task demonstrates loading on the forefoot. Danahue et al. reported that peak fifth metatarsal strain was 80% through the stance of walking (during forefoot loading). It has been hypothesized that players with a history of MT-5 fractures exhibit rearfoot inversion alignment and that their foot pressure is biased to the lateral region of the foot.All study protocols were approved by the Ethics Committee on Human Research of the university. This study conforms to the Declaration of Helsinki. All subjects were fully informed of the procedures and the purpose of this study and provided written informed consent. The participants were free to withdraw from participation at any time without fear of consequences.This study aimed to clarify the characteristics of playing position, foot alignment, and plantar pressure during heel raise tasks of players with a history of MT-5 fractures. Our main findings were that the foot with a history of MT-5 fractures (FF) exhibited reduced LHA during non-weight-bearing conditions compared with the healthy foot or the feet of controls; however, there was no difference in LHA under weight-bearing conditions. Additionally, the FF group exhibited greater forefoot inversion relative to the rearfoot than did controls. No difference in foot pressure was identified.In this study, everted rearfoot and inverted forefoot alignments were only observed during non-weight-bearing conditions. Monaghan et al. reported that alignment in the non-weight-bearing position reflects kinematics during walking and running [, ]. Moreover, during cutting and turning movements, the forefoot contacts the ground first; therefore, an inverted forefoot position may cause ground contact with the lateral part of the forefoot. The inverted forefoot may then create a high load at the lateral plantar part of the forefoot. It may be useful to note the alignment in the non-weight-bearing position in future studies. Contrary to expectations, the rearfoot performed everted, rather than inverted. After surgery, many players make contact with the ground on the medial side of their foot while walking because of pain and fear. Moreover, the load on the outside of the foot may have been corrected by rehabilitation. It is possible that several postoperative factors influenced this everted rearfoot alignment.Under weight-bearing conditions, previous literature has reported that players with a history of MT-5 fractures tended to have inverted rearfoot alignment [, ]. In the previous study, radiographs were performed to measure the weight-bearing rearfoot alignment, and lateral radiographs were used to calculate the calcaneal pitch angle. However, in our study, the pictures for evaluation were taken from the posterior side to measure the angle of the rearfoot. This difference in measurement methods may have caused contrary results.There were also no significant differences in plantar pressure among the groups. Hetsroni et al. researched plantar pressure in athletes who had sustained a proximal fifth metatarsal stress fracture during gait and reported that the loading pressure of the lateral part of the forefoot was low []. It is possible that a player who has experienced a fracture cannot bear a load on the lateral part of the forefoot because of fracture pain. In addition, the load on the outside of the foot may have been corrected by rehabilitation through weight-bearing training. In contrast to our findings, Azevedo et al. reported that young soccer players present with asymmetries in plantar pressure in the hallux, fifth metatarsal, and medial rearfoot specifically because of soccer. These contradictory results may have been due to differences between the evaluation procedures. Further studies are needed to analyze in detail the plantar pressure of each region of the foot [].There were no significant differences in arch ratio between the FF, CF, and NF groups. Teyhen et al. and Wong et al. reported that the center of pressure was more lateral in the high arch group during gait stance [, ]. However, this current study did not demonstrate a relationship between MT-5 fractures and high arch ratio. In future studies, the influence of the arch ratio on plantar pressure during cutting, jumping, and turning tasks should be determined.In addition, it is necessary to evaluate load movement before MT-5 fractures and one-leg movements such as cutting and turning.Midfielders had significantly higher rates of MT-5 fractures whereas defenders had significantly lower rates. There are few preliminary research reports on the relationship between playing position and MT-5 fractures. Dellal et al. investigated the physical activity of soccer players and reported that the running distance was significantly greater for midfielders than for defenders []. Orendurff et al. reported that the bending moment of the fifth metatarsal increased during the running acceleration phase []. Therefore, MT-5 fractures may be associated with increased running distance. In addition, MT-5 fractures are common in soccer players because of the combination of long running distance and cutting and turning movements, which are associated with MT-5 fracture. In future studies, movement characteristics and practice intensity for each position should be investigated.There was no significant difference in the occurrence of MT-5 fracture between the kicking foot and pivoting foot. Elite players use not only the dominant foot but also the non-dominant foot for kicking; therefore, the proportions of right foot use and left foot use for kicking will need to be investigated.A relationship between ankle sprain history and MT-5 fracture was not noted. In the present study, only ankle sprain severity was considered. It may be necessary to investigate the relationship between lateral instability of the ankle and MT-5 fracture.The results of the present study suggest that playing a midfield position and everted rearfoot and inverted forefoot alignments were associated with 5-MT fractures. These results provide evidence that alignment assessments may be helpful in risk screening for fifth metatarsal stress fractures. A more detailed load evaluation and prospective study are needed in the future."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0099-7", "title": "It's Time to Start Changing the Game: A 12-Week Workplace Team Sport Intervention Study", "authors": ["Andrew Brinkley", "Hilary McDermott", "Rachel Grenfell-Essam", "Fehmidah Munir"], "publication": "Sports Medicine - Open", "publication_date": "23 August 2017", "abstract": "A 12-week multi-team sport programme was provided to employees of a large services organisation and conducted in workplaces. This programme was used to investigate the short-term effect of regular sports team participation on individual employee and organisational health.", "full_text": "Within Europe, almost half of working age adults are failing to meet physical activity (PA) guidelines []. Modifiable inactive behaviours are associated with non-communicable diseases (e.g. coronary heart disease and type-2 diabetes) and the prevalence of premature mortality []. An inactive workplace has been linked with diminished organisational health outcomes, increased sickness absence, reduced productivity and workplace engagement []. Workplace PA interventions can positively influence employee and organisational health []. Exercise/gym classes, walking, active-transport, educational training, active work stations and activity challenges have been utilised in the workplace with varying degrees of effectiveness []. However, research to date is yet to comprehensively examine the efficacy of sport and team sport on health outcomes []. Indeed, a further critique of workplace PA literature has been the failure to assess the efficacy of participation against social group and organisational health outcomes (e.g. communication, team performance, job satisfaction) [].Participation in workplace team sport has the capacity to improve individual, social group and organisational health outcomes []. Several randomised control trial (RCT) and quasi-experimental studies demonstrate that participation in competitive or non-competitive team sports can significantly improve cardiorespiratory fitness, musculoskeletal function [\u2013] and psychological wellbeing []. These factors have the capacity to contribute to the reduced prevalence of non-communicable diseases associated with sickness absence, diminished productivity and all-cost mortality [\u2013]. Qualitative studies suggest participation in team sports may also improve workplace relationships, communication and team cohesion [\u2013].Improvements in individual health and social group outcomes are known to contribute to the function of the organisation []. However, perhaps due to a limited level of funding and expertise, and the non-clinical community setting (i.e. the workplace), the research examining participation in team sport has been limited by the reliance on qualitative methods (e.g. focus groups and individual interviews) and interventions that lack strong theoretical underpinnings []. Researchers are yet to examine the impact of participation in workplace team sport on social group health outcomes (e.g. cohesion, communication, interpersonal relationships) with validated measures. Furthermore, the type of sport played is reported in most manuscripts but not the intensity it is played at, or the duration, volume and frequency of participation [\u2013]. This challenges researchers in determining what \u2018dose\u2019 of team sport equals the benefits reported in the literature [].Participation in team sport is challenged by social comparisons with colleagues and superiors, the organisational culture and the facilities available within the workplace [], and psychological barriers relating to autonomy, competence and relatedness [, ]. Self-determination theory (SDT) [] suggests supporting people\u2019s innate needs for autonomy (i.e. feeling free and fully volitional to engage in team sport), competence (i.e. feeling capable to complete a skill in team sport) and relatedness (i.e. feeling supported, understood and valued by a social group) through the provision of an activity (e.g. team sport) may promote wellbeing and autonomous motivation [\u2013]. Good evidence indicates supporting basic needs is an effective behaviour change strategy within a workplace, sports and exercise setting [\u2013]. For these reasons, workplace team sport interventions should be underpinned by behaviour change theories with a focus on the social environment [\u2013].This study evaluated the impact of a workplace team sport intervention (i.e. \u2018Changing the Game\u2019; CTG) underpinned by SDT. The primary intention of this study was to examine the impact of CTG on aerobic fitness (estimated VO max). Secondary intentions were to investigate participation in CTG\u2019s impact on individual, social group and organisational health outcomes. These included subjective vitality, leisure-time PA, quality of life, occupational stress and fatigue, group cohesion, relationships with superiors and colleagues, communication, job satisfaction, individual and team job performance, and work engagement.The intervention study was evaluated using a process evaluation underpinned by the RE-AIM approach []. This was conducted to evaluate the effectiveness of the intervention approach and its theoretical underpinning [] and will be reported elsewhere (i.e. findings due for publication in late 2017).The current study examined the impact of a 12-week workplace team sport intervention on individual, social group and organisational health outcomes. Results indicate workplace team sport can improve aerobic fitness, PA behaviour and interpersonal communication within teams. These results suggest team sport may be an effective and viable form of health promotion within a workplace setting. Promoting forms of PA such as team sport within workplace settings is required to meet UK public health guidelines and reduce the financial and societal burden faced as attributable to an inactive population []. Therefore, it remains important to continue to understand why employees choose to participate in team sport and promote programmes which encourage participation. The current study suggests this may be achieved by promoting team sports which are supportive of autonomy, competence and relatedness. Researchers should consider testing the efficacy of a multi-team sport programme within a workplace setting over the long term with cluster RCT designs (i.e. randomise on workplace team level) and further objective measures of health (e.g. objective measures of physical activity, skinfold, DEXA scans, sickness absence)."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0093-0", "title": "Observational Review and Analysis of Concussion: a Method for Conducting a Standardized Video Analysis of Concussion in Rugby League", "authors": ["Andrew J. Gardner", "Christopher R. Levi", "Grant L. Iverson"], "publication": "Sports Medicine - Open", "publication_date": "14 July 2017", "abstract": "Several professional contact and collision sports have recently introduced the use of sideline video review for club medical staff to help identify and manage concussions. As such, reviewing video footage on the sideline has become increasingly relied upon to assist with improving the identification of possible injury. However, as yet, a standardized method for reviewing such video footage in rugby league has not been published. The aim of this study is to evaluate whether independent raters reliably agreed on the injury characterization when using a standardized observational instrument to record video footage of National Rugby League (NRL) concussions.", "full_text": "We present the first objective and reliable coding form for rugby league to capture the game situation, the mechanism of injury, and possible signs of concussion.Rugby league is a high-intensity collision sport []. The game is played continuously in two 40-min halves, and game-play involves two teams of 13 on-field players and four interchange players who may be switched in and out of the game. The published incidence rates of concussion in rugby league vary []; at the National Rugby League (NRL) level, medically diagnosed concussions in three clubs from the 2013 season revealed an incidence rate of 14.8 concussions per 1000 player match hours [], while a rate of 28.3 concussion per 1000 player match hours were reported from one NRL club over a 15-year (1998\u20132012) period []. The incidence of use of the concussion interchange rule (CIR) was 24.0 (95% CI 20.7\u201327.9) uses of the CIR per 1000 NRL player match hours [] and 44.9 (95% CI 38.5\u201352.3) uses of the CIR per 1000 National Youth Competition player match hours [].One method that has becoming increasingly relied upon to assist with improving the identification of possible concussion has been the review of video footage on the sideline. The use of video for reviewing a concussion may identify signs of injury that may have been blocked from view or otherwise missed by medical staff. A number of professional contact and collision sports have recently introduced the use of sideline video review for club medical staff to help identify and manage concussions []. A number of studies of video footage have been conducted in a variety of sports, for example, rugby league [\u2013], rugby union [], and Australian Rules Football [, , ]. Other sports, such as boxing [], soccer [], taekwondo [], ice hockey [, , , ], and lacrosse [], have also reported on the use of video footage for understanding the circumstances and mechanisms of injury unique to their sports. A risk prediction model among National Hockey League (NHL) players reported that both visual signs of concussion and information pertaining to mechanisms of injury improved a clinician\u2019s ability to identify athletes who should be removed from play and evaluated []. Specifically, the study indicated that suspected loss of consciousness, motor incoordination or balance problems, being in a fight, having an initial hit from another player\u2019s shoulder, and having a secondary hit on the ice were all associated with increased risk of subsequent concussion diagnosis.Sport-specific coding criteria of concussion for game situational factors and injury mechanisms have been developed for hockey (e.g., the \u2018Heads-Up Checklist\u2019 []), but these criteria do not generalize to other sports like rugby union or rugby league. Video criteria and coding forms require validation in each individual sport [, ]. In a more recent NHL study examining the predictive ability of visual signs of concussion, loss of consciousness, motor incoordination, and blank/vacant look had a positive association with concussion diagnosis, whereas slow to get up and clutching of the head, despite occurring frequently, had low positive predictive values [].Several video studies have examined signs of concussion, together with player characteristics, injury characteristics, and match situational factors, in professional rugby league [\u2013]. In 2014, video reviews of injury have been implemented in the NRL to help medical staff and promote player health and safety. The aim of this study was to present a standardized observational recording form and to determine whether independent raters agreed on the antecedent events, mechanisms of injury, and concussion signs when using the form to code digital video records of concussions in the NRL.For the expert raters, 19 of 20 (95%) components of the form had  values of between .90\u20131.00 (\u2018almost perfect\u2019 agreement) and one (5%) had moderate agreement. The expert raters had perfect ICC for 2 of 2 (100%) interval/ratio variables. For the concussion signs, the expert raters had 1 of 6 (17%) of concussions signs with  values between .80\u2013.90 (\u2018strong\u2019 agreement); 4 (67%) of were classified as \u2018moderate\u2019; and 1 (17%) was classified as \u2018weak\u2019 agreement. No signs classified by either the na\u00efve or expert raters had a \u2018minimal\u2019 or \u2018none\u2019 level of agreement.There were nine components that were all rated with \u2018almost perfect\u2019 agreement by both the na\u00efve and the expert raters (game time, score, whether the concussed player was a ball carrier or a tackler, the number of players involved in the tackle, whether the offending player was placed on report by a match official, the initial contact, the region of contact, whether the player was removed from play, and how the player left the field). The level of agreement between the expert raters and between the na\u00efve raters was also very consistent for the tackle height and whether the injury occurred as a result of foul play (i.e., the offending player was penalized). The na\u00efve raters had a \u2018strong\u2019 level of agreement for these components. The na\u00efve raters had a moderate level of agreement on whether the game was played during the night or day, the tackle number in the set, the anatomical location of the impact, and the location of the field where the concussion took place, whereas all of these components had an \u2018almost perfect\u2019 level of agreement between the two expert raters. The expert raters also achieved an \u2018almost perfect\u2019 level of agreement on the secondary contact, whether the concussed player had anticipated the impact that caused the injury, and the time taken to leave the field of play. However, the na\u00efve raters only had a \u2018minimal\u2019 level of agreement on these components. For type of play, and whether or not the player returned to play, the na\u00efve raters had a \u2018weak\u2019 agreement on these components compared to the \u2018almost perfect\u2019 agreement by the expert raters. Whether or not there was secondary contact was the most difficult component to agree upon; the expert raters\u2019 level of agreement was \u2018moderate\u2019 for this component, and the naive raters\u2019 level of agreement was \u2018minimal\u2019 (see Table\u00a0).Regarding concussion signs, slow to get up had the best level of agreement between expert and na\u00efve raters of all possible concussion signs (strong and moderate agreement, respectively), whereas a blank or vacant stare had the worst agreement (both rater groups had a \u2018weak\u2019 level of agreement). Clutch or shake head, gait ataxia (or having wobbly legs), unresponsiveness, and post-impact seizure-like features had moderate agreement for both expert and na\u00efve raters.Rugby League is a full contact collision sport that has high concussion incidence rates [\u2013]. The in-game management and decision-making process surrounding concussion is a challenge. Video review is increasingly being used as one method for improving this in-game decision-making process for medical staff, although a standardized approach to the use of such information had not been published. Although there is a large body of research examining on-field markers of concussion and their association with outcome [\u2013, , , , , , , , , , , , \u2013, \u2013], very few of these studies have been focused on possible signs of concussion at the time of injury (versus collected later as part of a questionnaire or interview with the athlete). This study presents a standardized observational form and examines intra-rater and inter-rater agreement on the antecedent events, mechanisms of injury, and concussion signs. Overall, the results of this study suggest that a certain level of knowledge about the game is required to complete the form components accurately. Expert rates achieved an \u2018almost perfect\u2019 level of agreement on 21/22 (95%) of components compared to only 9/22 (41%) components for the na\u00efve raters.In a similar study conducted with the \u2018Heads-Up Checklist\u2019 for National Hockey League (NHL) concussions, the na\u00efve raters also had worse agreement across components pertaining to the antecedent events and mechanism of injury compared to the expert raters. Of the 15 components in version 1 of the Heads-Up Checklist, na\u00efve raters 7 (47%) had weak or minimal agreement, compared to only 1 of the 15 (7%) components for the expert raters []. For the Heads-Up Checklist, the acceleration of the head (which was not considered a component or review item in our form) was the single component with the worst agreement across na\u00efve and expert raters. Rating secondary contact was also challenging in the hockey study as it was in the current study. The location of the playing surface where the concussion occurred and the time in the game when the concussion occurred were the two components with the strongest agreement by na\u00efve and expert raters for the hockey study []. For the current study, the time in the game was rated well. However, the location on the field did not have a high agreement for the na\u00efve raters. The discrepancy between na\u00efve raters for the hockey study compared to this rugby league study may have occurred for at least three reasons. First, we divided the playing surface in our study into 12 different components and the hockey study used fewer zones. Second, the hockey study designated offensive ends and defensive ends, whereas the rugby league study required the raters to record the direction of the play, and some of the disagreement between the na\u00efve raters for the location on the field was due to the indication of the direction of the play. Finally, the hockey study used na\u00efve raters who where more familiar with their sport (i.e., \u2018individuals with limited experience who might have played or coached [ice] hockey at a competitive level\u2019), whereas our na\u00efve raters were complete novices, who had limited to no experience even watching the sport as fans and certainly no experience identifying concussions.In the current study, there were a number of variables that appear to rely on knowledge, understanding, and experience with rugby league match play (i.e., the expert raters outperformed the na\u00efve raters). For example, there were large differences between the coding by expert and na\u00efve raters of variables such as secondary contact and anticipation of impact. There was also a large difference between the coding by expert and na\u00efve raters on whether the player returned to play. This variable required the raters to watch the remainder of a game (following the injury) to determine if the injured athlete subsequently returned to the field of play. Interchanges can occur during play or during a stoppage in play, and they are not always announced on the broadcaster footage. It appears that the na\u00efve raters were not as savvy in identifying the return to play of an interchanged athlete and/or did not identify the athlete as being re-involved in match play following their return to the field of play.As with our previous video reviews of concussion signs [, ], we once again found that determining whether a concussed player had a blank or vacant stare was difficult to agree upon. We had weak agreement between na\u00efve (0.44, 95% CI\u2009=\u20090.15\u20130.71) and expert (0.50, 95% CI\u2009=\u20090.23\u20130.76) raters in this study, and our previous work has also revealed difficulty with agreement between raters (i.e., 0.36 (95% CI\u2009=\u20090.29 to 0.43) [] and 0.62 (95% CI\u2009=\u20090.37 to 0.88) []). In a recent Australian Football League (AFL) video review, inter-rater reliability for the blank/vacant stare on first review was reported to be 0.24 (95% CI\u2009=\u20090.04 to 0.41) and minimal improvements were observed on second review [0.26 (95% CI\u2009=\u20090.07 to 0.43)]. The intra-rater reliability in the AFL study was somewhat better for the two raters over the two rating sessions [i.e. 0.63 (95% CI\u2009=\u20090.50 to 0.74) and 0.36 (95% CI\u2009=\u20090.18 to 0.51)]. The concussion sign \u2018blank/vacant stare\u2019 was reported to have 9% sensitivity, 100% specificity, 100% positive predictive value and 58% negative predictive value in the sample of AFL concussions []. When the quality of the video (including the zoom capacity to see the players face) is limited, attempting to code the presence or absence of a blank or vacant stare from video is challenging []. This supports the notion that good-quality video from multiple camera angles are crucial for effective video surveillance of injuries []. In the current study, however, this was not a limitation, suggesting that it is also important to have clear definitions, including the inclusion and exclusion criteria for coding concussion signs []. In a recent series of video reviews of concussions from the AFL [, , ], Makdissi and Davis indicated that video review may be an avenue that facilitates the assessment of the mechanism and impact of injury and allows for the identification of brief early signs of concussion []. The authors suggest that video analysis may be a useful adjunct to the sideline assessment of possible concussion [] and that the implementation of a flowchart may improve the timely assessment of concussion [].We recently completed a study on the frequency (or base rates) of concussion signs in NRL match play (Gardner et al., under review). That study reviewed every game (\u2009=\u2009201) from the 2014 NRL season, which included 127,062 tackles, and found unresponsiveness occurred 52 times [24 (46%) were diagnosed with a concussion], slow to get up occurred 2240 times [60 (3%) were diagnosed with a concussion], clutching or shaking the head occurred 361 times [38 (11%) were diagnosed with a concussion], gait ataxia occurred 102 times [35 (34%) were diagnosed with a concussion], blank or vacant stare occurred 98 times [45 (46%) were diagnosed with a concussion], and a post-impact posturing or seizure occurred 4 times [3 (75%) were diagnosed with a concussion]. The unresponsiveness sign had 40% sensitivity, 91% specificity, 46% positive predictive value, and 89% negative predictive value. The slow to get up sign had 100% sensitivity, 50% specificity, 27% positive predictive value, and 100% negative predictive value. Clutching or shaking the head had 63% sensitivity, 46% specificity, 18% positive predictive value, and 87% negative predictive value. Gait ataxia had 58% sensitivity, 79% specificity, 34% positive predictive value, and 91% negative predictive value. Blank or vacant stare had 75% sensitivity, 84% specificity, 46% positive predictive value, and 95% negative predictive value. Post-impact seizure had 5% sensitivity, 100% specificity, 75% positive predictive value, and 85% negative predictive value in the 2014 NRL season (Gardner et al., under review).One of the unusual and unexpected findings of this study was the discrepancy observed between the na\u00efve raters in coding variables that were conceivably thought to be obvious (e.g., game time, score, day/night game). The na\u00efve raters did not always have 100% agreement. Because rugby league is a continuous sport, it is common for the game to continue despite an injury, and therefore, the game clock also does not stop. As such, an injury can occur well before the game and the game clock is stopped. The discrepancies in the \u2018time in game\u2019 variable are explained by this issue; one of the na\u00efve reviewers recorded the time correctly (i.e., when the injury occurred), whereas the other na\u00efve rater often recorded the time when the game clock was stopped. In terms of the \u2018game score\u2019 variable, it is possible that the na\u00efve raters were unfamiliar with teams, and therefore, errors were made in coding the score of each team. For the \u2018day/night game\u2019 variable, there were a number of games that were played during twilight, as well as the footage of some of those cases being zoomed in, and the wide view did not make the day/night difference obvious to the na\u00efve raters who do not watch NRL games.Video review appears to be a useful adjunct to traditional methods for making in-game decisions pertaining to the identification of potential concussion (and an athlete subsequently being removed from play). However, to better understand and quantify the value of this process, future research should be conducted under time limits and/or during a game to replicate the real-world/practical pressure, neither of which was replicated in this study. Future studies might focus on whether agreement between experts improves under \u2018ideal circumstances\u2019 (i.e. as many reviews as required without time limitations) versus \u2018real-world circumstances\u2019 (i.e. a quick decision required to identify a possible injury and immediately remove the athlete from play).The current study has several limitations. Firstly, clubs used their own personnel and methods for identifying possible injuries on the field and diagnosing concussions on the sideline, which presumably makes the final specific criteria for a \u2018medically diagnosed concussion\u2019 variable across clubs. The current study does not generalize to the real-world use of in-game video analysis because the study was not conducted under the time pressure associated with in-game decision-making. Further, the sample size is small, and only two na\u00efve and two expert reviewers were used. Whether the current results hold true for more cases and a greater number of raters is unknown.The present study suggests that determining the presence or absence of a blank or vacant stare is challenging for both na\u00efve and expert raters to rate reliably, but that showing unresponsiveness (i.e. possible LOC), clutching or shaking of the head, a post-impact seizure, or being slow to get up are more reliably rated signs. However, in this study, there was no variability in the clinical outcome measure, as our sample came from a pool of individuals who were all medically diagnosed with a concussion. Therefore, the predictive value of any one component or concussion sign, or a combination of these items, is unknown and may be the focus of future research. Given the variability of in-game decision-making in professional rugby league [\u2013], we sought to provide validation of a standardized approach for collecting information surrounding possible concussions to help inform the in-game decision-making process. Although the form was created for all levels of competition, it only had a good level of agreement among experienced raters. Therefore, it might only be useful for those teams or clubs that have experts available to them (i.e., the professional level). For lower levels of competition, the form may have less of a benefit, because the na\u00efve raters had a low level of agreement on many components of the form. It is important to note, however, that the management of suspected concussion at these lower levels should always be conservative. If a concussion is suspected, then the athlete should be removed from play and not returned to play the same day []. At the professional level, data collected from this form may allow for a thorough understanding of the situational and contextual factors related to concussion, which may be used to strategize future interventions to reduce the risk of concussion at this level."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0094-z", "title": "\n                     ", "authors": ["E. Couv\u00e9-Deacon", "D. Postil", "O. Barraud", "C. Duchiron", "D. Chainier", "A. Labrunie", "N. Pestourie", "P.M. Preux", "B. Fran\u00e7ois", "M.C. Ploy"], "publication": "Sports Medicine - Open", "publication_date": "16 August 2017", "abstract": "\n                           ", "full_text": "\n                         (SA) is a leading cause of both community-acquired and healthcare-associated infections. Community-associated methicillin-resistant  (CA-MRSA) emerged worldwide during the last decade. CA-MRSA causes skin and soft-tissue infections such as multiple abscesses among healthy individuals, as well as life-threatening necrotizing pneumonia in children and young adults []. There is a strong epidemiological link between CA-MRSA causing deep primary skin infections and Panton-Valentine leukocidin (PVL). PVL-producing SA strains have a peculiar antibiotic susceptibility profile and are more susceptible to antimicrobials as compared to healthcare-associated MRSA []. The prevalence of CA-MRSA seems to be low but is increasing in Europe, although prevalence in clinical isolates vary according to the country from less than 0.5 to 15% across published studies [].Skin-to-skin contact and poor hygiene are risk factors for CA-MRSA outbreaks, and populations at risk of CA-MRSA infection include prisoners, military personnel, and sports teams []. Most studies of SA infections among athletes have been conducted in the USA, and also in the UK, Germany, and Japan [, ]. In the USA, CA-MRSA infections among athletes have become very frequent, the most at-risk sports being those involving physical contact, such as American football, rugby, and wrestling [, ]. CA-MRSA infections have also been linked to sports involving less physical contact but shared equipment, such as fencing, martial arts, cross-country running, volleyball, basketball, football, baseball, and weight-lifting []. Other sport-related SA infection risk factors include a high body mass index, use of equipment resulting in skin abrasion, and poor personal hygiene (sharing of personal items, failure to protect skin lesions) [].The anterior nasal cavities are the most common SA carriage site with the oropharynx (10\u201350%), but skin carriage is also frequent, especially on the hands (27\u201390%), perineum (22\u201360%), and axilla (8\u201319%), the intestinal tract, vagina, and skin lesions are also common carriage sites []. Nasal carriage has been associated in literature with a higher incidence of SA infections []. Twenty to 25% of healthy volunteers were reported to be permanent nasal carriers, 60% intermittent carriers and 20% permanent non-carriers []. In the US general population, 1.3% were nasal MRSA carriers, compared to 5.4% of athletes and individuals in daily contact with MRSA carriers [, ].In France, the reported SA nasal carriage rates were about one third of the general population. However, unlike the USA, there are very few data on the prevalence of CA-MRSA in France, and none on carriage among athletes at risk of CA-MRSA infection. The aim of this study was to determine the SA carriage proportion among athletes practicing physical contact sports in a French county, and the proportion and ST types of isolates resistant to methicillin and/or producing the PVL. We also analyzed SA carriage according to the sport, hygiene habits, and medical history.We observed an overall  carriage proportion of 61% (95%CI: 51.0\u201370.0) among 300 French athletes. A previous European study showed a carriage rate in France of 21.1% (95%CI: 17.4\u201325.4) among 3870 healthy patients [], but only the nose was sampled. The prevalence of SA carriage found here in our study is higher than that observed in the general population in other studies with multiple sampling sites [\u2013]. This high SA carriage among athletes is not surprising, as SA transmission is facilitated by sharing of equipment and skin-to-skin contacts. In addition, athletes are known to have poor personal hygiene during sports, as reflected by reports of CA-MRSA outbreaks in sports teams [, ]. We report a high variability of team SA carriage, from 0.0% in a karate team to 100% in a wrestling team and a baseball team. Clonal transmission of SA within sports teams has already been demonstrated as well as the increase of SA isolates in the sweat of SA carriers during sport practice []. However, the typing of the strains did not show clear evidence of clonal transmission within the studied teams. Even though skin contact is known as the main transmission risk in SA outbreaks in sports team, the SA carriage out of outbreak period is probably more complex. Host susceptibility and endogenous flora are probably in balance with skin contact transmission within a sports team.We observed a higher carriage in the throat (\u00a0=\u00a0143, 47.7%) than in the nose (\u00a0=\u00a0111, 37.0%), as recently reported in the general population worldwide [, , ]. Interestingly, throat only carriage represented 31.1% (\u00a0=\u00a057) of our carriers (\u00a0=\u00a0183), and represented 19% of the total population (\u00a0=\u00a0300). Nose positivity accounted for 37% (111/300 subjects), which is comparable with many other studies on nasal SA carriage in the general population. However, when enriched with throat-only carriers, which are often not sought for in other studies, it added 57 cases, thus inflating the prevalence to close to 60% (168/300 subjects), which is much higher than most other studies. This raises the question as to what is the genuine rate of  carriage in the general population if throat swabbing would also be performed and highlights the fact that  does not only colonize the nose [\u2013]. Due to this peculiar observation we could not really compare our high carriage proportion to other studies. This implies that a combination of throat and nose sampling should be used to detect SA carriage.In the USA, CA-MRSA is endemic. However, only one publication reported no difference in CA-MRSA nasal carriage between college athletes (1.8%) and the general population (1.5%) []. Based on these reports, we expected to find a CA-MRSA carriage proportion of 1.0\u20132.0% among French athletes. However, in our study, none carried PVL producing MRSA.One handball player carried a PVL-negative MRSA, in the nose and skin lesion, the strain belonged to the ST5, the second more frequent clonal complex isolated in France after ST8 []. In athletes, MRSA carriage differed according to the studies. A study reported that 23.1% (\u00a0=\u00a044/190) of high-school football players in the USA had nasal colonization with methicillin-susceptible SA, and none with MRSA []. On contrary, another study reported nose and skin MRSA carriage in 35.0% of 233 healthy athletes in the USA, and found that most isolates also harbored the PVL-encoding gene []. When our study started, most published data on MRSA carriage in the general and athlete populations came from the USA. A European study published during our study showed an MRSA carriage of 0.4% in the French general population [], close to the 0.3% observed here in French athletes.The link between SA carriage and outbreaks of skin and soft-tissue infections is not always clear. Outbreaks of PVL-positive MRSA infection have been reported in sports teams with high carriage of methicillin-susceptible SA (23.0 to 48.5%) but not significantly linked to CA-MRSA carriage [\u2013]. In a 1-year surveillance study of MRSA nasal carriage among student athletes, colonization alone appeared insufficient to trigger outbreaks []. However, CA-SA infection outbreaks remain low in European athlete population compared to USA athlete population. The reported outbreaks are usually linked to poor hygiene practices and the implementation of simple hygiene measures can efficiently halt the transmission []. In our study, we did not find any PVL-positive MRSA carrier, nonetheless, the high SA carriage associated with skin contact during sport practice increases the risk of SA transmission between players [] and therefore the risk of SA carriage and infection []. Previous study reported a higher risk of  carriage overtime in contact sports athletes and that efforts should focus to prevent transmission of  among these athletes []. Prevention of SA carriage and infection in that population could be achieved by simple hygiene measure promotion as skin wound disinfection and covering during practice, showering after practice and competition, using liquid soap rather than bar soap, washing clothes and equipment after practice, refraining from cosmetic body shaving, and discouraging athletes from sharing towels and personal hygiene items [, ]. Our data did not allow identifying risk factors for SA carriage. We found that long-term antibiotic therapy was associated with SA carriage, but this classical risk factor was no longer significant in multivariate analysis. As French data on SA carriage among athletes were lacking, we based our sample size on American data; we therefore overestimated the carriage proportion, which partly explains the lack of statistical power. Sample size was a major limitation to the study. Despite showing a high carriage, it was only out of 300 athletes. Finally, one interesting control would be to apply the same swabbing technique to a sample of the Limousin general population.We sampled our athletes only once, mainly during the first half of the sports season. Two previous studies showed variations in MRSA carriage according to the athletic activity. In a one-year surveillance study of student football and lacrosse athletes, MRSA carriage ranged from 4.0 to 23.0%, with a peak during the period of maximal athletic activity []. Moreover, a 3-year surveillance study of CA-MRSA skin and soft-tissue infections in a collegiate football team, obtained evidence that variations in MRSA carriage might be related to the time of the competitive season or to outbreaks or isolated cases of CA-MRSA infection [].In our population of French athletes, we found a high SA carriage but a low MRSA carriage. Our local epidemiology was quite different than that reported in similar types of athletes in the USA, e.g., that the USA300 clone had not spread (yet) spread among our test population. We found a very high percentage of throat-only positive cultures, which substantially inflated the nasal carriage proportion. Further studies on  carriage should include throat sampling.Staphylococcal carriage prevention in that population could consist in simple hygiene measures promotion, known to efficiently halt SA transmission in sports team."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0100-5", "title": "Single-Leg Assessment of Postural Stability After Anterior Cruciate Ligament Injury: a Systematic Review and Meta-Analysis", "authors": ["Tim Lehmann", "Linda Paschen", "Jochen Baumeister"], "publication": "Sports Medicine - Open", "publication_date": "29 August 2017", "abstract": "Previous reports of single-leg assessment demonstrated functional deficits in postural stability following anterior cruciate ligament (ACL) injury. However, quantified measures describing postural stability vary among investigations and results seem not to be clear. The first aim of this systematic review was to quantify postural deficits in eyes open single-leg stance in patients after ACL injury. Moreover, the second aim was to examine the potential of traditional center of pressure (CoP) measures in order to distinguish postural stability between ACL patients and healthy controls.", "full_text": "Injuries to the anterior cruciate ligament (ACL) are the most frequent knee injuries in sports and cause immediate disability for athletes followed by long-term consequences in terms of functional deficits in motor coordination [, , , ]. Generally, functional stability of the knee joint during voluntary movement is predominantly regulated by the sensorimotor system. As a dynamic system, it contributes to the transmission and integration of somatosensory, vestibular, and visual information to the central nervous system, in order to provide adaptability to the environment [, , ]. Alterations of afferent sensory information, potentially caused by mechanoreceptor damage, may subsequently contribute to disturbances of sensorimotor control [, , ]. Based on this rationale, research has shown altered sensorimotor control after ACL injury [\u2013, , , , ], whereas contradictory findings indicate that these alterations may not necessarily be correlated with postural control of standing balance [, , ].Postural control is defined as the ability to monitor body position and alignment in space, involving multimodal interactions of musculoskeletal and neural systems []. It is comprised of two components: postural orientation and postural stability. While postural orientation describes the visually and vestibular-guided ability of monitoring the interrelationship between body segments relative to the environment, postural stability predominantly incorporates somatosensory information to control the center of mass (CoM) in relationship to the base of support []. To date, center of pressure (CoP) trajectories, as the vector of total force applied to the center of the supporting surface [, ], are measured by laboratory-based force or pressure-sensitive platforms in order to assess postural stability [, , , ]. After ACL injury, measures describing postural stability vary among investigations and results seem not to be clear [].Postural stability as a crucial determinant for functional movement reflects a multimodal interaction of the sensorimotor system []; however, there is no gold standard to assess postural stability in patients after ACL injury. Therefore, the purpose of the present systematic review and meta-analysis was to investigate postural stability after ACL injury. The second aim was to examine the potential of CoP measures to distinguish postural stability between ACL patients and healthy controls. Given that there is no clear consensus about the feasibility of these measures in ACL research, this meta-analysis on postural stability measures may further the development of valid tools to examine functional outcomes after rehabilitation or reconstruction in ACL patients.Conducting this meta-analysis, the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) provided by Moher et al. [] were followed and adapted to the current data properties.A systematic literature search in the databases PubMed and Scopus was conducted from their inception to December 2016 to capture all pertinent articles investigating postural stability in ACL patients. The search strategy included the key terms: (postural control OR postural balance OR vestibular OR posture OR balance) AND (\u201cACL\u201d OR \u201canterior cruciate ligament\u201d). Since there is no universal definition of postural control and balance, this search strategy comprised a widespread spectrum in order to cover all potentially relevant studies. Search limitations were imposed to full access articles in English language and studies investigating human species. Additionally, reference lists of articles found were inspected, and relevant review articles [, , ] were scrutinized to identify further evidence.The objective of this meta-analysis was to quantify postural stability during single-leg stance in patients after ACL injury compared to healthy controls. The comprehensive analysis revealed that postural stability was decreased in patients after ACL injury. During eyes open single-leg stance, patients showed significantly increased sway magnitudes and velocities in the injured limb. Additionally, postural sway was significantly increased in AP and ML direction. However, the non-injured side demonstrated no differences in sway magnitude or velocity compared to matched controls. Similarly, no difference in sway velocity has been observed among ACL patients between the injured and non-injured side, but sway magnitude significantly differed.Following the model of Kapreli and Athanasopoulos [], mechanoreceptor damage may lead to a disturbance of sensory transmission, contributing to alterations of afferent feedback and stabilizing reflexes that may implicate increased body sway [, , , ]. In line with Howells et al. [] and Negahban et al. [], the present findings support that postural sway is altered in patients after ACL injury. Medium to large effects were found for increased total sway magnitudes [, , , , , ], as well as increased anteroposterior and mediolateral sway in the injured leg [, , , ]. On the other hand, two studies [, ] found decreased postural sway in ACL patients compared to healthy controls. Differences in post-injury and post-surgical rehabilitation may explain the inconsistency of these results. While rehabilitation protocols commonly include balance training to positively influence clinical status and postural stability of ACL-injured patients [], healthy controls may not be trained comparably for specific balance tasks and finally achieving worse results. Furthermore, the increases in AP and ML direction are consistent with previous reports [, , , ], also demonstrating postural impairments along these two axes. Since postural adjustments are limited in the knee joint, ankle and hip strategies may compensate for modified conditions to control the center of mass in AP and ML direction in relationship to the base of support [, , ]. Although compensational motor strategies may take part, ACL patients exhibit deficits in postural stability, supporting the supposition of a systematic change in sensorimotor control.The present meta-analysis found no differences of postural sway between the non-injured and matched control leg. Previous systematic reviews [, ] have shown the non-injured leg to be affected by ACL injury. Similar to other reports [, ], one study [] in this meta-analysis showed less postural sway in the non-injured limb compared to healthy controls. Nevertheless, other studies indicated a bilateral deficit of postural stability in ACL-injured patients [, , ]. Thus, higher-level sensorimotor control may be affected in addition to sensory afferent transmission. In fact, Baumeister et al. [] found increased cortical processing in the brain related to ACL injury, also demonstrating significantly higher frontal brain activity in both the injured and non-injured leg.However, in contradiction to earlier reports [, , , ], within-group differences of sway magnitudes were found between the ACL-injured and ACL-non-injured leg in the present meta-analysis. Future studies should apply neurophysiological measures to investigate the underlying mechanisms of sensorimotor processing after ACL injury.Parameters of sway velocity were investigated in seven studies [, , , , , , ] revealing significant differences with medium to large effects for the comparison between the injured and matched leg. The mean sway velocity is arithmetically related to total path length of the CoP trajectory. It is usually calculated by total path length divided by trial duration [, ]. Thus, an increase in sway velocity may naturally be accompanied by increased sway magnitude, as demonstrated in this meta-analysis. With respect to the mathematical formula for mean sway velocity, the trial duration chosen for the assessment of postural stability may therefore crucially affect the outcomes. Moreover, other confounding variables may relate to differences in limb-matching procedures applied in the included studies. Howells et al. [] suggested considering leg dominance as an influential factor for the comparison of ACL and healthy subjects. They found greater impairments in postural stability of the ACL group when compared to the dominant leg of healthy control. However, solely two of 11 included studies reported leg dominance. When evaluating a potential influence to the outcomes, future studies may explicitly provide detailed information about leg dominance.The present meta-analysis indicates that postural stability in a standardized single-leg stance is impaired in patients after ACL injury. Furthermore, CoP measures appear to be suitable to differentiate ACL patients and healthy controls with respect to postural stability. Thus, the proposed measurement procedure may help physicians and physiotherapists to identify patients at greater risk for suffering a subsequent ACL injury and consequently allow adjusting their treatment or return to play strategies. Nevertheless, caution should be exercised when using the non-injured leg as a reference measure. However, the potential of these measures to provide further insights into underlying mechanisms of altered postural control is limited to theoretical considerations. While current investigations mainly describe motor responses to multimodal sensory feedback, further etiological approaches may assess neurophysiological mechanisms underlying functional deficits in ACL patients, providing valuable indications for diagnostics, rehabilitative treatment, or return to play assessment."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0086-z", "title": "The Contribution of Individual Exercise Training Components to Clinical Outcomes in Randomised Controlled Trials of Cardiac Rehabilitation: A Systematic Review and Meta-regression", "authors": ["Bridget Abell", "Paul Glasziou", "Tammy Hoffmann"], "publication": "Sports Medicine - Open", "publication_date": "5 May 2017", "abstract": "While the clinical benefits of exercise-based cardiac rehabilitation are well established, there is extensive variation in the interventions used within these trials. It is unknown whether variations in individual components of these exercise interventions provide different relative contributions to overall clinical outcomes. This study aims to systematically examine the relationship between individual components of the exercise intervention in cardiac rehabilitation (such as intensity and frequency) and clinical outcomes for people with coronary heart disease.", "full_text": "While ongoing improvements in diagnosis and treatment have resulted in a steady increase in survival rates from major coronary events [, ], the burden of coronary heart disease on public health remains a substantial problem. With an increasing number of patients surviving acute cardiac events, the impetus to use effective secondary prevention strategies grows. However, this need is not being met, with up to 40% of all coronary events occurring in patients who have previously been diagnosed or hospitalised with the disease [\u2013], and well-documented evidence-practice gaps in the use of effective therapies [, ]. Reducing the frequency of these recurrent events by enhancing the uptake of effective pharmacological and non-pharmacological interventions should therefore be considered an important health care priority.While the benefits of exercise-based cardiac rehabilitation in the secondary prevention of coronary heart disease are well established [\u2013], the complex nature of this intervention presents a substantial challenge to its implementation. Individual trial results vary considerably in terms of effectiveness, as well as in the type and \u2018dose\u2019 of exercise intervention provided, making it difficult to synthesise and translate these findings into practice in a way which provides optimal patient benefit. This problem has been compounded by incomplete reporting of intervention details in a substantial proportion of cardiac rehabilitation trials [, ]. In turn, this has hampered past attempts to understand how, and which, intervention characteristics relate to clinical outcomes.Given the substantial variability of interventions in cardiac rehabilitation trials, it is pertinent to explore whether the differences in exercise interventions are contributing to the differences in observed effectiveness. While meta-analyses of these trials have provided evidence for the overall effectiveness of cardiac rehabilitation, vital content about the individual interventions and their effective components had been lost in the process of pooling these studies. An opportunity exists however to open this \u2018black box\u2019 of pooled interventions, by using meta-regression techniques to perform a more robust examination of the key intervention characteristics which may be associated with positive clinical outcomes [, ]. Past attempts to use this technique with cardiac rehabilitation have however only used a crude measure of exercise dose [, , ], excluded exercise interventions without a multi-faceted secondary prevention approach [] or examined only intermediate outcomes such as cardiorespiratory fitness [, ].The aim of this systematic review of randomised controlled trials of exercise-based cardiac rehabilitation for patients with coronary heart disease was to use meta-regression and subgroup meta-analysis to explore the contribution of individual exercise characteristics to clinical outcomes. This review expands on previous analyses by separating the exercise intervention into its smallest component parts, as well as obtaining and incorporating as many details as possible about previously unpublished intervention characteristics directly from trial authors.This meta-analysis demonstrates exercise-based cardiac rehabilitation to be effective in reducing total and cardiovascular mortality, as well as myocardial infarction, in participants with coronary heart disease. This effect was largely consistent across subgroups of patients who received various types of usual care, more or less than 150\u00a0min of exercise per week, and of differing cardiac aetiologies. Additionally, the effectiveness of cardiac rehabilitation to reduce PCI procedures displayed borderline significance, which may be important given that the funnel plot displayed a possible absence of small studies reporting positive effects.While the largest proportion of included trials and interventions remained traditional group-based programs of supervised aerobic (and often resistance) training, our review also found an expanding range of interventions including home walking programs, high-intensity interval training, case management and Internet technologies. Consequently, the prescribed dose of exercise training varied widely, with some interventions offering a substantially greater volume of training than others. However, we were unable to demonstrate evidence for the effectiveness of any one specific exercise component, such as intensity, frequency, session time or type, on reducing mortality outcomes. A relationship was observed however between increasing levels of adherence to exercise training and a reduction in subsequent mortality. In contrast, we found a detrimental effect of increasing the prescribed exercise time and intensity on myocardial infarction and PCI outcomes respectively.Overall, the results of our pooled analyses are generally consistent with previous meta-analyses of exercise-based cardiac rehabilitation. These studies also found a significant reduction in the relative risk of cardiovascular mortality, with a similar magnitude of effect observed in the most recent analyses [, ], which also included patients of mixed aetiologies. The inability of cardiac rehabilitation to significantly reduce the frequency of CABG and PCI events was also reported in these two earlier studies. The previously observed effects of cardiac rehabilitation on myocardial infarction and total mortality have however been mixed.While earlier meta-analyses found significant reductions in total mortality of up to 20\u201330% [, , , ], the most recent review [], which represented a broader range of participants and trials conducted in the modern treatment era, failed to find such a benefit. While we observed a significant 10% reduction in total mortality in our sample, this effect was less robust to missing data in sensitivity analysis and provides some evidence to support the hypothesis that the effect of cardiac rehabilitation on total mortality may be attenuated when considering a wider range of participants in the modern treatment era.This meta-analysis has shown that there is a place for exercise-based cardiac rehabilitation in the modern treatment of coronary heart disease, particularly where it plays a role in increasing the use of other secondary prevention therapies, many of which are currently suboptimal. There is little differential effect of variations in individual exercise training components, particularly on mortality outcomes. For this reason, it may be more important to offer programs which focus on achieving increased adherence to the exercise intervention, regardless of what format it may take."},
{"url": "https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-017-0085-0", "title": "Polygenic study of endurance-associated genetic markers ", "authors": ["Seema Malhotra", "Kiran Preet", "Arvind Tomar", "Shweta Rawat", "Sayar Singh", "Inderjeet Singh", "L. Robert Varte", "Tirthankar Chatterjee", "M. S. Pal", "Soma Sarkar"], "publication": "Sports Medicine - Open", "publication_date": "26 April 2017", "abstract": "Gorkhas, a sub-mountainous population of the Himalayan region, are known for strength and bravery. In the present study when \u201cGorkha\u201d is used without brackets, we are mentioning Gorkhas of Tibeto-Burman origin. Physical capability, strength and endurance are important components of fitness associated with genetic traits. The aim of this study was to examine the endurance potential of male Gorkha soldiers, based on endurance-related genetic markers ", "full_text": "\n                         (also spelled as \n                        ) are a sub-mountainous population of the Himalayan region (Nepal) and make excellent soldiers. During the Anglo-Nepalese war of 1814\u20131816, the British were greatly impressed by the bravery of the Nepalese soldiers and started recruiting Nepalese to the Gurkha regiments of the British Indian Army []. The soldiers in the British army were mainly recruited from the \u201ctrue Gorkha martial tribes\u201d of , , , , ,  and  []; the Indian Army continues to recruit from the same brigade of Gorkhas. , , , ,  and  are associated with Tibeto-Burmese cultural traditions and physical features conventionally labelled as Mongoloid while  () and  castes are associated with Aryan cultural traditions and have physical features conventionally labelled as Caucasoid []. Gurkha Service opened an opportunity for the soldiers to settle in different parts of the British Empire including India, and their descendants are present in Assam, Sikkim, Darjeeling and Dehradun [].The Gorkha soldiers are best known for their physical strength, fighting tenacity, bravery and fearlessness in battle [, ]. Physical capability, strength and power are important components of fitness. Himalayan Sherpas are well known for their physical strength and endurance in the high-altitude terrain. Himalayan Sherpa elite climbers demonstrated high functional reserve with maximal oxygen uptake (VO) of 66.7\u2009\u00b1\u20093.7\u00a0ml\u00a0min\u00a0kg, maximal cardiac frequency of 199\u2009\u00b1\u20097\u00a0beats\u00a0min and ventilatory anaerobic threshold of 62\u2009\u00b1\u20094% of VO []. Higher frequency of  allele of  (angiotensin-converting enzyme gene, location: 17q23.3) have been reported in Sherpas [].  allele is associated with endurance-related events [, ] and exercise performance in atmospheric hypoxia []. Predominance of  genotype and  allele was demonstrated in male Gorkha soldiers []. Many other polymorphisms are associated with endurance-related performance [].  polymorphism of  (alpha actinin 3 gene, location: 11q 13.1) (functional  allele and non-functional  allele) is associated with generation of rapid forceful contractions [] and muscle performance [] with frequency of -null genotype (loss of alpha actinin 3) being higher in endurance athletes [, ]. In an earlier study,  genotype of  polymorphism was observed to be present in 23% of Gorkhas []. The  polymorphism of  (muscle-specific creatine kinase gene, location: 19q13.32) is associated with energy-buffering in the skeletal muscle fibres along with tolerance to skeletal muscle damage [].  allele and  genotype were significantly higher in endurance athletes and were associated with high values of VO []. Physical fitness test scores in military recruits were also associated with  allele [].  (endothelial nitric oxide gene, location: 7q36) polymorphism is linked with endurance performance and endurance elite status [, ].  encodes the rate-limiting enzyme for nitric oxide (NO) products []. Higher frequency of wild  allele of  polymorphism was reported in high-altitude natives from Ladakh suggesting advantageous consequences in high-altitude environment [, ].High functional reserve, physical strength and endurance in the hostile high altitudes coupled with higher frequency distribution of some of the endurance-related performance enhancing genetic markers in the mountain population indicates possibility of the mountain population being genetically endowed for endurance- related activities. This natural endowment would set the stage for investigation of prospects of the mountain people for distinctive performance in endurance-related elite sports activities. In the present study, we chose to investigate in male Gorkha soldiers, (i) the genotypic and allelic frequency distribution of four genetic variants associated with endurance performance: , ,  and ; (ii) determine the probability for the occurrence of an \u201coptimal\u201d polygenic endurance profile using the four polymorphisms in the population and assess whether individuals were likely to exist who harboured \u201cpreferable\u201d genotypes for endurance; and (iii) generate a \u201ctotal genotype score\u201d (TGS) [] for finding a likely distribution of genetic endurance potential of the male Gorkha soldiers and compare with virtual data of other populations of male Indian soldiers.The Gorkha population is largely an understudied population and genetic information on the population is scanty. This study reports for the first time the genotypic and allelic frequency distribution of endurance-related four polymorphic markers in the male Gorkha soldiers of Tibeto-Burman linguistic cluster and the polygenic endurance potential of the population. Ethnic heterogeneity with respect to  polymorphism was noted with a trend of higher frequency of  genotype and  allele in Tamangs. It may be mentioned here that the Gorkhas included in this study were main ethnics (tribes): Gurungs, Magars, Rais, Tamangs and Limbus. They look similar but are very different ethnics and could be having their own distinct genetic makeup. Thus, the frequency differences within the subgroups (although not significant after Bonferroni correction, \u2009<\u20090.00250), could be attributed to their evolutionary adaptation. The observed genotypic frequency of  in Tamangs was in agreement with frequency distribution reported from inhabitants of Kotyang, majority of whom were Tamangs []. Tamangs are indigenous inhabitants of the western Himalayan regions and one of the major Tibeto-Burman speaking communities who trace their ancestry to Tibet and further back to Mongolia (). Higher  allele frequency in Tamangs, similar to that found in Sherpas, suggests enhanced physical activity in the group. Predominance of homozygous  genotype and  allele of  in Gorkhas in the present study is in agreement with that observed in Gorkhas reported earlier, [] majority of whom belonged to Indo-Aryan ethnicity. Predominance of  allele in the Gorkhas suggests endurance and muscle efficiency [, ], the key determinants of performance and also associated with enhanced performance at high altitudes [].  allele influences human physical performance and trainability [, ] and is also related to cardiorespiratory efficiency [, ]; although, contrary reports also exist showing no relationship between  polymorphism and cardiorespiratory fitness [].\n                         gene is associated with performance and genotype across multiple cohorts of elite power athletes and also supported by gene knockout mouse model [].  () allele affects endurance ability of elite athletes while  () allele affects sprinting [, ]. It would be interesting to further investigate the polygenic potential of  genotype (associated with sprinting) along with power-related muscle performance genes in this population. ACTN3  () allele and the  () genotypes are significantly associated with certain groups of elite endurance athletes [, ]. The frequency distribution of  allele in the male Gorkha soldiers is observed to be 0.394 with genotype frequency being 0.155  and 0.366 . Interestingly, the  genotype frequency in the male Gorkha soldiers was observed to be lower than Gorkha and Indian lowlander soldiers belonging to Indo-Aryan linguistic phylum as well as Caucasians (Table\u00a0). Similar to the frequency observed in male Gorkha soldiers (Tibeto-Burman), the  genotype frequency has been reported to be ~0.170 in other Asian populations viz., HAN Chinese (CHB, Sino-Tibetan linguistic phylum) [] and Japanese (HapMap Phase 3) while in Caucasians, the  frequency is 0.22 (HapMap Phase 3). At the moment, we do not have an explanation for less  genotype frequency in the Gorkha (Tibeto-Burman) population compared to Caucasian population; this appears to be a population-specific difference between Caucasian and Asian (Tibeto-Burman/Sino-Tibetan) population. The derived 577X allele has been shown to increase in frequency with distance from Africa, reaching the highest frequencies on the American continent [].Creatine kinase is an important enzyme in energy metabolism which catalyzes phosphorylation of creatine to phosphocreatine, an energy storage molecule and source of ATP []. Muscle-specific creatine kinase gene () correlates with athletic performance and  genotype is considered as one of the genetic markers associated with predisposition to endurance-related sports activity []. The  allele probably influences gene expression and results in a decrease in muscle isoform of creatine kinase activity in myocytes leading to enhanced activation of oxidative phosphorylation and endurance development []. The frequency of the  allele varies from 85% in the Chinese population [] to 68% in white Americans [] with Caucasoids having 65\u201371% []. In the male Gorkha soldiers, frequency of  allele was 80.34% and  genotype 65.50% which was comparable to that observed in high-altitude natives from India. Interestingly, frequency of  genotype was significantly higher in Gorkhas of Tibeto-Burman ethnicity as compared to Gorkhas of Indo-Aryan ethnicity. Higher frequency distribution of allele of  compared to that of  allele in the Gorkhas, similar to the frequencies observed in high-altitude natives, suggests adaptive advantage of  allele through increased production of nitric oxide (NO). Higher frequency of  allele has been reported in Sherpas [] and Quechuas of Andean  []. Higher exhaled NO has also been observed in Tibetan and Bolivian Aymara population [, ].Maximum oxygen uptake capacity (VO), the maximal amount of oxygen per unit of time that can be delivered to the peripheral organs including the skeletal muscle (where it is used to sustain muscular contraction at peak exercise), is a bench mark measure of physical performance/work capacity []. It provides an index of functional reserve of the organ systems involved and limitation that can be encountered at peak exercise [, ]. In the present study, the overall VO in the population was found to be 51.34\u2009\u00b1\u20097.22\u00a0ml kg\u2009\u00a0min\u2009 with Tamangs showing the highest maximal oxygen uptake compared to other subgroups (\u2009<\u20090.05) (Table\u00a0). Higher VO is reflected in better performance in running, hill climbing and endurance work [, ]. VO in the male Gorkha soldiers was higher as compared to the values reported from Indian general population [\u2013] suggesting overall better endurance potential of Gorkhas (Tibeto-Burman) at the physiological level. We did not, however, observe statistically significant association between VO, and the studied polymorphisms in the Gorkhas either individually with four genetic variants or when combined genotype profile of each individual was analysed in the 122 volunteers who participated in the assessment of VO (Additional file : Table S3 and Additional file : Table S4). It is probable that the small cohort of individuals for VO may be limiting the interpretation of the result or it may also be probable that VO is indeed not associated with the studied polymorphisms. Rankinen and co-workers [] analysed elite endurance athletes with VO values over 83\u00a0ml\u00a0kg and found no trend for excess  allele or low number of  homozygotes of . A genomic scan for maximal oxygen uptake in Caucasian families of the HERITAGE Family study reported many potential candidate gene loci on many chromosomal regions, but no linkage was observed on chromosome 17 on the  locus []. It appears logical to suggest that the study of influence of genotypes on the VO should be conducted in an increased cohort of healthy individuals for genetic association to be more evident. In addition to maximal rate of oxygen uptake, at least two other endurance phenotypes (economy of movement and lactate/ventilator threshold) also contribute to endurance performance phenotype (time taken to travel a given distance) as seen in elite competition []. A fourth phenotype, namely oxygen uptake kinetics, may also be discretely related. These phenotypes are yet to be associated with specific genetic polymorphisms in a healthy adult population. Unbiased genome-wide approaches have been used in search for genomic region, transcripts and DNA variants linked or associated with endurance performance related trait [\u2013]. Bouchard et al. [] studied association of 324,611 SNPs with the response of VO to endurance training in 473 Whites from HERITAGE. None of the SNPs reached genome-wide significance even though there were several SNPs moderately associated with VO trainability []. A meta-analysis of genome-wide association study of two cohorts of elite endurance athletes and controls revealed only one statistically significant marker ( at  locus, \u2009=\u20090.0002), and no panel of genomic variation common to the elite candidate athletic group was identified [].Human physical capability is influenced by many environmental and genetic factors, and it is generally accepted that physical performance phenotypes are highly polygenic [, ]. Potential for elite human physical performance is limited by the similarity of polygenic profiles with 99% of people differing by no more than seven genotypes from the typical profile []. The predicted mean TGS for favourable endurance profile in endurance elite athletes was significantly higher (70.2\u2009\u00b1\u200915.6) compared to general Spanish population (60.70\u2009\u00b1\u200912.21) and about 3000 individuals out of a total population of ~41 million people were predicted to have theoretically optimal polygenic profile (TGS\u2009=\u2009100) for seven candidate genes []. In the simulated population of Gorkhas in the present study, mean TGS obtained was 69.01\u2009\u00b1\u200915.40 for favourable endurance. It was interesting to note that 4% of the population (15 individuals out of 374) exhibited an optimal TGS (100) for endurance with another 16% of the population (60 individuals out of 374) showing a TGS of ~87. Compared to Gorkhas, only 2 and 10% of Indian lowlanders had TGS 100 and TGS 87, respectively. Analysis of TGS 100 and TGS 87.5 between Gorkha (Tibeto-Burman) and Indian lowlanders, using 3\u2009\u00d7\u20092 contingency table (), demonstrated significant difference between the cohorts (\u2009<\u20090.0001) suggesting an overall more favourable polygenic endurance profile in Gorkha population. With 4% of the male Gorkha soldiers (who are otherwise drawn from the general population) having the optimal endurance profile, this would mean that nearly 1.7 million, out of an estimated 43 million (31 million of Nepalese from Nepal and about 12 million Nepalese domiciled in India, Nepal Census 2011) would have the optimal endurance profile. Even if new candidate polymorphisms are scored, probability of presence of individuals with optimal genotypes for endurance still remains large in this population. The Gorkhas are thus genetically endowed with the endurance phenotypes; they have the genetic advantage with higher co-occurrence of  in the population. Genetic endowment coupled with high-intensity training which considerably increases maximal oxygen uptake, of young adolescent Gorkhas will definitely bring performance improvement in many sporting events. It may not be farfetched to postulate that such individuals can rewrite the world records. It is accepted that in addition to genetic potential and physical environment, a performance record is a function of economic and social opportunity. Individuals from small geographical area hold many world athletic records. Although genetic explanations is lacking, there are considerable regional and ethnic variations in typical frequencies of many genotypes e.g., frequency of  is higher in some regions of Oceania than most of Europe while athletes of north and east African descent excel in endurance events and those of west African descent excel in sprint events []. Interestingly, 5% of high-altitude natives (Tibeto-Burman) also had TGS 100 and 19% had TGS 87 similar to the TGS frequency observed in Gorkhas (Tibeto-Burman). The observation presents a scope for us to state that Tibeto-Burman mountain population per se has higher TGS score linked to endurance genotype than the lowlander Indian population.The present study provides a novel perspective on endurance genetics based on polygenic profiling in the Gorkha population. The study reports for the first time the genotypic and allelic frequencies of four endurance-associated genetic markers in the male Gorkha soldiers (Tibeto-Burman). The study also highlights that nearly 4% of the Gorkha soldiers (Tibeto-Burman) exhibit an optimal total genotypic score (100) for endurance, indicating genetic potential of this population for achieving excellence in endurance-related elite sports activities."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0353-8", "title": "A case of incisional hernia repair using Composix mesh prosthesis after antethoracic pedicled jejunal flap reconstruction following an esophagectomy", "authors": ["Atsushi Yasuda", "Takushi Yasuda", "Hiroaki Kato", "Mitsuru Iwama", "Osamu Shiraishi", "Yoko Hiraki", "Yumiko Tanaka", "Masayuki Shinkai", "Motohiro Imano", "Yutaka Kimura", "Haruhiko Imamoto"], "publication": "Surgical Case Reports", "publication_date": "29 June 2017", "abstract": "An incisional hernia in a case of antethoracic pedicled jejunal flap esophageal reconstruction after esophagectomy is a very rare occurrence, and this hernia was distinctive in that the reconstructed jejunum had passed through the hernial orifice; a standard surgical treatment for such a presentation has not been established. Herein, we describe a case of repair using mesh prosthesis for an atypical and distinctive incisional hernia after antethoracic pedicled jejunal flap esophageal reconstruction.", "full_text": "The overall reported occurrence of incisional hernias post laparotomy varies within a range between 11 and 23% in different studies [\u2013]. However, an incisional hernia in a case of antethoracic pedicled jejunal flap esophageal reconstruction is a very rare occurrence. Antethoracic jejunal esophageal reconstruction\u2014an alimentary tract reconstructive method following esophagectomy\u2014is conducted only in patients who have undergone gastric surgery previously. The distinctive feature of an incisional hernia in such patients is that the pulled-up jejunum passes through the hernial orifice together with the herniating intestine. Incisional hernias can be conventionally repaired by two methods: simple re-suturing or repair with mesh prosthesis; however, it is recommended that incisional hernias measuring more than 10\u00a0cm in diameter be repaired with mesh prosthesis because of the high recurrence rate with simple re-suturing [, ]. However, a mesh prosthesis cannot be conventionally used in an incisional hernia after antethoracic jejunal reconstruction, and consequently, the standard surgical treatment for this condition remains unclear.We, herein, describe our clinical experience of a case of incisional hernia, which developed after a postesophagectomy antethoracic pedicled jejunal reconstruction and was successfully repaired with our originally modified parastomal hernial repair method using a mesh prosthesis.A 77-year-old woman who had undergone distal gastrectomy (Billroth I) for early gastric cancer 3\u00a0years earlier then underwent a subtotal esophagectomy for esophageal squamous cell carcinoma that was followed by esophageal reconstruction with an antethoracic pedicled jejunal flap using the supercharge technique. No postoperative complication, except grade 1 surgical site infection according to the Clavien\u2013Dindo classification [, ], occurred throughout the postoperative course. For the last 8\u00a0years, the patient had chronic constipation caused by a lumbar intervertebral disk herniation, and usually strained to pass stool, with resultant high abdominal pressure.To our knowledge, this is the first case to use a mesh prosthesis for repair of an incisional hernia that developed after a subtotal esophagectomy with antethoracic reconstruction by a pedicled jejunal flap. According to a national cancer registry in Japan, reconstructions using the jejunum accounted for 5.4% of all esophageal cancer resections []. The incidence of incisional hernia after this surgery was unclear.In this case, we consider the following two factors to be responsible for wound dehiscence: one was fragility of the abdominal wall that resulted from a surgical site infection following a previous operation, and the other was the high abdominal pressure when straining to pass stool. A simple primary closure, therefore, seemed insufficient for surgical treatment. Moreover, the hernial orifice in this case was huge, and the abdominal wall around the orifice was weak; therefore, we decided to repair the hernia using a mesh prosthesis. In such a case of incisional hernia, with the pulled-up jejunum passing through the hernial orifice, a mesh prosthesis is required not only to reinforce the abdominal wall but also to allow the pulled-up jejunum to pass through the mesh without disturbing food passage. Therefore, we decided on a parastomal hernia repair technique using a mesh sheet, particularly the keyhole repairing technique [\u2013], for this case. This surgical technique using mesh, frequently reported to be effective since Rosin and Bonardi developed it [], is superior to the suture repairing technique with regard to recurrence [rate (range) 17.2% (11.9\u201323.4%) vs 69.4% (59.7\u201378.3%)] and wound infection [rate (range) 1.9% (0.4\u20135.5%) vs 11.8% (6.1\u201320.2%)] [].There is an another famous parastomal hernia repair technique, the Sugarbaker method, in which no hole is made in the mesh but the bowel going to the stoma is lateralized and covered by the mesh. It is desirable to tightly adhere the subcutaneous tissues to the skin from the perspectives of infection, recurrence, and visualization, but in this method, the skin and mesh might be distorted and a gap might occur in the subcutaneous space; thus, we did not choose this method.Bowel fistula is a postoperative complication that should be feared exceedingly, but J\u00e4nes et al. reported that it did not occur within 5\u00a0years of observation in their randomized trial []. Steele et al. reported a low incidence (3%) that was acceptable []. However, caution must be exercised with regard to this complication even if the rate is low because it is a consequence of placing the mesh in direct contact with the bowel and accounts for the hesitation with the use of this prosthesis.Thus, in our case, we created a mesh prosthesis with a diameter that was a few millimeters larger than the diameter of the pulled-up jejunum and sutured it tightly between the skin flap and the mesh at the edge of the oval edge circumferentially to keep the jejunum away from the polypropylene mesh. Moreover, we thought that a composite mesh with a porous external surface to encourage tissue integration and a smooth microporous internal surface to prevent adhesions when placed in contact with viscera would be better than the usual polypropylene mesh for prevention of unintended adhesion and, therefore, used it in this case. However, there was a problem in that the manufacturer does not recommend incisions or cuts on the Composix Kugel Patch (C.R. Bard Inc.), because a reshaped patch might cause complications such as bowel perforation or surgical site infection. Therefore, in order to avoid these complications, we placed the reshaped mesh in the subcutaneous space instead of in the abdominal cavity to ensure the intestine was kept away from the mesh. Moreover, we folded down the cut ends of the shape-memory ring and fixed them to the mesh tightly with sutures to prevent tissue injury (Fig.\u00a0).Three years and 5\u00a0months have elapsed without hernia recurrence or complications since the operation. This parastomal hernia repair using a composite mesh seems to be a useful treatment for incisional hernias developing after esophagectomy with antethoracic pedicled jejunal flap reconstruction."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0358-3", "title": "A large retroperitoneal lymphatic malformation successfully treated with traditional Japanese Kampo medicine in combination with surgery", "authors": ["Toko Shinkai", "Kouji Masumoto", "Fumiko Chiba", "Nao Tanaka"], "publication": "Surgical Case Reports", "publication_date": "17 July 2017", "abstract": "Current treatment options for lymphatic malformations (LMs) are multimodal. Recently, the effectiveness of treating LMs with Eppikajyutsuto (TJ-28) has been reported. TJ-28 is a kind of oral herbal medicine classified as the traditional Japanese Kampo medicine.", "full_text": "Lymphatic malformations (LMs) usually appear as soft compressive masses at birth and are found in all age groups and in various parts of the body. LMs develop in various spectrums from localized masses to diffuse infiltration. Morphological types of LMs are categorized according to size and location, i.e., macrocystic, microcystic, mixed macrocystic and microcystic, and diffuse types [, ]. Although LMs are benign malformation, sometimes, its treatment can be challenging, depending on the tumor size and location. Current multimodal treatments include observation, sclerotherapy, radiofrequency ablation, laser therapy, and surgery []. Recently, oral medications such as sildenafil, propranolol, and sirolimus are shown to be effective for treating LMs [\u2013]. In addition, in Japan, the most common treatment options include sclerotherapy using OK-432 and/or surgical resection. Recently, several investigators have reported on the effectiveness of Eppikajyutsuto (TJ-28) in treating LMs [, ], which is a Japanese Kampo medicine manufactured by Tsumura & Co. (Tokyo, Japan). We experienced a successfully treated case of large retroperitoneal LMs with the combination treatment of TJ-28 and surgery. Therefore, we herein reported the clinical course of this case and showed the effectiveness of our treatment.We reported a case of successfully treated retroperitoneal LM with the combination treatment of TJ-28 and surgery. Based on our experience, this TJ-28 treatment option may be very useful in treating cases of LMs having surgical difficulties because of size and/or location."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0357-4", "title": "Non-functioning parathyroid carcinoma: a case report", "authors": ["Nobuyasu Suganuma", "Hiroyuki Iwasaki", "Satoru Shimizu", "Tatsuya Yoshida", "Takashi Yamanaka", "Izumi Kojima", "Haruhiko Yamazaki", "Soji Toda", "Hirotaka Nakayama", "Katsuhiko Masudo", "Yasushi Rino", "Kae Kawachi", "Yohei Miyagi", "Akio Miyake", "Kenichi Ohashi", "Munetaka Masuda"], "publication": "Surgical Case Reports", "publication_date": "19 July 2017", "abstract": "Non-functioning parathyroid carcinoma is a rare disease that is difficult to distinguish from other diseases based on the lack of hyperparathyroidism. This is a report of non-functioning parathyroid carcinoma diagnosed by reverse transcription polymerase chain reaction (RT-PCR) targeting parathyroid hormone (PTH) messenger RNA.", "full_text": "Parathyroid carcinoma (PTC) is a rare type of endocrine tumor that accounts for less than 5% of all cases of primary hyperparathyroidism. Most PTC patients have clinical manifestations of hyperparathyroidism. However, in some cases, patients may not present increased serum calcium or parathyroid hormone (PTH) levels. This type is called non-functioning parathyroid carcinoma, which is an extremely rare type reported in less than 30 cases. It is occasionally difficult to distinguish this disease from other diseases based on the lack of hyperparathyroidism. This is the first report of non-functioning parathyroid carcinoma diagnosed by reverse transcription polymerase chain reaction (RT-PCR) targeting PTH messenger RNA (mRNA).A 67-year-old male presented with hoarseness and was admitted to our hospital. Cervical examination revealed a 5-cm hard mass at the lower portion of right thyroid lobe fixed with the swallowing.Laboratory findings demonstrated that all thyroid and parathyroid functions are within the normal range: thyrotropin (TSH) 3.68 \u03bcIU/m (normal range 0.50\u20135.00), free triiodothyronine (T3) 3.10\u00a0pg/ml (2.30\u20134.00), free thyroxine (T4) 1.06\u00a0ng/dl (0.90\u20131.70), thyroglobulin (Tg) 31\u00a0ng/ml (\u226432.7), anti-thyroglobulin antibodies (TgAb) 10.0\u00a0IU/ml (<28.0), serum calcium 9.4\u00a0mEq/l (8.8\u201310.1), serum phosphorus 3.2\u00a0mg/dl (2.6\u20134.4), and intact parathyroid hormone (PTH) 49\u00a0pg/ml (15\u201365).Fine-needle aspiration revealed cells with a large N/C ratio. No cells exhibited nuclear grooves or intranuclear cytoplasmic inclusion, suggesting that it was a poorly differentiated thyroid carcinoma (PDTC) derived from a follicular tumor.The post-operative course was satisfactory, and no dysphagia was observed. The patient was discharged 5\u00a0days after the operation. Seven months later, a palpable lymph node of approximately 2\u00a0cm was detected on the right neck during a physical examination. The US and CT scans revealed lymphadenopathy in the right upper deep cervical lymph nodes. The patient was cytologically diagnosed with the parathyroid carcinoma. Then, right modified neck dissection was performed 9\u00a0months after the operation. Given that prophylactic irradiation therapy has a low evidence level and has not been established, we suggested the patient undergo radiation therapy, but he refused. Unfortunately, re-operation was needed for the right neck recurrent lymph nodes 25\u00a0months after the first surgery. We finally decided to apply radiation therapy to the neck after the third operation. No distal metastasis has occurred, and the patient is still alive.Parathyroid carcinoma is a rare disease with a prevalence of 0.015 per 100,000 persons and is estimated to account for 0.005% of all cancers []. Most patients with parathyroid carcinoma exhibit hyperparathyroidism, i.e., 2-5% of all primary hyperparathyroidism cases [\u2013]. The median age is 48\u00a0years old. The male to female ratio for parathyroid carcinoma is approximately 1:1, whereas that for primary hyperparathyroidism is 1:3.5. More females are affected than males []. Clinical findings of parathyroid carcinoma include neck tumor, high PTH and associated hypercalcemia, and generalized fibrous osteitis [, ]. In addition, a report also states that findings such as D/W ratio \u22651 in the ultrasound scan and a tumor growing into the thyroid gland could be useful findings indicating malignancy []. Cytology is contraindicated as it may induce dissemination. Hence, it is necessary to develop treatment strategies against cancer when parathyroid carcinoma is suspected based on the above mentioned clinical findings. The gene involved in the onset of parathyroid carcinoma is hyperparathyroidism 2 (HRPT2), which is a tumor suppressor gene on a long arm of the chromosome 1 (1q31). Genetic mutations of this gene inactivate parafibromin. This gene has been identified as a causal factor of hyperparathyroidism-jaw tumor syndrome, a hereditary hyperparathyroidism condition, in 2002 []. Mutations in this gene have also been detected in sporadic parathyroid carcinoma [].In addition, non-functioning parathyroid carcinoma is a very rare disease, with approximately less than 30 cases reported to date [, ]. Genetic mutations in HRPT2 have also been reported with this disease []. However, given that insufficient data are available, further accumulation of cases is awaited. Unlike functioning parathyroid carcinoma, this condition does not develop hypercalcemia and associated clinical symptoms. Hence, non-functioning parathyroid carcinoma is typically discovered with neck tumor, hoarseness, dysphagia, or dyspnea []. Our case was also discovered with hoarseness without hypercalcemia or high PTH value; it was difficult to make a diagnosis preoperatively. A histopathological study revealed fibrous band formation inside the tumor, trabecular alignment of tumor cells, mitotic figures, and capsule/blood vessel invasions as described in the criteria proposed by Schantz and Castleman [] and compatible with those of parathyroid carcinoma. In addition, we also performed immunostaining methods to differentiate this case from other diseases, such as PDTC, medullary thyroid carcinoma (MTC), carcinoma exhibiting thymus-like differentiation of the thyroid (CASTLE), paraganglioma, and thymic carcinoid. According to the results, staining for PDTC, TTF-1 and Tg was negative, and no differentiated cancer tissues were detected. For MTC, calcitonin was negative. For CASTLE, CD5 was negative. For paraganglioma, S-100 protein was negative. Hence, these diseases were excluded. For thymic carcinoid, it was difficult to differentiate using the immunostaining technique as both are neuroendocrine tumors. Although the tumor cells were slightly positive for PTH, the diagnosis of non-functioning parathyroid carcinoma was finally made by detecting PTH by RT-PCR, which offers increased sensitivity compared with immunohistochemical staining.For the treatment of non-functioning parathyroid carcinoma, it is crucial to perform radical excision in the initial operation. It is difficult however to achieve a definitive diagnosis of this disease pre-operatively unless we keep this disease in mind, as it exhibits neither hypercalcemia nor PTH elevation. Radical excision should not be performed in the initial operation in approximately 86% of cases []. The lesions on the right recurrent nerve, muscular layer of the esophagus, and the membranous wall of trachea were all resected with the tumor during the initial operation. In our case, lymph node recurrence in the right neck occurred after 9 and 25\u00a0months from the initial operation, and irradiation to the neck was required after the second lymph node dissections. No distal metastasis occurred, and the patient is still alive.This is a report of a patient with non-functioning parathyroid carcinoma, which is clinically very rare. We diagnosed this tumor as non-functioning PTC using RT-PCR for PTH mRNA."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0361-8", "title": "A case of successful conversion from everolimus to surgical resection of a giant pancreatic neuroendocrine tumor", "authors": ["Asahi Sato", "Toshihiko Masui", "Nao Sankoda", "Kenzo Nakano", "Yuichiro Uchida", "Takayuki Anazawa", "Kyoichi Takaori", "Yoshiya Kawaguchi", "Shinji Uemoto"], "publication": "Surgical Case Reports", "publication_date": "20 July 2017", "abstract": "Although pancreatic neuroendocrine tumors generally have a far better prognosis relative to pancreatic cancer, the varied manifestations lead to treatment-related challenges. Everolimus therapy is generally recommended for patients with advanced pancreatic neuroendocrine tumors; however, its efficacy in a neoadjuvant setting remains unclear. Here we present a case of a giant pancreatic neuroendocrine tumor with a portal tumor thrombus that became resectable after everolimus therapy.", "full_text": "Pancreatic neuroendocrine tumor (PNET) is a rare type of malignancy [], with an estimated prevalence of 2.23/100,000 and annual onset incidence of 1.01/100,000 in Japan []. Although the prognosis of PNET is better than that of pancreatic cancer [, ], some types of PNET, such as those presenting with multiple metastases at the initial diagnosis or with advanced locoregional diseases, are difficult to cure. Recently, molecular targeting agents such as everolimus and sunitinib were found to be effective for advanced PNETs [, ], and the National Comprehensive Cancer Network guidelines accordingly recommend these agents for the treatment of locoregional unresectable or metastatic PNETs []. However, the preoperative therapeutic efficacies of these agents have not been elucidated.In this case report, we describe the successful treatment of a giant PNET that occupied most of the pancreas and had extended via the portal vein to the level of the hepatic hilus at the time of diagnosis. Although the tumor size was not reduced significantly by everolimus, the extent of portal vein invasion was reduced enough to allowing a conversion to surgical resection. This case suggests the potential role of everolimus as a neoadjuvant agent for locally advanced PNETs.This is a case report to describe the efficacy of preoperative everolimus therapy for locally advanced PNET. We did not initially intend to administer everolimus as a neoadjuvant therapy, following complete resection after 2\u00a0years of treatment. This suggests that the treatment plan for highly advanced locoregional PNETs must be organized individually because the tumor character is different from each other. In addition, we have to carefully estimate the therapeutic effect of the treatment for locally advanced tumor. In our case, although everolimus therapy did not appear to reduce the size of the tumor, it decreased the extent of the portal vein invasion, thus allowing us to perform surgical resection.To the best of our knowledge, there was no English literature which suggested the effectiveness of preoperative therapy by everolimus. However, in a Japanese case report, Takano et al. reported the improvement of arterial invasion by using everolimus and somatostatin analog preoperatively []. Combined with our case, it might be a treatment of choice to take everolimus before resection in locally advanced PNETs with vascular invasion.Everolimus, an inhibitor of the mammalian target of rapamycin (mTOR) signaling, was shown to effectively treat advanced PNETs in the RADIENT-3 (RAD001 in Advanced Neuroendocrine Tumors-3) study []. Generally, mTOR pathway abnormalities in malignancies are associated with increased aggressiveness [], and 15% of sporadic PNETs harbor mutations in this pathway, according to an exome sequencing analysis []. In our patient, everolimus was especially effective for ameliorating the extent of portal vein disease, which had restricted the possibility of complete resection, suggesting the potential use of this agent as a neoadjuvant therapy for locally advanced PNETs. Additionally, our case also suggests the importance of a close follow-up of patients with unresectable locally advanced PNETs, as drug therapy may change the surgical eligibility in some cases.In the present case, the tumor began to grow slightly after a 2-year course of everolimus therapy, leading us to speculate an adaptive response such as resistance. Although Yao et al. described the existence of an adaptive mechanism in response to everolimus [], the acquisition of this mechanism remains under investigation. Recently, Vandamme et al. demonstrated that resistance to everolimus could be overcome using a novel phosphoinositide-3 kinase (PI3K)-Akt-mTOR inhibitor []. However, those results were obtained through experiments using cell lines, which differ considerably from in vivo PNETs []. Additional clinical experiences and the establishment of animal models that could mimic human PNETs will be needed to overcome these adaptive responses.At present, adjuvant treatment for resected PNETs has not been established. Despite the absence of postoperative therapy, our patient has remained recurrence-free for more than 3\u00a0years after surgical resection. Our patient showed disease-free status of the lymph nodes. Hashim et al. revealed that lymph node metastasis is predictive of a poor outcome among patients with PNETs [], and Ge et al. recently revealed the prognostic significance of lymph node metastasis regarding non-functioning PNETs []. Additionally, Taki et al. reported a significant difference in 5-year overall survival between patients with and without lymph node metastasis []. These suggest that lymph node metastasis is a risk factor for poor prognosis. In this sense, if our patient has a positive lymph node metastasis, it is reasonable to treat with adjuvant therapy. Further studies are needed to determine the indications and applications of adjuvant treatments.In our present case, we successfully resected a giant, locally advanced PNET, which was unresectable at the time of diagnosis, after a 2-year course of everolimus treatment. We have to carefully follow up treating a locally advanced tumor to estimate the possibility of resection at any time."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0356-5", "title": "Recurrence with pagetoid spread arising 17\u00a0years after surgery for intramucosal rectal cancer: a case report", "authors": ["Taichi Matsubara", "Yuta Kasagi", "Kippei Ogaki", "Yu Nakaji", "Ryota Nakanishi", "Yuichiro Nakashima", "Masahiko Sugiyama", "Hideto Sonoda", "Hiroshi Saeki", "Eiji Oki", "Yoshihiko Maehara"], "publication": "Surgical Case Reports", "publication_date": "26 July 2017", "abstract": "Perianal Paget\u2019s disease (pPD) is uncommon, with only about 180 cases documented in the literature. Anorectal carcinoma with pagetoid spread is even rarer.", "full_text": "Paget\u2019s disease (PD) is an uncommon intraepidermal adenocarcinoma and is classified as mammary and extramammary. Mammary PD is an adenocarcinoma originating mainly in the mammary duct, and extramammary PD (EPD) is an epidermotropic neoplasm arising from the apocrine glands of organs such as the vulva, penis, scrotum, perineum, and perianal region [\u2013]. PD is characterized by the presence of atypical Paget cells with clear cytoplasm and large pleomorphic nuclei in histological diagnosis [].Perianal PD (pPD) is a rare disease with only about 180 cases documented in the literature []. It is confirmed in approximately 20% of cases of EPD [], and some studies have shown that pPD may be associated with underlying malignancy []. The rate of malignancy associated with pPD ranges from 33 to 86% [], and it is also associated with underlying anorectal carcinoma [, ]. In the case of pPD with underlying malignancy, the carcinoma cells can spread in the anal canal mucosa and perianal skin, and may show histological features similar to Paget cells. Such spread, therefore, is referred to as pagetoid spread []. Some case reports describe anorectal carcinoma with pagetoid spread [, ]; however, most of those cases were synchronous primary anorectal cancer. To the best of our knowledge, metachronous recurrence rectal cancer with pagetoid spread has never been reported previously.Thus, we report a rare case of locally recurrent rectal cancer with pagetoid spread arising from intramucosal rectal cancer that was treated by surgery 17\u00a0years ago.The diagnosis of PD must be confirmed histologically. Armitage et al. [] reported that Paget cells are usually positive for CK7, GCDFP-15, CEA, and human milk fat globule glycoproteins 1 and 2 by immunohistochemical staining. In contrast, pPD associated with rectal adenocarcinoma is usually positive for CK20 but negative for GCDFP-15 []. In the present case, histological findings revealed that the resected specimen contained Paget cells in the squamous epithelium as well as adenocarcinoma components in the submucosal layer. Immunohistochemical staining revealed that CK7 and CK20 expression was positive in Paget cells and adenocarcinoma tissues. However, GCDFP-15 activity was not detected in any of the lesions. The patient had a history of intramucosal rectal cancer 17\u00a0years ago, and the tumor recurred on several occasions before the present episode. Therefore, we diagnosed the lesion as recurrent rectal cancer with pagetoid spread.Pagetoid spread is thought to be a type of secondary EPD that can occur with some visceral carcinomas, especially of the rectum and anal canal, leading to epidermotropism with intraepidermal invasion or metastasis. This is known as pagetoid phenomenon [, ]. pPD associated with anorectal cancer is also included in secondary EPD, although such cases are rare. Therefore, clinical experience is limited and the literature consists mainly of sporadic case reports and small series. Kubota et al. [] reviewed 28 cases of pPD associated with anorectal carcinoma, and Goldman et al. [] reported that pPD with underlying anorectal carcinoma comprises 33% of pPD cases. However, almost all of those reported cases were pPD associated with synchronous primary anorectal cancer. The present case was metachronous rectal cancer recurrence with pagetoid spread with a long-term clinical course after the first operation. This is believed to be the first report to describe such a case.In the present case, the patient had been diagnosed with intramucosal, well-differentiated adenocarcinoma at the first operation, but final pathological findings revealed that the recurrent tumor was moderately to poorly differentiated adenocarcinoma with pagetoid spread. During perineal tumor resection and Miles\u2019 operation, the tumor exhibited rapid growth and bilateral inguinal lymph node metastases. In general, it is believed that an epigenetic physiological phenomenon is essential for aging and carcinogenesis, and such abnormality mutants are definitive risk factors for cancer []. These findings led to speculation that her long-term clinical course, aging, and frequency of resections stimulated changes in cancer epigenetics or tumor characteristics, which might have caused cancer genotype transformation and poor prognosis.Generally, the prognosis of PD confined to the epidermis is good, however, it is poorer in patients with underlying anorectal carcinoma [, ]. pPD associated with malignancy is reported to have a high risk of local recurrence []. According to Kubota et al. [] the 1- and 2-year survival rates of patients with anorectal carcinoma were 91 and 44%, respectively. Armitage et al. [] indicated the need for multidisciplinary therapy, such as adjuvant chemotherapy and radiotherapy, after the treatment of local recurrence. In our case, the patient and her family did not want any more therapy; therefore, we did not carry out additional chemotherapy and radiotherapy after surgery and the disease took its natural course. Seven months later, she died from multiple metastases of cancer. Our findings suggest that rectal cancer recurrence with pagetoid spread progresses rapidly and has poor prognosis, and it seems to be better to perform additional therapy after surgery.In conclusion, this is believed to be the first report of a rare case of locally recurrent rectal cancer with pagetoid spread arising from intramucosal rectal cancer.\u00a0The present case suggested that radical surgery followed by additional therapies should be considered when recurrence of a rectal cancer with pagetoid spread is diagnosed."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0360-9", "title": "Successful video-assisted thoracoscopic surgery in prone position in patients with esophageal cancer and aberrant right subclavian artery: report of three cases", "authors": ["Koji Shindo", "Eishi Nagai", "Toshinaga Nabae", "Toru Eguchi", "Taiki Moriyama", "Kenoki Ohuchida", "Tatsuya Manabe", "Takao Ohtsuka", "Yoshinao Oda", "Makoto Hashizume", "Masafumi Nakamura"], "publication": "Surgical Case Reports", "publication_date": "28 July 2017", "abstract": "An aberrant right subclavian artery (ARSA) with an associated nonrecurrent right inferior laryngeal nerve (NRILN) is a relatively rare anomaly that occurs at a frequency of 0.3 to 2.0% of the general population. NRILN has been mainly documented in the head and neck region; it has been rarely described in patients with esophageal cancer, especially those undergoing thoracoscopic surgery. Video-assisted thoracoscopic surgery for esophageal cancer (VATS-E) is becoming more widespread as a reliable minimally invasive surgical procedure associated with reduced perioperative complications.", "full_text": "Video-assisted thoracoscopic surgery for esophageal cancer (VATS-E) is a reliable minimally invasive surgery that provides an enlarged view and reduced perioperative complications such as wound infection and pulmonary complications []. All involved staff members can share the enlarged, detailed view, which is useful for quality control of surgical procedures. Moreover, use of the prone position during VATS-E helps to keep the operating field clear by avoiding the pooling of blood and leachate. However, lymphadenectomy in the upper mediastinum is still a stressful procedure for many surgeons because of the difficulty of exploration for the recurrent nerve. Injury to this nerve may decrease the patient\u2019s quality of life because of the development of hoarseness and an increased risk of aspiration pneumonia. A nonrecurrent right inferior laryngeal nerve (NRILN) is a relatively rare anomaly of the recurrent nerve that is associated with an aberrant right subclavian artery (ARSA). It is essential that surgeons who perform esophagectomy are familiar with the anatomy of the vessels and nerves in this region, including their variations. We herein describe three patients with esophageal cancer and an NRILN with an associated ARSA who underwent VATS-E and highlight the key points to remember in such cases.In conclusion, we have herein reported three cases of successful VATS-E for treatment of esophageal cancer in patients with an ARSA and NRILN with no morbidity and discussed key points to be noted when performing esophagectomy. Knowledge of this type of anomaly is very important in esophagectomy. VATS-E in the prone position can help to reduce the risk of complications with an enlarged and clear view."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0362-7", "title": "Resection of rectal cancer resembling submucosal tumor that was preoperatively diagnosed with endoscopic ultrasound-guided biopsy", "authors": ["Akimitsu Tanio", "Hiroaki Saito", "Keigo Ashida", "Shouichi Urushibara", "Manabu Yamamoto", "Naruo Tokuyasu", "Teruhisa Sakamoto", "Soichiro Honjo", "Yoshihiko Maeta", "Yoshiyuki Fujiwara"], "publication": "Surgical Case Reports", "publication_date": "26 July 2017", "abstract": "Colorectal cancer (CRC) resembling submucosal tumor (SMT; CRC/SMT) is very rare. Because its biopsy is challenging, accurate preoperative diagnosis is also very rare.", "full_text": "Submucosal tumor (SMT) usually arises from tissue in the wall of digestive tract, and its surface is therefore covered with normal mucosa in most cases. In contrast, gastrointestinal (GI) carcinomas arise from the epithelium, and most of their mucosal surfaces typically consist of cancerous tissue. SMT-like growth is an unusual presentation for GI carcinomas, especially in colorectal cancer (CRC). We herein report a rare case of rectal cancer resembling SMT (CRC/SMT) in which endoscopic ultrasound-guided fine needle aspiration biopsy (EUS-FNAB) was useful in accurate preoperative diagnosis. We also review the literature and discuss CRC/SMT.Carcinomas of the digestive tract arise from the mucosal epithelium; most of their mucosal surfaces consist of cancerous tissues. Although cancer cells then infiltrate both horizontally and vertically, the dominant direction depends on the nature of the cancer cells. Some intestinal cancers dominantly infiltrate in a vertical direction without massive invasion along the horizontal plane. As a result, the internal intestinal surfaces over these tumors are mostly covered with normal mucosa, and therefore manifest as SMTs. This type of tumor is sometimes called a carcinoma resembling an SMT; and although it is extremely rare, has been reported in the esophagus [], stomach [], and colon and rectum [, ]. A report that reviewed 70 reported cases of CRC/SMT found it to be characterized by small tumor size, high rates of poorly differentiated adenocarcinoma or mucinous adenocarcinoma, and invasiveness (including high rates of lymph node metastasis and lymphatic vessel invasion) [].Secondly, our case clearly showed that EUS-FNAB is useful for accurate preoperative diagnosis of CRC/SMT. EUS-FNAB was recently shown to be useful in accurate preoperative diagnosis of gastrointestinal stromal tumor (GIST) [] and pancreatic cancer []. Previous reports have shown the difficulty of preoperative CRC/SMT diagnosis, because of the difficulty of obtaining biopsy specimens. Therefore, EUS-FNAB should be considered to make accurate preoperative SMT diagnoses in colon and rectum. To our knowledge, our case is the first to use EUS-FNAB for an accurate preoperative diagnosis of CRC/SMT.We should keep in mind that colorectal cancer can present with SMT-like growth. Furthermore, EUS-FNAB is useful for preoperative accurate diagnosis of this rare colorectal cancer."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0355-6", "title": "Hereditary breast cancer associated with Cowden syndrome-related PTEN mutation with Lhermitte-Duclos disease", "authors": ["Fuyo Kimura", "Ai Ueda", "Eiichi Sato", "Jiro Akimoto", "Hiroshi Kaise", "Kimito Yamada", "Mari Hosonaga", "Yuko Kawai", "Saeko Teraoka", "Miki Okazaki", "Takashi Ishikawa"], "publication": "Surgical Case Reports", "publication_date": "24 July 2017", "abstract": "Cowden syndrome is characterized by multiple hamartomas in various tissues, including the skin, brain, breast, thyroid, mucous membrane, and gastrointestinal tract, and is reported to increase the risk of malignant disease.", "full_text": "Estimates show that up to 15% of breast cancer patients have one or more first-degree relatives with breast cancer []. Inherited breast cancer is caused by penetrant susceptibility genes and most often involves germ line mutations of the  and  genes in about 15% of familial breast cancer patients. Tumor protein 53 (), cadherin 1 (), liver kinase B1 (), and phosphatase and tensin homolog () are rarely associated with the development of breast cancer, which occurs in only about 3% of patients with familial breast cancer. Breast cancer in approximately half of women with a familial history may also result from unexplained genes [].Cowden syndrome (CS) is an autosomal dominant inherited cancer syndrome associated with germ line mutations in , a tumor suppressor gene. CS is characterized by multiple hamartomas and developing breast, thyroid, and endometrial malignancies. Dysplastic cerebellar gangliocytoma called Lhermitte-Duclos disease (LDD) is also associated with CS [, ]. The cumulative lifetime risks of any cancer diagnosis is 89% for CS patients, and the morbidities are 85% in breast cancer, 32% in LDD, 21% in thyroid cancer, 19% in endometrial cancer, 15% in renal cancer, 16% in colon and rectum cancers, and 15% in kidney cancer [, ]. Herein, we report a case of -mutated hereditary breast cancer with LDD.In conclusion, we report the case of a rare familial breast cancer syndrome of CS. Unlike other familial breast cancers such as those with  mutation, breast cancers with CS are generally hormone receptor-positive and may have a favorable clinical course."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0365-4", "title": "Aggressive angiomyxoma of the liver: a case report and literature review", "authors": ["Koki Sato", "Masahiro Ohira", "Seiichi Shimizu", "Shintarou Kuroda", "Kentaro Ide", "Kohei Ishiyama", "Tsuyoshi Kobayashi", "Hiroyuki Tahara", "Noriyuki Shiroma", "Koji Arihiro", "Michio Imamura", "Kazuaki Chayama", "Hideki Ohdan"], "publication": "Surgical Case Reports", "publication_date": "23 August 2017", "abstract": "Aggressive angiomyxoma (AAM) is a rare mesenchymal tumor that occurs almost exclusively in the soft tissue of the pelvis and perineum. AAM has both locally infiltrative and recurrent characteristics. Very few cases of AAM occurring outside of the pelvis and perineum have been reported. Here, we report a case of AAM originating in the liver of a 33-year-old female patient.", "full_text": "Aggressive angiomyxoma (AAM) is a rare mesenchymal tumor with myxoid and vascular components that usually occurs in the pelvi-perineal region of females, as first described by Steeper and Rosai in 1983 [].AAM lesions nearly exclusively arise in the soft tissues of the pelvi-perineal region of adult women. However, uncommon cases have been reported [\u2013] in which the neoplasm has features consistent with AAM but occurs in the head, neck, or lung regions.Here, we also discuss the differential characteristics of AAM occurring outside of the pelvi-perineal region.To the best of our knowledge, only one case of AAM originating from the liver has been previously reported, and that case was reported by Qi et al. in 2015 []. This report describes an additional case of primary AAM arising from the right lobe of the liver in a 33-year-old woman.The postoperative course was uneventful, and she was discharged from the hospital 9\u00a0days after surgery. The patient was followed for 10\u00a0months postoperatively. There were no signs of recurrence or distant metastasis.Regarding diagnosis with AAM, the characteristic MRI findings of AAM include the following: On T1-weighted MRI, it has low signal intensity. That is, it is of the same signal intensity as the skeletal muscle. On T2-weighted MRI, it has high signal intensity. These appearances likely relate to the loose myxoid matrix and high water content of angiomyxoma []. The CT findings demonstrated that the tumor, later identified as AAM, had a well-defined margin and an attenuation less than that of a muscle []. On ultrasonography, AAM appears as a hypoechoic or cystic mass []. In our case, the liver mass showed high signal intensity on T2-weighted MRI, as an attenuated mass on CT, and as a hypoechoic cystic region on ultrasound (Fig. ). In our case, CT and FDG-PET unable to detect any mass in other regions including the pelvi-perineal region. Therefore, the AAM was concluded as of liver origin.Immunohistochemically, AAM has been found to be positive for vimentin, desmin, CD 34, ER, PgR, and \u03b1-smooth muscle actin and negative for MSA and S-100 []. Table  described immunohistochemical features of AAM which occurred in other than pelvi-perineal region. In our case, immunostaining was positive for vimentin, desmin, CD34, and ER, PgR and was negative for S100, EMA, CD99, HMB45, and CK19, which was conclusive of the diagnosis of AAM. In the liver, myxoid or fibrous tumors with abundant vasculature are not frequent. The differential diagnosis of these tumors includes myxoid neurofibroma, angiomyolipoma (AML), and angiomyofibroblastoma (AMF). Myxoid neurofibroma showed a positive immunohistochemical staining for S-100, and AML showed specifically positive for HMB-45. AMF is also characterized by the immunoreactivities of vimentin, desmin, and CD34. AAM can be distinguished from the aforementioned tumors by the proliferation of spindle- or satellite-shaped cells in a myxoid background with a prominent vascular component []. From these findings, our case was distinguished from AMF by the features including subcutaneous location, smaller tumor size, sharply circumscribed margins, and delicate smaller tumor vessels.The ideal treatment of AAM is complete surgical excision with a wide margin, because AAM is a locally aggressive tumor. However, another study asserted that the recurrence rate in patients with narrow surgical margins is not higher than that of patients with wide surgical margins []. Although we aim for complete resection, incomplete or partial resection is acceptable, especially when high operative morbidity is anticipated or preservation of fertility is an issue. Long-term follow-up and careful monitoring with imaging techniques are essential for timely identification of recurrence []. Distant metastasis is rarely reported [, ].Radiation therapy and chemotherapy should not be recommended for AAM because the mitotic index of AAM is usually low. Hormonal treatment could be used as an adjuvant therapy for recurrent and residual tumors, because most AAM express estrogen and progesterone receptors []. In cases with large tumors requiring extensive resection, arterial embolization and/or hormonal treatment may be used initially, followed by surgical resection []. Regarding AAM derived from extra-pelvic regions, all six patients underwent surgical resection without any additional adjuvant therapies (Table ).This report describes the second case of AAM originating from the liver, which is an uncommon location for this particular tumor. Reporting a large series of these tumors may lead to a better understanding of how AAM may occur outside the pelvi-perineal region."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0364-5", "title": "Petersen\u2019s hernia after living donor liver transplantation", "authors": ["Sodai Sakamoto", "Ryoichi Goto", "Norio Kawamura", "Yasuyuki Koshizuka", "Masaaki Watanabe", "Minoru Ota", "Tomomi Suzuki", "Daisuke Abo", "Kenichiro Yamashita", "Toshiya Kamiyama", "Akinobu Taketomi", "Tsuyoshi Shimamura"], "publication": "Surgical Case Reports", "publication_date": "23 August 2017", "abstract": "Hepaticojejunostomy may be used for biliary reconstruction in certain cases of liver transplantation. In this occasion, Roux-en-Y biliary reconstruction is predominantly performed. Petersen\u2019s hernia is an internal hernia that can occur after Roux-en-Y reconstruction, and it may lead to extensive ischemic changes affecting incarcerated portions of the small bowel or Roux limb resulting in severe complications with a poor prognosis.", "full_text": "Petersen\u2019s hernia is a complication of Roux-en-Y reconstruction and involves incarceration of a small bowel loop beneath the mesenterium of the Roux limb through Petersen\u2019s peritoneal defect []. Petersen\u2019s hernia may cause necrosis of incarcerated small bowel and ischemic injury of the Roux limb and is associated with a poor prognosis []. Hepaticojejunostomy with a Roux-en-Y limb is performed as part of biliary reconstruction in certain cases of liver transplantation (LT). Once this type of hernia develops in patients who underwent transplantation, it may lead to ischemic damage to the anastomosis of the hepaticojejunostomy, leading to severe complications and graft failure. To avoid the reanastomosis of the hepaticojejunostomy, which is a high-risk procedure in emergent situations, the extent of injury to the Roux limb should be accurately assessed.We report a case of Petersen\u2019s hernia observed in a LT patient. Petersen\u2019s hernia is a relatively rare complication after LT. However, it should be considered as a cause of bowel obstruction in recipients with Roux-en-Y reconstruction. Early diagnosis and prompt and proper surgical intervention including second-look surgery are crucial for the treatment of this type of hernia."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0371-6", "title": "Tracheoesophageal fistula after total resection of gastric conduit for gastro-aortic fistula due to gastric ulcer", "authors": ["Yayoi Sakatoku", "Masahide Fukaya", "Hironori Fujieda", "Yuzuru Kamei", "Akihiro Hirata", "Keita Itatsu", "Masato Nagino"], "publication": "Surgical Case Reports", "publication_date": "23 August 2017", "abstract": "Tracheoesophageal fistula (TEF) is a rare but life-threatening complication after esophagectomy. It has a high mortality rate and often leads to severe aspiration pneumonia. Various types of surgical repair procedures have been reported, but the optimal management of TEF is challenging and controversial. Treatment should be individualized to each patient.", "full_text": "Tracheoesophageal fistula (TEF) is a rare but life-threatening complication after esophagectomy. It has a high mortality rate and often leads to severe aspiration pneumonia [, ]. Various types of surgical repair procedures have been reported, but the optimal management of TEF is challenging and controversial. Treatment should be individualized to each patient.Herein, we report a patient with a TEF after total resection of a gastric conduit for gastro-aortic fistula due to a gastric ulcer, successfully repaired with a pectoralis major muscle flap through a cervical approach. A TEF located near the cervicothoracic border was successfully treated with a pectoralis major muscle flap through a cervical approach. Because total resection of a gastric conduit in the posterior mediastinum carries a risk of tracheobronchial injury, thoracotomy with creation of an intercostal muscle flap should be performed in preparation for a tracheobronchial injury in such situations. If such an injury occurs, surgeons should be able to repair the injury using a suitable flap depending on the injury site."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0367-2", "title": "Laparoscopic repair of parahiatal hernia after esophagectomy: a case report", "authors": ["Yuji Akiyama", "Takeshi Iwaya", "Fumitaka Endo", "Takehiro Chiba", "Takeshi Takahara", "Koki Otsuka", "Hiroyuki Nitta", "Keisuke Koeda", "Masaru Mizuno", "Yusuke Kimura", "Akira Sasaki"], "publication": "Surgical Case Reports", "publication_date": "23 August 2017", "abstract": "Diaphragmatic hernia is a potential complication of esophagectomy, which usually occurs as a hiatal hernia and more frequently after minimally invasive esophagectomy. Parahiatal hernia is a rare form of diaphragmatic hernia, and to the best of our knowledge, parahiatal hernia after esophagectomy has not been previously reported. Here, we report a case of parahiatal hernia after esophagectomy that was successfully managed laparoscopically.", "full_text": "Diaphragmatic hernia is a potential complication of esophagectomy, which usually occurs as a hiatal hernia and more frequently after minimally invasive esophagectomy. [\u2013]. Parahiatal hernia is a rare form of diaphragmatic hernia [], and parahiatal hernia after esophagectomy has not been previously reported. Here, we report a case of parahiatal hernia after esophagectomy that was successfully managed laparoscopically.A 73-year-old man was admitted to our hospital with a diagnosis of esophageal squamous cell carcinoma in the middle thoracic esophagus in October 2014. The clinical diagnosis was T4b (left main bronchus) N2M0 stage IIIC carcinoma, according to the seventh edition of the Union for International Cancer Control TNM Classification of Malignant Tumors. Accordingly, he was initially treated by triple induction chemotherapy comprising docetaxel, cisplatin, and 5-fluorouracil. Febrile neutropenia and neutropenic enterocolitis were observed as adverse events. Downstaging to T3N2M0 stage IIIB carcinoma was achieved after two courses of chemotherapy. Thoracoscopic esophagectomy for esophageal cancer with gastric tube reconstruction via the posterior mediastinum was performed in January 2015. An abdominal approach was applied using open laparotomy. A jejunostomy catheter was placed for early postoperative enteral nutrition. Ileus due to  was a postoperative morbidity, with hepatic portal venous gas and sepsis on postoperative day (POD) 10. Left pleural effusion was pooled and treated with drainage between POD 11 and 25. The patient recovered and was discharged on POD 28. He was then identified as having intestinal obstruction with volvulus at the site of the removed jejunostomy catheter and underwent surgery with laparotomy to correct this in August 2015.This was a rare case of parahiatal hernia following thoracic esophagectomy involving gastric tube construction via the posterior mediastinum. Laparoscopic surgery was found to be effective for the diagnosis and repair of a parahiatal hernia."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0363-6", "title": "Hydrodynamic rupture of liver in combat patient: a case of successful application of \u201cdamage control\u201d tactic in area of the hybrid war in East Ukraine", "authors": ["Igor Khomenko", "Vitalii Shapovalov", "Ievgen Tsema", "Georgii Makarov", "Roman Palytsia", "Ievgen Zavodovskyi", "Ivan Ishchenko", "Andrii Dinets", "Vladimir Mishalov"], "publication": "Surgical Case Reports", "publication_date": "15 August 2017", "abstract": "The hybrid war of Russia against Ukraine has been started in certain districts of Donetsk and Luhansk oblasts within the Donbas area in 2014.", "full_text": "The hybrid war of Russia against Ukraine has been started in certain districts of Donetsk and Luhansk oblasts within the Donbas area in 2014. Although the actual nature of this armed conflict is a war, it is officially entitled by the Ukrainian government as an antiterroristic operation or ATO [, ]. The pro-Russian terroristic forces apply modern weapons such as multiple-launch rocket systems (MLRS) \u201cGrad\u201d, \u201cSmerch\u201d, or \u201cUragan\u201d against the Armed Forces of Ukraine during the hybrid war. It has resulted in various injuries of military personnel, which is not typically seen in known armed conflicts [\u2013]. Out of all injuries, thoraco-abdominal fragmental wounds are considered as the most severe [, , ]. First aid is provided during the evacuation or at the place of injury (I level), whereas hospital stage of medical care in Ukraine to the combat patients is organized at the levels II, III, IV, and V [, ]. The surgical care (II level) is provided in district hospitals to be deployed very close to the battlefield line, contributing to the implementation the principle of the \u201cgolden hour\u201d.Specialized surgical care (level III) is provided in Kharkiv and Dnipro. The IV level of medical care is provided in national (Kyiv) or regional Military Medical Clinic Centers (Vinnytsya, Odesa, Lviv), in which specialized surgical aid and high-tech medical equipment are available.Damage control (DC) is a clinical approach in combat conditions to be applied in the US army and military forces of NATO, and in some other armies to improve the management of combat patients [, ]. In brief, a DC is a combination of DC surgery and DC resuscitation aiming to control hemorrhage and microbial contamination followed by intraperitoneal or thoracic packing and rapid closure of wounds, resuscitations to correct fluid homeostasis, to control coagulopathy in the intensive care unit (ICU) as well as application of re-exploration [, , ].The experience of medical departments of the US army and NATO is considered as modern, and its application to military personnel wounded in the hybrid war in East Ukraine is reasonable. Furthermore, DC is not frequently applied in Ukraine and little is known about consequences of MLRS application to military personnel.We report a clinical case of a combat patient who was injured after the MLRS \u201cGrad\u201d shelling, diagnosed with hydrodynamic liver rupture and treated with application of DC tactic.The patient received first medical aid immediately at the place of injury from other soldiers of his team. This included superimposed aseptic dressings and painkillers. Then, the patient was subsequently evacuated by sanitary transport to the second level of medical care to get primary surgical aid. The time taken between injury and evacuation was less than 60\u00a0min; thus, the principle of the \u201cgolden hour\u201d was achieved.To receive basic surgical aid, the patient was transported to the Military Medical Hospital #66 at Pokrovsk of the Donetsk oblast. At this level, the patient was diagnosed with having a penetrating thoracic wound. The blood loss was approximately 1500\u00a0ml, indicating hemorrhagic shock. The blood product transfusion of red was applied by using red blood cells and blood plasma.For further treatment, the patient was transported to the Mechnikov Regional Clinical Hospital in Dnipro, where further resuscitation was continued. Antibacterial therapy was administrated as well as other symptomatic therapy and daily dressings were applied.Three days after the level III, the patient was transported to the ICU at the National Military Medical Clinical Center of Ukraine in Kyiv. The general condition of the patient remained severe. Drainage from the subhepatic area showed daily bile volume up to 300\u00a0ml. Such a volume of bile was caused by the intrahepatic biliary hypertension as a result of post-traumatic edema of liver parenchyma. In order to eliminate biliary hypertension (sixth day after the combat injury), the following intervention was performed: endoscopic papillosphincterotomy, endoscopic retrograde cholecystopancreatography, and stenting of the common bile duct. To provide enteral nutrition, the feeding tube was placed behind the Treitz ligament. Implementation of endoscopic decompression of the bile duct had resulted in gradual reduction of bile volume from the subhepatic drainage from up to 50\u00a0ml per day as compared to 300\u00a0ml before the decompression.The patient was discharged from hospital in good condition on the 49th day after receiving injuries (Fig. ). The patient received 45\u00a0days of further rehabilitation according to military medical commission decision before continuing his return to military duties (V level).In this study, we reported a successful application of DC for the combat patient who was severely injured in the Donbas area in the hybrid war in East Ukraine, which is in line with other studies of larger cohorts of combat patients [, ]. To our best knowledge, this is the first report to describe a case of medical care service in the hybrid war in East Ukraine. The hydrodynamic rupture of the liver is an uncommon lesion to be diagnosed in relation to thoraco-abdominal injuries in combat patients as showed in large cohort studies [, , ]. These injuries were caused within the battlefield area of ATO by multiple-rocket launcher system \u201cGrad\u201d, resulting in mine-explosive trauma due to simultaneous effect of the several damaging factors such as an air blast wave, a gas-flame composition, as well as primary shrapnel, which is created from the covering of the explosive ammunition of MLRS \u201cGrad\u201d. Another cause of trauma was due to injury by debris to be formed as a result of the blast wave influence to such objects as bricks, gravel, concrete etc., which is also shown in soldiers from operations by the US Army [, , ]. An acoustic trauma and barotrauma also were considered because of the high-energy blast wave effect to the human body [].Furthermore, in this case report we showed that combat injuries to be received from attacks by the MLRS such as \u201cGrad\u201d demonstrate specific features as compared to mine-explosive injuries received from the mortar and tank shelling, the undermining at the tripwire mine, or the hand grenade explosion. The mechanism of combat injury due to mine-explosion is associated with direct action of the air blast wave, which lead to significant anatomical destruction of the body\u2019s parts (usually limbs) as showed in previously published studies of conflicts in Iraq, Afghanistan, and Chechnya [\u2013]. However, in hybrid war in the Donbas, Ukrainian military personnel are injured by rocket shelling from MLRS \u201cGrad\u201d and \u201cSmerch\u201d, which are associated with the impact of high-energy damaging factors as compared to other types of lethal weapons, which is demonstrated in our case of patient with hydrodynamic rupture of the liver. It is worth to mention that frequent application of MLRS \u201cGrad\u201d by pro-Russian separatists against Ukrainian soldiers is one the specific features of the hybrid war in East Ukraine. Furthermore, high-energy effect of the MLRS to military personnel or civilians resulted in changes of the structure in combat trauma as compared to military operations for instance in Iraq, Afghanistan, and Chechnya [\u2013]. In the battlefield of East Ukraine, we observe a lower number of traumatic limb amputations, but higher frequency of massive traumatic disruptions of various body parts, which are lethal in majority of cases. Still, lives could be saved after MLRS application, which is demonstrated by the present case.It is worth to mention that high-energy shrapnel is associated with additional explosive and hydrodynamic effects because of their high kinetic potential, which is more typical for bullet gunshot injuries with bullet speeds above 150\u00a0m/s. Moreover, hydrodynamic effect is associated with ballistic pressure waves, occurring while high-energy fragments pass through the human body. The hydrodynamic effect is strong, causing significant anatomical destruction of tissues and organs outside of the trajectory of the formed wound channel, and could result even in bone fractures, which is described as hydrostatic shock by Courtney et al. []. The hydrodynamic effect is also described in the present study of the combat patient with a severe injury including hydrodynamic liver rupture. In our case, the hydrodynamic effect occurred when the shrapnel (or caused by shrapnel, the side shock waves) with high kinetic energy penetrated the liver, which is a fluid-filled organ. In our case, hydrostatic effect caused the injury of the right dome of the diaphragm and rupture of the liver S without shrapnel channel in the abdomen. Moreover, the entrance wound was detected in the region of 5\u20137 ribs between the anterior and the posterior right axillary lines, whereas an exit wound was not detected indicating unknown path of the shrapnel and possible damage of abdominal organs []. Not surprisingly, ruptures of S segments of the liver were diagnosed at laparotomy. However, shrapnel in the abdominal cavity was not identified, indicating different mechanism of the liver damage such as hydrodynamic shock [].Given the described clinical case and specific features, high-energy wounds from rocket launchers shelling, we consider that it is necessary for all patients with penetrating thorax wounds to perform both thoracocentesis and laparocentesis in order to identify and manage possible thoraco-abdominal injury. For these patients, a CT scan is highly informative; however, it is not available within the \u201cgolden hour\u201d period within ATO area in Ukraine. Similar to Kotwal et al., we followed the principle of \u201cgolden hour\u201d, aiming to reduce the time between injury and medical care to increase the rate of saved lives of combat patients and in agreement with [].For our patient, the DC tactic was applied, which is a highly effective medical approach for management of combat patients. According to DC, we delayed the resection of liver S for 16\u00a0days because of hydrodynamic ruptures, which is in line with other studies [, , , ]. According to DC, a critical status of the patient was considered and performing of surgery for patients could be fatal. By using DC, a more careful assessing of the viability of the liver ischemic areas was performed and liver resection with maximum preservation of viable liver parenchyma was possible []. Hence, out of four liver segments to be planned for resection at the first laparotomy, two liver segments were preserved because of DC application, and recovering of liver viability through the collateral circulation during 16\u00a0days after injury was achieved. This report is a demonstration that patients with severe liver injuries associated with critical health condition after MLRS shelling should undergo DC tactic in condition of hybrid war.In the presented clinical case, we also showed that suturing of the liver ruptures with omentopexy was not effective and resulted in partial failure of hepatic sutures and bile leakage. Such complication could be avoided if appropriate conditions for adequate drainage of the place of potential sutures failure was considered at the first place to prevent the influence of the bile to injured organs. Furthermore, perihepatic packing could also be considered along with the DC approach []. In the current case, the bile outflowing first was noticed from the drainage from the pleural cavity whereas the drainage from subhepatic space was not functional, probably due to the location of the wounds within diaphragmatic surface of the liver, which were directly adjoined to the place of diaphragm rupture. Considering the severe condition of the patient and high operative risk of reoperation, we performed the endoscopic decompression of the bile ducts and to postpone resection of liver, which is in agreement with other studies applying DC control [, , , ]. Unfortunately, we were not able to fully use this time for adequate preparation of the patient to the liver resection because of the bleeding from the erosive defect of the right hepatic artery to be diagnosed at the tenth day after the endoscopic decompression. Thus, endoscopic transpapillary decompression of the biliary tracts was effective in condition of the suture failures.To summarize, we demonstrated the case of a combat patient with severe penetrating wounds after MLRS shelling from the hybrid war in East Ukraine. To our best knowledge, this is the first report from the Donbas battlefield describing a successful application of DC approach. From this case, we hypothesized that application of DC tactic at all levels of combat medical care could save more lives. Effect of high-energy from MLRS \u201cGrad\u201d could result in liver damage due to hydrodynamic impact of the shelling."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0366-3", "title": "Lateral lymph node metastasis in a patient with T1 upper rectal cancer treated by lateral lymph node dissection: a case report and brief literature review", "authors": ["Hiroyuki Tanishima", "Masamichi Kimura", "Toshiji Tominaga", "Shinji Iwakura", "Yoshihiko Hoshida", "Tetsuya Horiuchi"], "publication": "Surgical Case Reports", "publication_date": "23 August 2017", "abstract": "Lateral lymph node (LLN) metastasis may occur in patients with advanced rectal cancers of which the lower margins are located at or below the peritoneal reflection. However, LLN metastasis from a T1 rectal cancer is rare. Here, we report a case of LLN metastasis from a T1 upper rectal cancer that was successfully treated by sequential LLN dissection.", "full_text": "The standard treatment for T1 rectal cancer is total mesorectal excision (TME) without preoperative chemoradiotherapy in Western countries and TME without lateral lymph node (LLN) dissection in Japan [\u2013]. LLN metastases are detected in approximately 15% of patients with rectal cancer; however, LLN metastases from T1 rectal cancers are rare [\u2013]. Here, we report a case of LLN metastasis from a T1 upper rectal cancer that was successfully treated by sequential LLN dissection.LLN metastasis can occur in patients with T1 upper rectal cancer and no risk factors for lymph node metastasis, and sequential LLN dissection is useful for the treatment of this condition. Although LLN metastasis occurs very rarely in this patient population, a careful perioperative evaluation of the LLN should be performed. In cases involving LLN metastasis, LLN dissection could be considered a therapeutic option if performed curatively."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0370-7", "title": "Intestinal perforation after nivolumab immunotherapy for a malignant melanoma: a case report", "authors": ["Koji Yasuda", "Toshiaki Tanaka", "Soichiro Ishihara", "Kensuke Otani", "Takeshi Nishikawa", "Tomomichi Kiyomatsu", "Kazushige Kawai", "Keisuke Hata", "Hiroaki Nozawa", "Yuri Masui", "Yukako Shintani", "Toshiaki Watanabe"], "publication": "Surgical Case Reports", "publication_date": "25 August 2017", "abstract": "Nivolumab is a monoclonal antibody against programmed death 1 and has become a standard treatment of advanced melanoma because of its durable response and survival benefits. In this report, we present a case of severe intestinal perforation after nivolumab immunotherapy for malignant melanoma.", "full_text": "Immune checkpoint blockade is a new therapy that uses cytotoxic T-lymphocyte antigen 4 (CTLA-4) and programmed death 1 (PD-1) inhibitors to treat melanoma and squamous cell lung cancer. In the last decade, studies have clarified the integral role of the immune regulatory pathway involving PD-1 (a receptor expressed in activated T and B cells) and PD-1 ligands (PD-L1 and PD-L2) in the downregulation of antitumor immunity. Thus, inhibition of this immune regulatory pathway by using blocking monoclonal antibodies (mAbs) against PD-1 or PD-L1 is emerging as an effective therapy for achieving tumor regression in patients with advanced disease []. Nivolumab is an anti-PD-1 mAb that provides a durable response in various advanced malignancies [, ]. For example, in cases of melanoma, nivolumab provides 1- and 2-year overall survival rates of 62 and 43%, respectively []. Thus, nivolumab has recently become a standard treatment for patients with advanced melanoma [].However, the use of immune checkpoint inhibitors can lead to novel autoimmune-related adverse events, including interstitial pneumonia, colitis, vitiligo, autoimmune hepatitis, and endocrine dysfunction [, ]. In this report, we describe a case of advanced melanoma with intestinal perforation that developed shortly after the start of nivolumab therapy. The patient underwent surgical treatment for the intestinal perforation and medical management of sepsis and recovered successfully without complications. The patient provided informed consent for publishing this report.Initial evaluation revealed dehydration and tachycardia (97 beats/min), a low-grade fever, abdominal distension, tenderness on palpation, no peristalsis, rebound phenomenon, and muscle stiffness. Laboratory testing revealed the following results: white blood cells, 7100/\u03bcL; hemoglobin, 13.7\u00a0g/dL; platelets, 190,000/\u03bcL; creatinine, 0.55\u00a0mg/dL; and C-reactive protein, 0.05\u00a0mg/L. Based on these findings, he underwent emergency surgery, during which purulent free fluid, no necrosis of the colon and small intestine, and extensive adhesion of the small intestine with perforation of the small intestine were found. The perforation was found in the ileum, 240\u00a0cm away from the Treitz ligament. The patient had generalized peritonitis and underwent an emergency operation in a poor general condition; partial resection of the small intestine including the perforation area was performed. We decided not to perform intestinal anastomosis but to perform ileostomy after considering the risk of sutural insufficiency. Therefore, we performed small intestinal resection with ileostomy and cavity lavage. During the surgery, extensive adhesion caused by past surgery was found in the abdominal cavity of the patient. The intestinal wall distal from the perforation area was injured during surgery; partial resection of the small intestine including the injured area was required. We accordingly resected 50\u00a0cm of the small intestine. No surgical complications occurred during the perioperative period, although the patient was admitted to the intensive care unit because of hemodynamic instability. During the postoperative period, the patient exhibited signs of systemic inflammation and was prescribed several antibiotics, which included vancomycin hydrochloride. The patient required mechanical ventilation on the fifth postoperative day, and the patient\u2019s hemodynamic status remained stable without the need for vasopressors. Thus, we maintained the intermediate therapy. The patient had a bowel movement on the tenth postoperative day, and oral administration of water was initiated, which he tolerated well. The patient was discharged at 42\u00a0days after the surgery, with no complications, with adequate gastrointestinal function, and with the ability to tolerate an oral diet.In recent years, biological and molecular targeted drugs have been developed and implemented for the treatment of malignant melanoma. These drugs include vemurafenib (a selective BRAF V600 kinase inhibitor) [], ipilimumab (an antibody preparation that targets CTLA-4 and inhibits T-cell activation) [], and nivolumab (a mAb to PD-1) []. PD-1 is an inhibitory and co-stimulatory factor that is expressed on activated T cells and was first identified by Ishida et al. in 1992 as a protein that is upregulated during T cell apoptosis []. In this context, PD-L1 and PD-L2 are expressed in cancer cells, where they bind to PD-1 and deliver inhibitory signals to T cells. Thus, nivolumab is a novel immune checkpoint inhibitor that exerts its anti-tumor effect by inducing PD-L1/PD-L2 binding and maintaining T cell function []. Clinical studies have confirmed that nivolumab is effective for treating malignant melanoma [, ], as it provides a 1-year survival rate of 72.9%, compared with 42.1% among patients who received dacarbazine monotherapy []. Currently, nivolumab therapy is indicated for patients who are refractory to conventional chemotherapy, which includes dacarbazine, and is typically administered intravenously at a dose of 2\u00a0mg/kg every 3\u00a0weeks. Treatment time with nivolumab is short because it does not require premedication to prevent the occurrence of nausea, and outpatient treatment is available, as no serious cytopenia occurs after treatment.Nivolumab therapy has potential efficacy for malignant melanoma refractory to existing pharmaceutical therapies, such as in the present case. Moreover, nivolumab therapy could improve overall survival and quality of life among patients with malignant melanoma, and is expected to become a central or combination therapy for patients with unresectable malignant melanoma. However, nivolumab therapy can cause serious side effects (e.g., intestinal perforation in the present case), and patients who receive nivolumab therapy should be monitored for the onset of possible side effects. Nevertheless, the accumulation of additional cases or large-scale studies are needed to validate our findings.We treated a patient with a malignant melanoma, who subsequently developed intestinal perforation due to nivolumab therapy."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0359-2", "title": "Usefulness of three-dimensional image navigation system for evaluation of hepatic artery before living donor liver transplantation: a case report", "authors": ["Michinori Matsumoto", "Shigeki Wakiyama", "Hiroaki Shiba", "Yuichi Ishida", "Yoshiaki Kita", "Katsuhiko Yanaga"], "publication": "Surgical Case Reports", "publication_date": "28 July 2017", "abstract": "The evaluation of the hepatic vascular anatomy in living liver donors is increasingly being performed by three-dimensional (3D) computed tomography (CT) angiography. However, details of hepatic artery anatomy obtained by 3D CT angiography are not always superior to those obtained by angiography. Here, we report a case in which the 3D image navigation system helped to detect segment II, III, and IV arteries (A2, A3, and A4, respectively) that individually originated from the proper hepatic artery (PHA); this could not be detected by 3D CT angiography.", "full_text": "The preoperative evaluation of donors for living donor liver transplantation (LDLT) aims to select a suitable donor with optimal graft quality and to ensure donor safety. Owing to extensive research on and rapid technical development of computed tomography (CT) scanners and three-dimensional (3D) workstations [], the evaluation of hepatic vascular anatomy in living liver donors is increasingly being performed by 3D CT angiography. However, details of hepatic artery anatomy obtained by the 3D CT angiography are not always superior to those obtained by angiography []. On the other hand, 3D image navigation software systems enable not only the calculation of total liver volume and the volume of each vessel\u2019s (both the portal and hepatic venous branches) territory but also the visualization of the anatomy of the donor\u2019s hepatic vessels [], which facilitates preoperative surgical planning. However, to our knowledge, the usefulness of these 3D image navigation software systems for evaluating the anatomy of the hepatic segmental arteries has not yet been reported.Here, we describe a case in which the 3D image navigation system could detect the segment II, III, and IV arteries (A2, A3, and A4) that individually originated from the proper hepatic artery (PHA); this phenomenon could not be detected by 3D CT angiography.A 46-year-old Japanese man with end-stage primary biliary cirrhosis was admitted to our hospital for evaluation as a candidate for LDLT. On admission, the model for end-stage liver disease (MELD) score was 20, and the updated Mayo risk score was 11.3. The patient\u2019s younger sister, aged 43\u00a0years, was the only living donor candidate. Her height and body weight were 153\u00a0cm and 57\u00a0kg, respectively.Multidetector row CT (Siemens Somatom Definition Flash, Siemens Healthcare Japan, Tokyo, Japan) was employed for preoperative dynamic CT. As a contrast material, iopamidol (Iopamiron 370, Bayer, Tokyo, Japan) with an iodine concentration of 370\u00a0mg I/mL was intravenously administered (600\u00a0mg I/kg, maximum dose of 150\u00a0mL) over 30\u00a0s. Images in the arterial phase were obtained by the bolus tracking technique, and the scanning began 15\u00a0s after the trigger threshold, which was set at 80\u00a0HU above the aortic baseline CT number, was reached.In conclusion, the reconstruction of hepatic vasculature using the region-growing method with 3D CT software might be a useful adjunct for surgical planning in the evaluation of the hepatic arteries in live liver donors."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0372-5", "title": "Successful conversion surgery for gastric cancer with multiple liver metastases treated after S-1 plus cisplatin combination chemotherapy: a case report", "authors": ["Masashi Tsunematsu", "Naoto Takahashi", "Keishiro Murakami", "Takeyuki Misawa", "Tadashi Akiba", "Katsuhiko Yanaga"], "publication": "Surgical Case Reports", "publication_date": "29 August 2017", "abstract": "Gastric cancer with multiple liver metastases have poor prognosis. Recently, stage IV gastric cancer patients who respond well to systemic chemotherapy can be treated by gastrectomy. We herein report a case of advanced gastric cancer with liver metastases who was successfully downstaged by systemic chemotherapy and underwent conversion surgery.", "full_text": "Systemic chemotherapy is the standard treatment for stage IV gastric cancer (GC) []. During the last decade, several new agents with promising activity against GC have been identified, including S-1, docetaxel, oxaliplatin, and irinotecan []. In Japan, S-1 plus cisplatin is currently recognized as a standard treatment for unresectable and metastatic GC with an overall survival (OS) of 13\u00a0months in the SPIRITS trial [].In the field of colorectal cancer, resection is now actively performed following chemotherapy, particularly in cases of liver metastasis []. Conversion surgery is currently recognized as a significant factor for improving life expectancy in cases of advanced and recurrent colorectal cancer.Conversion surgery for GC is a new issue. It is defined as a surgical treatment aiming at an R0 resection after chemotherapy for tumors that were originally unresectable or marginally resectable for technical and/or oncological reasons [].We herein presented a case of conversion surgery for GC with multiple liver metastases treated by S-1 plus cisplatin combination chemotherapy.We reported a case of successful conversion surgery for GC with multiple liver metastases. Conversion surgery may improve the poor prognosis of GC, while further studies and careful assessment are necessary to determine the optimal regimen, as well as the number of courses that will be the ultimate treatment for each case."},
{"url": "https://surgicalcasereports.springeropen.com/articles/10.1186/s40792-017-0373-4", "title": "Primary colonic well-differentiated\u00a0/\u00a0dedifferentiated liposarcoma of the ascending colon: a case report", "authors": ["Hiroshi Sawayama", "Naoya Yoshida", "Yuji Miyamoto", "Tomoyuki Uchihara", "Tasuku Toihata", "Taisuke Yagi", "Yukiharu Hiyoshi", "Masaaki Iwatsuki", "Yoshifumi Baba", "Hideo Baba"], "publication": "Surgical Case Reports", "publication_date": "30 August 2017", "abstract": "Primary colonic and dedifferentiated liposarcomas are both remarkably rare. This work describes a case of primary colonic well-differentiated/dedifferentiated liposarcoma and reviews the clinical characteristics and current therapies for liposarcoma tumors.", "full_text": "Soft tissue sarcomas arise from skeletal or extraskeletal connective tissue in the extremities, retroperitoneum, head and neck, and subcutaneous tissues. Liposarcoma is the single most common soft tissue sarcoma, accounting for at least 20% of all sarcomas and over 50% of retroperitoneal sarcomas []. The most recent World Health Organization classification recognizes five categories of liposarcomas: well-differentiated or atypical lipomatous tumor\u2014which includes the adipocytic, sclerosing, and inflammatory subtypes; myxoid; high-grade myxoid; pleomorphic; and dedifferentiated []. Primary colonic liposarcoma is rare, while dedifferentiated liposarcoma is even more so. This report describes the surgical resection of a primary colonic well-differentiated/dedifferentiated liposarcoma and discusses the disease characteristics and current treatments.We performed a radical resection with lymph node dissection. General anesthesia was induced with full muscle relaxation, and a skin incision was made in the abdomen. A firm tumor was confirmed in the ascending colon, but the abdominal cavity was devoid of metastatic or disseminated lesions. The ileocolic vein and artery, as well as an accessory right colic vein, were identified, ligated, and cut, and the main lymph nodes on the anterior surface of the inferior mesenteric vein from gastrocolic trunk to ileocolic vein were dissected. We resected along the fusion fascia between the mesocolon and retroperitoneum; the resultant surgical specimen completely encompassed a firm tumor.A firm mass of dedifferentiated sarcoma was completely resected. However, there were some atypical cells in the adipose tissue around the firm tumor and these were diagnosed as well differentiated liposarcoma. The well-differentiated liposarcoma component extended to the surgical margin in places; however, only normal adipose tissue was detected at most of the surgical margin. A positive surgical margin was detected only for the parts of the well differentiated sarcoma near the firm mass. No lymph node metastasis was detected. Microscopic remnant of well-differentiated liposarcoma was confirmed according to pathological findings, but post-surgical FDG-PET CT and enhanced CT could not detect the remnant tumor around the resected lesion. The patient received no postoperative therapy. We performed follow-up evaluation with enhanced computed tomography or magnetic resonance imaging every 3\u00a0months No recurrences have been observed 12\u00a0months after surgery.This report describes an extremely rare case of ascending colonic well-differentiated/dedifferentiated liposarcoma. PubMed was queried for articles published in English from January 1990 to September 2016 with the terms \u201cliposarcoma\u201d and \u201ccolon\u201d. Seventy-six articles were returned, of which 13 and 2 described primary colonic well-differentiated and dedifferentiated liposarcoma, respectively. Primary colonic liposarcoma is rare, and only a few cases of dedifferentiated liposarcoma have been reported [, ]; therefore, considerations of the characteristics and treatments of primary colonic liposarcoma generally reference those for retroperitoneal tumors.Dedifferentiated liposarcomas arise from their well-differentiated counterparts and thus exhibit cytogenetic similarities. Immunohistochemistry for CDK4, MDM2, and p16 expression aids in the differential diagnosis of well-differentiated and dedifferentiated liposarcoma from other adipocytic neoplasms []. Despite this, the two display significantly different biologic behaviors. Low-grade, well-differentiated tumors can recur as high-grade, dedifferentiated disease characterized by increased local recurrence rates and metastatic potential. For instance, retroperitoneal well-differentiated and dedifferentiated liposarcomas were associated with 3-year local recurrence rates of 31% and 83%, respectively. Approximately 20\u201330% of dedifferentiated liposarcomas result in distant recurrence, as opposed to only 3% of well-differentiated tumors\u2014of which all belonged to the sclerosing subtype. Moreover, dedifferentiated liposarcoma is associated with a 4\u20136-fold increased risk of death when compared with well-differentiated cases [, ].In this case, the dedifferentiated liposarcoma formed a firm mass that was easily identified; however, the well-differentiated sections were soft and difficult to distinguish from the fat tissue surrounding the ascending colon during the operation. We therefore recommend that when dedifferentiated liposarcomas are resected, an adequate margin of soft fatty tissue around the solid tumor also be resected to ensure that no well-differentiated liposarcoma remains. Given that it is difficult to diagnose dedifferentiated liposarcoma preoperatively, submucosal tumors suspected of being high-grade sarcomas need to be resected with adequate margins that include soft tissue around the firm tumor.Positive margins increase the risk of local recurrence, their influence on overall survival is less clear. Positive microscopic margins (R1) are reportedly associated with higher distant recurrence rates and poorer prognoses than negative microscopic margins (R0) [], but other studies have failed to demonstrate this relationship [, ]. Moreover, 72% of patients with positive margins had no recurrence []. Positive margins were poor prognosis than negative margins in the high-grade tumor; however, surgical margins were not related with prognosis in the low-grade tumor [].Postoperative treatment for liposarcoma remains controversial. Adjuvant radiotherapy limits the risk of local recurrence, but no definite survival benefit has been established [] and patients often experience severe adverse effects attributed to cell death in the surrounding normal tissues\u2014primarily the small bowel []. In 1054 patients undergoing resection of primary retroperitoneal sarcoma, 276 patients underwent radiotherapy. But the radiotherapy had no significant impact on prognosis (HR, 0.95; 95% CI, 0.78\u20131.15; \u2009=\u20090.6) []. Recent study reported the survival benefits of adjuvant radiotherapy, but the postoperative radiotherapy effect does not depend on surgical margin status (HR 0\u00b793; 95% CI 0\u00b779\u20131\u00b710; \u2009=\u20090.38) []. Comparatively, adjuvant chemotherapy with a doxorubicin-based regimen ((EORTC 62771: doxorubicin, dacarbazine, cyclophosphamide and vincristine; EORTC 62931: doxorubicin and ifosfamide) is an independent favorable prognostic factor for relapse-free, but not overall, survival [, ]. In this case, low-grade, well-differentiated liposarcoma were positive but dedifferentiated liposarcoma was completely resected, and no adjuvant therapy was given based on previous studies.The following recommendations for evaluation during follow-up of retroperitoneal sarcoma have been published by the Trans-Atlantic RPS Working Group. Because the median time to recurrence of high-grade retroperitoneal sarcoma is less than 5\u00a0years after definitive treatment, the intervals between follow-up evaluations should be 3 to 6\u00a0months for 5\u00a0years. After 5\u00a0years, annual follow-up is considered adequate. After incomplete resection of retroperitoneal sarcoma patients should be followed indefinitely because the risk of recurrence does not plateau [].This report describes an extremely rare case of ascending colonic liposarcoma consisting of well-differentiated and dedifferentiated sections. Dedifferentiated liposarcomas are high-grade sarcomas; additionally, well differentiated liposarcoma was detected around the main tumor. The dedifferentiated tumor was completely resected, but the aggressive nature of dedifferentiated liposarcoma need periodic examination."},
{"url": "https://progressinorthodontics.springeropen.com/articles/10.1186/s40510-017-0183-z", "title": "Removable thermoplastic appliances modified by incisal cuts show altered biomechanical properties during tipping of a maxillary central incisor", "authors": ["Phillipp Brockmeyer", "Katharina Kramer", "Florian B\u00f6hrnsen", "Rudolf Matthias Gruber", "Sarah Batschkus", "Tina R\u00f6dig", "Wolfram Hahn"], "publication": "Progress in Orthodontics", "publication_date": "28 August 2017", "abstract": "The present study aimed to evaluate the force delivery of removable thermoplastic appliances (RTAs), modified by different sized incisal cuts, during tipping of a maxillary central incisor in palatal and vestibular direction.", "full_text": "Removable thermoplastic appliances (RTAs) can be used in patients as a less visible alternative to conventional, fixed, orthodontic treatment appliances [\u2013]. Minimal malocclusions, slight arch expansions, and corrections of a deep overbite are indication ranges []. Bodily tooth movements are restricted, as aligners primarily transfer forces produced by point contact between the appliance and the tooth [, ]. This results mainly in tipping and intrusive movements []. Forces required for tooth movement can be generated by local and whole body deformation of the aligner when it is placed onto the dental arch. This deformation results from a discrepancy between the actual and the intended tooth position incorporated into the appliance [\u2013]. The appliance manufacturing process increases material stiffness by incorporating bends and edges into the aligner. This increase in stiffness is mainly located at the incisal edge []. Therefore, it might be appropriate to attenuate the generated force component by inserting cuts into the material in this region.The measuring device used in the present investigation is comparable to the one already described previously [\u2013]. A rigid connection between measuring tooth and sensor was used. So far, it has been impossible to simulate the periodontal ligament (PDL) using a measuring device [, ]. Therefore, the measured forces must be interpreted as an immediate and transient sequence, as no significant tooth movement can be expected at this point due to the PDL\u2019s visco-elastic properties [, ].The results confirm a direct dependency between the aligner material and the magnitude of force delivery, as has been described in preliminary investigations [, \u2013]. The Fx and Fz values for the Biolon\u00ae aligners were significantly higher than those for the two other materials tested (Erkodur\u00ae and Ideal Clear\u00ae). Apart from different material properties comprising individual elastic moduli [], the manufacturing process plays a crucial role in force delivery strength. The Biolon\u00ae appliances were prepared by using 6\u00a0bar overpressures; while in the Erkodur\u00ae and Ideal Clear\u00ae appliances, a 0.8\u00a0bar under pressure was used. Thermoforming using high positive overpressure leads to more precise fitting of the appliance on the individual oral structures. Depending on the degree of better play of the appliance in the anterior region, where a tooth is tipped, complete deformation of the aligner results. Superior fit and, consequently, higher friction generate higher restoring force components. A space holder foil of 0.05\u00a0mm was additionally used for Erkodur\u00ae appliance preparation. Friction was decreased, resulting in a reduced force application compared to the two other test materials.For the first time, the present investigation demonstrated a possible inversion of the Fz component during palatal tipping as Fz values changed from a weak intrusive force for aligners without cut to an extrusive force for those with a cut of increasing size. One reason for this might be the different morphology of the vestibular (convex) and palatal (both convex and concave) surfaces of the experimental tooth, resulting in a unique interaction between inclination and shape of the interior aligner surface and the anatomy of the tooth in the contact area during force generation. The contact point between appliance and tooth, where the force is generated in uncut aligners, is located near the incisal edge at an inclined inner surface of the appliance [, ]. During tipping, this inclined surface results in a single point force application. With incisal cut, the aligner material can move more horizontally at the contact area. In this case, the force components that are generated are less at an inclined surface, which results in reduced intrusive force values (Fz). What is more, during palatal tipping, the incisal parts of the tooth slip relatively in an apical direction on the palatal aligner surface with high friction. This could explain the extrusive Fz values (Fig. ). Compared to uncut aligners, the material at the area around the contact point of appliances with a cut can be more deformed with lower resulting forces.The amount of orthodontically induced inflammatory root resorptions has been shown to be directly correlated with the force magnitude applied [\u2013]. By using removable aligners, less inflammatory root resorptions are caused, even with higher forces [], which have also been reported by Barbagallo and colleagues during premolar tipping []. Therefore, modifying the aligner by an incisal cut could reduce even more the risk of orthodontically induced inflammatory root resorptions."},
{"url": "https://prc.springeropen.com/articles/10.1186/s41155-016-0047-3", "title": "Expert panel\u2019s modification and concurrent validity of the Teacher Stress Inventory among selected secondary school teachers in Nigeria", "authors": ["Victor O. Lasebikan"], "publication": "Psicologia: Reflex\u00e3o e Cr\u00edtica", "publication_date": "18 September 2016", "abstract": "The Teacher Stress Inventory (TSI) is an instrument used for assessing occupational stress among teachers but has not been adapted or validated for use in Nigeria.", "full_text": "The impact of the teaching profession on the psychological health of teachers is of utmost public health importance all over the world (Abel and Sewell ; Baldacara et al. ; Chan ; Johnson et al. ), and the construct of \u201cteachers\u2019 stress\u201d which has been conceptualized as having both emotional and physical substrates resulting from some aspect of their work (Freudenberger ) is well recognized. Despite this, teachers\u2019 stress is largely determined by varying contextual perception and sociocultural factors (Bhagat et al. ).In the developed countries of the world, assessing teachers\u2019 stress with valid instruments has been found to be significantly helpful in order to improve their health and to build policy decisions regarding health care services. This led to the development of a teacher-specific valid instrument for the assessment of teachers\u2019 work-related stress (Fimian ).Before the development of the Teacher Stress Inventory (TSI), the phenomenon of teachers\u2019 stress was difficult to access as it was equated with burnout. Therefore, the psychometric constructs of teachers\u2019 stress were devised with the understanding that burnout is the product of long-standing stress (Fimian and Fastenau ). Thus, in an attempt to specifically assess \u201cdifferent stress experiences\u201d that are particular to the teaching profession, the TSI was developed (Fimian ). However, such instrument should be culturally and contextually applicable in Nigeria, requiring its modification and comparison with the original version in order to achieve this. The original version of the TSI is a psychometrically valid and reliable self-administered instrument used in the measurement of teacher stress developed by Fimian in 1984 for the assessment of occupational stress in teachers (Fimian ). The TSI has been widely used in North America, where it was developed and also in Africa (Atindanbila ; Paulse ). This 49-item instrument assesses issues relating to time management, work-related stressors, discipline and motivation, professional investment, emotional manifestations, fatigue manifestations, cardiovascular manifestations, gastronomical manifestations, and behavioral manifestations. The original version of the TSI has ten factors, of which the item content was based on the experiences of public school teachers working in both regular and special education schools. When the psychometric property of the TSI was determined in South Africa, items 1 (I easily over-commit myself), 3 (I have to try doing more than one thing at a time), and 6 (I feel uncomfortable wasting time) were omitted (Boshoff ).In the Nigerian context, given differences in ethnicity, religion, demographics, political, organizational, and social support systems compared to the Western world, where the TSI was developed and adapted for use, sources and reaction to stress are expected to differ significantly. Even within Nigeria, there is a wide cultural variation of a stressor that will be significant enough to constitute a stress reaction (Sheikh et al. ). Thus, instruments that will be applicable to assess teachers\u2019 stress in Nigeria must consider these contextual factors. Specifically, among teachers in Nigeria, lack of resources for teaching, dilapidated teaching infrastructures (Okebukola and Jegede ), organizational problems such as high student-pupil ratio, high work load (Akpochafo ), delay in wage payment spanning almost a year in some instances, delay or absence of promotion, and lack of teaching materials (Duyilemi ) are some of the factors that have been identified as being stressful in their industry. From previous studies in Nigeria, an identified gap is the lack of standardized and valid instruments in the assessment of teachers\u2019 stress (Adeoye and Okonkwo ; Olaitan et al. ).Thus, our main aim was to modify the original version of the TSI within the Nigerian context using an expert panel on the subject matter and to also pilot test the modified version among selected secondary school teachers. In the current study, the original and the modified versions of the TSI were compared for correlation.This study is an important contribution in the assessment of Nigerian teachers\u2019 stress. It is also a major contribution to the modification and validity of an instrument that is applicable to a specific population. This study supports the reliability and validity of the Nigerian version of the TSI. The results show that construct validity, internal consistency, and concurrent validity of the modified version of TSI along with its corresponding subscales were generally supported by this representative sample of Nigerian teachers. Findings from the present report support that stress sources and stress manifestations are the two factors of the TSI. The internal reliability analysis of the TSI and its ten subscales showed satisfactory alpha coefficients, within the range of other studies (Kourmousi et al. ) and the original version of the TSI (Fimian ). It was previously noted that a modest Cronbach\u2019s coefficient of 0.7 could be regarded desirable during scale validation (Nunnally and Bernstein ). Figures that were above 0.7 were reported in all the ten factors of the modified version. Also, all the ten factors of the modified version of the TSI showed significant correlations. This was achieved by a rigorous and meticulous process of adaptation of the instrument during the pilot study.The concurrent validity analysis indicated that the modified TSI was extremely correlated with the original version and all the correlation coefficient of the factors were 0.95 and above. Beyond these, higher TSI scores were generally reported for the stress sources rather than stress manifestations. This is consistent with findings from other studies which showed stress sources to be most frequently reported compared to stress manifestations (Kourmousi et al. ), probably because sources of stress may be easier to identify than its common manifestations. Also consistent with previous studies was the observation that emotional (Olivier and Venter ) and fatigue manifestations (Tsai et al. ) were the most common stress manifestations reported in the present study.It this study, respondents who were married were found to have higher stress level, which could be adduced to additional responsibility of being married. An interesting finding is the differential level of stress between public and private sector teachers, with teachers in the public sector having a lower level of stress despite the highly deplorable structural and functional factors that are potential sources of stress for these teachers, which may be suggestive of cognitive dissonance (Festinger ) among them.The observed high level of stress among teachers of science subjects, followed by those teaching arts subjects, and also among those teaching combined senior and junior classes has corroborated documented reports that it is often easy to identify sources of stresses among teachers (Kourmousi et al. ).All of the abovementioned findings support the evidence that the modified version of the TSI can discriminate groups with high stress levels in this sample of Nigerian teachers\u2019 population.In the present study, standardized procedures, including modification, deletion, and inclusion of some questions, were adopted. Using focus group discussion and the use of an expert panel, the face and the content validity of the modified TSI were established. The findings from the pilot test conducted before the present study revealed that the TSI wordings of the instrument were well understood by the teachers and the instrument had acceptable administration time.The expert panel members agree on the following issues: question 8 which states \u201cI rush in my speech\u201d was modified to \u201cI rush in my speech or find it difficult to communicate fluently in English when under pressure\u201d because not all teachers are fluent in English when under pressure, making some teachers converse in the native language under such circumstances. Question 14 which states \u201cthere is too much administrative paperwork in my job\u201d was deleted because it may be included in question 10. Question 14 was then substituted with \u201cthe school has structural and functional problems.\u201d This is because a number of issues create stress for teachers in the state where the study was carried out. Many schools are dilapidated, and students on some occasions bring their own tables and chairs to school everyday. Question 16 which states \u201cI am not progressing on my job as rapidly as I would like\u201d was deleted as it duplicates question 15. It is replaced by \u201cI have frequent teacher-teacher or teacher- superior authority conflict, for example, with the ministry or board of directors.\u201d This took into consideration interpersonal and organizational factors that lead to stress. Question 18 which states \u201cI receive an inadequate salary for the work I do\u201d was modified to \u201cI receive an inadequate or late wages for the work I do.\u201d This is because late payment of salary is very frequent and constitutes significant stress to the teachers in this environment. Question 46 which states \u201cI respond to stress by using over-the counter drugs\u201d was modified to \u201cI respond to stress by using over-the counter drugs or prescription medication.\u201d This is because several over-the-counter drugs are prescription medication within this population. Question 47 which reads \u201cI respond to stress by using prescription drugs\u201d was modified to \u201cI respond to stress by using illegal drug.\u201d This was to incorporate the use of illegal drugs such as cannabis in time of stress.In the current study, the original TSI was validated in order to identify and resolve the peculiar stress-related problems of Nigerian teachers. This is believed to be a crucial step in the production of context-dependent instrument that will determine or actually define what constitutes teachers\u2019 stress.The strength of this study lies in the rigorous way of validating the TSI. Although there are cultural variation in beliefs and practice in Nigeria, the inclusion of the major ethnic group during the validation process is highly invaluable.This study has certain limitations. It has not carried out a factor analysis of the modified version. This will be addressed in future reports.Also, the absence of a clinically diagnosable group with DSM-IV or ICD-10 stress pathology makes comparison difficult in order for the modified version of the TSI to show evidence of its discriminative ability.In conclusion, the present study provides evidence that the modified TSI is a reliable and valid instrument for measuring stress in Nigerian teachers. It has yielded information for some demographic and related factors of stress."},
{"url": "https://opengeospatialdata.springeropen.com/articles/10.1186/s40965-017-0029-0", "title": "Remote sensing of burned areas via PCA, Part 2: SVD-based PCA using MODIS and Landsat data", "authors": ["Nikos Alexandris", "Nikos Koutsias", "Sandeep Gupta"], "publication": "Open Geospatial Data, Software and Standards", "publication_date": "24 August 2017", "abstract": "Singular value decomposition (SVD), as an alternative solution to principal components analysis (PCA), may enhance the spectral profile of burned areas in satellite image composites.", "full_text": "In the article \u201d [], we present in-depth the concepts of PCA []; past scientific literature of PCA in remote sensing applications []; the link of PCA to burned area mapping []; the implications of centering and scaling []; and finally suggest that the uncentered-unscaled SVD-based PCA variant may further improve the spectral enhancement of burned area clusters compared to the conventional centered and EVD-based PCA.In multi-spectral imagery, burned areas build homogeneous clusters of low internal heterogeneity. Their mean spectral value is distanced from the composite\u2019s overall mean and they present lower projections, in some dimensions, in both uni- and multi-temporal composites. In the latter case, it is well noted that burned surfaces are absent in the prefire dimensions.The pre-processing options to center and scale the image composites before the matrix decomposition, can be combined in different ways []. Their application influences the transformation of the spectral properties of burned area clusters. The impact of the transformations, is most evident in some of the higher order principal components. A non-centered SVD, captures in the first component greater amounts of information around the mean value of the input composite []. This can be advantageous in isolating burned clusters in some of the higher order components. Not scaling the input data may as well allow for subtle, yet useful, transformations applied in the initial dataset to be expressed in the restructured principal components. In this article, we demonstrate numerically the theoretical concepts of spectrally enhancing remotely sensed burned areas via SVD-based PCA. We apply and discuss the performance of four SVD versions. In addition, we go through an example-based quantitative discussion on the selection of the  principal components obtained via SVD.The Landsat5 TM scenes were acquired in summer 2007 (Julian day 248, postfire) and in summer 2003 (Julian day 237, prefire). These are already pre-processed data of Level-1 and delivered as scaled digital numbers. Since we do not cross-compare data from different sensors, and burned areas feature distinct spectral profiles, no further pre-processing was performed.The selected MODIS scenes (Fig. ) cover the Peloponnese peninsula (South Greece) with a total surface of 22,068 \n                        \n                         (main land of about 21,405 \n                        \n                         incl. surrounding islands on East, South). The Landsat5 TM products (Fig. ) illustrate a region North of Athens\u2013including Mt Parnitha\u2013of about 1027 \n                        \n                        . Both areas were severely damaged by large and uncontrolled wildland fires at the end of the summer 2007.The employed methods were performed using free and open source software. Geospatial processing was performed using GRASS-GIS [], QGIS [] and FWTools []. The SVD-based PCA algorithm was applied via R\u2019s function  []. Multi-Response Permutation Procedures (MRPP) statistics were estimated using the  and  functions, part of the R-package  []. The J-M index was implemented via custom R functions.We discuss hereafter the results of the transformations and their impact on spatial distances within and between the sampled land cover classes. In addition, we compare the performance of the four SVD-based PCA versions in terms of the spectral enhancement of burned area clusters via the Jeffries-Matusita index. Next, we evaluate the principal components visually and numerically. Regarding the latter, we thoroughly review the case of the bi-temporal MODIS data set (2a), how its variance is redistributed among the principal components. Finally, we justify the selection of the components that hold the highest separabilities.The statistical evaluation shows that centering and scaling, prior to the application of SVD, operate on the input multi-dimensional matrix generally in a non-destructive way. If performed, centering modifies the way that data clusters are intercepted by the transformed axes. Effectively projecting spectral information related to unchanged patterns in higher order components. This works rather against the spectral enhancement of burned area clusters. Scaling smooths out fine variations existing in the original data. The latter may neutralise minor to moderate\u2013but potentially useful details.Within the framework of burned area mapping, the spectral separability estimations between burned and major land cover samples, point to the uncentered-unscaled SVD-based PCA version as the most suitable one. The uncentered-scaled version is rather expectedly not useful as it appears to have random effects. The centered-unscaled and centered-scaled versions should be tested. Yet, we generally discourage the use of scaling the original data if it is important to retain fine details after the transformations.Since SVD is not optimised for class separability, centering or not centering the input data matrix, should be examined carefully. Even small improvements might be significant in further analysing the transformed data.\n                         eigenvector decomposition\n                         Distributed by the Land Processes Distributed Active Archive Center (LP DAAC), located at USGS/EROS, Sioux Falls, SD. \n                     \n                         Local Granule ID: \n                        \n                     \n                         Local Granule ID: \n                        \n                     \n                         Available from the U.S. Geological Survey, .\n                         Scene ID:\n                     \n                         Scene ID: \n                     \n                         Landsat Processing Details, \u201dUSGS - Landsat Missions,\u201d (accessed April 16, 2017)\n                         Driven by the sample size restriction in GRASS-GIS\u2019  module, an implementation of the algorithm [] to perform supervised image classification\n                         We use the term \u201cclass\u201d in place \u201cgroup\u201d as used originally in the MRPP test\n                \n                \n              \n                        \n                         can be seen as  or  which determine the direction of the principal components\n                \n                \n              \n                        \n                         represent the variance of the original data contained in the principal components"},
{"url": "https://prc.springeropen.com/articles/10.1186/s41155-016-0051-7", "title": "Burnout syndrome and abdominal adiposity among Primary Health Care nursing professionals", "authors": ["Magno Concei\u00e7\u00e3o das Merces", "Douglas de Souza e Silva", "Iracema Lua", "Daniela Sousa Oliveira", "Marcio Costa de Souza", "Argemiro D\u2019Oliveira J\u00fanior"], "publication": "Psicologia: Reflex\u00e3o e Cr\u00edtica", "publication_date": "21 November 2016", "abstract": "Accumulation of abdominal adiposity (AA) constitutes a risk factor for heart and coronary diseases and for metabolic complications. Research suggests that stress is related to adipogenesis. The burnout syndrome (BS) is linked to stress due to the chronicity of work stress. The objective of this study is to estimate the association between BS and AA in Primary Health Care (PHC) nursing practitioners.", "full_text": "Stress has as initial function adjusting homeostasis and improving the individual\u2019s ability in order to ensure their survival. The stimulus or stress agent is an element that affects the homeostatic balance of the body, and stress is the response to this stimulus. The way we deal with stress can make the difference between health and illness (Benevides-Pereira ). The body sets off a series of neurological and hormonal processes aiming to face the stress stimulus. In their genesis, systemic responses to stress are considered beneficial; however, chronic stress can cause systemic problems. The duration and intensity of stress can cause short- or long-term effects, disrupting homeostasis, even resulting in a pathological process (Smeltzer et al. ).Thus, the stress response is related to the hypothalamic-pituitary-adrenal (HPA) axis. An increase in the routine stress can result in physiological and neuroendocrine changes, which stimulate the ingestion of calorie foods and, consequently, the increase of adipogenesis (Miller et al. ; Oliveira et al. ).Stress related to work, known as occupational stress, whose stressors are linked to the work environment, is constituted by the association of various symptoms presented by the body and can trigger physical and mental diseases (Ribeiro et al.  p. 436).Linked to occupational stress is the burnout syndrome (BS). Characterized by emotional exhaustion (EE), depersonalization (DP), and low professional accomplishment (LPA), it can affect those professionals whose work requires direct contact with the public (Tironi et al. ; Sousa and Medon\u00e7a ). Burnout involves negative attitudes and behaviors with regard to labor activities, resulting in emotional and labor practice problems (Murofuse et al. )\n                     Some authors emphasize that this syndrome is inherent to the working environment, being triggered by failures to cope with chronic work stress making the professional less sensitive to environmental and labor issues (Benevides-Pereira ; Gil-Monte ; Limongi-Fran\u00e7a and Rodrigues ). Thus, chronic workplace stress provides its emergence (Bj\u00f6rntorp ).Stressful work situations stimulate the response of the HPA axis leading to insulin resistance as a result of an excessive cortisol production (Costa et al. ). Increased cortisol levels released by the adrenal medulla, associated in turn by stimulation of the adrenocorticotropic hormone (ACTH) released by the pituitary, would be related to abdominal adiposity, as there is mobilization of lipids from the adipose tissue, and of glucose from liver glycogen, to increase the amount of available energy for stressful situations (Rosmond and Bjorntorp ).A study shows that too much occupational stress can lead the professional to develop BS and, consequently, metabolic diseases (Merces et al. ). Some other studies have shown a positive association between abdominal adiposity (AA) and cardiovascular responses to mental stress (Goldbacher et al. ; Steptoe and Wardle ; Davis et al. ; Waldstein et al. ). Among the risk factors for the development of AA and its metabolic and cardiovascular consequences, situations of occupational stress are found (Chandola et al. ; Tsigos and Chrousos ).AA accumulation constitutes a risk factor for heart and coronary diseases and for metabolic complications. It is not just an aesthetic issue but an important indicator for the workers\u2019 health (Martins and Marinho ; Ammar et al. ). Excess of AA can increase up to ten times the risk for developing type 2 diabetes and is a major risk factor for hypertension in adults (Francischi et al. ). Noteworthy is the higher health risk when compared to other forms of body fat distribution (Oliveira et al. ).In the interim, the impact of work on health is nowadays being studied and discussed. The health conditions of health workers have emerged as a commonly discussed problem, taking into account the complexity of the demands of the health sector. In this scenario, the nursing professionals who make up a large contingent of this labor force are often overwhelmed by the accumulation of work and professionally undervalued. In the context of Primary Health Care (PHC), little is known about the health conditions of the professionals inserted in this level of caretaking (Davis et al. ).Among the factors that contribute to the emergence of stress and hence to the BS in PHC nursing professionals, we find the complexity of interpersonal relationships, a work environment typically inserted into a community, the relationship with users, and an inadequate human and material resources planning (Merces et al. ; Ribeiro et al. ).Given the above information, this study aimed to estimate the association between burnout syndrome and abdominal adiposity in PHC nursing practitioners.We conducted a confirmatory, cross-sectional, epidemiological study with Primary Health Care nursing practitioners of nine municipalities in Bahia, Brazil, namely, Anguera, Catu, Guanambi, Ibirapitanga, Igapor\u00e3, Inhambupe, Maca\u00fabas, Pojuca, and Santa Barbara, linked to the multicenter project entitled \u201cS\u00edndrome de Burnout e S\u00edndrome Metab\u00f3lica em Profissionais de Enfermagem da Aten\u00e7\u00e3o B\u00e1sica \u00e0 Sa\u00fade do Estado da Bahia\u201d (Burnout Syndrome and Metabolic Syndrome in Primary Health Care Nursing Professionals of the State of Bahia). The total amount of family health units (FHU), the study\u2019s locus, corresponded to 73, and among the 211 nursing professionals, who have developed their working practices regularly, 195 met the inclusion criteria.It is noteworthy that 16 professionals were excluded because of the following: being on medical leave, develop purely administrative activities, pregnant women, and professionals who reported a diagnosis of depression before entering the position assumed.From the professionals included in the study, 189 (96.9%) participated and answered all the questions of the questionnaire, specifically designed for this study, containing information on sociodemographic and job conditions, lifestyle, and human biology as well as a tool to assess the presence of BS, called the Maslach Burnout Inventory (MBI) (Maslach et al ). We evaluated the internal reliability of the MBI instrument categories using the Cronbach\u2019s alpha coefficient. Alpha values above 0.70 indicate acceptable internal consistency, maintaining the reliability of the instrument, with 0.82 for EE, 0.79 for DP, and 0.81 for LPA.Data collection was conducted through interviews from January 2015 to March 2016, and all the PHC nursing professionals were invited to participate in the survey by telephone, on which occasion the scope and objectives of the survey were explained to them, emphasizing the nature of voluntary participation. Only six participants refused to participate in the study. We performed a pilot study to make the necessary adjustments in the questionnaire. Moreover, viewing to ensure a uniform application of the questionnaires, we opted for a calibration of the research\u2019s assistants by means of the application of questionnaires in ten professionals of the hospitals\u2019 field. This procedure was eligible to calculate the rate of agreement between them, using the Kappa index, finding a 0.87 value which is considered acceptable with an optimal classification (Seigel et al. ).The main independent variable of this study was the BS; for the definition of the individual with BS, we remark that the MBI is a validated instrument which was translated into Portuguese in Brazil by Benevides-Pereira (). It is composed of 22 questions that explore the three dimensions: EE (1, 2, 3, 6, 8, 13, 14, 16, and 20), DP (5, 10, 11, 15, and 22), and LPA (4, 7, 9, 12, 17, 18, 19 and 21), scored by a Likert scale of five points which is as follows: \u201c1\u201d never, \u201c2\u201d rarely, \u201c3\u201d sometimes, \u201c4\u201d frequently, and \u201c5\u201d always.From the sum of questions for each dimension, the score was obtained with the following cutoff points: EE, high (\u226527 points), medium (19\u201326 points), and low (<19 points); DP, high (\u226510 points), medium (6\u20139 points), and low (<6 points); and LPA, high (\u226433 points), medium (34\u201339 points), and low (\u226540 points) (Moreira et al. ). The BS was dichotomized according to the criteria by Ramirez et al. (), as present (yes) or absent (no), by considering the existence of high scores in the dimensions of EE and DP and low scores on the LPA.The dependent variable was represented by abdominal adiposity, measured by the individual\u2019s waist circumference. The most representative anthropometric verification of intra-abdominal fat, considered simpler measurement, is the measurement of abdominal circumference (AC) or waist circumference (WC) due to its high relation to the amount of visceral adipose tissue (Sociedade Brasileira de Endocrinologia e Metabologia ; Scarcela and Despres ). It is the best indicator of visceral fat mass being strongly related to cardiovascular complications (Martins and Marinho ).The WC standardization and measurement followed the standards recommended by the Universidade de S\u00e3o Paulo, Laborat\u00f3rio de Avalia\u00e7\u00e3o Nutricional de Popula\u00e7\u00f5es () and the nutrition department. Thus, the WC measurement was obtained using an ISP\u00ae inelastic tape made of fiberglass material with a capacity of 150\u00a0cm and 1-cm accuracy. The measure was taken at the midpoint of the distance between the lower edge of the rib cage and the iliac, at horizontal plane. The surveyed professionals were measured in standing position, arms at their sides, feet together, weight divided between the legs, and eyes on the horizon. We requested the professionals to elevate their shirts to measure accurately (Habicht and Butz ).The following cutoffs were used: normal WC (<88\u00a0cm for women and <102\u00a0cm for men) and high-risk WC (\u226588\u00a0cm for women and \u2265102 for men) (World Health Organization ).After data collection, the questionnaires were examined, numbered to facilitate registering, entered, and processed, using the Statistic Package for Social Sciences (SPSS). We checked for errors and inconsistencies and validated the data presenting the percentage values, the means, and the standard deviations of the variables.Subsequently, the statistical analysis of the data was carried out using SPSS 22.0 software for Windows and STATA for Windows, in order to assess the predictive conceptual model having AA as dependent variable and BS as an independent main variable. The independent variables (covariates) considered as potential effect modifiers were as follows: hypertension, diabetes mellitus, and alcohol consumption; the confounding variables were as follows: gender, education, working time at the Health Unit, physical activity, and anxiety.Initially, we carried out a descriptive analysis by means of absolute and relative frequencies of all the variables of interest, enabling to estimate prevalence from the main independent variable (BS) and the dependent variable (AA). The bivariate analysis was performed to verify the associations of the independent variables with the dependent variable. Therefore, we calculated their prevalence ratios (PR), their respective confidence intervals (CI) of 95%, and  value through Pearson\u2019s chi-square test or Fisher\u2019s exact test to assess the statistical significance of the associations.To confirm the modification of the effect, we calculated the PRs of the main association per stratum of each preselected covariate and their respective 95% CI. Verification of effect modification was done through the intuitive method (PR of a stratum should not be contained in the confidence interval of another stratum and vice versa) and the Breslow-Day homogeneity test with a  value \u22640.05. As for the confirmation of the confounding variables, we used the variation between crude PR and Mantel-Haenszel\u2019s adjusted PR, taking as a selection criterion variation \u226520%. In processing the analyses, we found no effect modifying or confounding variables.To assess the association between BS and AA, controlled by the covariates of interest in the predictive conceptual model, we used conditional logistic regression with the retrograde selection process (\u201cBack Ward\u201d), with analysis criterion value \u2009\u2264\u20090.05. To obtain the final logistic regression model, we followed the procedures described by Hosmer and Lemeshow ().It is noteworthy that the logistic regression model produced odds ratio measures (OR) which is not a suitable association measure for cross-sectional studies, and this measure overestimates associations in studies with high-prevalence outcomes (over 10%); therefore, the prevalence ratios (PR) and 95% confidence intervals were estimated using Poisson\u2019s regression with robust variance (Coutinho et al. ; Francisco et al. ).The adequacy of the final regression model was evaluated by the model\u2019s goodness of fit test (Hosmer and Lemeshow) and the area under the receiver operating characteristic (ROC) curve.The study was approved by the Ethics in Research Committee involving human beings of the State University of Bahia, in decree no. 872.365/2014. We highlight that all participants signed the consent form.BS prevalence was 10.6%. Regarding the dimensions of BS, 77 (40.7%) had average EE and 39 (20.6%) high exhaustion; 74 (39.2%) scored average DP and 60 (31.7%) high DP; 93 (49.2%) had mean LPA and 91 (48.1%) high LPA (Table\u00a0).Although we did not identify an effect modifying and confounding variables in the stratified analysis, in front of the relevance of the variables of gender, age, education, physical activity practice, alcohol consumption, hypertension, diabetes, anxiety, and working time in a PHC unit, we maintained them in the multivariate model to adjust according to the knowledge of their influence both on the exposure factor and on the outcome.Regarding the independent variable, our study found a prevalence of 10.6% for BS, especially with high EE (20.6%), DP (31.7%), and LPA (48.1%). A study with PHC professionals found a prevalence of 39.3%, indicating that about one third of primary care professionals have a high burnout level (Navarro-Gonz\u00e1lez et al. ). Another survey showed high EE (20.6%), DP (21.1%), and LPA (21.5%) (Martins et al. ), with a high prevalence of AA in the surveyed professionals (54%). Studies with nursing workers found inadequate AA in 61 and 50.72% (Silveira et al. ; Sousa et al. ).According to our knowledge, this is the first Brazilian research with PHC nursing professionals that studies the association of burnout with AA. No studies were found in the databases aiming to investigate this association. We highlight the positive association presented here, even after adjusting other variables of interest (adjusted PR 1.63; 95% CI, 1.29 to 2.06).The existence of a mechanism which mediates the relationship between AA and BS is stated. Based on the biological plausibility proposed in this study, the HPA axis would be related to burnout effects, since it is the result of prolonged exposure to chronic work stress (Miller et al. ; Oliveira et al. ). It is noteworthy that a study of chronic stress in British workers found an association between chronic occupational stress and the presence of metabolic syndrome (MS), with AA being a variable belonging to MS (Marmot and Brunner ).Stimulation of the concerned axis would be associated with adipogenesis which results in general obesity and central obesity besides appetite stimulation (Brunner and Marmot ). Fat accumulation occurs predominantly in visceral adipose tissue of the abdomen, which has more cells per mass unit. In turn, AA is a risk factor for atherosclerosis and chronic cortisolemia, increasing the risk of cardiovascular diseases (Vale ).In addition to the direct effects inherent to stimulation of the HPA axis, some authors mention the consumption of alcohol and the increased intake of foods rich in fats and sugars as indirect effects inherent to stress confrontation and relief and consequently to burnout (Kitaoka-Higashiguchi et al. ).A cross-sectional population-based study conducted in Brazil with a sample of 1054 company workers, aiming to investigate the factors associated with overweight and AA in both sexes, showed that the variables associated with AA in men were age (PR 1.02), high household income (PR 1.05), smoking (PR 1.36), hypertension (systolic blood pressure PR 1.41 and diastolic blood pressure PR 1.85), and hypertriglyceridemia (PR 2.29). In women, they were age (PR 1.02), alcohol consumption (PR 1.42), hypertriglyceridemia (PR 1.44), diastolic blood pressure (PR 1.65), and hyperglycemia (PR 1.71). The authors emphasize that the increase in body fat puts workers at possible risk of morbidity and early mortality (Ara\u00fajo et al. ).In this context, in our study, we found the following variables to be associated with AA: age (crude PR\u2009=\u20091.61; 95% CI, 1.24 to 2.10); education (crude PR\u2009=\u20091.6; 95% CI, 1.20 to 2.11); hypertension (crude PR\u2009=\u20092.02; 95% CI, 1.74 to 2.35); diabetes (crude PR\u2009=\u20091.80; 95% CI, 1.64 to 2.16); working time in a PHC unit \u22655\u00a0years (crude PR\u2009=\u20091.7; 95% CI, 1.31 to 2.24); and high EE (crude PR\u2009=\u20091.39; 95% CI, 1.05\u20131.84).In the scientific literature, we encountered a longitudinal study aimed to investigate the effects of burnout on risk factors for atherosclerotic disease in manufacturing company managers in Japan. The results showed new cases of increased WC, body mass index (BMI), and MS among the risk factors. Changes in WC, body weight, and BMI were significantly higher in professionals with burnout. The chances\u2019 ratio for the burnout group was 2.80 for hypercholesterolemia which was statistically significant after adjustment for age (Kitaoka-Higashiguchi et al. ).Significant differences between the group of healthy professionals and the BS group in relation with WC were found. While the healthy group had a mean estimated variation of 0.6-cm WC, the average variation in the burnout group was 2.4\u00a0cm. While the healthy group usually lost weight, the group with BS gained it. Thus, burnout resulting from prolonged exposure to chronic work stress may be associated with risk factors for atherosclerotic disease and adipogenesis (Kitaoka-Higashiguchi et al. ).Concerning the fact of not having identified statistically the presence of confounders in this study, some care was taken in the methods to reduce interference of possible variables that have an influence both on AA and BS. Thus, the covariates sex, age, education, physical activity practice, alcohol consumption, hypertension, diabetes, anxiety, and working time in a PHC unit were kept in the final analysis model.The age and working time variables remained adjusted in the final analysis model. In this context, Benevides-Pereira () emphasizes that burnout can express itself from the beginning of the employment with a possible externalization in a later stage, being an exhausting process that increases with time. Thus, Oliveira et al. () demonstrated the inadequacy in WC and BMI according to increasing age in women and men, respectively.The results of this study allow us to conclude that there is a positive association between BS and AA in the analyzed PHC nursing professionals. We found significant prevalence of burnout and AA. On the other hand, some limiting aspects may have weakened the interpretation of the findings of this study. Among them, there is the difficulty in understanding the chronological order of events or confirming causal relationships in cross-sectional studies, with the possibility of reverse causality, since the AA can act as exposure to the SB.In this conjuncture, attempting to expand the knowledge of the discussed hypothesis, studies with more robust designs should be encouraged, including intervention and cohort studies, in an effort to observe if there is a reduction of the risk of AA with BS prevention and control so that preventive measures, such as the creation of public policies for occupational health, can become more pervasive."},
{"url": "https://prc.springeropen.com/articles/10.1186/s41155-016-0052-6", "title": "Self-report personality tests and medical school selection", "authors": ["Isabel Lourinho", "Maria Am\u00e9lia Ferreira", "Milton Severo"], "publication": "Psicologia: Reflex\u00e3o e Cr\u00edtica", "publication_date": "15 December 2016", "abstract": "There has been a growing interest on the assessment of personality when selecting medical students. However, how faking may affect its usefulness has been poorly addressed. Therefore, we aimed to assess the faking effect on self-report personality tests in the selection process of graduates to a medical school.", "full_text": "Medical schools aim to select persons who besides becoming competent physicians in the future also express other competencies such as behaviour skills (Mahon et al. ). The importance of these competencies has been widespread in the medical education field. In our country, the \u201cMedical Graduate in Portugal\u201d is a document that defined 112 competences organised in five domains (knowledge, professional attitudes and behaviour, clinical skills and practical procedures, communication skills and general skills (Victorino et al. ).Although for years medical students have mainly been selected based on academic achievement, there has been a recent and growing interest in the assessment based on personal attributes with particular emphasis on personality (Ferguson et al. ; Hojat et al. ; Lumsden et al. ). Some of the available medical selection tools which seek to choose other non-academic characteristics are interviews, mini multiple interviews (MMI) and situational judgement tests (Patterson et al. ).In a historic perspective, admission to a medical school in Portugal has been dominated by young school-leavers, typically aged 18\u201319\u00a0years, and selection is based solely on their previous scholar achievement. However, since the 2007/2008 academic year, a graduate entry mode has been in force and each of the eight Portuguese medical school has its own criteria for admitting graduates. For this quota, some medical schools have a written examination followed by a MMI as selection process while others have chosen the combination of previous achievement and admission interview.Few studies that relate personality traits with the existent medical selection tools show that different selection processes call upon different personality traits (Azman et al. ; Griffin and Wilson ; Jerant et al. ; Schripsema et al. ; Schripsema et al. ). For instance, if MMI performance is associated with extraversion (Griffin and Wilson ; Jerant et al. ), it has been shown that the admitted medical students with higher top pre-university grades have higher conscientiousness scores when compared to the lottery-admitted group (Schripsema et al. ; Schripsema et al. ). Nevertheless, the research on the selected personality traits with regard to the application of direct personality assessment by self-report personality tests is practically non-existent.It has already been shown that under guidance, individuals can fake personality tests (Viswesvaran and Ones ). Faking consists of the deliberate false presentation of one\u2019s self that may be favourable (fake good) or unfavourable (fake bad) (Hayes et al. ). Various theories exist to explain the faking behaviour that can occur due to personal characteristics of an individual or as a result of contextual variables (McFarland and Ryan ; Snell et al. ; Tett and Simonet ). The majority of the faking research is cross-sectional, and participants receive instructions either to answer honestly (\u201chonest conditions\u201d) or to make a good impression or to make a specific impression of themselves (\u201cfaking conditions\u201d) (Shoss and Strube ; Tett et al. ; Topping and O\u2019Gorman ). In the endeavour to identify and avoid the faking behaviour when answering to personality tests, some strategies were devised such as the use of social desirability scales or the use of response times (Donovan et al. ; Holden and Lambert ). However, how faking may affect personality assessment usefulness in the medical selection field has been poorly addressed. With this study, we aimed to assess the faking effect on self-report personality tests in a real-life medical school selection process.This study has shown that participants faked on the personality traits of conscientiousness and neuroticism at the baseline assessment. Moreover, it was also found that participants with higher desirability levels were more honest at the follow-up conditions.We believe that these results are due to the existence of some individual and contextual components of the faking process (Tett et al. ). It is possible that our participants could have not only the natural ability to fake (Tett and Simonet ) but also lower scores under honest conditions setting on the above-mentioned personality traits, having greater opportunity to fake at the faking conditions setting (Tett and Simonet ; Tett et al. ). We cannot ignore the fact that in our country like in so many others, there are much more applicants than available places in medicine (Patterson et al. ). In particular, the FMUP has had the highest access ratings for secondary school-leavers in Portugal over the last decades. Since our participants already hold a degree, and most of them wanted to study medicine since they were younger, we are in the position to assume that motivation was very high at this selection process. In addition, our participants came from a two-stage selection process in which stage 1 was based solely on previous achievement. It has been suggested that achievement is related to  (McManus et al. ) and also that cognitive ability (higher ) may facilitate the faking behaviour as brighter students seem to better identify which traits are job-relevant and therefore they fake accordingly (Tett et al. ).However, the fact that had been already pre-selected by their previous achievement when they completed the personality measure is simultaneously a limitation of this study. As far as other study limitations, like other studies about medical student selection and personality testing, this is a single-centre study, which can lead to a selection bias because applicants usually apply to particular medical schools based on their personal preferences (Abbiati et al. ). In addition, unlike the personality measure, the social desirability scale was only administered at the follow-up assessment but we did not assess if desirability changed and if the association between social desirability and personality traits at baseline assessment was stronger or weaker when compared to the follow-up assessment. Furthermore, although important contributions to the faking research have been made with similar or even lower  (Shoss and Strube ; Robie et al. ), our small sample may have hidden results for other personality traits. Finally, our sample only comprises graduate participants whose average age is closer to the thirties whereas the high-school entrants are closer to the twenties which calls the generalisation of these findings to high-school entrants into question.Nevertheless, our study also has important strengths as it is one of the first studies on medical selection that assesses the faking effect on self-report personality tests. Also, it was carried out in a real selection process setting and not in an imaginary faking setting with manipulated faking instructions (Shoss and Strube ; Tett et al. ; Topping and O\u2019Gorman ). The most used self-report personality test was applied (Hojat et al. ), one that is already validated for the Portuguese population (Magalh\u00e3es et al. ). Moreover, it is a longitudinal study while most of the published research about faking is cross-sectional (Shoss and Strube ; Donovan et al. ).In conclusion, our study shows some evidence that the incorporation of personality self-report tests in medical student\u2019s selection it is not advisable and should be confirmed in other settings with larger samples and using different personality instruments.We agree that selection processes must be rigorous and publicly defensible (Prideaux et al. ) and that personality assessment may play an important role to the selection of medical students. But we also believe that faking it is a demanding and complex task for the combination of all the existent components (Tett et al. ).If medical schools select skilled applicants who are able to present a desirable image on personality assessment, they will be in danger of admitting someone low in the future physician-relevant traits (Tett et al. ). Moreover, they may especially get away with negative behaviours as medical students and as physicians.Research is required to evaluate the faking effect on indirect personality assessment, namely through the tools that aim to select non-academic characteristics."},
{"url": "https://prc.springeropen.com/articles/10.1186/s41155-016-0053-5", "title": "Heart disease and the stress hypothesis in the mid-twentieth century: a historical review", "authors": ["Heather L. Rogers"], "publication": "Psicologia: Reflex\u00e3o e Cr\u00edtica", "publication_date": "30 November 2016", "abstract": "In the 1920s, heart disease (a noncommunicable disease), was the new leading cause of death in the USA. Simultaneously, experimental progress in the study of stress provided scientific justification for a new type of risk factor. The objective of the present work is to examine the history of heart disease as a public health problem and the contribution of advancements in scientific knowledge about stress in the 1930s\u20131960s supporting the hypothesis of stress as one cause of disease.", "full_text": "At the end of the nineteenth century and in the early twentieth century, the prevailing paradigm in medicine was that of etiological specificity of disease. Doctors began to specialize and focus on particular organs or organ systems (Rosen ). At the same time, successful preventive measures were bringing communicable diseases under control. Thus, by the 1920s, mortality from communicable diseases was declining in significance relative to noncommunicable conditions. Public health attention was drawn towards important chronic diseases, especially heart disease, which had become a leading cause of death in the USA (National Center for Health Statistics, ). As the field of public health experienced a shift of focus in the type of disease that needed preventing, the old, single-cause risk factor paradigm based on the determination of a microbiologic agent was no longer sufficient to explain these new diseases. A new, multicause risk factor paradigm was needed. Meanwhile, experimental progress in the study of stress provided scientific justification for a new type of risk factor. The objective of the present work is to examine the history of heart disease as a public health problem and the contribution of advancements in scientific knowledge about stress in the 1950s and 1960s supporting the hypothesis of stress as one cause of disease.Today, it is clear that Selye\u2019s description of GAS was not entirely accurate. However, at the time, \u201cstress\u201d was appropriately vague and nonspecific enough to be hypothesized to cause heart disease in the white, middle class professional male population of the 1950s. Interestingly, \u201cstress\u201d is still used today as a mechanism to explain why individuals with  socio-economic status are at higher risk for heart disease in the twenty-first century.In the 1950s, numerous scientific research articles highlighted the importance of emotions on heart attack, sudden cardiac death, and congestive heart failure (e.g., Chambers and Reiser ). However, the link between thoughts/emotions and the physiological stress response would not be scientifically evidenced for approximately another 25\u00a0years. First, science had to discover that the hypothalamus, an organ that was clearly located in the brain, controlled the physiological stress response via release of corticotrophin releasing hormone (CRH) to the anterior pituitary (Fink ). John W. Mason, M.D., a psychoendocrinologist, attempted to re-conceptualize stress, even before this discovery. He suggests that stress is primarily a psychological rather than physiological phenomenon; thus, an organic response is secondary to the psychological one (Mason ). Mason () also argued against the nonspecificity concept of Selye\u2019s stress theory, suggesting experimental manipulation and clarification of stress terminology.In the late 1950s, the idea of stress was merging with psychological constructs like tension. The December 3, 1956, issue of Newsweek described Hans Selye\u2019s \u201cNew Approach to Tensions\u201d because Selye had just published his book entitled  (Selye ), in which he explained his theories about the true origins of disease in terms that the general public could understand. The Reader\u2019s Guide to Periodical Literature in \u20131953 categorized articles relating to \u201ctension,\u201d referring to psychological distress. \u201cStress\u201d was not a subheading until the \u20131955 edition. \u201cStrains and stresses\u201d (in addition to \u201ctension\u201d) classified the few early public interest articles describing stress on the body. Stress was likened to the physicist\u2019s sense of the word\u2014force or pressure on an inanimate object that can be measured up to a breaking point (Time, January 18, ). By the \u20131957 edition, \u201cstress\u201d was a category by itself in the Reader\u2019s Guide to Periodical Literature and under \u201ctension\u201d; one would also be referred to the \u201cfatigue\u201d category. In the \u20131959 edition, \u201ctension\u201d was eliminated as an individual category and readers were referred to the \u201cstress\u201d entries.In 1966, Richard Lazarus, a psychologist, published his book . His work focused on how humans interpret the environment around them. In the appraisal process, humans make automatic, often unconscious, assessments of what is happening and what it means to them, which causes emotion and may lead to the physiological stress response depending on secondary appraisals of their resources to manage the situation (Lazarus ).The consequence is that stress is all around us yet quite ambiguous. It may refer to external stimuli in the environment (e.g., the extreme cold of a windy winter day or the noise of the traffic on a city street) or it may be situational (an argument with a family member or losing one\u2019s job and having to pay the mortgage or driving and someone tries to cut you off). Stress can be positive (e.g., the birth of a new child) or negative (e.g., the death of a parent) and acute (e.g., an electric shock) or chronic (e.g., job strain). Moreover, the same term, stress, can be used to describe the mental or physical internal states that result from any of these stimuli or situations. For instance, tension, anxiety, irritation, and anger are all synonyms for stress.Further, as Mason and Lazarus proposed, individuals can vary widely in how they interpret, experience, and hence respond to a given stimulus. What is stressful to some people may be experienced by others as merely trivial, boring, or even amusing. Determining when a detrimental \u201cstress response\u201d occurs and how to uniformly measure it are two facets that make this new paradigm of research concerning stress and disease so difficult. To further complicate matters, genetic differences, earlier experiences, education, etc. are likely to also contribute to disease and there is a long period of time between exposure and disease outcome.Today, in the twenty-first century, are we any closer to an answer to the question is stress a cause of heart disease? Do we know more than we knew in the mid-twentieth century? Still today, psychology, physiology, and public health research fields are often separate areas of investigation. For over a hundred years, tension was thought to cause mental disorders, but not affect physical disease states. In , U.S. News and World Report interviewed Edward Jacobson regarding a new stress relaxation technique called progressive muscle relaxation. Any possible public health benefit that might have resulted from the use of this technique (or other psychological interventions) at the time in preventing chronic physical diseases such as heart disease was lost because the scientific evidence that fits nicely into medical paradigm was not, and some would argue still is not, available.Large scale, prospective epidemiologic studies, meta-analyses, and systematic reviews, as well as smaller scale basic science studies, have established the relationship between stress (and depressive symptoms in particular) and heart disease development and progression. For instance, a scientific statement from the American Heart Association in 2014 concluded that a preponderance of evidence supports depression after a heart attack as a risk factor for death and nonfatal cardiac events (Lichtman et al. ). Largely as a result of multidisciplinary training in cardiovascular behavioral medicine, many researchers are now equipped with a strong background in scientific methods and knowledge of advancements in human physiology, heart disease processes, and psychology. Their research examines biobehavioral mechanisms underlying this relationship. Stress, and depression specifically, is associated with traditional risk factors for heart disease such as hypertension, diabetes, and insulin resistance, as well as changes in platelet reactivity and inflammatory responses, and autonomic nervous system and hypothalamic pituitary adrenal axis dysregulation. Behavioral factors associated with stress are also risk factors for heart disease, such as smoking, heavy alcohol use, sedentary behavior, and poor adherence to medical recommendations. However, randomized controlled trials of medication and nonpharmacologic treatments for depression have not demonstrated improved survival. (See review in Wang et al. .)Thus, still today, the development of prevention and intervention strategies that will improve stress and simultaneously improve outcomes in heart disease is sorely needed. To accomplish this goal, professionals in the medical, psychological, and public health fields can no longer work separately approaching these problems from their individual paradigms. Health professionals must do more than collaborate together in working groups. More researchers in more countries need multidisciplinary training in order to advance the field and further our understanding of the link between psychosocial factors and chronic disease. Scientific and medical breakthroughs must translate into health policies and population-level prevention strategies so that, by the end of the current century, chronic diseases such as heart disease are no longer leading causes of death. Successful prevention measures helped control communicable diseases at the turn of the twentieth century. The challenge for twenty-first century health professionals is to control chronic diseases as well. Research and implementation science are crucial tools to understand how interventions that might work in one setting might transfer across socio-cultural contexts. The real question is: Are we up for the challenge?"},
{"url": "https://nanoconvergencejournal.springeropen.com/articles/10.1186/s40580-017-0099-9", "title": "Mechanical properties of paraformaldehyde-treated individual cells investigated by atomic force microscopy and scanning ion conductance microscopy", "authors": ["Seong-Oh Kim", "\u2020", "Joonhui Kim", "\u2020", "Takaharu Okajima", "Nam-Joon Cho"], "publication": "Nano Convergence", "publication_date": "20 March 2017", "abstract": "Cell fixation is an essential step to preserve cell samples for a wide range of biological assays involving histochemical and cytochemical analysis. Paraformaldehyde (PFA) has been widely used as a cross-linking fixation agent. It has been empirically recognized in a gold standard protocol that the PFA concentration for cell fixation, ", "full_text": "Understanding how cells behave at material interfaces holds wide importance for key biological applications such as cell\u2013material surface interactions [], mechanobiology [], and advanced cell analysis []. Among such applications, one of the most practical and widely methods used across the biological sciences is cell fixation, which is an essential process for histological analyses in clinical diagnosis. Typically, when cells are degraded or dehydrated, essential cell components, such as protein, membrane, and intracellular structures will also be altered or degraded []. The surface structure of these cells may also collapse and diffuse away during antibody incubation and washing steps. Cell fixation aims to maintain cells or cellular components in life-like state, preventing unexpected changes by preserving essential chemical and physical characteristics of cells for further observation. Furthermore, cell fixation provides an effective approach for immunostaining by allowing the antibodies to access intracellular structures [].Among various fixation agents for cross-linking cell membrane and cytoplasmic protein, paraformaldehyde (PFA) is one of the most widely used chemical agents for cell and tissue samples [\u2013]. PFA causes covalent cross-links between molecules, effectively gluing them together into an insoluble meshwork that alters the mechanical properties of the\u00a0cell surface. Previous studies report that the cell surface hardens after fixative treatment [\u2013]. Compared to an unfixed cell, the mechanical properties of a fixed cell are more uniform across the entire cell surface []. However, there is no systematic assessment of correlation between changes in mechanical properties of live and fixed cells. Indeed, little is known about how the mechanical properties of cells depend on the PFA concentration. Furthermore, it has been revealed that subtle adjustment in fixation conditions with, e.g., PFA condition, can have dramatic effects on the immobilization of molecules within cellular membranes []. Understanding the detailed process of cell fixation in various states from living cells to completely fixed cells provides an opportunity to optimize cell fixation protocols and to gain useful knowledge about the living cell fixation process.To address this outstanding question, we investigated the mechanical properties of cell surface structures as a function of the concentration of PFA (\n                        ) by using atomic force microscopy (AFM) and scanning ion conductance microscopy (SICM). These methods allow us to measure the elastic modulus and the surface fluctuation amplitude, respectively, of cells in both living and fixed states [\u2013]. These measurement approaches can be applied to living cells to investigate cell mechanical changes in response to the PFA concentration, \n                        . We found that both cell stiffness and cell surface fluctuation underwent a transition around \n                        \u00a0=\u00a010\u20134%, and these quantities were unchanged at a higher \n                         (>4%), indicating that the cell fixation is stabilized at \n                        \u00a0=\u00a0ca. 4%, which is consistent with the empirical concentration of cell fixation optimized in biological protocols.Herein, we have demonstrated a fundamental mechanical comparison between live cells and cells that were fixed with various concentrations of PFA. AFM and SICM measurements showed that the apparent surface fluctuation amplitude and elastic modulus of cells underwent transition when exposed to PFA concentrations between 0.1 and 4%. After complete PFA fixation, cell surface fluctuation decreased to 71% of live cell, while the Young\u2019s modulus increased by fivefold compared to that of live cells. These results provide a deeper understanding of how cells react to chemical treatment with PFA that takes into account not only the traditional chemical understanding of PFA\u2019s effect upon the cell, but now also the cell\u2019s surface-based mechanical properties that were targeted in this study. It is now apparent that PFA fixation enables the opening of distributed proteins across the cell surface, a critical process that facilitates widespread crosslinking. Cell membranes that are typically flexible and variable. But in a certain situation, such as chemical treatment, biological functions are changed, and morphological changes also occur. This is the reason why studying cell surface fluctuations are crucial for the understanding of cell function about cell dynamics. Given the general nature of these physicochemical mechanisms, we expect that similar effects of PFA treatment on the elastic modulus and membrane fluctuations would also be expected although the specific magnitudes and responses conferred upon PFA treatment might vary on an absolute scale. We have confidence in that the SPM techniques could well serve as a promising tool for quantitative studies of both fixed cells and live cells in order to further explore this exciting topic at the convergence of biology and nanotechnology."},
{"url": "https://progearthplanetsci.springeropen.com/articles/10.1186/s40645-017-0137-6", "title": "Potential impact of sea surface temperature on rainfall over the western Philippines", "authors": ["Julie Mae B. Dado", "Hiroshi G. Takahashi"], "publication": "Progress in Earth and Planetary Science", "publication_date": "15 August 2017", "abstract": "The study used a 5\u00a0km-resolution regional climate model, the Advanced Research Weather Research and Forecasting Model, to quantify the potential impact of sea surface temperature (SST) west of the Philippines on summer monsoon rainfall on the northwestern coast of the country. A set of control simulations (CTL) driven by ERA-Interim reanalysis data and the monthly National Oceanic and Atmospheric Administration Optimum Interpolation SST dataset was performed for the months of June to August of 1982\u20132012. A second set of simulations driven by climatological SST values was performed for the same period. The difference between these two simulation sets is analyzed to determine the sensitivity of rainfall to interannual variations in local SST, not remote SST, via a regional climate model. The CTL simulations represented spatial and temporal variations in rainfall well, yielding realistic climatological rainfall values with high spatial correlations with observations. The interannual correlation of monthly rainfall over the northwestern region of the Philippines was also high when compared to observations. The results showed that positive SST anomalies west of the Philippines induced positive rainfall anomalies in the northwestern Philippines via an increase in latent heat flux from the sea surface, implying that summer monsoon rainfall in the northwestern Philippines is modulated by interannual variations in SST west of the Philippines. The impact of SST on latent heat flux and rainfall were 20\u201340%, greatly exceeding the 7% approximation from the Clausius\u2013Clapeyron equation, which can be explained by the enhancement of low-level winds and a weak warming of surface air temperature over the ocean.", "full_text": "The Philippines is located in an area primarily affected by the Asian summer monsoon. Specifically, the northwestern side of the country is affected by this system, which is known locally as the southwest monsoon (SWM). The SWM is part of the western North Pacific Monsoon within the Asian-Pacific summer monsoon system (Wang and LinHo ). The subsystems within the Asian summer monsoon are known to interact with each other. The monsoon onset of regions to the west of the Philippines, specifically Indochina Peninsula and South China Sea (SCS), is associated with the development in the circulation and convective features in the tropical East Indian Ocean (Ding and Chan ). The Asian monsoon is a complex system, but the past decade have seen significant advances in understanding climate over the region through the Monsoon Asian Hydro-Atmosphere Scientific Research and Prediction Initiative (MAHASRI) project (Matsumoto et al. ).Rainfall is an important characteristic of the SWM season because these winds carry warm, moist air, which has a higher potential for convective activity. The northwestern regions of the Philippines experience a rainy season during June to September. A significant percentage (43%) of the mean annual rainfall in the Philippines is associated with the SWM season, which is essential for supplying water for agriculture, energy, and domestic use (Asuncion and Jose ; Cayanan et al. ). Although there has been a general decline in the SWM mean rainfall over northwestern Philippines (Cruz et al. ), intense rainfall episodes still occur during the SWM season, which can result in heavy flooding.The oceans surrounding the Philippines have a major impact on its regional climate. Large-scale oceanic impacts, including El Ni\u00f1o\u2013Southern Oscillation (ENSO,\u00a0Lyon and Camargo ; Villafuerte and Matsumoto ), have been examined in many studies. However, the impact of local sea surface temperature (SST) on the country is not well understood. Thus, there is a need to examine the local SST impact on regional rainfall over the Philippines.Studies have noted the relationship between SST and atmosphere through the exchange of surface fluxes (Deser et al. ). The amount of near-surface moisture for convective rainfall is sensitive to SST through the Clausius\u2013Clapeyron relationship and is strongly associated on interannual timescales over oceanic regions (Wentz and Schabel ). In general, higher SSTs correspond to increases in precipitation (Vecchi and Harrison ; Trenberth and Shea ). In addition, a recent study showed that precipitation tends to increase linearly with increasing SST over tropical monsoon basins, suggesting a noticeable impact of SST on tropical precipitation (Roxy ).In the western North Pacific (WNP) region, the correlation between local rainfall and SST in the boreal summer is negative indicating that atmospheric dynamics are more dominant than ocean conditions in determining the rainfall variability of this region (Trenberth and Shea ; Wang et al. ). This relationship may be explained by the monsoon activity during this season. The summer monsoon drives rainfall but it also leads to SST cooling, hence the negative correlation between local rainfall and SST. Moreover, Trenberth and Shea () also showed the covariability between surface temperature and precipitation, and although relationship is negative in WNP, it would be interesting to isolate the SST effect alone. Note that impact of local SST on rainfall variation in this region has not been evaluated yet, although these previous studies have indicated that SST cannot be the main driver of rainfall in this region.However, it is difficult to quantify the impact of SST on rainfall from observational datasets because of strong air\u2013sea interactions (Peralta and Narisma ). This is similar to a mid-latitude winter monsoon case over the Sea of Japan (Takahashi and Idenaga ; Takahashi et al. ), although the range in surface air temperature is quite different. Higher SSTs bring higher latent heat fluxes under the same atmospheric conditions due to higher water vapor pressures on the sea surface, which is explained by the Clausius\u2013Clapeyron equation. At the same time, the evaporation over the sea surface decreases the SST. In SWM condition, the strong monsoon westerly also increases latent heat flux which brings abundant rainfall to the northwestern Philippines. And since rainfall on the northwestern Philippines is predominantly controlled by the SWM airflow, the SST effect is seemingly hidden in all these dynamics. These dynamics are highly coupled, and while it is difficult to isolate the one-way impacts from observational data alone, numerical experiments are useful for understanding and quantifying such one-way impacts.The SWM months also coincide with high tropical cyclone (TC) activity in the WNP basin including the Philippines (Cinco et al. ; Lyon and Camargo ; Takahashi and Yasunari ). This high frequency of TC makes a substantial contribution on rainfall over the monsoon trough (Takahashi et al. ), especially in the northern island of Luzon (Cinco et al. ). It has also been shown that SWM rainfall is enhanced in the presence of TCs (Cayanan et al. ; Kubota and Wang ), and as such, this study also attempts to isolate the TC effect in the analysis of SWM rainfall.To determine the contribution of SST to rainfall in a statistically robust manner, long-term or multiple-case experiments are required, as the atmospheric response to SST anomalies can vary under different atmospheric conditions (Peng et al. ). Moreover, long-term climate simulations can be used to investigate the extent to which interannual variations in SST affect regional climate.In this study, we examined the response of rainfall associated with the SWM from SST forcing located west of the Philippines. We attempted to isolate the impact of SST alone, neglecting feedback from the atmosphere to the ocean, by applying SST as the boundary condition in a regional climate model. We focused on the effects of SST located west of the Philippines, as SWM winds pass through this area before arriving on the northwestern coast of the country. The SST\u2013rainfall relationship is important in further understanding the hydroclimate within this region which translates in its significance in economic sectors such as agriculture. The methodology is discussed in detail in the \u201c\u201d section, and an evaluation of the performance of the model and quantification of the SST impact is described in the \u201cResults\u201d section. Changes in latent heating and the lack of a tropical cyclone (TC) effect are discussed in the \u201c\u201d section, and a summary is presented in the \u201c\u201d section.In this study, we explored the potential impact of SST west of the Philippines on summer monsoon rainfall on the northwestern coast of the country. We performed two sets of simulations using the same initial and lateral boundary conditions, but with different SST conditions, from June to August for 1982\u20132012.The spatial pattern of monthly rainfall was simulated well by the CTL simulation, with a clear horizontal delineation between rainfall in the western and eastern regions of the Philippines during the summer monsoon months. The monthly rainfall values were in close agreement with the two observational datasets used.Positive SST anomalies in the WSST region corresponded to positive rainfall anomalies in the WPH region. Based on the regression analysis, rainfall in the northwestern region of the Philippines is modulated by interannual variations in SST west of the Philippines by 80~100\u00a0mm\u00a0K SST warming. This translates to 20~40% of the rainfall change in the WPH region relative to climatological values in the region.The change in rainfall exceeded the theoretical approximation of 7% due to the Clausius\u2013Clapeyron contribution, which may be attributed to the increase in the latent heat flux in the WSST region. This latent heat flux increase is associated with an increase in surface winds coupled with a weak near-surface warming. We isolated the impact of TCs and noted that this mechanism holds true for years with low or no TC effects, which are more representative of the SWM climate."},
{"url": "https://nzjforestryscience.springeropen.com/articles/10.1186/s40490-016-0074-y", "title": "European gypsy moth (", "authors": ["Fernando Castedo-Dorado", "Gorka Lago-Parra", "Mar\u00eda J. Lombardero", "Andrew M. Liebhold", "Mar\u00eda F. \u00c1lvarez-Taboada"], "publication": "New Zealand Journal of Forestry Science", "publication_date": "13 September 2016", "abstract": "Like most pines, radiata pine (", "full_text": "The gypsy moth,  L. (Lepidoptera: Erebidae), is a serious defoliator of forests throughout much of its native range. It has also colonised eastern North America and is one of the most damaging invasive pests in that region (Doane and McManus ; Tobin et al. ). This species has three recognised subspecies: the European gypsy moth,  L., and the two Asian gypsy moths,  Vnukovskij and  Motschulsky (Pogue and Schaefer ). In most populations of European gypsy moth, females are flightless, but in Asian gypsy moths, females are largely flight capable (Baranchikov ; Keena et al. ).All subspecies are polyphagous and can exploit over 300 deciduous and coniferous host species (Liebhold et al. ; Tobin and Liebhold ). Gypsy moth caterpillars feed on a wide range of host tree species, but hosts vary in their susceptibility to defoliation. According to the system used by Montgomery () and Liebhold et al. (), forest tree species can be classified as \u2018susceptible\u2019, \u2018resistant\u2019 or \u2018immune\u2019 to defoliation. Susceptible tree species are described as those that are consumed by all larval stages, and resistant species are consumed by only some larval stages or when susceptible species are not available, while immune species are rarely, if ever, consumed by any larval stage. At a stand level, potential for defoliation is closely related to the proportion of basal area comprised of susceptible species (Davidson et al. ).Most pines, including radiata pine, are considered to be resistant to defoliation since they are only consumed by late larval stages or when susceptible species are not available (Miller and Hanson ; Liebhold et al. ). Moreover, several authors have reported that early instars cannot complete development on non-deciduous conifers (Rossiter ; Strom et al. ; Tobin and Liebhold ). In mixed pine-hardwood stands of eastern North America, defoliation by gypsy moth caterpillars is largely limited to hardwood hosts, and outbreaks generally do not occur in stands in which oaks or other susceptible hosts represent less than 20\u00a0% of host basal area (Campbell and Garlo ; Davidson et al. ).Radiata pine is native to California but has been widely planted for commercial forestry elsewhere, especially in the southern hemisphere. In the Northern Iberian Peninsula, extensive planting began in the in the 1950s (Mead ). Plantations in this region, along with those in other southern European countries, are the only areas where both radiata pine and European gypsy moths overlap, since the insect is absent in the southern hemisphere and radiata pine is virtually absent from gypsy moth\u2019s invaded range in North America.European gypsy moth has previously been reported to feed on  in Europe. For example, Romanyk () and Romanyk and Rup\u00e9rez () described outbreaks during 1952\u20131953 in the provinces of Pontevedra (north-western Spain) and Asturias (northern Spain), respectively. In the latter region, Dafauce and Cuevas () reported that 250\u00a0ha of  forest were chemically treated with insecticide between 1953 and 1966. In 1991, 290\u00a0ha of a 15-year-old plantation of radiata pine located in northwestern Portugal was severely defoliated (Leite ). These reports all demonstrate that European gypsy moth may feed on radiata pine and can cause severe defoliation but they do not provide important details regarding the outbreaks. For example, they do not indicate whether early instars completed development on radiata pine nor whether stands contained a component of oak or other susceptible hosts.In a laboratory trial, Miller and Hanson () demonstrated that European gypsy moth could complete development from egg to adult by feeding solely on radiata pine foliage. However, the ability of this moth to complete development on radiata pine in field conditions has never been described. These reports, although limited, are important since they suggest that radiata pine plantations elsewhere in the world could be at risk from European gypsy moth. However, most previous concern about potential impacts of gypsy moths on radiata pine plantations have focused on the risk associated with invasions by Asian gypsy moth subspecies (Walsh ; Matsuki et al. ; Withers and Keena ; Pitt et al. ; Bi et al. ; Troncoso ).The purposes of the present report are to: (i) describe a European gypsy moth population that completed development entirely in a pure stand of radiata pine and caused extensive defoliation; and (ii) highlight the potential threat of an accidental introduction of European gypsy moth to other regions of the world where radiata pine is used in planted forests.Outbreak populations were first detected in the radiata pine plantation in 2011, peaking in 2012 and 2013 and suddenly decreasing in 2014. During the 2\u00a0years of outbreak culmination, approximately 6\u00a0ha were severely defoliated during two consecutive years (2012 and 2013), and  40\u00a0ha were severely defoliated in 2013 (Fig.\u00a0). Moderate defoliation was observed in surrounding radiata pine stands (around 15\u00a0ha).Of the 23 egg masses sampled, the mean egg mass length was 2.93\u00a0cm (S.D.\u2009=\u20090.94\u00a0cm) and mean width was 1.61\u00a0cm (S.D.\u2009=\u20090.49\u00a0cm). Applying the regression equation of Moore and Jones (), these dimensions correspond to a mean fecundity of 422 eggs per mass. Such fecundity levels are typical of outbreak populations of the European strain of the gypsy moth in North America (Campbell ).Virtually all-noticeable defoliation was limited to radiata pine. Isolated  Ait. (maritime pine) trees within radiata pine stands were not affected (Fig.\u00a0) indicating a difference in susceptibility between the two species. Defoliation was also not evident in stands of  (a priori, a preferred host) located within 3\u00a0km of the radiata pine plantation. Large numbers of egg masses (mean\u2009=\u200921.8 and 4.1; S.D.\u2009=\u200913.0 and 4.1, egg masses per tree for moderately and severely defoliated plots, respectively) were present on stems of radiata pine trees (Fig.\u00a0). Larvae were observed feeding exclusively on radiata pine needles beginning with the first instar and continuing to pupation (Fig.\u00a0). In other parts of the world, early instars are observed feeding on susceptible hosts (e.g.  spp.) and then moving to resistant hosts (e.g.  spp.) in late instars (Lance and Barbosa ). However, this was not the case here since defoliation occurred in pure radiata pine stands and susceptible hosts were not present in the understorey.Results of DNA analysis from the sampled males were similar to other populations of European gypsy moth, which ruled out the possibility of an Asian gypsy moth introduction. The monitoring of the biological cycle showed that egg hatch occurred from mid-April to the end of May and that larvae fed until the end of August. According to captures in the lure traps, flying adults were present through early July to early October, with a peak at the end of August.Tree mortality (87\u00a0%) occurred during at least one year in plots suffering severe defoliation (Fig.\u00a0, ); much lower levels of tree mortality (around 5\u00a0%) occurred in plots with moderate defoliation, where the main effect was the reduction of tree growth. In the 2014 growth season, the radial and height growth of individual trees was, on average, 45 and 40\u00a0% lower, respectively, than that observed in non-defoliated trees (Lago-Parra ). Additionally, trees that had been weakened by defoliation were observed to be attacked by borers, especially bark beetles (Coleoptera: Scolytinae), from April\u2013May of 2014. The major species involved was  (Boern.), but other secondary species present were  (Payk.),  (F.), and  (Woll.). Most of the stands experiencing severe defoliation were clear-cut in July\u2013August, 2014, to avoid expansion of bark beetle outbreaks.The occurrence of a European gypsy moth outbreak in a pure radiata pine plantation contradicts previous observations that larvae of this moth species cannot complete development in stands comprised entirely of radiata pine. These results also have implications concerning the potential for damage that European gypsy moth may inflict where radiata pine is widely utilised in plantation forestry in New Zealand, Australia and elsewhere in the world. The potential for damage caused by invading Asian strains of the gypsy moth has been recognised and biosecurity management of imported automobiles and other pathways used by these strains are currently given high priority (Walsh ; Armstrong et al. ). However, this study suggests that European strains of the gypsy moth also hold great potential for damage to commercial radiata pine plantations and that continued vigilance in biosecurity efforts targeting this strain are appropriate."},
{"url": "https://nzjforestryscience.springeropen.com/articles/10.1186/s40490-016-0079-6", "title": "A method to maximise forest profitability through optimal rotation period selection under various economic, site and silvicultural conditions", "authors": ["Tohru Nakajima", "Norihiko Shiraishi", "Hidesato Kanomata", "Mitsuo Matsumoto"], "publication": "New Zealand Journal of Forestry Science", "publication_date": "26 January 2017", "abstract": "Maximising forest profitability is important from both economic and ecological perspectives. Managers of forest areas gain utility by optimising profits, and maximising the efficiency of a forest stand is also beneficial to the natural environment. This study presents a method to estimate and visualise forestry profitability based on variables defined in previous studies. The design space included economic and forest stand factors that can affect profitability. A contribution index analysis identified factors that significantly impact profitability, and these factors were then applied to data collected from a forest area in Japan. The effects of the two primary factors, discount rate and rotation period length, on a measure of profitability, the soil expectation value, were visualised in three-dimensional space.", "full_text": "There is a very long tradition of humans managing forests to obtain timber and various other products for both personal use and sale (Westoby ). Currently, there is a global consensus that sustainable forest management should be assured for the present and future. It is important that forest management is sustainable from both economic and environmental perspectives. Historically, uncontrolled logging has had detrimental effects on valued characteristics of forest environments, such as biodiversity, carbon stocks, aesthetic appeal and amenity value (Pukkala ). However, timber production does not necessarily affect all of these characteristics negatively. Instead, the relationships between timber production and some valued environmental characteristics can be synergetic rather than divisive (Cademus et al. ), and this synergy can exist between two or more forest products and services, such as biodiversity (Probst and Crow ), bioenergy-carbon sinks (Hoel and Sletten ) and multiple-use management (Hornbeck and Swank ). Moreover, Lu et al. () provided evidence for a synergistic relationship between soil organic carbon and soil total nitrogen during timber production. On the other hand, provisioning services, which include wild-food production and timber harvesting, often cause large trade-offs with ecosystem functions such as water quality, flood control and ecotourism potential (Marianov et al. ; Millennium Ecosystem Assessment Board ). For example, the reduction of CO emissions could be facilitated by both maintaining forests\u2019 carbon stocks and sustainably producing timber as a carbon-neutral material (IPCC ; IPCC ). It has also been shown that appropriate forest management, when compared to the practice of abandoning an area after planting, can result in more varied forest types with greater biodiversity and amenity value (Boyce ).Certain management practices help to maintain both economic and environmental sustainability in planted forests used for timber production. The rotation period is especially important in silvicultural management, as it determines logging intervals, and can dramatically affect stand conditions (Bettinger et al. ). For example, changes in the rotation period can alter the age distribution in a stand, potentially skewing it towards a population of younger trees. Previous studies have defined rotation periods in various terms, such as physical, technical and financial parameters, depending on the forest management objective (Bettinger et al. ; Hiley ). Numerous studies have also shown that the physical rotation age, which is based on the life span of a tree, varies greatly between species. For example,  (D.Don) Endl. and  Bong. have physical rotation values of over 1000 and less than 100\u00a0years, respectively (Harrington ; Olson et al. ). Another definition, the technical rotation age, is based on the size of tree a particular economic market requires.The Faustmann formula proposes that a rotation period that maximises the soil expectation value (SEV) will result in a final cutting age that is economically sustainable (Faustmann ). The Pressler formula () can also be used to calculate the optimal rotation period. However, Samuelson () suggested, following a discussion about the validities of the Faustmann and Pressler formula, that the Faustmann formula has better validity than other optimal rotation period formulae. The initial formula for maximising SEV has been modified to include variables that represent environmental characteristics (Hartman ). Hyyti\u00e4inen et al. () included the optimal rotation simulation formula into their process-based forest growth model. Formulae for the optimal rotation period were originally applied only to even-aged forests but have recently been used to calculate optimal rotation periods for uneven-aged and natural forests (Chang and Gadow ). Previous studies have also investigated how economic conditions affect forest profitability; for example, Parajuli and Chang () included timber price in their sensitivity analyses. On the other hand, Halbritter and Deegen () investigated the relationship between silvicultural practices and rotation period by analysing how planting density influences the optimal rotation period. Ultimately, the optimal rotation period is site-specific, and the SEV depends on economic, stand and site conditions, as well as silvicultural practices used. A previous study showed that a few subjective conditions can change the SEV and optimal rotation period when forest management regimes aiming to maximise the SEV are applied (Davis et al. ).However, there have been few (if any) attempts to determine and visualise forest profitability as a solution space optimised by calculating SEV from characteristics of the local forest environment.Although previous studies have analysed how discount rate affects SEV (Davis et al. ), there have been few attempts to identify long-term forest management strategies through a multifactorial analysis. There is a need for a comprehensive analysis in Japan since the government has proposed that the target for annual timber production should increase from 23\u00a0million\u00a0m to approximately 50\u00a0million\u00a0m in the future. This is an achievable target as the average annual growth of Japan\u2019s forest resources is estimated to be 80\u00a0million\u00a0m (Forestry Agency ). Thus, it is important to analyse the various factors that affect stand profitability so that timber production can be increased without jeopardising the sustainable management of forest resources. The objective of the presented study was to simulate how stand, site and economic conditions, as well as final cutting silvicultural practices, affect SEV distribution in a Japanese forestry area characterised by high productivity, such as the Miyazaki prefecture.The present study used a contribution analysis to determine how stand, site and economic conditions, along with silvicultural planning, affect forest profitability. The resulting variables were then used to generate a solution space displaying SEV maxima and optimal rotation periods on both forest and stand levels. Furthermore, the distribution of SEV based on rotation periods was estimated in an actual forest area. In addition to describing the methodology and presenting the results, we also provide forest management recommendations that aim to maintain economic sustainability.It was important to select only factors for the study that had significant effects on forest profitability, defined as having an effect of at least 0.1%. The contribution index analysis showed that the key factors affecting forestry profitability, in decreasing order, were discount rate, rotation period, site index, harvesting area and stand age. The discount rate and rotation period determined approximately 90% of the total profitability. However, a stratification analysis suggested that effects of these two variables could noticeably change with changes to various stand and site factors. Discount rate has been previously shown to significantly affect profitability estimates (Davis et al. ; Richard and Puneet ), so we expected it to be one of the main factors in our study. Rotation periods determine the harvest intervals for a given stand, so variations in these periods will also affect the profitability of a forest area (Gunalay and Kula ). Site index is a measure of the productivity of a stand and thus is likely to reflect harvesting frequency. Increasing the harvesting area can also improve profitability due to the fixed costs of harvesting timber, such as labour and machinery.When the various small stands are harvested separately, moving the harvesting machines between each site will be associated with certain costs and inefficiency. These costs can be reduced, and the harvesting efficiency could improve if these small stands are aggregated into a large harvesting area. The distribution of costs over a large area rather than individual small stands could boost profitability.Stand age had the smallest effect of the five significant factors, but this variable can influence profitability due to the increase in timber volume as stand age increases. Distances from forest and strip roads did not meet the criteria for inclusion as significant factors, although they influence accessibility of harvesting areas and hence harvesting costs.An increase in stand area also increased the effects of rotation period, as shown in Fig.\u00a0. The figure illustrates that the optimal rotation period decreases as stand area increases under certain conditions (i.e. Table\u00a0, Additional file : Table S1) defined in this study. Although it is relatively difficult to get large amounts of high-quality timber (i.e. the large diameter timber) under shorter rotation periods, the large diameter timber does not have a significant price premium in the target timber market. If we considered the timber market for specific traditional buildings, such as shrines and temples, then the optimal rotation period and silvicultural system would most probably shift.This is consistent with expectations, as a larger stand area provides greater timber volume per harvesting operation because it can be harvested more efficiently per hectare than a small area, and the rotation period can be reduced while maintaining profitability.Thus, as shown in the simulations in Fig.\u00a0, small stand areas could be harvested simultaneously to increase profitability in situations where average stand area at the site is smaller than the minimal viable harvesting area (Nakajima et al. ). The relationship between site index and optimal rotation period is also shown in Fig.\u00a0. This result was expected since site index is a key determinant of tree growth rate and usually measured in terms of growth.The SEV is more sensitive to changes in the discount rate when the discount rate is low (1\u20132%) than when it is high (2\u20135%), as shown in Figs.\u00a0 and . Interestingly, the SEV is more sensitive to changes in rotation period when the discount rate is higher. The optimal rotation period increased when the discount rate increased from 3 to 5% (Fig.\u00a0). Furthermore, Fig.\u00a0 illustrates that optimal rotation period decreases as the harvesting area increases. Thus, the best rotation period for profitability depends on various economic and stand conditions, and a shorter rotation period is not ideal for all situations.The discount rate is determined by a country\u2019s economic conditions. If we regard forestry as a competitive industry, a discount rate of more than 4% may be appropriate. However, the SEV of certain stands was negative at discount rates under 5%. Hence, there are currently a limited number of profitable stands at the study site, even though it is located in one of the most productive areas for the Japanese forestry industry. In this way, the profitability of an average Japanese forestry area would be even lower than what was estimated in this study. However, as expected, when the discount rate decreased to 3%, and to 1%, the number of stands with a positive SEV increased.It is not straightforward to solely consider the effects of discount rates for the management of natural resources, as there may be strong ecological, ethical and political pressures to conserve the resources (Heal ). However, a discount rate of 1% is very low for a competitive industry, such as forestry (if the complicating aspects are neglected). In this context, stands that have a negative SEV at a discount rate of 5% should not be considered as profitable forestry areas. However, large Japanese timber mills require immense amounts of harvested timber for processing to remain profitable. Failure to provide enough raw material to meet these mills\u2019 operational demands would severely impair Japanese forestry and associated businesses. Therefore, if the mills are highly profitable, it may be worthwhile, from an economic perspective, to harvest forest stands with slightly negative SEV (via appropriate redistribution of profits) to maintain forestry product supply chains. Another complicating factor is that according to current national forest management plans, more than 30% of the total clear-cut area in Japan should no longer be maintained as planted forests (Forestry Agency ) but regenerated as natural stands, such as mixed hardwood forests. However, such stands should also be predominantly in areas with low profitability (and hence strongly negative SEV and poor fertility).The densities of the contours in Fig.\u00a0 illustrate the stability of SEV over varying rotation periods and discount rates. A higher density represents lower SEV stability. For example, the SEV is unstable between discount rates of 1 to 2%, as small changes in discount rate strongly affect profitability, but changes in the discount rate above 2% have weaker effects on SEV. This pattern has been noted in previous studies (Bettinger et al. ; Chladn\u00e1 ; Davis et al. ). The density of contours increases as harvesting area and/or site index increases, as shown in Fig.\u00a0. This suggests that the selection of an optimal rotation period is more important in large areas with high productivity than in small stands (unless numerous small stands can be harvested simultaneously, or they can be harvested simultaneously with large stands).In addition to these solution spaces, the multiple regression analysis provides further evidence for how the factors included in this study influence SEV. Generally, a larger harvesting area translates into better harvesting efficiency. In this way, it would be reasonable that as harvesting area increases, SEV improves, as was shown in our analyses. Furthermore, previous studies have shown that a higher discount rate and/or site index will decrease both the SEV and net present value of a stand (Davis et al. ; Nakajima et al. ). The results of our analyses are consistent with previous studies (Bettinger et al. ; Davis et al. ) and demonstrate that the rotation period length, as well as stand, site and socioeconomic conditions, all have the expected effects on SEV.Over the last 35\u00a0years, the price of timber has decreased, which has reduced forestry profitability and subsequently caused almost all of Japan\u2019s forest owners to become dependent on government subsidies. Previous studies have indicated that the intensity of silvicultural practices in an area, including planting, weeding, pruning, pre-commercial thinning and thinning, strongly correlates with the amount of national subsidy available (Hiroshima and Nakajima ). Based on previous research and the current forestry situation in Japan, subsidies could be considered as another variable to represent a socioeconomic condition in the calculation of forestry profitability (Nakajima et al. ).In relation to the categories of factors that affect forestry profitability defined in this study (Table\u00a0), previous studies have considered the stumpage price (Penttinen ) and planting density as an economic condition and silvicultural system, respectively. Planting density is also strongly related to future stand conditions. These variables based on the additional data collection could be included in the presented model to determine how they affect forestry profitability. Furthermore, as the simulation system proposed in this study has also been applied to Japanese carbon emission reduction systems (Nakajima et al. ), it could be used to estimate how the carbon price would affect rotation periods. A previous study (Coordes ) has shown that this kind of rotation analysis could be applied to adaptive forest management under various uncertainties. Because forest-level adaptive management is related to the aggregation of stand-level simulations within the target forest area, it would be possible to apply these results to adaptive forest management by combining the presented stand-level and forest-level analyses to a selected Japanese forest area.Various studies have shown how forest profitability is dependent on the discount rate and numerous other variables, such as the carbon tax rate (Chladn\u00e1 ). This study employed a solution space to visualise the effects of various factors on forest profitability, measured through SEV. When the results are compared with previous studies (Loisel ; Price ), our study provides additional information, such as the stability of forest profitability, which is shown by the density of contour lines in the solution spaces (Fig.\u00a0). This stability can provide additional information for forest managers and owners, helping them determine the optimal rotation period.The multiple regression model used here includes harvesting with a logging vehicle based on relatively high road density. Therefore, the regression analysis should be recalculated if estimates of forest profitability under other harvesting systems, such as skyline harvesting, are desired. However, this result suggested that forest profitability can be expressed through simplified regression models. In this way, the presented regression model has an advantage in that it can estimate forest profitability through simple calculations. Additionally, the Japanese government hopes to increase road construction to encourage harvesting with logging vehicles in the future (Forestry Agency ). Therefore, the regression analysis applied in this study, which is based on harvesting with logging vehicles, is logical for estimating forest profitability in Japan.Certain factors, such as site index, are highly insensitive to the influence of human activity. The application of appropriate fertilisers could increase the productivity of a site, but this practice is not popular in Japanese silviculture, so factors based on natural resources should remain constant. Other site condition factors, such as the harvesting area and rotation period, can be optimised quite easily. The modification of these factors does not raise costs, so it would be easy to increase profitability through, for instance, the synchronisation of harvest times and other silvicultural operations.Modifications to harvesting area and rotation age are based on the decision-making of forest owners and a harvesting operational schedule, so the modification of these factors does not raise tangible costs, such as additional harvesting equipment expenses or labour costs.If the total harvesting area in a local forestry area dramatically increases, then supply and demand effects, such as hiring a forestry crew, will increase during periods of high demand. However, even if the total harvesting area remains the same, the synchronisation of harvest times and other silvicultural operations that results from aggregating small stands into a larger area would increase harvesting effectiveness (Hansmann et al. ; Kittredge ).Although we did not consider the possibility that the intangible costs (i.e. time spent negotiating the synchronisation of harvest times and other silvicultural operations amongst owners of small stands (Kittredge )) might increase, it would be in the best interest of all forest owners to synchronise harvest times and silvicultural operations by aggregating small stands.On the other hand, factors such as the establishment of forest roads incur initial physical costs, so changes to these types of factors are not ideal from an economic viewpoint. The most feasible strategy would be to increase the harvesting area with the consensus of forest owners.The optimal rotation period may have to be recalculated when the harvest times of adjacent stands are synchronised. However, the rotation period should be based on the areas with the highest profitability, as changes in the rotation period of these areas can strongly affect overall SEV, and results of the local forest management analysis confirmed the importance of minimising changes to rotation periods in high profitability stands when the rotation period is optimised. The Japanese government has plans to increase forest road density and encourages the simultaneous harvesting of adjacent stands, as well as certain stands that are connected by a forest road (Forestry Agency ). Furthermore, according to the forest association interviews, the simultaneous harvesting of adjacent stands, even if they are owned by various private foresters, would be possible in the studied forest area.An increase in the number of stands harvested per harvesting operation, as suggested, would increase the areas in which the SEV exceeds 0, and abandoned stands would be subject to active silvicultural management. Abandoned stands usually have high densities, large dead wood contents and poor conditions for sustainable forestry (Nakajima et al. ). Thus, the simultaneous harvesting approach described above could increase the profitable stand area by increasing the area of actively managed stands.The importance of increasing the total harvesting area by increasing the harvest of stands smaller than 1\u00a0ha (following assumption 2) was confirmed through an analysis of how local-scale variations affect rotation period, as illustrated in Fig.\u00a0. Both efficiency and profitability would increase if groups of small adjacent stands were harvested simultaneously. The figure also shows that the rotation period increases when the discount rate rises, due to future returns becoming more attractive.The national government has suggested that the final cutting area should be expanded to increase national timber production. It is possible that profitable stands could be harvested on a priority basis, as profitable stands typically have shorter rotation optimal periods than less profitable stands. It has also been suggested that the harvested stands should be replanted, with a management objective of maintaining stand and site conditions that most affect forest profitability.Other studies have analysed uncertainties by including risk variables, such as biomass resources and fire hazards, in the calculations (Shettles et al. ; North Carolina Use-Value Advisory Board ). Spatial uncertainty in a large forest area has also been addressed (Wei and Murray ).This study provides a starting point for the further analysis of uncertainties related to various factors in forest management. For example, one of the main risks in forest management in Japan is wind. By combining techniques presented in this study and previous research, it would be possible to estimate, and predict, the uncertainty of forestry profitability in Japan associated with wind and other risk factors.This study presents a method to estimate and visualise forestry profitability based on variables defined in previous studies. Dimensions of the design space were constructed from previously published forestry inventory data and consisted of two stand condition factors, three site condition factors, one economic condition factor and one silvicultural planning factor. This study used previously published inventory data regarding stand age, site index and tree species. Additionally, the forestry profit simulator was used to estimate the optimal rotation period in terms of soil expectation value. The relationships between SEV and these significant factors were then graphically visualised. The significant factors identified as described above were used to estimate SEV-based profitability distributions, based on the inventory data used to construct the design space and optimal rotation periods, for the studied forest.The design space included economic and forest stand factors that can affect profitability. A contribution index analysis identified factors that significantly impact profitability, and these factors were then applied to data collected from a forest area in Japan. The effects of the two primary factors, discount rate and rotation period length, on a measure of profitability, the soil expectation value, were visualised in three-dimensional space.Changes in rotation period affected forestry profitability. However, the effect depended on stand, site and economic conditions. In scenarios characterised by relatively low site productivity index and harvesting area, which results in low profitability, rotation period changes did not have a strong effect on profitability. On the other hand, it was vital to select the optimal rotation period for high profitability areas as even a small deviation had a significant impact on profitability. Furthermore, it was shown that by synchronising the harvesting times of small, adjacent stands, the overall profitability increased through reductions in forest management costs.These results can help local forest management increase profitability through cooperation with individual forest owners. The presented method also has risk management applications, as it could be used to estimate the effects of external uncertainty variables on forest profitability."},
{"url": "https://opengeospatialdata.springeropen.com/articles/10.1186/s40965-016-0010-3", "title": "Tools to operate and manage early warning systems for natural hazards monitoring in El Salvador", "authors": ["Jacqueline Yamileth Rivera"], "publication": "Open Geospatial Data, Software and Standards", "publication_date": "9 August 2016", "abstract": "This article explores the concerns from the institutions to include information from the field into geographical information systems (GIS). This task is becoming an essential action to strengthen and speed up the response to an imminent threat. Specially creating strategies supported by national plans such as risk reduction national plans, and climate change national plans among others.", "full_text": "Natural disasters such as tsunamis, earthquakes, volcanic eruptions, etc. are produced by natural forces and induced by internal dynamic process in the earth. The ones generated by dynamic process on Earth surface can be landslides, mudslides and avalanches. In addition, another type of disaster can be generated by meteorological or hydrological phenomena such as hurricanes, tropical storms, and drought.Latin America and the Caribbean region is classified among the regions with high vulnerability to suffer natural disasters. The estimated economic lost per year due to the consequences of the disaster is 2000 million dollars per year. Based on this information, Latin-American countries are the most vulnerable on the planet, and more specific, El Salvador ranks among the ten most vulnerable countries [].The geographical characteristic of El Salvador, located in the tropical climate zone and in the Ring of Fire, makes the country to be in a constantly zone risk to natural hazards. It is characterized by intense seismic and volcanic activity generated by the subduction and the activation of both geological faults. Besides this, meteorological events affects the country year by year where due to its location hurricanes, tropical storms, seasonal rains and intense rainfall events occurs very frequently. Additionally, due to the accelerated climate change, meteorological events are becoming more frequent and destructive. One of the more destructive cases was recorded between October and November 1998: the hurricane Mitch, hitting the countries in Central America, causing several damages to lives as well as infrastructure.El Salvador has performed various actions in monitoring disasters by locating automated sensors in the field as well as upgrading the instruments continuously. The telemetry technology has started to be one of the main system for data collection.According to historical records, since 1983 a telemetric network collecting data from the field was implemented for monitoring the seismic risk in order to facilitate real-time response []. In 1998, due to the effects caused by hurricane Mitch, the central government in El Salvador installed telemetric stations with satellite transmission systems to record information about precipitation, wind, river water levels, and temperature. Nowadays, the mentioned systems are operating and expanding along the country. Meanwhile, different areas were functioning separately for more than 15\u00a0years.The Ministry of Environment and Natural Resources of El Salvador (MARN for its acronym in Spanish) defined to implement a culture for recovering the environment and reducing environmental risk. One of the institution vision is to focus on the EWS as a system implemented to alert in advance from a potential or imminent risk in order to protect life and property. There are different types of early warning systems based on the use of measuring devices in the field, some of them are with automatic sensors while others use satellite technology.The main reason to establish a monitoring center for natural hazards in El Salvador is the scientific approach application of the early warning systems to manage the telemetric station network and monitoring natural hazards in El Salvador. Since 2001, the center is implementing different technology devices, software among other element for the vigilance and to develop the most accurate forecast information, besides other activities related to the risk assessment.The Monitoring Center is composed by different areas, the Hydrological, Meteorological, Landslide, Oceanography, Volcanology, and Seismic area (Fig.\u00a0). It involves the communication with the rest of the state, local governments and other regional actors such as civil society, business community, universities and the population in general. The communication with local observers in the risk areas is an essential element in the system in order to strengthen this communication and to make it faster and effective.The information is shared with the public via different communication modes such as phone, radio telecommunication, social networks and websites. The aim is that communication reaches the entire population within a reasonable time, allowing preventive actions.Through a Risk Reduction Program in 2011 a powerful data center was established to support and improve the communication capability with the aim of achieving considerable benefits to increase the capacity, speed of storage and access to information.The center performs actions oriented to improve tasks on data collection and sharing to the public in a clear way. The main sources of data are remote sensors in the field, satellite imageries, radars, and geographical information. Big data is feeding continuously all the serves in the center. This data needs to be processed into different tasks to generate early information, forecast information, and other tasks related to assessment.The country of interest for this study was El Salvador, located in the Pacific coast of Central America (tropical climate). The geographical characteristic of El Salvador, located in the tropical climate zone and in the Ring of Fire , makes the country to be in a constantly zone risk to natural hazards. Besides this, meteorological events affects the country year by year.The research is based on the case of the Ministry of Environment and Natural Resources of El Salvador (MARN for its acronym in Spanish) that has performed various actions in monitoring disasters by locating automated sensors in the field as well as upgrading the instruments continuously. The telemetry technology has started to be one of the main system for data collection.The implementation of geographic information systems (GIS) is one of the tool to manage data. Among this, different software classified in principle as an open or proprietary source exists. For the monitoring center such a software is one of the main bases for the daily tasks that needs to be accomplished.The research explain the different tools that combine the use of open source software and proprietary software to develop applications that are easily understood by people and offers easy access and transmission in real time, tasks on data collection and sharing to the public in a clear way. The main sources of the information has been provided by developers involved in the monitoring center develop application to manage big data, and to generate product such as maps with different characteristics for different purposes (rainfall, soil moistures, landslide, geological characteristic). Additionally, lessons to learn from developed countries, taking the plans performed by the Korean government.This theoretical bases were analyzed in order to establish relationships, differences, stages, or the current position of the knowledge we have about managing EWS implemented for monitoring natural hazards. To propose an integrated and comprehensive system, sharing information in the shortest time possible, and achieving a big database available to make risk assessments would be the main goals of such a system.It has been known that for the recent year the implementation of geographic information systems (GIS) is one of the tool to manage data. Among this, different software classified in principle as an open or proprietary source exists. For the monitoring center such a software is one of the main bases for the daily tasks that needs to be accomplished. The center has started to use different tools that combine the use of open source software and proprietary software to develop applications that are easily understood by people and offers easy access and transmission in real time. As an example, online land information system is operating in a decentralized manner with updated information since 2010. The information basically includes: flood risks status, landslide risk, droughts, tsunamis, earthquakes and volcanic eruptions.Countries exposed to different natural hazards such as El Salvador have to introduce and implement certain technologies for monitoring and taking comprehensive actions. The actions are mainly reflected in the creation of strategies based on EWS implementation for natural threats monitoring. Since natural hazards are also one of the most important challenges in land development, enhancing the actions is an important issue to solve. Therefore, we discuss two case examples to elaborate on the importance of this study.The first needs that have raised in this field in El Salvador is the digitization as well as improving application systems, and standardization of the data among the different institutions.A comprehensive approach implies to establish monitoring centers to integrate different social areas, essential issues such as security, transportation, taxation, etc. Comprehensive plans involve different institutions, privates and publics. Besides, cadastral and census information can be included in different task, economical estimate of losses after a disaster, infrastructure losses, etc. Involving central and local government institutions.ASM, average soil moisture; CAFFG, Central America Flash Flood Guidance; CSS, cascading style sheets; EWS, early warning systems; FFFT, forecast flash flood threat; FFG, flash flood guidance; FMAP, forecast mean areal precipitation; GDP, gross domestic product; GEO, group on earth observations; GIS, geographical information systems; GMAP, gauge mean areal precipitation; ICT, information and communication technology; IFFT, imminent flash flood threat; MAP, mean areal precipitation; MARN, Ministry of Environment and Natural Resources in El Salvador (Ministerio de Medio Ambiente y Recursos Naturales in Spanish); PFFT, persistence flash flood threat; SAAPIS-HD, automated system for acquisition and processing of satellite images in high definition in spanih Sistema Automatizado para la Adquisici\u00f3n y Procesamiento de Imagines Satelitales; SMNM \u2013WRF, automatic numerical modeling of mesoscale system with the weather research and forecast model in Spanish of Sistema Automatizado de Modelaci\u00f3n Num\u00e9rica de Mesoescala; UNISDR, United Nations office for disaster risk reduction; WMS, web map service; WRF, weather research forecast model"},
{"url": "https://nzjforestryscience.springeropen.com/articles/10.1186/s40490-017-0095-1", "title": "Comparative studies of the response of larch and birch seedlings from two origins to water deficit", "authors": ["Runmei Gao", "Xiaodong Shi", "Jian R. Wang"], "publication": "New Zealand Journal of Forestry Science", "publication_date": "26 July 2017", "abstract": "Early developmental stages of plants are expected to be a major bottleneck to recruitment. Information on the response of seedling to anticipated water availability is urgently needed in regions where tree seedlings may experience more frequent water deficits. In this paper, we focused on the influence of water deficit on different species (larch vs. birch) and origins (xeric vs. mesic).", "full_text": "The impact of climate change on the structure and function of forest ecosystems has been observed in many places, and this knowledge is continuously expanding to cover biodiversity, distribution, growth, productivity and mortality (Allen et al. ; Lei et al. ; Kim et al. ). Forests have globally and regionally suffered from drought and heat events (Kozyr ; Anenkhonov et al. ). These include pine and oak species in Mediterranean regions (Ruiz et al., ), larch forests and pine forests of the lower forest belt in Southern Transbaikalia, Russia (Kozyr ), and coniferous forests in South Korea (Kim et al. ). The common causal factors in these examples are elevated temperatures and/or water stress, raising the possibility that the world\u2019s forests are increasingly responding to ongoing warming and drying. There is limited opportunity for tree species to adapt to changing climatic conditions due to the combination of rapid climate change and the long life-span of trees (Eilmann et al. ). Climate-induced forest mortality seems to be an emerging global phenomenon, and investigation into the physiological mechanisms through which dry and hot climatic conditions drive tree death and forest die-off represent a rapidly growing research area (Allen et al. ; Levesque et al. ; Anenkhonov et al. ). Understanding the impact of future climate change on forest species is important for forest managers to generate adaptation and mitigation strategies.The sensitivity of trees to climate change differs among species and ecozones (Zhang et al. ; Lei et al. ). Therefore, knowledge of species- and region-specific responses to climate is needed. Boreal forests have been found to be severely affected by climate change (Chenlemuge et al. ; Ashraf et al. ). Larches ( spp.) are widely distributed in cool temperate and boreal regions of the Northern Hemisphere (Hiranoa et al. ) and are some of the most sensitive tree species to climate change (Levesque et al. ; Lei et al. ; Kim et al. ). About half of the carbon that has accumulated in Eurasian forest communities is contained in larch forests (Cai et al. ). Precipitation in spring and autumn could affect growth of eastern larch ( (Du Roi) K. Koch) in Manitoba, Canada (Girardin et al. ). Future climate changes could facilitate stand growth and accelerate mortality of Changbai larch ( A. Henry) in north-eastern China (Lei et al. ). Increased drought has been found to be detrimental to larch species, such as Dahurian larch ( (Rupr.) Kuzen.) in central Siberia (Sidorova et al. ) and the lower forest belt in Southern Transbaikalia, Russia (Kozyr ), European larch ( Mill.) in Central Europe (Levesque et al. ), and Siberian larch ( Ledeb.) in north-western Khentey, Mongolia (Dulamsuren et al. ; Chenlemuge et al. ) and in the South Siberian forest-steppe landscape (Anenkhonov et al. ). Differences in the capacity of plants to withstand water deficits may exist among plants native to different climatic conditions (Levesque et al. ). Commonly, species adapted to dry environments tend to survive and grow better during drought than mesic-adapted species when grown together in common-garden experiments or in natural ecotones (Engelbrecht et al. ).Survival of seedlings is often considered the most important factor affecting recruitment, which would significantly affect future forest stand development processes and dynamics (Cai et al. ; Xiang et al. ), and significant climatic effects on tree recruitment have also been detected. Temperature conditions associated with water availability were found to be an important determining factor affecting tree recruitment in semi-natural  forests in north-eastern China (Xiang et al. ). The Siberian larch forests in Central Asia have experienced a lack of regeneration attributed to decreasing summer precipitation in the course of climate change (Dulamsuren et al. ; ). Drought-induced tree mortality appeared to be more likely for either young or old trees. Seedlings are the most critical development stage because they are more vulnerable to environmental constraints than mature trees due to their poorly developed root systems (Allen et al. ; Walck et al. ; Galiano et al. ). Therefore, information about seedling response to anticipated decrease in water availability is urgently needed, especially in regions likely to experience more frequent water deficits for the coming decades (Walck and Dixon ; Baeten et al. ; Walck et al. ).Larches are the fourth most abundant tree genus in China, accounting for 6.5% of the forest area and 6.8% of the forest volume in this country (Lei et al. ). Prince Rupprecht\u2019s larch ( var.  (Mayr) Pilg.) is one of dominant species of cold temperate coniferous forests in northern China, mainly distributed at mountainous regions with elevations of 1400\u20142800\u00a0m (Di et al. ). The species plays significant ecological roles in water conservation, ecotourism and biodiversity, and it is also relatively fast growing with good-quality timber (Li et al. ; Zhang and Meng ). Eastern larch is native to North America. It has a continuous distribution in Canada from the Yukon in the west to Newfoundland in the east and south to the northern and north-eastern USA (Berg and Chapin ). It is a commercial species, particularly for short-rotation pulpwood plantations. Prince Rupprecht\u2019s larch and eastern larch are both of ecological and commercial importance but responses of seedlings of these species to water deficits are still unknown. Populations of Prince Rupprecht\u2019s larch have experienced serious drought since the 1990s (Qian and Zhu ). Eastern larch is mainly confined to hydric sites in Canada, so is tolerant to spring flooding in wetlands (Islam and Macdonald ).Investigating multiple species and comparing how each would respond to the same soil moisture regime are beneficial to understanding how a single species responds to differences in soil moisture (Nishimura and Laroque ; Jansons et al. ). Larch species are commonly found to be mixed with birch species in natural forest (Cai et al. ; Di et al. ; Kozyr ; Anenkhonov et al. ; Xiang et al. ; Jansons et al. ; Schaedel et al. ; Hiranoa et al. ). Larch and birch species have been reported to respond differently to climate change. For example, in Southern Transbaikalia, Russia, the habitats of the subalpine belt had also become drier, particularly in the upper parts of the slopes where populations of larch ( L.) was consequently less stable and was in a declining state; while at the foot of the slopes, dwarf birch ( L.) showed an increase in height due to increased soil moisture (Kozyr ). In a study of plasticity comparisons of climate-growth relationships of seven tree species, European larch was sensitive to the temperature in the dormant period and silver birch ( Roth) appeared to be the most robust against fluctuations in weather (Jansons et al. ). In northeastern China, increases in the severity and frequency of fires due to climate change may prompt shifts from a larch-dominated forest to an increasingly birch-dominated landscape (Cai et al. ). Water crises are expected to occur as a consequence of warmer temperatures and lower levels of precipitation in northern China (Zhang et al. ), so may affect species pair of Prince Rupprecht\u2019s larch and white birch ( Sukaczev) in mountainous forests of northern China. In eastern subarctic Qu\u00e9bec, Canada, warmer temperatures since the 1990s have triggered shrub expansion of tundra dwarf birch ( Michx.) and might have hindered seedling establishment of eastern larch (Dufour-Tremblay et al. ). Also, climate change could affect the species pair of eastern larch and paper birch ( Marshall) in boreal forests of eastern Canada.In this paper, we selected Prince Rupprecht\u2019s larch and white birch of Chinese seed origin, eastern larch and paper birch of Canadian seed origin to compare their drought responses. Considering the different original habitats of the two species pairs, we hypothesised that: (i) the growth of birch seedlings is more likely to be affected by water deficiency than that of larch seedlings; and (ii) seedlings of larch and birch originating from a xeric region will grow better than those from a mesic region in response to water deficit.Seedlings showed varied responses to water deficit according to the species origins and the species traits. Birch seedlings are more sensitive to water deficit than larch. In low-watered condition, no mortality is occurred in larch seedlings, whereas survival rate of birch seedlings is reduced by 35.4%. Biomass, shoot height, root collar diameter, total leaf area, total root area, and chlorophyll concentration of two birch species were all significantly decreased. Contrary to common opinion, species pairs of xeric origins are more affected by water deficit than those of mesic origins, probably because the narrower range in mountainous regions of Prince Rupprecht\u2019s larch results in lower adaptation to water deficits, while larger distribution along coastal regions of eastern larch contributes to its higher phenotypic plasticity of variable soil conditions. Better performance of paper birch might also be a result of higher adaptation due to its larger range than white birch. That is, plasticity or adaptation of seedlings outweighs seed origins in determining their drought responses."},
{"url": "https://opengeospatialdata.springeropen.com/articles/10.1186/s40965-017-0018-3", "title": "OpenDragon: software and a programmer\u2019s toolkit for teaching remote sensing and geoinformatics", "authors": ["Kurt Rudahl", "Sally E. Goldin"], "publication": "Open Geospatial Data, Software and Standards", "publication_date": "13 March 2017", "abstract": "Continued progress in geoinformatics research and applications depends on educating new generations of theorists and practitioners. Effective education and training in turn depend on the availability of appropriate tools, including software.", "full_text": "Continued progress in geoinformatics research and applications depends on educating new generations of theorists and practitioners []. There are many open source projects that provide a framework for research in remote sensing and geoinformatics [, ] and can be potentially used for educational purposes [], but none have been specifically engineered for educational purposes. Effective education and training in turn depend on the availability of appropriate tools, including software. The OpenDragon Project was established in 2004 to help satisfy this requirement. Details of the project can be found at .The primary software currently offered by the project is OpenDragon, which is derived from the commercial Dragon/ips\u00ae package originally developed by Goldin-Rudahl Systems, Inc. []. OpenDragon is simple, portable, easy to use, and modest in its hardware requirements. Furthermore, it is fully internationalized, so that producing versions in languages other than English is a straightforward process of translating a few dozen pages of text. Currently, OpenDragon includes a variety of languages, including Thai, Traditional Chinese and Bahasa Indonesia.OpenDragon can be used as a turnkey platform to teach undergraduates and even secondary school students [] about geography, agriculture, geology, meteorology, forestry and other natural resource disciplines. In this role, it supports the education of practitioners and users of geoinformation. Due to its modular design and the developers\u2019 professional computing background OpenDragon can be used in conjunction with the Dragon Programmer\u2019s Toolkit to provide a tool for teaching about the underlying software. The combination allows students and researchers to explore new geoinformatics algorithms and data structures.This paper describes OpenDragon, its history and its capabilities. It also explains the system architecture and the organization of the Programmer\u2019s Toolkit, and includes some examples of how we have used these capabilities to support student research and to teach geoinformatics programming.OpenDragon is based on Dragon/ips, which was the first commercial remote sensing image processing software package developed for off-the-shelf personal computers. The authors began the development of Dragon/ips in the mid-nineteen eighties after spending 2\u00a0years teaching remote sensing image processing at the Asian Regional Remote Sensing Training Center (ARRSTC), at the Asian Institute of Technology in Bangkok Thailand. For laboratory exercises, the courses offered at ARRSTC used expensive, proprietary minicomputer- and mainframe-based image processing software which required special graphics display hardware. Twenty to thirty students each term had to share five workstations, which meant that students had limited opportunities for hands-on practice. Recognizing the need for affordable software for remote sensing education, the authors decided to create remote sensing software that could be used with the IBM PC and similar computers.Dragon/ips was released in 1987 by Goldin-Rudahl Systems, Inc. (GRS), who focused keeping the system cost low and the hardware requirements modest. In 2003, the authors returned to Thailand to take teaching positions at King Mongkut\u2019s University of Technology Thonburi (KMUTT). The authors also convinced the university to provide 3\u00a0years funding for the OpenDragon project, to create and disseminate a free version of the software for educational organizations in Southeast Asia.After KMUTT funding ended, GRS took over support for OpenDragon, providing web hosting and continued development, and in 2010, GRS made OpenDragon free worldwide. Any user, anywhere in the world, can now freely download the system after completing a simple registration process.To increase the utility of the software, in 2011 the authors published [] which is available in ebook and print from Amazon.com. This text explains the basic theory underlying core remote sensing image processing operations and provides extensive hands-on exercises using OpenDragon.Currently OpenDragon is free (but not open source). The package has been downloaded by users in more than 100 countries. If funding can be found, the intention is to merge the capabilities of Dragon/ips into OpenDragon, create versions for Linux and MacOS/X, and make the resulting combination both free and open source.OpenDragon provides a simple yet powerful platform for hands-on lab exercises and student projects. The fact that it is available in multiple languages and can be internationalized without changing any code gives the system a world-wide audience.Although other software packages are available for teaching remote sensing and GIS at an introductory level, most if not all focus on application-oriented users. OpenDragon is unusual in that it also provides support for students and researchers who need to program their own algorithms or who want to understand the internal workings of remote sensing and GIS software.The OpenDragon Programmer\u2019s Toolkit provides a level of extensibility that is rare in the world of geoinformatics software. It offers a testbed for research as well as a framework for hands-on class projects. By revealing the inner workings of geoinformatics software to the students of today, OpenDragon helps build the skills of the people who will develop the spatial analysis software of tomorrow.Project name: OpenDragonProject home page: \n                     Operating system(s): currently Windows; eventually platform independentProgramming language: C, C++, Java, and othersOther requirements: minimum screen size of 1024\u2009\u00d7\u2009768 (so-called VGA size)License: currently must register to download; eventually FreeBSD or similarRestrictions to use by non-academics: no"},
{"url": "https://progressinorthodontics.springeropen.com/articles/10.1186/s40510-016-0156-7", "title": "The effect of a lidocaine/prilocaine topical anesthetic on pain and discomfort associated with orthodontic elastomeric separator placement", "authors": ["M. Abu Al-Melh", "L. Andersson"], "publication": "Progress in Orthodontics", "publication_date": "9 January 2017", "abstract": "The initial placement of orthodontic elastomeric separators can be uncomfortable and painful. Therefore, it is important to relieve this disturbing sensation to create a discomfort or pain-free orthodontic visit. The purpose of this study was to investigate the effect of a lidocaine/prilocaine topical anesthetic on pain and discomfort associated with the placement of orthodontic elastomeric separators.", "full_text": "Orthodontic discomfort and pain can be disadvantageous to the patient\u2019s compliance and response to treatment. This unpleasant experience may occasionally cause loss of interest, poor compliance, compromised treatment results, and even eventual termination of treatment [, ]. The various painful and distressing orthodontic procedures include separator placement, archwire insertion and activation, application of orthopedic forces, use of elastics, and debonding procedures []. The accompanying uncomfortable sensations experienced by patients during orthodontic treatment are often described as feelings of pressure, tightness, soreness of the teeth, and pain [].It was demonstrated in some studies that amongst the discomfort and pain causing orthodontic procedures is the placement of separators, which can be elastomeric, brass wire, spring type, steel, and latex separators [\u2013]. These studies have reportedly shown that discomfort was clearly associated with separator placement. The sensation of discomfort often starts within 4\u00a0h of separator placement, gradually increasing over the next 24\u00a0h, and then tends to decrease within 7\u00a0days [, ]. The orthodontic patients\u2019 response to the initial placement of separators seems to be overlooked, and there is no current information in the literature that analyzed the patients\u2019 initial response to such a procedure.There are several methods for managing orthodontic pain and discomfort that were covered in the literature. The most common method used to manage orthodontic pain and discomfort was the use of systemic analgesics [, \u2013]. Chewing on something hard such as a chewing gum or a plastic wafer, during the first few hours of appliance activation, has been recommended for reducing the orthodontic pain [\u2013]. Other methods such as low-level laser therapy (LLLT), transcutaneous electrical nerve stimulation (TENS), and vibratory stimulation were also advocated for managing orthodontic pain [, \u2013].Topical anesthetics have been used in different dental procedures for reducing or eliminating pain. It was demonstrated that pain from needle stick injections in the maxillary vestibular and palatal mucosae could potently be reduced or eliminated by using a combination of 2.5% lidocaine/2.5% prilocaine topical anesthetics in a creamy eutectic mixture, known as EMLA\u00ae (EMLA, AstraZeneca UK Limited, Bedfordshire), or a thermosetting gel, known as Oraqix\u00ae (ORAQIX, DENTSPLY International, PA, USA) [\u2013]. Other studies reported different useful applications of lidocaine, adrenaline, and tetracaine (LAT) gel and EMLA\u00ae cream, such as suturing of the facial and soft tissue lacerations and minor biopsies [, \u2013]. One study concluded that lidocaine and prilocaine topical anesthetics could be used in oral mucosal lacerations prior to suturing without the risk of adverse tissue reaction [].The effect of the lidocaine/prilocaine topical anesthetic Oraqix\u00ae on pain reduction from orthodontic procedures has been studied previously. The findings of one study suggested the potential usefulness of Oraqix\u00ae in performing orthodontic procedures such as band placement and cementation, archwire ligation, and band/bracket removal []. The advantage of the topical anesthetic gel Oraqix\u00ae was its delivery method, which simply introduced the gel into the gingival crevice. The suggested indication for use was correlated with the reduction of pain during scaling in gingival pockets. The gel hardened with intraoral temperature and hence was easily contained within the gingival crevice. Also, the application procedure was reportedly simple and completely painless []. Since Oraqix\u00ae has not yet been used for discomfort and pain relief from the initial elastomeric separator placement, extending the use of Oraqix\u00ae to relieve patients from the associated sensation of discomfort would be an interesting achievement.This study was aimed at comparing the topical anesthetic effect of a 2.5% lidocaine/2.5% prilocaine gel (Oraqix\u00ae) with a Vaseline\u00ae placebo on the reduction of discomfort and pain from the initial placement of orthodontic elastomeric separators.Fifty subjects, between 20 and 35\u00a0years of age, were included in this study, 47 females and 3 males. The subjects were undergraduate fifth- and sixth-year dental students, staff members, and dental assistants from the Faculty of Dentistry, Kuwait University. A written consent was obtained from all subjects participating in this study. The study\u2019s experimental design and protocol were approved by the Ethical Committee of the Health Sciences Center, Kuwait University.The inclusion criteria of this study involved the presence of healthy gingival tissues, complete intact posterior occlusion, intact maxillary dentition with the exception of the third molars, and tight contacts between the posterior teeth which was checked with a piece of floss. The exclusion criteria comprised of the existence of inflamed gingival tissues and periodontal disease, missing posterior teeth, spacing between the posterior dentition, retained deciduous posterior teeth, and interproximal carious lesions and/or restorations between the first molar and the second premolar. Subjects with systemic diseases and/or are taking systemic analgesics were excluded from the study.Only the subjects were blinded during the study by wearing sunglasses with gauze taped to the inner side of the shades. The gingival tissues of the first molars and the second premolars from both sides of the maxillary arch were first dried using gauze and the air-water syringe. A suction device and cotton rolls were used to achieve a dry field prior to the application of agents and throughout the procedure. The subjects had their mouths open during the entire experimental procedure. This was done to prevent accidental distribution of the topical anesthetic, Oraqix\u00ae, to the placebo, Vaseline\u00ae, side.A few drops of Oraqix\u00ae were applied on the orthodontic elastomeric separator (Ormco Separators, Ormco Corporation, CA) prior to its placement between the teeth to ensure that the topical anesthetic agent reached the interproximal tissues adequately without the need to use a dispensing needle, which may cause discomfort or pain. Moreover, Oraqix\u00ae is a liquid gel when first applied, which is difficult to contain within the gingival tissues, unlike Vaseline\u00ae (Vaseline\u00ae, 100% pure petroleum jelly, Unilever, USA), which is more viscous and easily contained on and within the gingival tissues. In addition, placing a few drops of Oraqix\u00ae on the separator before its placement between the teeth did not facilitate its insertion as the contact points between the teeth which were tight to begin with.On the contralateral side, using an irrigation syringe with a blunt applicator tip, a small size amount of placebo Vaseline\u00ae was placed around the gingival margins of the first molar and the second premolar (Figs.\u00a0 and ). The sides, where the materials were applied, were randomly alternated in such a way that if the first subject received Oraqix\u00ae on the right side, then the next subject had it on the left side and so on. At the end of the study, half of the subjects received Oraqix\u00ae on the right side and the other half had it on the left side.A questionnaire was given to the subjects and was returned the following day. The survey contained comparative questions about the overall satisfaction, taste, numbing effect, presence of numbness after 30\u00a0min, personal preference, recommendation for routine use in orthodontic clinics, recommendation for application on adults and children, and the experience of any adverse reactions the following day.The results of this study showed that topical anesthesia could be a valuable tool in reducing pain and discomfort associated with several orthodontic procedures. The lidocaine/prilocaine topical anesthetic, Oraqix\u00ae, could be effective in relieving pain and discomfort related to the initial placement of orthodontic elastomeric separators.Based on clinical experience, the placement of orthodontic separators in some patients can cause immediate initial pressure, leading to discomfort and/or pain as soon as the separator is wedged between the teeth. The patients\u2019 degree of initial and delayed responses to the uncomfortable and painful orthodontic procedures can be attributed to several factors, including age, gender, and pain threshold, which can affect the patients\u2019 motivation for orthodontic treatment [, , ]. Therefore, some orthodontic patients do need special consideration, making it imperative to find a method that decreases the patients\u2019 initial discomfort and pain during the separator placement visit to ensure good compliance in the orthodontic visits.The use of topical anesthetics was found to be involved in various orthodontic procedures for relieving discomfort and pain. A study, for instance, reported the use of a benzocaine containing wax for the relief of oral mucosal irritation caused by orthodontic fixed appliances. In comparison with the unmedicated wax commonly used in orthodontic clinics, the benzocaine-medicated wax was found to be instantly effective, and its anesthetic effect kept increasing with time []. A preliminary study analyzed the effect of benzocaine mucoadhesive patches on orthodontic pain caused by elastomeric separators. It was revealed that the use of 20% benzocaine patches during the first 3\u00a0days after the separator placement significantly decreased the degree of pain []. This finding triggered the interest to investigate the effect of the lidocaine/prilocaine topical anesthetic, Oraqix\u00ae, on pain reduction from the initial placement of orthodontic separators. It would also be interesting to study the effect of Oraqix\u00ae on pain reduction after hours or days from separator placement.In this study, after applying Oraqix\u00ae, a waiting period of 2\u00a0min was chosen before the placement of the separators. This was due to the potency of Oraqix\u00ae as previous studies showed that it is efficient from the first 2\u00a0min after application to the vestibular and palatal mucosae [\u2013]. Regarding the study\u2019s duration, a total period of 10\u00a0min was selected as it usually takes less than 5\u00a0min for placing the orthodontic elastomeric separators and about 10\u00a0min is needed to give the instructions to the patient or guardian and to answer their questions before leaving the orthodontic clinic. It was best to have the patient relieved of any disturbing sensation during the visit to ensure proper attendance to appointments, good compliance to treatment, and a pleasant in-office experience.Concerning the VAS results of this study, regardless of the sides the materials were applied to, it was shown that the Oraqix\u00ae side reduced discomfort/pain earlier and significantly better than the Vaseline\u00ae side (Fig.\u00a0). The significant difference in the percentage of pain reduction between both materials was evident from the fourth and sixth minutes onwards. The early onset of action shown in this study coincided with the findings from two previous studies that compared the anesthetic effect of two lidocaine/prilocaine substances with benzocaine. Both studies showed that the lidocaine/prilocaine substances (EMLA\u00ae and Oraqix\u00ae) reduced pain significantly better than benzocaine as early as the first and second minutes after application [\u2013]. In this study, after 2\u00a0min from applying the materials, some subjects reported mostly pressure and discomfort after the instant separator placement, and this sensation was probably due to the fact that 2\u00a0min was not sufficient enough to anesthetize the gingival margins and periodontal ligament. This could be due to the thickness of the gingival tissues as well as the distance of penetration of the topical anesthetic to anesthetize the periodontal ligament. Moreover, due to the fact the subjects had different pain thresholds, some subjects reported low pain scores, while others reported higher pain scores during the entire duration of the study.The overall mean discomfort/pain score on the VAS was found to be significantly lower (\u2009<\u20090.001) with the topical anesthetic (3.9\u2009\u00b1\u20090.49 SE) than with the placebo (12.8\u2009\u00b1\u20090.90 SE) (Table\u00a0). As observed from the VAS graphs of both the placebo and TA sides, most subjects described the sensation as pressure discomfort giving lower percentages of discomfort/pain evaluation, while a few reported a painful perception immediately after the initial placement of the elastomeric separators, and hence reported higher percentages of discomfort/pain evaluation (Fig.\u00a0). This supported the previous findings that patients of different age, gender, ethnicity, psychosocial background, and pain tolerance and threshold could have varying responses to discomfort and pain [].Some studies looked at pain from orthodontic tooth separation by registering pain responses on a VAS at three time points: T1 (before insertion of the tab), T2 (immediately after insertion), and T3 (24\u00a0h after insertion) [, ]. In this study, the effect of the topical anesthetic Oraqix\u00ae versus the placebo Vaseline\u00ae on discomfort or pain from the very beginning of the placement of the orthodontic elastomeric separators was analyzed. It would be interesting to monitor the effect of Oraqix\u00ae 24\u00a0h after insertion. However, this is difficult to achieve this as the duration of action of Oraqix\u00ae is about 20\u00a0min in a dry field.In this study, the verbal scale showed that most subjects reported Oraqix\u00ae as the least painful side, and the effect was evident from the second minute of application (Table\u00a0, Fig.\u00a0). A few subjects did report less pain on the placebo side after 2 and 4\u00a0min. Those subjects explained that the anesthetic effect was bothersome and irritating as it was too strong. When those subjects were asked in detail about their dental history, some did mention a bad dental experience in the past with anesthetic needles, and that this sensation reminded them of this disturbing perception. Moreover, throughout the duration of the study, a few subjects reported no difference between the Vaseline\u00ae and Oraqix\u00ae sides with regard to reduction of discomfort/pain perception after separator placement. Regardless of their response in the verbal scale, by the end of this study, the subjects still preferred and recommended the use of Oraqix\u00ae for adults and children in orthodontic clinics.Regarding the overall satisfaction, a high number of subjects felt more satisfied and pleasant with the Oraqix\u00ae side as the pressure created from the separator was relieved (Table\u00a0). Some patients reported that the placebo side felt like there was a piece of foreign object or meat stuck between their teeth creating immediate pressure that was very annoying.The subjects\u2019 response to the taste was inconsistent (Table\u00a0). More subjects seemed to favor the taste of placebo Vaseline\u00ae, which was tasteless, as opposed to the taste of Oraqix\u00ae, which was bitter. A few patients were confused which side tasted better as at the end of the study, they could not remember the taste after rinsing their mouths. Also, some of the subjects mentioned that the whole oral cavity felt bitter, which meant that some of the Oraqix\u00ae material might have been mixed with saliva despite the vigilant use of saliva ejectors. Despite the bitterness of Oraqix\u00ae, most subjects did favor the taste of it. Again, most patients could not remember the difference of taste by the end of the study. In this study, the bitter taste of Oraqix\u00ae coincided with a previous study that compared the effect of two lidocaine/prilocaine substances, Oraqix\u00ae and EMLA\u00ae, on pain reduction after palatal needle sticks. Both materials had the same composition, but EMLA\u00ae was a cream and Oraqix\u00ae existed as a gel. The study showed that Oraqix\u00ae was more bitter than EMLA\u00ae as reported by the subjects []. Hence, since Oraqix\u00ae is FDA approved and registered for intraoral use, it was essential to suggest a recommendation to the manufacturer to improve the taste of Oraqix\u00ae to be more acceptable by patients, particularly children.All subjects reported more numbness on the Oraqix\u00ae side, and the anesthetic effect was described as intense (Table\u00a0). Seventy percent of the subjects mentioned that the anesthetic effect lingered with a reduced effect for 30\u00a0min after completion of the study, while 30% of the subjects reported complete absence of anesthetic effect after 30\u00a0min. As reported in one study about the onset and duration of action as assessed by probing of pocket depths, Oraqix\u00ae provided anesthesia after an application time of 30\u00a0s, with a mean duration of action of about 17 to 20\u00a0min []. The anesthetic effect of Oraqix\u00ae might eliminate the need for a preemptive administration of a systemic analgesic, and it may possibly limit the patient to only a postoperative dose of systemic analgesic after the anesthetic effect wears off.When the subjects were asked about their personal preference, recommendation for routine use at orthodontic clinics, and recommendation for use for adults and children, all subjects reported the Oraqix\u00ae side as the preferred side (Table\u00a0). This suggested that even though some subjects did report no difference between the sides in terms of overall satisfaction, they still felt that it would be of benefit for them as well as others. Moreover, all subjects reported no adverse effects experienced on both sides 1\u00a0day after the study (Table\u00a0). As concluded in one study, in terms of the systemic effects after the application of Oraqix\u00ae in periodontal pockets, there was a large safety margin. The plasma profiles of lidocaine and prilocaine following a single dose of Oraqix\u00ae to patients with advanced periodontitis were low as compared to those reported to cause initial signs of CNS toxicity [].In this study, there were some weaknesses that need to be addressed in future studies. Increasing the sample size and including more males would be advantageous besides facilitating the investigation of the effect of age, gender, ethnicity, and psychosocial factors on the outcome of the subjects\u2019 response to pain and discomfort related to placement of orthodontic elastomeric separators. It would also be motivating to explore new means to extend the effect of topical anesthetics in relieving pain and discomfort experienced after a couple of hours or even a day from the placement of orthodontic elastomeric separators.This study showed that the lidocaine/prilocaine topical anesthetic, Oraqix\u00ae, could potently relieve discomfort or pain experienced after the initial placement of the orthodontic elastomeric separators. This method could be useful for patients with a low pain threshold as well as apprehensive adults and children."},
{"url": "https://prc.springeropen.com/articles/10.1186/s41155-017-0061-0", "title": "Relations between mental workload and decision-making in an organizational setting", "authors": ["Mar\u00eda Soria-Oliver", "Jorge S. L\u00f3pez", "Ferm\u00edn Torrano"], "publication": "Psicologia: Reflex\u00e3o e Cr\u00edtica", "publication_date": "3 April 2017", "abstract": "The complexity of current organizations implies a potential overload for workers. For this reason, it is of interest to study the effects that mental workload has on the performance of complex tasks in professional settings.", "full_text": "Current organizational settings frequently imply high complexity and high demands from their workers. Occupational activity can therefore involve levels of demands that go beyond the person\u2019s cognitive capacity of analysis and decision-making (Ferrer & Dalmau, ; Rolo, D\u00edaz, & Hern\u00e1ndez, ). The existing empirical studies present indicators along these lines. Thus, recent official surveys in the European setting indicate a growing incidence of labor problems reflected in indicators related to mental demands: level of attention required, rhythm of work imposed, deadlines to be met, and the monotony of the task (INSHT, , ).The level of overload is important for workers\u2019 efficacy and well-being. For this reason, from different areas of knowledge, researchers have attempted to define constructs and instruments that allow its adequate assessment. Among them, the  is one of the most widely invoked concepts in ergonomic research and practice (Hancock & Meshkati, ; Salvendy, ; Wickens, ; Young, Brookhuis, Wickens, & Hancock, ). Mental workload (MWL) attempts to measure the extent to which occupational activity matches or exceeds the worker\u2019s resources. In this sense, it has been studied from two essential perspectives. The first one considers that MWL is a factor that depends exclusively on task demands to which the subject adapts. The second one, currently receiving more support, conceptualizes MWL as a consequence of the relation between task demands and the subject\u2019s skills in terms of a demand/resource balance (D\u00edaz, Rubio, Mart\u00edn, & Luce\u00f1o, ; Ferrer & Dalmau, ; Young et al., ). The multidimensionality of the concept of MWL and the subjective perception involved implies different fundamental aspects like characteristics of the task, characteristics of the operator, environmental context in which the performance occurs, time pressure, and subjective aspects related to it, such as stress or the perception fatigue (Hart & Staveland, ; Eggemeier, ; ISO, ). In an attempt to bring these dimensions together and provide a global definition of MWL, Young and Stanton () have suggested that MWL reflects the level of attentional resources required to meet both objective and subjective performance criteria, which may be mediated by task demands, external support, and past experience.The empirical works carried out have evidenced that MWL presents dynamic and complex relationships with performance, which may also vary according to subjects\u2019 characteristics. In this sense, it has been shown that an increase or decrease of MWL can be compensated to some degree by the investment of additional resources, thus maintaining performance at the cost of individual strain (Hancock & Warm, ; Matthews & Davies, ). If the intensity and the duration of the effort required to perform the task are balanced, activation will take place in an optimum area, which makes the subject efficient. However, if the effort must be sustained over a prolonged period of time, fatigue appears and the subject\u2019s functional efficiency is temporary altered (ISO, ; Pretorius & Cilliers, ; Young et al., ). On another hand, too little stimulation can lead to underload. In this case, as resources are either allocated elsewhere or otherwise decrease through underuse, the subject\u2019s performance may also be negatively affected (Young & Stanton, ; Wilson & Rajan, ; Young et al., ). Considering the former, as Young et al. () state, there is a strong consensus that mental underload can be just as detrimental to performance as mental overload, with both leading to performance degradation, attentional lapses, and errors. The relationship between MWL and performance may thus follow a similar pattern to the classical \u201cinverted U\u201d of Yerkes-Dodson\u2019s curve (Yerkes & Dodson, ), where optimal performance is located between low and high MWL (Brookhuis & Waard, ; Young et al., ).However, MWL literature has mainly been centered on measuring performance by means of operative tasks, like those related to traffic or transport research, real flight, flight simulation, air traffic control, peripheral detection, or formal memory/follow-up tasks (see Bowling & Kirkendall, ; Brookhuis & Waard, ; Hart, ; and Young et al., ). In this sense, only a few and very recent works have focused on relating MWL and one of the key concepts in skilled work settings: decision-making (Byrne, ; Baethge, M\u00fcller, & Rigotti, ; Jackson, Kleitman, & Aidman, ).The concept of decision-making (DM) differs in relevant characteristics from classical performance indicators used in MWL literature. It entails a sequence of actions that allow understanding how subjects face and solve complex problems in professional contexts (Hodgkinson & Starbuck, ). Decisions within professional settings have an interactive social dimension and are based on prior knowledge that, in itself, does not guarantee success when applying the decisions adopted (Argando\u00f1a, ; Offrage, ; Weick & Sutcliffe, ). They are, in turn, developed in contexts far from the classical paradigms of rational choice. Thus, decisions are conditioned by elements such as open problems; uncertain and dynamic settings; changing, multiple, and competitive goals; multiple feedbacks; time pressure and contrasts; consequences involving some risk; multiple decision-makers; and external standards (Orasanu & Connolly, ; Csaszar & Eggers, ). DM has critical effects on professional achievement and acquires special relevance when performance and actions involve direct consequences on people\u2019s integrity or well-being (Secchi, ).As mentioned, evidence about relationships between MWL and DM is scarce. In relation to clinical decision-making, Byrne () has suggested\u2014on the basis of the existing research\u2014that MWL and complexity of the information may be key factors to determine which kind of clinical decision-making is developed: schemata-based methods or conscious metacognition procedures. Baethge et al. () have shown empirically that the influence of MWL on performance quality is moderated by the way in which selection, optimization, and compensation of resources and goals are handled by nurses. Jackson et al. (), using driving simulation strategies, have provided support for the existence of a MWL zone of optimal DM performance, below or above which a worse quality of DM is produced, as has been suggested for the relationship between MWL and other classical performance measures. It is also worthwhile to mention the evidence provided by the works that have analyzed the relation between individual DM and stress. Recent reviews and meta-analyses postulate that stress occurs whenever a demand exceeds the regulatory capacity of an organism, particularly in situations that are unpredictable and uncontrollable (Dickerson & Kemeny, ; Koolhaas et al., ). Stress, in this sense, only refers to situations that are conceptualized within the range of MWL. In addition, the emergence of stress as a function of external demands is mediated by appraisal strategies (Lazarus, ). Thus, relations between stress and DM can only be applied to the field of MWL in a limited way. However, it is interesting to briefly mention the existing evidence about the relationship between stress and DM in natural settings. In this sense, stress is thought to relate to dysfunctional strategy use, altered feedback processing, heightened reward sensitivity, and lowered punishment sensitivity (Starcke & Brand, )According to the former considerations, our research question refers to the way in which MWL is related to DM in organizational settings. There are several reasons that support the relevance of this question, which the present work will address, specifically, the scarcity of works that explore the relation between MWL and DM, the relevance implied in the organizational setting, and the evidence of complex relations between MWL and DM. Our general objective is, therefore, to study in depth the effects of MWL on task performance in the work setting, empirically analyzing the relations between MWL and DM in workers who carry out their activity in real contexts. In this sense, our specific objectives are (1) to analyze the relation between the worker\u2019s expectation of MWL prior to task performance (Pre-Task WL) and the quality of DM after task performance; (2) to analyze the relation between perceived MWL after the task (Post-Task WL) and the quality of the DM performed after the task; and (3) to analyze the relation of differences between the expected MWL and the MWL perceived after the task (differential mental workload), on the one hand, and the quality of the DM carried out, on the other.To address the concept of MWL, we have drawn from the conceptualization model of the NASA-Task Load Index (NASA-TLX) (Hart & Staveland, ). The suitability of this method has been supported by numerous studies because it provides more accurate results than other techniques, such as the SWAT or the modified Cooper-Harper scale (Hill et al., ; Rubio, Diaz, Martin, & Puente, ). This evidence, along with the simplicity of its use, makes the NASA-TLX currently the most widely used instrument to assess MWL (Noyes & Bruneau, ; Rutledge et al., ). This method allows rating the task from a multidimensional perspective, so it has been proved useful due to its diagnostic capacity with regard to possible sources of workload (D\u00edaz et al., ). Its core strengths include its applicability in a naturalist work setting, as the workers can quickly rate the task carried out both a few moments after its performance and retrospectively (Recarte, P\u00e9rez, Conchillo, & Nunes, ). In studies carried out on retrospective assessments, high correlations have been found between the data extracted in this way and the immediate scores (Wierwille & Eggemeier, ). NASA-TLX measurement procedure is detailed more extensively in the \u201c\u201d section.In the case of DM, we used the conceptualization and measurement model elaborated by Soria-Oliver and her team (Sanz de Acedo Lizarraga, Sanz de Acedo Baquedano, Soria-Oliver, Closas, ; Soria-Oliver, ). In this model, DM is conceived as a complex sequence of actions that require attending to different parameters conditioned by the temporal and organizational needs. They are specified in the following steps: planning the process, defining the goals, generating options, evaluating them, and selecting the best by considering both the influence of personal variables and variables of the setting. According to the guidelines of Byrnes (), Cannon-Bowers, Salas, and Pruitt () and Cannon-Bowers and Salas (), this model assumes three sources of variables that characterize naturalistic decision-making: task, subject, and context variable. Task variables are associated with the nature of the decision itself, for instance, the uncertainty involved in each alternative, pressure of time and available money, quantity and quality of the information, proposed goals, and possible consequences of the decision. Subject or decision-maker characteristics include the performers\u2019 internal factors: motivation, thorough self-regulation of the decision stages, crucial information processing, expertise in a certain domain, and the emotions that almost always accompany a decision. Finally, the environmental characteristics define the context in which the decision takes place, specifically, factors that are not directly a part of the decision task itself: social and work influences and distracting events. This model, in turn, has led to the empirical elaboration of a questionnaire (Decision-Making Questionnaire, DMQ (Soria-Oliver, )) that measures the quality of DM in organizational contexts, considering the different theoretical dimensions of the model. Detailed characteristics of DMQ are presented in the \u201c\u201d section.Our study draws some tentative hypotheses that are conditioned, in any case, by its exploratory and correlational nature. Thus, we will explore the degree to which the relationship between DM, on the one hand, and Pre-Task WL and Post-Task WL, on the other, follow the general pattern that has been evidenced for other performance indicators (Brookhuis & Waard, ; Wilson & Rajan, ; Young & Stanton, ; Young et al., ). This pattern may be defined as follows: first, by a low expected or real perceived MWL range in which low stimulation may decrease the quality of DM strategies; second, by an optimum performance range of expected or real perceived MWL in which balanced stimulation adequately fits subjects\u2019 resources and DM quality is higher; third, a high expected or real MWL range that overwhelms subjects\u2019 resources and may yield worse DM quality. In relation to Differential WL, we expect that expected MWL (Pre-Task WL) may act as a moderator of relationships between Differential WL and DM. When subjects\u2019 expected MWL (Pre-Task WL) is low, real MWL (Post-Task WL) that fits this expectation (Adjusted Differential WL) or remains below it (Low Differential WL) may generate lower DM quality due to absence of stimulation. If subjects with low Pre-Task WL have to cope with higher real MWL than expected (High Differential WL), DM may be better if subjects\u2019 resources are not overwhelmed. When subjects\u2019 expected MWL (Pre-Task WL) is high, lower real MWL (Low Differential WL) would lead to worse DM quality due to stimulation decrease. If real MWL fits expected MWL (Adjusted Differential WL), DM quality will be high, as subjects have adequate stimulation. In this case, if real MWL is higher than expected (High Differential WL), DM may be better only if subjects can manage their resources to cope with the requested context demand. However, our empirical design, which measures professional decisions in a real setting without manipulation, does not allow us to predict a priori which expected or real MWL levels may be found and, consequently, whether they would cover all the different ranges proposed in our hypotheses.Design: ex post facto prospective design, with measurement of the independent variable (MWL) before and after task performance and measurement of the dependent variable (DM) after task performance.Sample: a total of 175 participants, representative of the professionals belonging to an institution of online higher education. The sample was stratified proportionally to the number of people in the different professional categories in the entire institution (managers, middle managers, professors, and tutors). Participants were selected randomly within each stratus. Inclusion criteria were as follows: having joined the institution more than 3\u00a0months ago to ensure adequate familiarity with the tasks, and adequate mastery of the Spanish language. The criteria of a 3-month period was established in accordance with prior consultation with the human resources department about the estimated time to achieve workers\u2019 adequate task performance autonomy, as was also reflected in  documents. The sample had a mean age of 34.5\u00a0years (SD\u2009=\u20097.4) and included 57.8% (\u2009=\u2009100) of women. It included managers (2.3%; \u2009=\u20094), middle managers (11.6%; \u2009=\u200920), professors (39.9%; \u2009=\u200969), and tutors (46.2%; \u2009=\u200980). The minimum sample size required to perform the study was estimated at 105 people. Sample size was calculated assuming a 95% confidence level, 80% power, a standard deviation of 60 in both comparison groups, and 10% of losses, to detect a minimum difference of 35 points in the score of the DMQ between the groups with high and low differential workload (see below for a definition).Measurements: The following variables were measured:Socio-demographic variables: sex, age, and occupational post within the organizationMental workload: It was measured by means of the dimensions considered in the NASA-TLX scale (Hart & Staveland, ), which measures workers\u2019 subjective experience of the workload. NASA-TLX uses six dimensions: (1) : how much mental and perceptual activity was required (e.g., thinking, deciding, calculating, remembering); (2) : how much physical activity was required (e.g., pushing, pulling, turning, controlling); (3) : how much time pressure workers felt was due to the rate or pace at which the task or task elements occurred; (4) : how successful was the worker in accomplishing the goals of the tasks; (5) : how hard did the worker feel she/he had to work (mentally and physically) to accomplish the level of performance; (6) : how insecure, discouraged, irritated, stressed, and annoyed did the worker feel during the task. Twenty-step bipolar scales are used to obtain ratings for these dimensions. In the classical procedure, a previous weighting phase is performed. This weighting phase requires a paired-comparison task to be performed prior to the workload assessments to combine the six individual scale ratings into a global score. However, recent research has shown the scarce utility of the weighting phase (L\u00f3pez, Rubio, & Luce\u00f1o, ). Therefore, we decided to use the subjects\u2019 rating directly on the six dimensions considered in the scale, without carrying out the prior pairwise weighting phase. As pointed out, expected MWL was measured before the task (Pre-Task WL) and perceived MWL was measured after performing the task (Post-Task WL). We also estimated a score that we called differential mental workload, obtained as a result of subtracting the Pre-Task WL score from the Post-Task WL score. In this indicator, negative values imply that the task was less burdensome than expected. Positive values, in contrast, reflect that the subject faced a greater burden than expected. Values approaching zero reflect a match between the expected MWL and the real MWL encountered.Procedure: Our research team presented the objective and procedure of the study to the Rectorate of the academic organization in which the field work would take place. The Rectorate decided that the proposed research was potentially useful for the organization and also that study procedures followed ethical criteria. Consequently, the workers\u2019 institutional e-mail was provided by the organization. The survey was thus disseminated by means of a link to which access was gained by an e-mail invitation. In the e-mail, we mentioned the nature of the research, the subjects\u2019 voluntary participation, and we guaranteed anonymity of the responses. As an e-mail attachment, subjects received the NASA-TLX questionnaire, which had to be printed to become adequately familiar with the instrument. The way in which participation was designed required the subjects to firstly fill in the adapted paper-and-pencil version of the NASA-TLX questionnaire before beginning a typical task of their job and to complete it again after completing the task. After completing both questionnaires, by means of the web link, access was gained to the pre-task and post-task NASA-TLX questionnaire to enter the data collected on paper. The DMQ was also accessed to allow completing it online directly on screen. A reminder was sent 2\u00a0weeks after the original invitation in order to promote participation. Out of a total 250 applications, we obtained 175 completed questionnaires (response rate of 70%).Analysis: We conducted descriptive and initial exploratory analyses, and we examined the reliability of the scales and subscales. Subsequently, we performed multivariate analysis of variance (MANOVA) test and linear regression to explore the relation between decision-making and the different indicators of mental workload. We used the statistical package IBM\u00ae SPSS\u00ae Statistics V. 22.0.In the present work, we have attempted to deepen our knowledge of the relations between a process of great relevance in complex organizational settings, decision-making (DM), and an indicator of occupational quality with a long trajectory, mental workload (MWL). For this purpose, we used theoretical models and instruments that have been contrasted in prior literature. We analyzed real tasks at the workplace in subjects who carry out their activity in an organization, and we focused on DM from a model that takes into account the particularities of professional settings. The results obtained reveal some consistent and expected evidence, but they also present innovative relations that are necessary to interpret.The initial analyses of our study reveal the psychometric robustness of the DMQ when used as a global score. It also shows good psychometric indicators for the DMQ-Task and DMQ-Subject subscales and somewhat worse indicators for the DMQ-Context subscale. Different subscales also show adequate reliability indicators (Uncertainty, Time/Money Pressure, Information and Goals, Consequences of Decision, Cognition, and Social Pressure), although there are others, especially those referring to Motivation and Self-Regulation, which show significantly lower psychometric indicators than those obtained in the initial studies that led to the construction of this instrument (Soria-Oliver, ). These results reinforce the suitability of the instrument as a whole and also its main dimensions. On another hand, the results point to the need to continue contrasting the application of the subscales and to determine the extent to which\u2014and with different populations\u2014the separate use of the subscales can be maintained.From the viewpoint of the descriptive statistics, some reflections are warranted. A notable aspect of our study is the obtention of indicators in a professional sample that, in turn, is representative of a specific organization. This type of samples is scarce in the literature (D\u00edaz et al., ), which has preferentially focused on laboratory tasks and student samples. On another hand, there are few studies that report data from the chosen approach, which suppresses the weighting phase of the NASA-TLX scale. From this perspective, it is not possible to establish comparisons with the absolute score when assessing the significance of the scores of our sample. Nevertheless, if we take into account that the mean levels of the measurement scale as a whole reach 60 points and that the maximum is 120 points, our sample is located around the medium-high level in absolute terms. Regarding decision-making, we can use previous works carried out with different professional groups as a reference (Sanz de Acedo Lizarraga et al., ). Based on them, it should be noted that our sample presents higher values of quality of DM than the samples used in the same geographic context.From the viewpoint of exploring the relations among the different variables, we found different patterns as a function of the moment and the way of implementing the MWL measure.In this sense, with regard to the first goal of this study, which was to explore the relation between the expected MWL, which we have called Pre-Task WL, and the quality of DM, results provide support for our hypothesis. Thus, from a global point of view, the relationship between Pre-Task WL and DM is compatible with the expected pattern proposed on the basis of the existing literature (Brookhuis & Waard, ; Wilson & Rajan, ; Young & Stanton, ; Young et al., ). Specifically, global DM as measured by the Total DMQ score and DM aspects related to the way in which the task is handled yielded better quality when intermediate levels of MWL were anticipated. Our results are concordant with those presented by Wilson and Rajan () and Young and Stanton () in the case of operative tasks and by Jackson et al. () specifically for DM, pointing out that both under- and overload could lead to poorer performance. In this sense, on the one hand, underload may lead to inadequate activation and scarce use of workers\u2019 available resources. Our results are also perfectly compatible with evidence indicating that an adequate process of activation is useful for better performance in cognitive tasks both in academic contexts (Pekrun, Goetz, Daniels, Stupnisky, & Perry, ) and organizational settings (Fisher, ; Loukidou, Loan-Clarke, & Daniels, ). However, a slightly different pattern was found for the relationship between expected MWL and the performance of DM strategies related to the subject dimension. In this sense, although DM performance referring to the subject dimension increases when expected MWL remains at a medium level, the quality of these strategies decreases when expected MWL is higher. This fact, taken with caution, indicates that subjects\u2019 ability to engage in the decision process in a motivated, thoughtful way and by self-regulating the process may not be affected, but maintained or improved when higher MWL is expected. This result may be explained by the fact that DM strategies referred to the subject dimension are more accessible to self-regulation than DM strategies that affect task or environment handling (Cannon-Bowers & Salas, ).Taking into account the above and, in any case, considering the study\u2019s limitations, our study provides evidence for the application of results to DM in organizational contexts. In this sense, our results indicate that anticipating levels of MWL that guarantee workers\u2019 adequate activation may yield better results in DM performance. The expectation of low task requirements may lead to workers\u2019 poorer quality of decisions. High levels of expected MWL could lead to a decrease in DM performance, but this decrease seems to be moderated by the kind of task that has been explored, related mainly to intellectual work, and there is no evidence for the dimensions related to subjects\u2019 motivation and self-regulation.With regard to the second goal: to explore the relation between Post-Task WL and DM, the results do not yield evidence of a relation between the two constructs. Different kinds of potential relationships were explored (linear and nonlinear), as was the potential moderation effect of Pre-Task WL, but none of our analyses showed relevant patterns. This indicates that, from our data, we cannot make direct inferences about the quality of DM that the worker performs as a function of the MWL experienced. This finding is not consistent with our hypothesis, which assumed that an \u201cinverted U\u201d pattern would be found between DM and real experienced MWL. Our hypothesis, as previously mentioned, was based on the existing knowledge about the general relation between performance and MWL (Brookhuis & Waard, ; Wilson & Rajan, ; Young & Stanton, ; Young et al., ) and some specific results referring to relationships between MWL and DM (Jackson et al., ). Some possible mechanisms could explain this lack of relation: (1) Our results could be conditioned by the levels of MWL present in the studied sample, which may not reflect the values of low or high MWL achieved by experimental manipulation in common simulation studies (Young et al., ). Thus, our results could be revealing a comfort zone, in which the variations in MWL scores do not reflect an important change in conditions affecting DM performance. However, this condition may not operate in the same way for expected MWL. In this sense, as shown previously, expected MWL\u2014although yielding a similar distribution\u2014showed relevant effects on DM. (2) Combined with this, experienced MWL in real organizational settings may be handled dynamically (Weick & Sutcliffe, ) by means of different strategies (e.g., rebalancing demands and capabilities) that may reduce activation level and, consequently, MWL impact on DM. In this sense, DM in natural settings may offer subjects more flexibility to buffer the effect of MWL than do closed experimental or operative tasks. (3) Additional variables, like subjects\u2019 expertise or stress reaction and appraisal, could be operating as mediators between MWL and DM (Weick & Sutcliffe, ; Starcke & Brand, ).In relation to our third goal, the exploration of the relations between decision-making and the differential mental workload, the pattern of relations found is striking and of great interest. However, it differed partially from our expectations according to our hypothesis. In this sense, we found that the relationships between Differential WL and DM are not moderated by expected WL (Pre-Task WL), as Differential WL and Pre-Task WL had no significant interaction and presented parallel relationship patterns with DM. Additionally, the results show that the workers whose expectations of MWL match the real MWL would have a worse quality of DM than those whose MWL is higher or lower than expected. The relation occurs both for global DM and the processes that affect the task and the subject, which are reflected to a lesser extent in the dimensions of decision-making that affect the setting. This pattern of results leads to considering that when there is a match between expectations and the real task, the task demands do not activate a better quality process of DM. According to our theoretical basis, this result should be explained by the fact that this match may not place subjects in the optimum activation range but in a potential underload zone that may lower DM performance (Wilson & Rajan, ; Young & Stanton, ; Jackson et al., ). The higher quality in DM in the group with more MWL than expected is also concordant with our hypothesis and may be revealing a higher cognitive demand that places subjects on a better performance range and, on another hand, does not exceed the limits to generate a deficient performance. This pattern has been previously shown in MWL for general performance (Young & Stanton, ; Young et al., ) and DM (Jackson et al., ). The presence of higher levels of DM quality in the group with lower MWL than expected is more difficult to explain because it is not consistent with the existing evidence from other studies (Jackson et al., ; Loukidou et al., ). This could be revealing the possibility of developing more qualified DM in the absence of task demands, but we have no basis to contrast this hypothesis with our data. In any event, it seems to reveal the need to consider dynamic and more complex processes, such as the reciprocal influence between the two constructs. The exploration and determination of these patterns of relation would be of great interest to adequately design the levels of MWL that allow more qualified organizational decisions.One strength of our study is that we measured MWL before and after the task, as well as the fact that it was carried out in a real professional setting. However, it has the limitation of not ensuring more extreme values of MWL, which would allow a better contrast of their effect on DM. Such values could be obtained by including some quasi-experimental manipulation or by accessing settings regularly submitted to higher mental loads. This would allow comparing the extent to which the patterns found are repeated, especially for the complex relations found between DM and differential WL. It may also be of interest to complement the quantitative indicators with a qualitative follow-up of the process undergone by the subjects, in order to further our understanding of the unexpected results we found. Our study design may also be improved if potential moderators of the relationship between MWL and DM are considered. In this sense, subjects\u2019 expertise could be one of the most relevant factors to be included in further studies.Our work reveals the existence of complex relations between the MWL experienced by workers in a setting with intellectual tasks and the quality of the DM they develop in their work. Quality of DM appears to be better when medium levels of WL are expected. Underload or overload may affect general quality of DM and, specifically, the strategies related to task management. However, decision strategies related to the subject\u2019s own motivation and self-regulation appear to be less affected by overload. This relationship pattern shows the potential positive effects of adequate activation prior to the task on DM performance. However, this effect does not seem to be linked to the real MWL experienced because we found no consistent pattern of relation between the quality of DM and the MWL experienced after having carried out the task. The relation between the differential WL, on the one hand, and the quality of DM, on the other, showed also a nonlinear pattern. Thus, the workers whose expected MWL matches the one they must face have a worse quality of DM than those who have a mismatch between the two, either in a positive or negative sense. This finding makes us consider a more elaborate pattern of relations, in which the possibility of the existence of a reciprocal dynamic influence between the two constructs should be contemplated. Our results lead to various questions, which can be taken up by future research to better understand the influence of the levels of overload in complex tasks that are dealt with in current organizational settings."},
{"url": "https://prc.springeropen.com/articles/10.1186/s41155-017-0066-8", "title": "Translation and validation of the Mind-Wandering Test for Spanish adolescents", "authors": ["Carlos Salavera", "Fernando Urcola-Pardo", "Pablo Us\u00e1n", "Laurane Jarie"], "publication": "Psicologia: Reflex\u00e3o e Cr\u00edtica", "publication_date": "2 June 2017", "abstract": "Working memory capacity and fluent intelligence influence cognitive capacity as a predictive value of success. In line with this, one matter appears, that of mind wandering, which partly explains the variability in the results obtained from the subjects who do these tests. A recently developed measure to evaluate this phenomenon is the Mind-Wandering Questionnaire (MWQ).", "full_text": "As human beings, we tend to be distracted by the activities we perform, which is when the mind tends to wander back to the past or to plan the future. This spontaneous tendency to produce thoughts and to freely allow our minds to wander, despite external stimuli, is considered a typical characteristic of the human mind (Smallwood and Schooler, ). Mind wandering is understood as a mental process during which attention is distracted from a task underway to focus on the contents that our minds intrinsically produce (Smallwood and Schooler, ). As it is one of the most common activities that the human mind performs, it occurs in practically all day-to-day activities, and individuals are gripped to their own mind events between 10 and 50% of the time they are awake (Kane, Brown, McVay, Silvia, Myin-Germeys & Kwapil, ; Killingsworth and Gilbert, ). Mind wandering presents wide inter-individual variability, and the mind-wandering trait appears as the personal characteristic of a tendency toward mind wandering for a given period of time (Mrazek, Smallwood, Franklin, Baird, Chin & Schooler, ).Repetitive thoughts are considered an adaptive function of human beings. Despite the negative connotations associated with this concept, mind wandering is not itself considered a negative characteristic. Similar negative connotations are attached to common terms like cognitive failures, resting state, rumination, distraction, attentional failures, absent-mindedness, repetitiveness, and the like (Baars, ). Planning the future is one of the most beneficial results connected with mind wandering as its appearance is associated with thoughts about the future, and not with the past or present (Schooler, Smallwood, Christoff, Handy, Reichle & Sayette, ). Thoughts that focus on the future are increased by self-reflection (Smallwood and O\u2019Connor, ) and by prioritizing personal goals (Stawarczyk, Majerus, Maj, Van der Linden and D\u2019Argembeau, ), which is reduced by negative moods (Smallwood and O\u2019Connor, ). Along these lines, mind wandering comes over as an adaptive advantage as it can diminish distress by predicting future events to better adapt to one\u2019s own environment (Bar, ). Mind wandering allows information that cannot be analyzed when a stimulus emerges to be systematized because the semantic manipulation of information cannot take place while a stimulus occurs (Binder, Frost, Hammeke, Bellgowan, Rao & Cox, ), and is thus associated with effective coping (Greenwald and Harder, ) and creativity (Sio and Ormerod, ). This anticipative capacity and planning of the future allow problems to be creatively solved (Baird, Smallwood, Mrazek, Kam, Franklin & Schooler, ).High levels of mind wandering are related with low moods (Killingsworth and Gilbert, ) and negative thinking (Smallwood, O\u2019Connor, Sudbery, and Obonsawin, ). An increase in negative thoughts in relation to mind wandering has been associated with individual levels of depression (Marchetti, Koster and De Raedt, ). This association may be due to mind wandering which, given the spontaneous emergence of thoughts, is associated with paying more attention to one\u2019s own thoughts, emotions, and experiences (Smallwood and Schooler, ). This marked increase in self-attention may mean being at more risk of self-assessment, which has been associated with negative emotions (Mor and Winquist, ). The appearance of repetitive thoughts is relevant for the appearance and maintenance of emotional disorders (Aldao, Nolen-Hoeksema and Schweizer, ) through brooding and worrying. Although both of these constructs are related with mind wandering, they are considered to semantically differ. Indeed, worrying is defined as expecting possible negative results in the future (Borkovec, Robinson, Pruzinsky and DePree, ), while brooding is defined as the repetitive response model that involves the constant development of distress symptoms, and of the causes and consequences of distress (Nolen-Hoeksema, Wisco and Lyubomirsky, ).Increased mind wandering and paying more attention to one\u2019s own thoughts, emotions, and experiences have been related with low levels of self-esteem (Mrazek, et al. ). Nevertheless, paying more attention to oneself is not necessarily considered a negative activity for self-esteem. So mindfulness is considered a construct that contrasts with mind wandering (Mrazek, Smallwood, and Schooler, ). The mindfulness construct has been defined in many forms, and all its definitions coincide in that it is a matter of paying intentionally more attention to the present time and not taking a judgemental attitude about experience (Brown and Ryan, ; Germer, ; Kabat-Zinn, ; Segal, Williams and Teasdale, ). This non-judgemental attitude makes mindfulness appear positively related with self-esteem (Kong, Wang, and Zhao, ; Rasmussen and Pidgeon, ). In turn, self-esteem is considered a predictor of satisfaction with life (Diener and Diener, ; M\u00e4kikangas and Kinnunen, ). Hence, the aforementioned factors may be considered modulators in the relation between mind wandering and satisfaction with life.As no valid scale exists to measure mind wandering, the usual way to assess it involves periodically interrupting individuals while they do a task, and asking them to report the extent to which their attention was related to on-task or on task-unrelated concerns (Mrazek, Smallwood, Franklin, Baird, Chin & Schooler, ). In the last few years, the Mind-Wandering Questionnaire (MWQ) was developed. It is a simple validated tool designed to directly measure mind-wandering trait levels. Its design offers good reliability and validity in both adult and adolescent populations (Mrazek, ), and has been validated to Chinese (Luo, Zhu, Ju and You, ) and Japanese (Kajimura and Nomura, ). In Spain, studies have been conducted on mind wandering by electroencephalography in relation to movement (Melinscak, Montesano and Minguez, ). However, no references about psychometric studies of the construct are available. For this reason, the objective of this work was to translate into Spanish and to validate the Mind-Wandering Questionnaire and to analyze its relation with the values of self-esteem, dispositional mindfulness, satisfaction with life, happiness, and positive and negative affects among adolescents.The objective of the present study was to translate into Spanish and analyze the psychometric properties of the Mind-Wandering Questionnaire (MWQ) with adolescents. Studies into this construct conducted with adolescent populations are much scarcer than those done with adult populations. Some research works have evaluated mind wandering in adolescent samples by identifying relevant indicators (Luo, Zhu, Ju, and You, ; Mrazek et al. ). The results revealed that the Spanish version of the MWQ for adolescents evidences validity and reliability.Regarding evidence for construct validity, a correlations analysis was firstly done with the five-scale items. The results showed some positive and significant results among them, with values above those obtained by Mrazek in 2013. These scores can be accounted for by the homogeneity that occurs in the scores of the variables that make up subjective well-being. Indeed, this situation has led several authors to consider the possible existence of some higher-order construct that covers several of what we now often consider to be synonyms, measured by different scales (e.g., subjective well-being, personal well-being, satisfaction with life, or happiness), which have shown significantly positive and generally high correlations with one another, and apparently overlap. Although these variables have clearly different characteristics, it is generally considered that their respective overall values are equally good indicators of subjective well-being. However, the observed correlations have not been high enough to be able to state that they measure identical constructs (Banati and Diers, ; Casas, Baltatescu, Bertr\u00e1n, Gonz\u00e1lez, and Hatos, ; Diener, et al. ; Nilsen and Bacso, ). The scores obtained with the MWQ would indicate that somehow this new construct could form part of subjective well-being.Secondly, the factorial structure of the MWQ was analyzed by a confirmatory factorial analysis. The results indicated good data fit, which corroborated the scale\u2019s dimensional structure, and also coincided with both the initial questionnaire postulates (Mrazek et al. \n                        ) and the factorial structure obtained in the original questionnaire version. The values obtained for scale reliability through Cronbach\u2019s alpha were acceptable in all the items and were similar to those found in not only the original version, but also in subsequent studies conducted in different contexts (Kajimura and Nomura, ; Luo et al. ). This could be an indication of the appropriateness of using this scale with an adolescent population. To examine the scale\u2019s concurrent validity, the structural equations model was tested, in which it was hypothesized that self-esteem, satisfaction with life, subjective happiness, positive/negative affects, and dispositional mindfulness predicted the results of the mind-wandering phenomenon. Here, gender differences were found as the results for the female participants in the MWQ scale obtained higher correlation indices with the MAAS Scale and PA, while the males\u2019 results were higher for self-esteem and NA. This gender discrepancy in the affects themes has already been pointed out by some authors (Salavera, Us\u00e1n, Anto\u00f1anzas, Teruel and Lucha, ). The multiple regression results showed that happiness, self-esteem, and satisfaction with life did not seem to influence the mind-wandering phenomenon. These three constructs have a lot to do with a person\u2019s disposition and with the subjective evaluation of his or her well-being. So up to a point, it would be logical to understand that with a phenomenon like mind wandering, the variables that require greater awareness about the subject\u2019s conscience state do not act as predictors, which was the case of the present research. Only PA had a significant and positive effect with the MWQ as high values for PA were associated with high MWQ values. There is an explanation for this as positive affect includes mood states and various emotions with pleasant, almost agreeable, subjective content, and with conditions or events that positively inform about how life is going (Luna, ), which falls in line with mind wandering. In the same way, dispositional mindfulness and NA predicted a negative and significant effect with the MWQ as high values for these variables were associated with low MWQ scores, which indicates that despite an increase in emotional regulation skills taking place in adolescence, an increase in negative affect states has also been detected during this life period (Larson, Moneta, Richards and Wilson, ). Thus the self-assessments that adolescents make can activate negative emotions, like fear, sadness, or rejection, which would explain why the mind-wandering process correlates inversely with negative affects.We should, however, point out that the present study has some limitations. Firstly, the evidence found for validity and reliability must be considered provisional as our sample size, especially males, was small. Future studies should verify gender effects with a larger study sample to evaluate the relation of these constructs over the years. Secondly, it would be necessary to test the instrument\u2019s factorial structure in different contexts. As future research lines, and like other works (Diener, Suh, Lucas and Smith, ; Hampel and Petermann, ; Mrazek, et al., ), we should make an in-depth examination of the interaction among mind wandering, psychological factors, and different life events, and continue to investigate the relation of mind wandering with different subjective well-being components (subjective happiness, self-esteem, and satisfaction with life and affects), and consider programs that promote the use of active strategies to enhance personal well-being in adolescents.To conclude, our results revealed that the Spanish version of the MWQ for adolescents offers preliminary evidence for validity and reliability, and along the same lines as the results obtained in the original version. In addition, this questionnaire could be useful for indirect measurement of the effectiveness of interventions with mindfulness. The inverse relationship found with the MAAS questionnaire (which measures mindfulness-trait) leads us to think that it can serve as an indicator of mind-wandering moments, which will decrease as the practice of mindfulness advances. Therefore, the Spanish version for adolescents may be considered a preliminary adaptation of the original questionnaire version, and the results justify its use for evaluating the mind-wandering phenomenon in the Spanish adolescent population. The present research results encourage us to continue seeking new questions to help us define new tools and methodologies and to find some answers to make progress in building mindfulness practices in adolescents."},
{"url": "https://progressinorthodontics.springeropen.com/articles/10.1186/s40510-017-0160-6", "title": "Skeletal and dentoalveolar bilateral dimensions in unilateral palatally impacted canine using cone beam computed tomography", "authors": ["Mariel Franchesca D\u00b4 Oleo-Aracena", "Luis Ernesto Arriola-Guill\u00e9n", "Yalil Augusto Rodr\u00edguez-C\u00e1rdenas", "Gustavo Armando Ru\u00edz-Mora"], "publication": "Progress in Orthodontics", "publication_date": "20 February 2017", "abstract": "The aim of this investigation was to compare skeletal and dentoalveolar measurements of subject with unilateral palatally impacted canine versus the unaffected contralateral side on cone beam computed tomography (CBCT).", "full_text": "In pathological terms, an impacted tooth can be defined as an abnormal state in which the tooth is completely or partially covered by mucoperiosteum and bone, distant from the site and time that it should be erupted in the oral cavity [\u2013]. Impacted canine in the palatal position occurs 3 to 6 times more often than buccal position [, ]. Impacted canines are twice as common in women as in men, and the incidence in the maxilla is more than double compared to the jaw []. Unerupted canines are the second most common group suffering impaction surpassed only by impacted third molars, its reported prevalence varies from 0.2% to 2.8% [, ]. Two main theories have been proposed to explain the emergence of palatal impacted maxillary canines: the \u201corientation\u201d and \u201cgenetic\u201d theories []. The lack of space in the dental arch can prolong retention of deciduous canines. Adjacent lateral incisors absence, root dilaceration, and ankyloses of the permanent canines are the most common local factors associated with maxillary impacted canines [\u2013].Investigations pointed out a lack of the accurate characterization of alveolar bone dimensions and the environment in the affected area [, ]. The impaction can lead to reduced bone dimensions, or affect dental angulations of the nearby teeth. There are a few studies [, ] comparing specifically the impacted area with the area that had adequate canine eruption in the same individual. These results indicate the consequences generated by the impaction of a canine. Kanavakis et al. [] concluded that the root of lateral incisors adjacent to palatal impacted canines is angulated more mesially compared to that of lateral incisors adjacent to normally erupted canines.With the advent of cone beam computed tomography (CBCT), three-dimensional representations (3D) of the teeth and bone are presented in high resolution. Tadinada et al. [] reported that alveolar bone dimensions (buccal-palatal width and height of the nasal floor to the alveolar ridge) and the perimeter of the arch are significantly reduced in the impacted side when compared with the non-impacted side. However, they did not evaluate lateral angulations of the long axis of incisors, neither the nasal cavity width nor lateral basal width, which also could be affected. It has been stated that maxillary transverse discrepancies increase the possibility of impacted canines []. According to Becker et al. [], three-dimensional and unilateral precise determination of the position of impacted canines is important for the clinician to determine the prognosis of an aligned tooth on the dental arch.The literature [, ] has little information about how the morphology and maxillary dimensions can affect the eruption and subsequent impaction of maxillary canines. For these reasons, the aim of this investigation was to compare skeletal and dentoalveolar dimensions in a sample with unilateral palatally impacted canines versus the unaffected side. Analyzing the characteristics of these dimensions and determining how they influence the impacted canines on vertical and transverse measurements using coronal and axial views on CBCT have been little reported in the scientific literature.This retrospective and cross-sectional study (including a split mouth design) was approved by the Ethics Committee of Cient\u00edfica del Sur University, Lima, Peru, with the No. of approval 000258.The sample consisted of CBCTs with unilateral palatally impacted canines. CBCTs of subjects attended in \u201cImaging Diagnostic Center CDI\u201d Lima, Peru, from January 2010 to December 2015 were included. The sample size calculation required 25 sides with and 25 sides without impacted maxillary canine. We calculated this sample considering a mean difference of 6\u00b0 in the lateral incisor angulation as a clinically relevant difference between sides with and without impacted canine. A standard deviation of 8.58\u00b0 was considered (obtained from a preliminary pilot study) with a two-sided significance level of 0.05 and a power of 80%. Although a minimum of 25 sides were required, we included 28 sides with and 28 sides without impacted maxillary canine (in overall 56 sides). This amount was selected from a sample of 960 CBCTs.The inclusion criteria for images selection were CBCTs of children or adults over 15\u00a0years old of both sexes, periodontally healthy, and with canines fully calcified, including unilateral palatally impacted canines located in the maxillary []. We included impacted canines located in sector 2 (if the cusp tip of the canine is between the major axes of the lateral and central) and sector 3 (if the cusp tip of the canine is between the major axis of the lateral and the first premolar) as rates by Erikson and Kurol []. Both groups were included as one in the sample. We include cases with deciduous canines in occlusion on the affected side to avoid a lack of arch development.Exclusion criteria were subjects with previous orthodontic treatment, dento-maxillary traumas, maxillary canine transpositions, agenesis, craniofacial malformations, odontogenic pathologies, and CBCTs including impacted maxillary bilateral canines and bucally impacted canines.Unilateral palatally impacted maxillary canines represent an asymmetric dentoalveolar and/or basal bone structure of the right or left anterior segment of the maxillae []. The main objective of this study was to compare the skeletal and dentoalveolar dimensions of the maxillae in a sample with unilaterally impacted canines versus the contralateral unaffected side. There are a few studies [, ] with similar methodology but did not use CBCT or did not include all variables like this study; also, coronal and axial views have been little reported in the scientific literature.In order to avoid measurement and sample selection bias, a pilot study was performed to ensure the reliability of the results. Furthermore, the intra-observer concordance was almost perfect for bone measurements and dentoalveolar dimensions of the sample, ensuring the reliability of the measurements.An important limitation to be considered was the relatively small sample size of this study, but this was due to the selection of CBCTs only in patients with unilateral palatally impacted canines, with a prevalence that ranges from a minimum of 0.92% to a maximum of 4.3% [, ]. We found this required sample from a total of 960 CBCTs (28 CBCTs, 2.94% of prevalence), although this prevalence of impacted canines in the sample did not depict the prevalence in the overall population because our sample was formed by CBCTs of patients that assisted to one radiological images center seeking orthodontic or surgical diagnosis.In this study, the most prevalent gender was females, confirming that the impacted upper canines are produced twice as common in women than in men, with a ratio of 2 or 3 to 1 [\u2013]. One main reason because the women were more prevalent in this sample added to etiological factor is probably the mere fact that women are esthetically more oriented to get orthodontic treatment.When comparing the bone heights and dentoalveolar dimensions of the sample according to the condition of the impacted canine side, no statistically significant differences were found. The study of Tadinada et al. [] showed different results, the alveolar bone dimensions (bucco-palatal width and length of the nasal floor to the alveolar ridge) and the maxillary arch perimeter were significantly lower on the impacted side, compared to that on the not impacted side. However, we think that the incisor heights should not affect because the sequence of eruption of incisors is prior to canines. In our study, all subjects had Latin American origin, and great variability related to the crown size was not expected; finally, we did not find difference into these heights between both sides (with and without impaction).Jacoby [] reported that 85% of palatal impacted canines were in patients with an adequate perimeter arc. Similarly, Stellzig et al. [] reported that there was enough perimeter of the arc in 82% of palatal impacted canines. However, in our study, significant differences were observed on the measurements from the mid-palatine raphe to the first premolar since the affected side was significantly lower than the non-impacted side. This was because the side of the impacted canine have not been sufficiently developed, compared with the unaffected side where canines have normally eruptions. In our sample, we found all deciduous canines in occlusion is possible that the lack of permanent canine eruption can affect the inter-canine distance and the transversal measurements; however, more future studies can compare these measures in cases with or without persistence of deciduous canine because the lack of tooth mass could account for a lack of arch development. Similar results were found by Tadinada et al. []; the length reduction of the arc on the affected side may also be due to the lack of eruption of the impacted permanent canine.The clinical significance of our findings with respect to treatment implies a greater attention to correct the transverse asymmetries mainly at level of the first premolar on the side that includes an impacted canine. The severity of this asymmetry (approximately 2\u00a0mm between both sides) should be corrected only with dental alignment; however, in cases of greater asymmetry including unilateral cross bite, the asymmetric expansions should be taken into account [].Likewise, statistically significant differences were observed when the lateral angulations of the long axis of incisors were compared according to the side of the impacted canine. The lateral angulation of the long axis of the incisors was lower on the impacted side presenting disto-angulated incisors on the side of impacted canine and mesial-angulated on the non-impacted side.Meanwhile, the lateral angulation of the long axis of the canines showed greater angulations on the impacted side compared to the non-impacted side with mesial tipping in the impacted canine. This was similarly presented in the study of Hanke et al. [] where the inclinations and lengths of vectors for impacted canines were higher (mesial tipping) than in those non-impacted canines. The inclinations of the long axis of the canines in relation to the three reference planes are particularly suitable for comparisons, and in their study, significant differences were detected (\u2009<\u20090.001). Similarly, this was also reported by Kanavakis et al. [], where the crown-root angulation of lateral incisors adjacent to palatal impacted canines differ compared to that of lateral incisors adjacent to normally erupted canine, but this study was made on the panoramic radiographs, where the long axis of the root of the lateral incisors adjacent to palatal impacted canines form a more mesial angle to the crown (approximately 2.5\u00b0), when compared to the lateral incisors adjacent to normally erupted canines.The present study included impacted canines located in sectors 2 and 3 as rated by Ericson and Kurol []; we did not included sector 1 because this condition is less frequent than the other two and the effect on the angulations of incisor is more expected, and ideally, we should form three groups according to this condition, but due to the small sample managed in the study, we did not classified the sample into these groups. It is recommended the use of this classification on future research. Other recommendation for future studies is the use of 3D analytic techniques for evaluation of shape differences between both sides with or without canine impaction, and this would provide other information about bone contours and bone volume; specifically, in this paper we attempt to compare measurements easily recognized by orthodontists and with clinical value, mainly in the coronal and axial views.Probably, the orthodontic treatment in unilateral palatally impacted canine requires its previous traction; the alignment of the incisors without distancing the impacted canine could expose the roots of the incisors with the impacted canine due to their distal angulation with respect to the opposite side without impaction. Furthermore, the orthodontists should have a greater attention to correct the transverse asymmetries mainly at level of the width from median raphe to first premolar on the affected side with an impacted canine.In conclusion, three measurements on the side of impacted canines were significantly lower than on the side without impaction; width from premolar to the mid-palatine raphe and lateral angulations of incisors (lateral and central) showed significant reduction, presenting disto-angulated incisors on the side of impacted canine.The orthodontic treatment of unilateral maxillary impacted canine should correct the transverse asymmetry mainly at level of the premolar width on the affected side with respect to the not impacted side and prevent the contact of the root of the incisors with impacted canines due to the disto-angulation of the lateral and central incisors on the affected side."},
{"url": "https://prc.springeropen.com/articles/10.1186/s41155-017-0068-6", "title": "A validation study of the Multidimensional Life Satisfaction Scale for Children", "authors": ["Cynthia Cassoni", "Edna Maria Marturano", "Susana Coimbra", "Anne Marie Fontaine"], "publication": "Psicologia: Reflex\u00e3o e Cr\u00edtica", "publication_date": "6 July 2017", "abstract": "Recent studies on the life satisfaction in children and young people have investigated its association with vulnerability, discrimination, the individual\u2019s school environment and network of relationships, and mental health. The growing interest in the area demands instruments with good psychometric properties.", "full_text": "Studies in the area of psychology have turned to examining the role of life satisfaction in the adjustment of people in life stages other than adulthood, as a reflection of society and the changes it has experienced. Research that initially focused on adults (e.g., Solomon & Winfield, ) was extended to adolescents and children in the 1970s and 1980s (see Daly & Carpenter, ; Lessing, ). A search made by the authors in the PsycNET database indicates that more than 6000 articles on life satisfaction have been published up to 2016, but only a quarter of them focus on adolescents (13\u201317\u00a0years) and schoolchildren (6\u201312\u00a0years).Satisfaction with life can be defined as the overall assessment of quality of life made by people according to their own criteria (Diener, Suh, Lucas, & Smith, ; Huebner, ) or as a cognitive response referring to evaluative judgments of satisfaction with life as a whole or in its multiple dimensions (Lucas, Diener, & Suh, ). This definition somehow implies the cognitive and emotional ability to consider and weigh several different sources of information.For Lucas, Diener, and Suh (), satisfaction with life is (along with positive and negative affect) one of the dimensions of subjective well-being that refers to the frequency of positive or negative feelings. In other studies, however, the concepts of satisfaction with life and subjective or general well-being are usually superimposed or interchangeable (Cheung & Lucas, ; Kuppens, Realo, & Diener, ; Lyubomirsky, King, & Diener, ; Pavot & Diener, ; Poletto & Koller, ).Studies on school-age children and adolescents have found associations between satisfaction with life and vulnerability and discrimination (Gilligan & Huebner, ; Poletto & Koller, ), school environment (Gilligan & Huebner, ; Poletto & Koller, ), work (Arteche & Bandeira, ), and an individual\u2019s network of relationships (Serafini & Bandeira, ). Satisfaction with life has also been associated with perceptions of support from teachers (Guess & McCane-Bowling, ) and parents (Jiang, Huebner, & Hills, ), as well as satisfaction with the neighborhood (Shin, Morgan, Buhin, Truitt, & Vera, ). Satisfaction with life also predicts externalizing and internalizing problems (Gao et al., ; Haranin, Huebner, & Suldo, ).Given the multiplicity and diversity of these associations, researchers have proposed models that cover the unidimensionality and multidimensionality of life satisfaction. The unidimensional, general, or global models (Diener, Emmons, Larsen, & Griffin, ; Pavot & Diener, ; Huebner, ) use a single scale to indicate levels of satisfaction with life in general. The multidimensional models (Giacomoni & Hutz, ; Huebner, ) provide a profile of satisfaction with life in various specific domains.Proctor, Linley, and Maltby () have reviewed the instruments developed from the unidimensional and multidimensional models to assess life satisfaction. Among the models of the second group, the authors present the Multidimensional Students\u2019 Life Satisfaction Scale (MSLSS) (Huebner, ), a 40-item self-report scale designed to provide a profile of life satisfaction within five specific domains (family, friends, school, context, and self) and an overall assessment of general life satisfaction. The MSLSS is applicable for use with students aged 8\u201318. High indices of internal consistency were reported for each dimension (.82 for family, .85 for school, .85 for friends, .83 for context, and .82 for self) and for the full scale (.92).Today, the MSLSS is used in different cultures, having been adapted and validated for young people in France (Fenouillet, Heutte, Martin-Krumm, & Boniwell, ), Iran (Hatami, Motamed, & Ashrafzadeh, ), Turkey (Irmak & Kuru\u00fcz\u00fcm, ), Serbia (Jovanovic & Zuljevic, ), and Italy (Zappulla, Pace, Cascio, Guzzo, & Huebner, ). The studies of adaptation for these countries clearly presented the stages of translation and semantic adaptation, exploratory and confirmatory factor analysis (CFA), and reliability indices of the instrument, maintaining the five-dimension structure, with good indices of fit and satisfactory internal consistency values (Cronbach\u2019s alpha greater than .70).The studies also explored the construct validity of the scale. For convergent validity, instruments were used to measure self-esteem (Jovanovic & Zuljevic, ; Zappulla et al., ), self-concept (Zappulla et al., ), positive relationships with others (Zappulla et al., ), positive affect (Jovanovic & Zuljevic, ), and other instruments of life satisfaction (Fenouillet et al., ; Irmak & Kuru\u00fcz\u00fcm, ). For divergent validity, instruments were used to measure depression (Fenouillet et al., ; Jovanovic & Zuljevic, ; Zappulla et al., ), mental health (Zappulla et al., ), and anxiety and stress (Jovanovic & Zuljevic, ).Giacomoni and Hutz () developed the Multidimensional Life Satisfaction Scale for Children (MLSS-C), a Brazilian instrument, based on the scale originally developed by Huebner (), seeking to adapt Huebner multidimensional model to the characteristics of the Brazilian culture. To ensure the ecological validity of the scale, the authors derived the MLSS-C items mainly from interviews conducted with Brazilian children aged 5\u201312.The MLSS-C contains 50 items, distributed among six dimensions, including domains of life satisfaction not covered in Huebner\u2019s original model, such as compared-self and non-violence. The  dimension consists of items that describe the self with positive characteristics, such as self-esteem, good mood, the ability to relate, and the ability to show affection. The  dimension groups items characterized by evaluations comparative with peers. The  dimension includes items with content associated with aggressive behaviors. The  dimension includes items that describe a healthy, harmonious, and affectionate family environment and satisfying relationships. Items in the  dimension refer to relationships with peers, the level of satisfaction in these relationships, and some indications of leisure, fun, and support. Finally, the  dimension includes items that describe the importance of school, the school environment, interpersonal relationships in this space, and levels of satisfaction with this environment.Giacomoni and Hutz () investigated the psychometric properties of the MLSS-C. They first conducted an exploratory factor analysis (EFA) with a sample of 661 students aged 7\u201312 (mean 10.6) from public and private schools in Porto Alegre in Rio Grande do Sul. The EFA provided a six-factor structure as the best solution, with factors corresponding to the proposed model. Five dimensions presented internal consistency rates ranging from .82 and .86; only one dimension, non-violence, obtained an index of less than .70 (.66). The total Cronbach\u2019s alpha was .93. In a further study with 230 children, the authors found moderate positive correlations between all dimensions and a measure of self-esteem, as well as moderate negative correlations with measures of depression and anxiety (Giacomoni & Hutz, ).The current evidence of adequate psychometric properties of the MLSS-C indicates that it is worth pursuing the validation process. As part of this process, there is a need to confirm the factorial structure derived from the EFA. Will the six-factor model maintain in a CFA? Given Brazil\u2019s cultural diversity, will data from children in another region fit the same model? These questions guided the present study, which aims to validate the structure of the scale using CFA in a sample of children living in the state of S\u00e3o Paulo, Brazil. The sample participated in a broader prospective study from the fifth to the sixth year of elementary school.In addition to the factorial validity, other contributions to the construct validity are presented through the study of some indicators of discriminant, convergent, and divergent validity. For the discriminant validity, each dimension\u2019s average variance extracted (AVE) must be compared to its squared correlations with other dimensions in the model (Fornell & Larcker, ).Convergent validity is demonstrated when there are correlations between instruments that measure similar traits or characteristics (Pasquali, ). Social skills and self-concept constructs were therefore chosen which (according to the literature) are moderately and positively associated with self, family, school and friendship dimensions of life satisfaction as measured by the MSLSS, an instrument similar to the MLSS-C (Jovanovic & Zuljevic, ; Zappulla et al., ). The hypotheses for the convergent validity analysis are that the dimensions of life satisfaction will positively correlate with positive self-concept and social skills.Divergent validity is achieved when no correlations are obtained between the measures (Pasquali, ). The hypothesis is that life satisfaction will not correlate with stress symptoms, since it is described as a different construct not associated with life satisfaction, as indicated by Jovanovic and Zuljevic ().Test-retest consistency, or stability, was assessed to verify the reliability of the instrument, as well as the internal consistency of the various dimensions of the MLSS-C using Cronbach\u2019s Alpha () and composite reliability (CR).This study aimed to contribute to the progressive ecological validation process of the MLSS-C, initially constructed for Brazil by Giacomoni and Hutz (). This instrument was based on the scale developed by Huebner (), and the aim of the current study was to improve its accuracy and appropriateness for use with Brazilian children. CFA was used to test the theoretical structure of the instrument from the empirical data (Mar\u00f4co, ). Through the CFA and other construct validity indicators, this study confirmed the feasibility of the model with six dimensions proposed by the authors.The dimensions of the MLSS-C cover different positive characteristics such as self-esteem, a good mood and the ability to relate, and perceived relationships within different contexts (family, school, and peers). The information assessed through the MLSS-C also comprises information regarding perceived satisfaction with fun and support, the ability to demonstrate affection, and interpersonal relationships in different contexts. It is a self-report measure that elicits self-assessment and social comparisons (with peers), leading to the production of both global and dimensional scores.In addition to the CFA, the convergent, divergent, and discriminant validity of this scale was investigated. Our results seem to support convergent validity, since all MLSS-C dimensions show a moderate positive correlation with self-concept and social skills. The results concerning social skills are in line with those reported for the MSLSS by Jovanovic and Zuljevic () and Zappulla et al. () with adolescents. Both studies showed associations between dimensions of life satisfaction and positive relationships. Concerning self-concept, there is also some previous evidence of association between life satisfaction and positive self-evaluations in children and adolescents. Giacomoni and Hutz () found a positive correlation between a measure of self-esteem and the six dimensions of the MLSS-C. More recently, Zappulla et al. () reported positive correlations between self-acceptance and all dimensions of the MSLSS.The results clearly support divergent validity for the school dimension of life satisfaction in relation to stress. For self, friendship, and non-violence, the correlations with stress were very low and negative. For compared self, family, and the total MSLSS score, low negative correlations with stress were observed. These last results are consistent with the findings reported by Jovanovic and Zuljevic () in their study with adolescents aged 16\u201319. Since we worked with younger subjects in a narrow grade school range, the pattern of results we found concerning stress deserves further attention, especially with regard to its longitudinal evolution.The discriminant validity was mostly satisfactory (below .50 for all factors). Results concerning AVE and the correlation matrix between each factor and total score suggest that the appraisals of life satisfaction are quite differentiated by domain among children of this age. The self and family dimensions were more associated with the global score, probably because they both include items that describe affection and positive characteristics of self and because they are still less influenced by meso- and macro systems\u2019 feedback.The internal consistency estimates were acceptable for five of the six dimensions. The non-violence dimension, however, presented a low Cronbach\u2019s alpha value and low CR. Possibly, the small number of items (there were only two: \u201cI fight a lot with my friends\u201d and \u201cFighting solves problems\u201d) influenced this result. It is worth noticing that a lower Cronbach\u2019s alpha value was also observed in the original version, which had four items, including \u201cI like fights\u201d and \u201cI\u2019m angry\u201d (Giacomoni & Hutz, ). This dimension thus seems less reliable for drawing conclusions about children\u2019s life satisfaction, probably because of social desirability and/or the lack of cognitive maturity for self-assessment.Given the low consistence of this dimension, it can be argued that it should be dropped out of the scale. Nevertheless, the authors consider it untimely to do so at this stage of MLSS-C validation, since later studies may develop or improve it. As Giacomoni and Hutz () suggested, one possible direction for this improvement would be to increase the number of the items of the subscale. The non-violence dimension needs further exploration because of its striking importance for primary and secondary prevention of social victimization.A possible limitation of this study lies in the timing of the retesting, which occurred after an important contextual change: the transition from elementary school I to elementary school II, between the fifth and sixth year. This transition requires adaptation in the children, as it involves changes that have effects ranging from the daily routine to the composition of the peer group, when students from the same class are allocated to different schools. This situation probably increased the variation of the results in the sixth year, as the transition is associated with positive consequences for some adolescents in terms of quality of life, while for others, it can lead to negative results (B\u00e9langer & Marcotte, ). The bias thereby introduced increases the probability that the stability of MLSS-C has been underestimated, since the children were experiencing changes due to the transition when responding to the scale for the second time.Another limitation of this study concerns the composition of the sample. While representativeness was ensured in relation to the pool of the city\u2019s public schools, the exclusive focus on public school students in their fifth year may have restricted the variability of the data, affecting the results of the correlations.The growing interest in the area of child and adolescent life satisfaction demands instruments with good psychometric properties. This study aimed to contribute to the progressive validation process of the MLSS-C. Together, the reported results broaden the evidence basis for the reliability and validity of this scale, suggesting that it is suitable for use with Brazilian schoolchildren in terms of both research and practice. Its multidimensional nature allows for obtaining global and specific scores, providing detailed information regarding the life satisfaction profile of the children. It would be important to add more waves to the research to investigate its longitudinal predictive power, as well as the relationship and/or differentiation between dimensions as children grow older and progress through school.Considering the considerable cultural and socioeconomic diversity of Brazil, future studies should include samples from different states and cities to consolidate the ecological validity of the instrument. For further evidence of the scale\u2019s external validity, it would also be interesting to triangulate information obtained through self-report with quantitative and qualitative information collected from parents, teachers, and other significant people in the life of the children."},
{"url": "https://progressinorthodontics.springeropen.com/articles/10.1186/s40510-017-0172-2", "title": "Stress and displacement patterns in the craniofacial skeleton with rapid maxillary expansion\u2014a finite element method study", "authors": ["J. Priyadarshini", "C. M. Mahesh", "B. S. Chandrashekar", "Abhishek Sundara", "A. V. Arun", "Vinay P. Reddy"], "publication": "Progress in Orthodontics", "publication_date": "10 July 2017", "abstract": "Rapid maxillary expansion (RME), indicated in the treatment of maxillary deficiency directs high forces to maxillary basal bone and to other adjacent skeletal bones. The aim of this study is to (i) evaluate stress distribution along craniofacial sutures and (ii) study the displacement of various craniofacial structures with rapid maxillary expansion therapy by using a Finite Element model.", "full_text": "Orthodontics and dentofacial orthopedics have changed from an opinion-based practice to evidence-based practice. Rapid maxillary expansion (RME) is indicated in the treatment of maxillary deficiency. During RME, high forces directed to the maxillary basal bone and perhaps to other adjacent skeletal bones can easily split the mid-palatal suture in young individuals and force the two maxillary halves laterally. Midfacial orthopedic expansion has been recommended for use in conjunction with protraction forces on the maxilla because it disrupts the circum-maxillary sutural system and presumably facilitates the orthopedic effect of the facemask [, ]. Various craniofacial areas especially in the areas of articulation of the maxilla are also effected []. A recent study used cone-beam computed tomography imaging to conclude that RME resulted in forward movement of the maxilla as well and also vertical dentoalveolar changes [].It is still an enigma as to where these areas of maximum force concentration are and how these heavy forces get dissipated. Sutures undergo anabolic changes such as increased sutural width, angiogenesis, and bone apposition in response to anteriorly directed forces. Studies provided histologic descriptions of sutural response to both tensile and compressive forces. Bone formation has been observed at the edges of the mid-palatal suture after rapid palatal expansion [].In recent years, finite element method (FEM) has been a powerful research tool for solving various structural mechanical problems. It is recognized as a general procedure for mechanical approximation to all physical problems that can be modeled by differential equation description [].Previous study assessed the stress and displacement in the maxilla alone []. The present study encompasses the changes produced in the entire craniofacial complex and was planned to explore how heavy transverse orthopedic forces generated by RME get dissipated within the craniofacial complex and to evaluate the pattern of stress accumulation, dissipation, and displacement of various vcraniofacial structures with RME appliance using a three-dimensional FEM study by using a model that better represents the human skull than previously presented.CT scan images of the skull were taken in axial direction parallel to the F-H plane at 1-mm interval. CT scan images containing cloud data point information were processed using Mimics software, and the required portion of the skull was exported into a stereo-lithography model. Files in stereo-lithography (STL) format were imported into rapid form software to create the surface data. Then, the surface model in IGES (Initial Graphics Exchange Specification) format was exported to HYPERMESH. The process of converting a geometric model into a finite element model is called meshing. The finite element model consists of 115,694 nodes and 537,684 elements. In this study, the model consisted of 694,164 degree of freedom. The discretized FE model is shown in different views in Fig.\u00a0 (frontal view of the three-dimensional FE model), Fig.\u00a0 (lateral view of the three-dimensional FE model), Fig.\u00a0 (basal view of the three-dimensional FE model), and Fig.\u00a0 (palatal view of the three-dimensional FE model).ANSYS software was used to solve the mathematical equation and to calculate the stress and displacement pattern of the skull. Post processing was the last stage of the FEM in which contour plots of the displacement and stresses was obtained from the results of the analysis performed.Skeletal Class III malocclusions appear in various conditions and patterns. It is caused by abnormal growth of the jaws or growth disharmony, as overdevelopment of the mandible, underdevelopment of the maxilla, or a combination of both. There are many controversies concerning the treatment modalities and treatment timing of skeletal Class III malocclusions with respect to skeletal and dental discrepancy, age, and residual growth. A reduction in the growth of the maxilla is caused not only by the antero-posterior divergence but also by a transverse variation, resulting, in many cases, in posterior crossbites [].Haas reported on the orthopedic effect of RME, which produced a forward and downward tipping of the maxilla with concomitant downward and backward mandibular rotation\u00a0[]. These orthopedic changes facilitated the correction of a mild Class III malocclusion. RME is effective for correction of transverse discrepancies and also for protraction of the maxilla by remodeling the nine circumaxillary sutures\u00a0[] .When Yu et al. compared the effect of maxillary protraction with and without RME, they concluded that opening the mid-palatal suture using a RME appliance and directing the protraction force inferiorly from the occlusal plane, passing through the maxillary center of resistance and also through the apical portion of the first premolar, maxillary protraction that is similar to normal downward and forward growth of the maxilla can be effectively achieved [].In yet another study by Park et al. to analyze displacement and stress distribution in the maxilla during maxillary expansion followed by protraction using bone-borne and conventional tooth-borne palatal expanders and a facemask via three-dimensional finite element analysis, they concluded that the group with only facemask resulted in more anterior displacement of the maxilla than the combination of facemask and bone-borne expanders. Further, tooth-borne expanders reported more anterior movement of the maxilla when protraction was combined with expansion and compared with protraction without expansion. This is due to the synergistic effect when combined with the facemask as reported by Yu et al. [].The three-dimensional FEM used in the present study provides the freedom to simulate orthodontic force systems applied clinically and allows analysis of the response of the craniofacial skeleton to the orthodontic loads in three-dimensional space. The FEM has several advantages, and the study can be repeated as many times as the operator wishes [].Sequential CT scan images were taken at 1-mm intervals to reproduce finer and detailed aspects of the geometry. Previous studies reported sections at 10-mm [, ], 5-mm [, ], and 2.5-mm [] intervals. This study differs from previous studies in the type of element used for meshing and solid tetrahedral elements used, each having 10 nodes, giving better stress transmissibility and bending deformations. Therefore, this model is a better representation of a real human skull than the previous models [, ]."},
{"url": "https://progressinorthodontics.springeropen.com/articles/10.1186/s40510-017-0180-2", "title": "A retrospective cephalometric study on upper airway spaces in different facial types", "authors": ["Roselaine Sprenger", "Luciano Augusto Cano Martins", "J\u00falio Cesar Bento dos Santos", "Carolina Carmo de Menezes", "Giovana Cherubini Venezian", "Viviane Veroni Degan"], "publication": "Progress in Orthodontics", "publication_date": "21 August 2017", "abstract": "Craniofacial growth pattern has been correlated with variations in size of the upper airway spaces. The objective of this study was to evaluate the nasopharyngeal, oropharyngeal, and hypopharyngeal airway spaces variations according to the craniofacial growth pattern, by comparing brachyfacial, mesofacial, and dolichofacial in Angle Class I individuals.", "full_text": "The upper airway is composed of the nasopharynx, oropharynx, and hypopharynx. Pharyngeal space size is determined primarily by the relative growth and size of the soft tissues surrounding the dentofacial skeleton [, ].A normal upper airway improves nasal breathing and is considered important in the growth and development of craniofacial structures [, ].An obstructive upper airway is present when obstructive processes of a morphological, physiological, or pathological nature occur, such as hypertrophy of adenoids and tonsils, chronic and allergic rhinitis, irritant environmental factors, infections, congenital nasal deformities, nasal traumas, polyps, and tumors cause functional imbalance and result in oral breathing patterns [].The upper airway dimensions may be influenced by the facial skeletal pattern, in which the relationship between the position of the maxilla and mandible in the anteroposterior direction has great influence on space [].There are studies in the literature about changes in the upper airways resulting from orthodontic treatment, orthognathic surgery or in individuals diagnosed with sleep apnea [\u2013]; however, few studies have shown evidence of the airspace related to facial types and Angle Class I individuals, and this information is relevant to assist in orthodontic planning.The aim of the present study was to evaluate the nasopharyngeal, oropharyngeal, and hypopharyngeal airway spaces in brachyfacial, mesofacial, and dolichofacial in Angle Class I individuals.The Intraclass Correlation Coefficient (ICC) showed excellent replicability (0.9636).In the comparison of the three groups for each of the 14 sleep apnea cephalometric measures, statistically significant difference was verified among the groups for the median posterior palatal space (\u2009=\u20090.020), with the complementary Tukey test pointing out difference between the brachyfacial and dolichofacial groups.For the other measurements, there was no statistically significant difference (\u2009>\u20090.05). However, for the dolichofacial group, it was observed that on an average, numerically, the inferior pharyngeal space measurements (12.79\u2009\u00b1\u20094.72), atlas-maxilla distance (35.22\u2009\u00b1\u20093.25), and posterior airway space (12.31\u2009\u00b1\u20092.86) were shown to be smaller in comparison with those of the other facial types.The contribution of this study was to present the use of a cephalometric analysis, commonly used in orthodontics, with the aim of identifying anatomical changes in the upper airways, which may predispose to respiratory disorders.Cephalometric performed by means of lateral teleradiography has been shown to be an important instrument in the multidisciplinary field for evaluating the upper airways [, ] because it is easily accessible and low cost, highly reproducible, and the individual is submitted to a low dose of radiation [, ]. Thus, innumerable studies have sought associations of the physical characteristics related to these airway spaces, as a way of predicting pathologies [\u2013].However, there are studies that use computed tomography for morphological evaluation of the airway spaces, particularly due to the possibility of measuring areas and volumes, which is impossible to do by means of other radiographic exams [\u2013]. Therefore, the authors point out that one of the limitations of the present study refer to not measuring the airway volumes, due to the type of exam used for evaluation []. There was also difficulty in the methodologies with two-dimensional radiographs when performing superimposition of tracings []; however, in the present study, no superimpositions were made. Many studies have evaluated the airways by using lateral cephalograms and associated their dimensions with the vertical skeletal pattern of the face and facial morphology [, , ]. A recent longitudinal study also used lateral cephalometric radiographs for associating changes in the morphology of the nasopharyngeal space in different facial patterns [], which did not make this method of evaluation unfeasible.In this study, the authors opted to use the sleep apnea cephalometry instrument because it has been validated for Brazilians and presents standard values that may be used as reference [].The authors were able to identify reduction in the median posterior palatal space in individuals with a dolichofacial pattern. A previous study also observed changes in the dimensions of the upper airway related to the reduction in the medial posterior palatal space in individuals with the obstructive sleep apnea syndrome (OSAS) []. This measurement expresses the distance from the soft palate to the posterior wall of the pharynx and has a close relationship with the dimensions of the soft palate. The increased length of which was related to presence of OSAS in other researches [, , ] and the present study. The highest alteration values in upper airway dimensions in OSAS patients occur in the oropharynx [] and were related to the reduction of the median posterior palatal space []. In individuals with a vertical pattern, the mandible is normally retracted and rotated downwards and backwards, thus diminishing the oropharyngeal space [] Furthermore, the base of the tongue accompanies the direction of mandibular rotation, being positioned downward and backward, thus the soft palate is in a more retrusive position, diminishing the median posterior palatal space.Some authors have pointed out that when the nasopharyngeal space was reduced, there would be a tendency towards neuromuscular adaptation, leading to vertical growth of the face that is associated with a dolichofacial pattern [, ]. However, in this study, no difference was found in the upper airway dimensions in the nasopharyngeal region in the studied volunteers with this facial type. This could be attributed to different sample characteristics in others studies in which the sample was composed of the youngest participants [, ]. This aged group could be more susceptible to narrower nasopharyngeal airway spaces due to adenotonsillar hypertrophy, for example [, ]. In addition, authors [] compared Angle Classes I and II, differently from the present study, in which the sample was composed of only Class I patients.Obstruction of the upper airways forces the patient to breathe through the mouth, and in addition to OSAS, these factors cause oral dysfunction, such as lip incompetence, low position of the tongue in the floor of the mouth, tongue thrust, and may lead to unbalanced muscle and function [] disturbances in swallowing, mastication, speech [], and stability of occlusion [].This study was performed using two-dimensional digital lateral cephalogram that is a limitation. Therefore, it is important to recognize that three-dimensional evaluation of the airways by means of cone-beam computed tomography, respecting legal and ethical aspects, due a higher dose of radiation, could be useful for improved assessment in further studies to minimize this limitation.Under the conditions of this study and considering the results, the authors could conclude that there were no differences in nasopharyngeal and hypopharyngeal airway spaces in brachyfacial and mesofacial individuals. Oropharyngeal space reduction was observed in Angle Class I dolichofacial individuals, characterized by reduction in the median posterior palatal measurement."},
{"url": "https://progressinorthodontics.springeropen.com/articles/10.1186/s40510-016-0157-6", "title": "Comparison and evaluation of stresses generated by rapid maxillary expansion and the implant-supported rapid maxillary expansion on the craniofacial structures using finite element method of stress analysis", "authors": ["Varun Jain", "Tarulatha R. Shyagali", "Prabhuraj Kambalyal", "Yagnesh Rajpara", "Jigar Doshi"], "publication": "Progress in Orthodontics", "publication_date": "16 January 2017", "abstract": "The study aimed to evaluate and compare the stress distribution and 3-dimensional displacements along the craniofacial sutures in between the Rapid maxillary Expansion (RME) and Implant supported RME (I-RME).", "full_text": "The rapid maxillary expansion is the treatment of choice in cases of malocclusion involving the transverse maxillary deficiencies and the class III malocclusion. In case of transverse maxillary deficiency, the orthopedic forces of rapid maxillary expansion will bring about the dental as well as the skeletal expansion of the narrow maxilla to fetch the space for relieving of the crowding or the proclination or to level the bite [, ] and it also increases the nasal permeability and nasal width and straightens the nasal septum [\u2013].Whereas, in class III malocclusion cases, the loosening of the circumzygomatic sutures will make the maxilla pliable enough to respond to the orthopedic protrusive forces of the protraction face mask []. All the above said changes are applicable to the patients who are growing, and the adult patients who require the similar changes have to undergo surgically assisted rapid maxillary expansion [] procedure, which is quite invasive. The alternative is to go ahead with the ankylosed tooth as a support [] or else to utilize the osteosynthesis plates for expansion. But these have their own set of disadvantages like invasive operation, with a higher risk of infection and speech problems as the appliance limits the tongue movement [, ]. Apart from this, the traditional RME appliances at certain times are bound to produce the side effects like root resorption, bony dehiscence, and decreases in the thickness of the buccal cortical plate, undesirable tooth movements, and relapse and loss of buccal cortical bone at the anchorage teeth.As a replacement, we can utilize the properties of orthodontic implants to apply the force on the palatal shelves through the medium of appliance to obtain the orthopedic changes and such appliance are known as implant-supported rapid maxillary expansion appliances (I-RME). These appliances apply the force directly on to the implant embedded in the bone, thus overcoming the disadvantages of the earlier appliances. As they are anchored to the palate, it is anticipated that a more efficient skeletal expansion and decreased undesired dental effects are produced [\u2013].The literature pertaining to the impact of rapid maxillary expansion on different circumzygomatic bones is only limited to the traditional appliances, and there are very few articles which have explored the possibilities of the implant-supported RMEs []. Different designs of micro-implanted supports for anchorage control are different from one study to the other. Thus, the current study plans to compare the effects of the traditional RME with that of the implant-supported RME using the finite element method of the stress analysis. The finite element analysis (FEA) has proven its worth in the field of orthodontics since long [, \u2013], and the present study utilizes FEA\u2019s ability of virtual model construction and the stresses analysis with the hypothesis that the implant-supported RME produces the similar effects as that of the simple RME on the different craniofacial sutures.Initial step in the creation of the finite element model of the skull involved the obtaining of the CT scan images of the skull of the 12-year-old boy using an X-Force/SH spiral CT scan machine (manufactured by Toshiba, Japan). The CT scan sections were obtained from DICOM images (Digital Imaging and Communication of Medicine). The CT section were obtained at the interval of 2.5\u00a0mm intervals in the parallel horizontal planes as the obtained images at this interval were capable producing better geometric models [] than the models used in the previous studies [, ].The implants were constructed using reverse-engineering process. Reverse engineering has become a viable method to create a three-dimensional virtual model of an existing physical part; it involves measuring an object and then reconstructing it as a three-dimensional model. Dentos implant design: SH1312-08 [AbsoAnchor, Dentos Inc, Daegu, Korea] i.e., 1.3\u00a0mm (diameter) \u00d7 8\u00a0mm (length) was modeled.The constructed finite element model had nine sutures (midpalatal suture, naso-maxillary suture, zygomatico-maxillary suture, pterygo-maxillary suture, intranasal suture, fronto-maxillary suture, naso-frontal suture, zygomatico-temporal suture, zygomatico-frontal suture) and the spheno-occipital synchondrosis. The model allowed independent movement of the bones adjacent to the cranial sutures in response to the stimulated orthopedic forces.The biomechanical changes produced by the RME can be studied using various tools like conventional cephalometrics, strain gauge, photoelastic, or the halographic technique. The disadvantage of all these techniques is the failure to depict the results in three-dimensional spaces. Finite element method of stress analysis is the best method to check out the changes produced by the RME in three-dimensional space by the creation of virtual model and the possibilities of stimulating the clinical situation is innumerable with such techniques. [] Thus, the present study utilized the benefits of the FEM to compare and analyze the difference between the traditional RME and the recently developed implant-supported RME.In case of implant-supported RME, usually, the clinician places either four implants or two implants with the RME screw attached to it. And they can be tooth supported or completely bone supported. The placement of implant is in between two premolars bilaterally in case of two implants or else two implants in the anterior region and the two implants in the posterior region in case of four implant design [, ].For this particular study, complete bone-supported design with the two implants was chosen. The implants were placed between the second premolar and the first molar area, as the region was predicted to be the safe zone for the placement of implants in the palatal region [] and in few of the studies, it was the site of choice for the implant placement []."},
{"url": "https://opengeospatialdata.springeropen.com/articles/10.1186/s40965-017-0023-6", "title": "A combined change detection procedure to study desertification using opensource tools", "authors": ["Anna Zanchetta", "Gabriele Bitelli"], "publication": "Open Geospatial Data, Software and Standards", "publication_date": "24 April 2017", "abstract": "The paper presents a combination of two unsupervised techniques for change detection studies in arid and semi-arid areas. Among Remote Sensing change detection techniques, unsupervised approaches have the advantage of promptly producing a map of the change between two dates, but often the interpretation of the results is not straightforward, and requires further processing of the image. The aim of the research is to propose a new time effective and semi-automated reproducible technique in order to reduce the weakness of the unsupervised approach in change detection. Two techniques, Change Vector Analysis (CVA) and Maximum Autocorrelation Factor transform of Multivariate Alteration Detector components (MAD/MAF) are chosen to serve the purpose.", "full_text": "In Remote Sensing (RS) of the environment, change detection covers the capability to identify the differences occurred on the Earth surface in time. This task can be performed through various techniques, and choosing between them depends also on the characteristics of the surface that need to be detected. Often change detection analysis is referred to as Land use/Land Cover Change (LULCC) analysis, referring to the characterization of the terrestrial surface in terms of natural and man built types of cover. A basic condition for change detection studies is the preliminary co-registration of the images and the correction for external factors of change on the surface reflectance, such us atmospheric conditions and differences in Sun angle, plus the intrinsic characteristics of the single sensors. The selection of appropriate images, in terms of time of the year and weather conditions, allows to reduce the impact of these factors [].LULCC analysis requires in general a first identification of the surface covers, then a mechanism that compares the images along a temporal interval. The identification of the different types of cover is called image classification and can be carried out through several techniques and procedures [\u2013]. Most techniques process the single pixel and are so called pixel-based techniques. They differentiate from the relatively recent object-based analysis, that groups the pixels not only depending on their spectral behavior but also on geometrical characteristics, such as shape, color, size and similarity with neighboring pixels [].Considering pixel-based procedures, the categorization of the different types of classification techniques varies widely from one author to another, but mainly two different approaches can be recognized: supervised and unsupervised classification. In the first approach, supervised classification, the single images to be compared are classified separately, based on a preliminary knowledge of training areas for the different classes. The user identifies on the image some polygons of interest that correspond to specific land cover types. These training areas are then assigned to a software, that recognizes the spectral characteristics of each class and performs, through a selected algorithm, the classification of the rest of the image. The algorithms include maximum likelihood and minimum distance classification, among others.Unsupervised procedures on the other hand are used in absence of samples or previous information on the ground. In this case the software makes use of the spectral characteristics of the pixels in order to group them in different classes. The techniques for unsupervised classification include clustering techniques like the ones used in unsupervised classification (maximum likelihood, minimum distance, k-means, ISODATA etc.) and also techniques that make use of simple algebraic procedures or more complex statistical and mathematical analysis procedures as a preliminary step for the classification. The algebraic procedures include differencing, ratioing, indexes (like Normalized Difference Vegetation Index \u2013 NDVI), while the mathematical analysis techniques include data transformations (like Principal Component Analysis - PCA, Tasselled Cap Transform \u2013 TCT, Multivariate Alteration Detector - MAD), Artificial Neural Network (ANN) analysis, fuzzy logic, Bayesian network and others. Some unsupervised procedures promptly give as output a map of the change that must then be interpreted assigning a threshold, in order to differentiate the significant change from the pixels which are unchanged. The threshold choice, through customary or statistical criteria, is the key for the change interpretation [, ].The choice of the type of techniques to be used depends on the purpose of the research and on the availability of ground truth data, accessory material and knowledge of the studied area. Supervised classification is an ideal procedure, but often finding both spatial and semantic accuracy can be challenging and relies on the skills of the classifier. Moreover the availability of knowledge on the field can not always be complied and an accurate classification can be time consuming. On the contrary, unsupervised approaches can be almost automatic and more time-efficient, but require the researcher to intervene in a second phase to interpret the results. Therefore, according to the aim of the researcher, the change detection analysis can be performed using a mixture of more techniques, the so-called hybrid approach, that combines two or more techniques in order to improve the results [, ].For this research, which was missing ground truth data, two semi-automated unsupervised techniques have been chosen: the Change Vector Analysis (CVA) applied to the TCT features, and the Maximum Autocorrelation Factor (MAF) transformation of the MAD.CVA, developed in the 80s [], gives as an output the direction of the change between two dates for each pixel of an image, having the advantage of not needing an image classification to be performed in advance. The CVA needs in input two (or more) spectral characteristics (bands or spectral features) of the surface, that have a physical meaning and whose combination in the bands space can give a physical interpretation of the change. For this aim TCT features are ideal candidates and have been widely used in several RS studies [\u2013]. In contrast to PCA, which must be calculated for each single image, TCT is meant to be image independent, meaning that it can be applied to any image, provided that the TCT coeffiecient have been previously calculated for its specific satellite sensor.The second RS methodology used in the present study is the combined MAD/MAF analysis. MAD transformation was introduced by Nielsen et al. [] to improve the simple image differencing technique, by considering the difference images with maximal variance, obtaining a new set of uncorrelated images. Although similar, MAD was intended to be an improvement over PCA transform []. PCA in fact is a statistical procedure used, not only in RS but in several fields of Science, to transform a set of correlated variables to a new set of uncorrelated variables (called ) that consider the principal directions in which the data are spread in the bands space. PCA is useful to reduce the size and redundancy in the original data, since most of the information is contained in the first components, while the latter ones contain mainly noise. To do so, PCA considers the maximum variance in the dataset, whereas MAD considers maximum autocorrelation, since it takes into account the maximum variance of the difference images. Doing such, MAD eliminates issues related to the possibility that a dominating element in the image affects the PCA components, with a disproportioned high variance compared to other elements of the image. Besides this, MAD has the huge advantage of being invariant for linear transformations of the data, making it insensitive to the application to raw DN or transformed images [, ].Like PCA, though, MAD analysis is locally applied to the single pixel and therefore doesn\u2019t retain the spatial context of the adjacent pixels. With this consideration in mind, Nielsen introduced the application of the MAF transform to the MAD components, which is intended to produces variates ordered by spatial autocorrelation []. The MAD/MAF method thus provides a \u201cstatistically rigorous system to determine the spatially coherent patterns of major change in an image sequence\u201d [].Several comparison studies have been carried out on the performance of these three techniques, CVA, PCA and MAD, also in desertification studies [, , ]. In particular Pannenbecker [] concluded that CVA and MAD were the most effective techniques, in terms of desertification indicators and in terms of results. Considering also the added value, described above, of the MAF applications to the MAD components, in our research we chose to use CVA and MAD/MAF analysis, and further developed them in order to improve the analysis by combining the results of the two.In desertification studies, change detection covers a prominent role, since it allows the identification of surface elements that changed over a large scale, both spatial and temporal, and constitutes an effective method for observing, monitoring and characterizing the dynamism of drylands []. One of the biggest challenge in this type of studies is the identification of common desertification indicators, since different ways of interpreting analogous factors can bring to different results [, ]. Several indicators have been proposed in literature, considering in particular the soil reflectance contribution and the vegetative mass, whose deterioration can be a good indicator of desertification, being the major biological production of desert areas []. In particular different authors used indicators of: land degradation (soil erosion, soil salinization []), land use changes (expansion of agricultural or urban areas), bare soil expansion, drought and changing vegetation (perennial plant cover and biomass) [, , ]. The usage of the change detection techniques commonly used for vegetated areas, requires though some care in applications to desert areas: since vegetative cover is low, the reflectance of the exposed soil surface highly influences the pixels value [, ]. On the other hand, drylands have advantageous weather characteristics for change detection studies, exhibiting dry and cloudless conditions which remain unchanged for a long part of the year. These are ideal for change detection studies, both on a short (seasonal) and long (years) scale, solving the issue connected with the atmospheric correction, a basic prerequisite for change detection studies [].The aim of the research is to reduce the weakness of unsupervised techniques, mainly absence of ground truth information, by the joint use of two of these techniques in a hybrid approach, thus introducing a new and reproducible change detection procedure. In specific, the effectiveness of a combination of CVA and MAD/MAF is investigated in change detection studies in arid and semi-arid areas, naturally prone to desertification processes.The research was conducted using Geospatial Free and Open Source Software (GFOSS). The use of GFOSS allowed to implement unavailable techniques and to adapt existing ones to the needs of the research. Several OSGEO (Open Source GEOspatial Foundation) products, namely Grass GIS, QGIS and Orfeo Toolbox, were used through user\u2019s specified scripts in BASH and Python. For the aim of this research, a new addon for Grass GIS performing CVA was also implemented.The research passed through several combinations and tested them on the Oasis case, prior to extensively testing the chosen methodology. Hence an application on the Lake case was carried out.As a first attempt, a combination of CVA and MAD was considered. The first step was the comparison of several RGB combinations of MAD components in QGIS, in order to select the ones that visually detect the expected change on the surface (Fig.\u00a0), as commonly performed in MAD application studies. The components 1, 5 and 6 were selected, and then imported in Grass GIS where the \u00b12\u03c3 threshold was applied to each of the three MAD components, giving in output a map of positive and negative values beyond the threshold.At this point, a visual correspondence was noticed between the CVA results and the thresholded MAD image, where the positive values are generally correlated with the first and forth quadrant (therefore positive Brightness) of the CVA, while the negative values are correlated with the second and third CVA quadrant (therefore negative Brightness). To make this more clear, a new map was created with only two raster categories, grouping all positive values to one and all negative to another (see Fig.\u00a0 for the component 5 as an example). To quantify the similarity between the maps, analogous to visually overlapping the maps, the Grass GIS  module was used, between CVA and two components of MAD.  creates an output raster map representing all unique combinations of category values in the input rasters, and outputs also a table summarizing the occurring categories. The number of possible combinations (not shown here) was still large, and their interpretation was not straightforward.With reproducibility in mind then, the combination MAD-CVA is not satisfying, as it depends on the choice of the number and orders of the significant MAD components, that can differ from case to case. Moreover, the high amount of unique combinations between the MAD and CVA classes leaves a complex classification, rather than an easy-to-interpret change map.Next, the option of the transformation to MAF variates was considered. Like with MAD components, the results contained a high degree of detail, but the meaning of the change was not yet clear (Fig.\u00a0). The first MAF variate (MAF1), which was imported in Grass GIS and visually compared with the CVA change map, showed a high spatial agreement. It was therefore taken as the only input for the combined methodology. Higher order MAF variates are not shown here but mostly contained noise, as expected.In terms of surface cover, the CVA detects more changed pixels, with an extension up to two or three times more than MAF in some specific classes (see the Bare soil and bare sand expansion class, with more than 5% pixels undetected from MAF, out of 6.93% of the CVA). On the other hand, pixels identified by MAF, but not considered from CVA, cover in total only 0.58% of the Oasis area.A comparison with the results of a previous CVA change detection analysis, carried out in the same study area and in the same time interval with similar conditions [], was visually performed. The major difference between the use of the CVA alone and its use combined with MAF is given by the lesser spatial extent of the detected change, as shown by the results in Table\u00a0. The change classes are correctly detected, but CVA tends to overestimate the change, detecting wider areas, especially in the bare soil expansion class (see Northwestern and Western part of the study area). The combined technique shows more precise identification of the surface elements who changed, in particular the pools and ditches on the East side of the Oasis who went through a drying out process.The high correspondence of the CVA and MAF results, together with the improvement in the CVA spatial detection, induce to propose the combination of the two techniques as a new methodology for change detection studies, where CVA gives the semantic interpretation and MAF the spatial extent of the change (Fig.\u00a0).Considering the new methodology valid, an application to the second case study was implemented, in order to evaluate an antithetical situation, of water replenishment in a semi-arid area. The steps performed in the Oasis case were repeated in the second case, as shown in Fig.\u00a0 Also in this case, the MAD components 1,5 and 6 were chosen for the analysis, but the MAD and CVA combination showed the same weak points seen before. The MAF1 visually exhibited a high spatial correspondence and the Grass GIS  module was applied, giving the results shown in Table\u00a0. The possible combinations were the same seen for the Oasis case, with one stray pixel matching the first CVA quadrant with the positive MAF1. Like in the Oasis case, MAF results are almost entirely overlapped by CVA, with a missed detection of just 0.23% of the image (compared to the 0.58% of the Oasis area).A visual interpretation of the results indicates that the combined methodology finds the abandonment of the fields areas South of the dam along the river banks, and again the MAF reduces the spatial extent of the CVA bare soil expansion class. The expected occurrence of a new vast water basin upstream the dam in the North is also correctly detected. However the MAD/MAF contribution eliminates from the change map the central part of the basin, that was correctly detected by CVA (3.19% of pixels, belonging to the Higher moisture land and water bodies, not detected by MAF). This issue could be possibly solved if considering higher order MAF variates, and this could be discussed in further work.Summarizing, the MAD/MAF finds an equivalent change to the CVA and at the same time the CVA allows the physical interpretation of MAD/MAF technique. This gives a new perspective for the use of MAD/MAF, since in other cases found in literature the interpretation of the results is always dependent on the image considered as a case study. The research shows that the use of the two unsupervised techniques in a hybrid approach allows an improvement in understanding the results of both.A combination of two pixel-based unsupervised change detection techniques for desertification studies was analyzed, taking as case studies two areas in the Middle East region. The first, Al Azraq Oasis, was chosen as a representative of drying conditions, while the second one, South Hasakah Lake, was considered for water bodies replenishment.A first attempt to combine two change detection techniques was carried out using Change Vector Analysis (CVA) applied to the Tasselled Cap Transform (TCT) features combined with the output of Multivariate Alteration Detector (MAD) analysis. The results, applied to Landsat images between 1984 and 2015, were positive but not satisfying, due to the nature of the MAD components, where the noise is spread among the bands, and to the difficulty in overlapping the many categories given from the two techniques together.A second approach took into account the Maximum Autocorrelation Factor (MAF) transformation of the MAD components, that groups the images differences, called variates, by decreasing autocorrelation. In this case, one single variate was chosen and combined with the CVA output.The results of the application to a case previously analyzed by the authors, Al Azraq Oasis, show that the two techniques can be successfully used in a combined way because they give complementary information: while CVA gives a meaning of the surface change, MAF gives a slightly more detailed map of the change on the surface. In this sense, the first MAF variate, appropriately thresholded, can be used like a first step for the identification of the changed features, while a second step is taken assigning a meaning to the features by the CVA semantic classification. This procedure is corroborated by the high correspondence between the change found by the individual techniques: once thresholded and reclassified, MAF positive values match with the first and fourth CVA classes, indicative of changes towards drier conditions, while MAF negative values correspond to the second and third classes of CVA, indicative of changes towards more wet and vegetated conditions.The results are validated through the comparison with a previous research applying CVA on its own to the same study area surrounding Al Azraq Oasis. The combined methodology detects the expected change on the surface, namely the shrinking of the Oasis and the development of new rural areas, but the MAD/MAF contributes by more precisely delineating the changed areas. The combined methodology therefore is a conservative procedure, since it excludes a small percentage of pixels from each technique, thus allowing to exclude over-estimation of the change.Extending the results to the second case study, the expected visual superficial change is again detected by the combined methodology, namely the replenishment of water in the Lake and the loss of cultivated areas along the river. Missing ground truth data, though, an accurate validation of the results can\u2019t really be performed in this case. Only the correspondence with the results obtained for Al Azraq Oasis can give some positive feedback about the methodology, beside the fact that the visual change on the surface is correctly identified.An important output of the research is the improvement in the use of MAD/MAF technique in change detection studies. Previously, the interpretation of the MAD/MAF was not straight-forward and in general relied on the single researcher\u2019s choices. The research has showed how using a statistical criteria for the threshold determination, and relying on the correspondence between the results of CVA and MAF\u2019s first component, the combined technology provides an almost automated, time effective and reproducible technique. As such, it can be used as either a complementary procedure or a first interpretation step in change detection studies. The fast detection can be used for example to perform a preliminary selection of relevant time intervals, on which to perform more accurate change studies with the support of ground data. The methodology then has been found effective for desertification studies in arid and semi-arid areas, detecting the expected change on the surface.A limitation of the methodology could be given by the choice of the threshold applied to both techniques, that relies on the researcher. Another weak point is that the method has been employed just in a restricted number of cases and could be tested in other geographical regions. Future works can be considered in order to validate the technique, both by verification of the results against ground truth data and by comparing them with other change detection techniques. The implementation of higher order MAF variates could also be taken into account, for a complete investigation of the technique.An added value of the research is the integration of a CVA calculation module into the Free and Open Source Software (FOSS) project Grass GIS, making the implementation of the technique available to future users."},
{"url": "https://nzjforestryscience.springeropen.com/articles/10.1186/s40490-017-0097-z", "title": "Factors affecting macropropagation of bamboo with special reference to culm cuttings: a review update", "authors": ["Syandan Sinha Ray", "Md. Nasim Ali"], "publication": "New Zealand Journal of Forestry Science", "publication_date": "29 August 2017", "abstract": "The gap between demand and supply of bamboo is increasing daily due to the destruction of natural bamboo resources. Therefore, there is a pressing need to find suitable methods for large-scale propagation of bamboo. Currently, bamboo is propagated mainly using vegetative means since seed supplies are often variable or limited.", "full_text": "From the presented knowledge, \u201cculm cuttings\u201d could be the best regeneration option to meet the global need for bamboo planting stock. Rooting and sprouting, as well as survival of cuttings, depend strongly on the season, species and plant growth regulators used. To maximise the success rate, in the absence of prior knowledge of the propagation characteristics of a species, we suggest the following approach. Use culms with one to two nodes from the middle of mother stock plants that are 2- to 3-years old, and set horizontally in appropriate substrate, preferably sand, during summer and maintain proper humidity (80\u201390%). Irrespective of age of plant, or growth regulator concentration, the best response was recorded in summer with winter being the worst. The available data are inadequate for deciding the best doses and duration of plant growth regulators (auxins) which again depend on season. So, doses and duration of auxins need to be standardised for different seasons and species. The interactions of species, auxins and seasons still need to be studied. Further research is also required on the selection of better growth-promoting substrates, to improve the potential of the culm-cutting method. It is worth mentioning that addition of biofertilizers like nitrogen fixers  sp. or  (PSB) to the growth medium was effective for macropropagation of banana (Sajith et al. ) and pomegranate (Damar et al. ) respectively. Addition of compost was also found effective for propagation of another woody plant,  (Kumar et al. ). To date, growth media were restricted to sand only in bamboo macropropagation. Others, like soil, biofertilisers, vermicompost, compost, vermiculite, and their possible combinations, are yet to be explored for improving rooting and survival rates of bamboo propagules. The cut ends of the cuttings are the entry sites of the pathogens causing several diseases. Research is needed for effective control of those infections which decrease the success rate in cutting-based propagation. Successful propagation under different agroclimatic zones is essential for wider acceptably. To date, no multi-locational trial of any cutting-based method has been reported. The immense potential of the cutting method must meet the demand, if a few more factors are addressed properly through future research."},
{"url": "https://nzjforestryscience.springeropen.com/articles/10.1186/s40490-016-0085-8", "title": "Gradient (elevation) vs. disturbance (agriculture) effects on primary cloud forest in Ecuador: floristics and physical structure", "authors": ["Randall W. Myster"], "publication": "New Zealand Journal of Forestry Science", "publication_date": "20 January 2017", "abstract": "Cloud forests are common in the Neotropics and an important part of its hydrological cycle. An investigation on how elevation and recovery from agriculture affects cloud forest floristics and physical structure in Ecuador was undertaken.", "full_text": "Gradients and disturbances are two of the most important forces that shape plant communities because plants often respond to cues created by or associated with them (Pickett and White ; Turner and Dale ; Whittaker ). Indeed, landscapes can be largely seen as a vegetative mosaic created by these two forces. Common gradients include precipitation, soil nutrients, flooding, elevation, and temperature, and disturbances range from the very severe (e.g., landslides) to the moderately severe (conversion to agriculture) to the least severe (tree fall, selective logging).In the Andean mountains, cloud forests exist along a large elevational gradient (1300\u20134000\u00a0m above sea level (a.s.l.); Bushush and Silman ) and are important to the biogeochemical cycling of surrounding forests (e.g., the hydrological cycle; Hamilton et al. ). They are also subject to a variety of large-scale disturbances such as landslides (Myster ), conversion to agriculture (Myster ; Myster ; Myster ), and natural tree fall (Myster a). In these forests as elevation increases, net primary productivity decreases (Girardin et al. ), height of the canopy and of emergent trees decreases, number of strata or canopy layers also decreases, plants take on different ecotypes (Myster and Fetcher ), and growth forms such as buttresses and climbers give way to various kinds of epiphytes (Whitmore ). Also as elevation increases, trees become shorter and bryophytes become more common especially where cloud condensation becomes more persistent (Grubb et al. ). Exposure to wind-driven fog and rain can also cause trees to take on a bent and gnarled physiognomy, and bamboos can replace palms in the understorey (Kappelle and Brown ).Consequently, cloud forests offer excellent opportunities to examine how gradients (in the form of changes in elevation) and disturbances (in the form of old fields recovering after abandonment from agricultural use) affect forest structure, function, and dynamics, where those effects may actually be due to factors associated with gradients (e.g., temperature, humidity) or conversion to agriculture (e.g., soil changes, past crop). For example, Holder () found, in Guatemala, that fog precipitation was greater in a cloud forest at 2550\u00a0m a.s.l. compared with a cloud forest at 2100\u00a0m a.s.l. Likewise, Veneklaas and van Ek () found an increase in interception in Colombian cloud forests with an increase in elevation (2550\u00a0m a.s.l. to 3370\u00a0m a.s.l.) and Weaver () found a trend of decreasing interception with increasing elevation in cloud forest of Puerto Rico, where the elevational range of sites varied from 930\u00a0m a.s.l. to 1015\u00a0m a.s.l. Recovery from agricultural uses is slow for cloud forests, similar to temperate old-field rates with small-seeded tree species arriving first followed by understorey trees (Myster and Pickett ). Overall richness and structure may take decades to reach primary cloud forest levels (Myster ).A field study in Ecuador was implemented to expand on past samplings of Neotropic cloud forests (Myster ; Nadkarni et al. ; Tanner ; Weaver et al. ) in order to better understand the floristics and physical structure of primary cloud forests and how the natural variation of elevation and the human treatment of agriculture changed them. Plots were installed and measured as follows: (1) four 2500\u00a0m (50\u00a0m\u2009\u00d7\u200950\u00a0m) plots in a primary cloud forest at Maquipucuna Reserve; (2) two 500\u00a0m (50\u00a0m\u2009\u00d7\u200910\u00a0m) plots in secondary, regenerating cloud forest at Maquipucuna Reserve 17\u00a0years after sugarcane () or banana ( sp.) cultivation, and (3) one 2500\u00a0m (50\u00a0m\u2009\u00d7\u200950\u00a0m) plot in a primary cloud forest at Yanacocha Reserve. Taxonomic authorities for all species referred to in this paper are available from the Plant List .Three major questions were addressed: (1) how does an increase in elevation change the floristics (family, genera, species) and forest physical structure (stem density, mean stem size, four stem size classes, total basal area, above-ground biomass, canopy closure) of cloud forest in the Andean Mountains of Ecuador; (2) how does conversion to agriculture change the floristics (family, genera, species) and forest physical structure (stem density, mean stem size, four stem size classes, total basal area, above-ground biomass, canopy closure) of a cloud forest in the Andean Mountains of Ecuador; and (3) which aspects of cloud forest floristics and physical structure are affected by which of these two forces, and does that suggest how they may interact to produce cloud forests?There were several more species in common between the primary Maquipucuna cloud forest and the secondary Maquipucuna cloud forest than between the primary Maquipucuna cloud forest and the primary Yanacocha cloud forest. This suggests that cloud forest floristics is affected more by an increase in elevation (here 2000\u00a0m) than by past sugarcane and banana cultivation (here 17\u00a0years after abandonment). In terms of forest physical structure, with an increase in elevation, there is a gain in tree stems of all sizes but a loss of some diversity, especially at the species level. This leads to a more closed and structurally (but not floristically) complex cloud forest at the higher elevations. Large trees are absent in the recovering sugarcane and banana plots, which produces large differences in physical structure, and thus, the secondary forest at MR may reach floristic similarity before structural similarity.Comparing the two study primary cloud forests to the primary cloud forest at Monteverde, Costa Rica, also sampled for all trees at least 10\u00a0cm dsh, there was a similar number of stems (555/ha) but more basal area (62\u00a0m/ha: Nadkarni et al. ) but the same reverse J size distribution. There were many families in common: Asteraceae, Cecropiacea, Fabaceae, Lauraceae, Melastomataceae, Moraceae, Myristicacea, Myrtaceae, Myrtaceae, Piperaceae, Rubiaceae, Solanaceae, Tiliaceae, Urticaceae, and Verbenaceae, but only Fabaceae and Lauraceae were abundant.  was the only genus in common as well.The stem density for all trees at least 10\u00a0cm dsh in another primary cloud forest sampling in Costa Rica was comparable to YR (553/ha: Heaney and Protor ), but low in Jamaica (Tanner ) and Ecuador (Grubb et al. ), which puts the MR sampling somewhere in between. In Puerto Rican \u201cdwarf\u201d cloud forest, there were many small stems (3671/ha) but the basal area was comparable to Monteverde in Costa Rica (49.1\u00a0m/ha: Weaver et al. ). In cloud forests closer to Ecuador (Venezuela) sampled to a dsh of at least 10\u00a0cm, the stem density (365\u2013850/ha) and species richness (9\u201314/ha) was also comparable (Schwarzkopf et al. ).Previous studies of the sugarcane and banana plots have compared floristics and physical structure like in this study (Myster ); for example, relationships between richness and productivity (Myster ) and dominance-diversity curves (Myster , ). These studies have also included field experiments into regeneration mechanisms and tolerances at MR (Myster ; Myster a) and have shown that seed predators took most of the seed and had their greatest effect in closed-canopy primary cloud forest, a medium effect in tree-fall gaps in primary cloud forest, and least effect in secondary cloud forest (Myster a).Field experiments into how elevation affects seed processes were also done at Guandera biological station also in Ecuador (also at 3400\u00a0m a.s.l.) and at MR (Randall Myster: unpub. data). In that study, I found: (1) the two study sites were significantly different for all three seed processes but where seed predation dominated Maquipucuna, seed pathogens dominated Guandera; (2) closed-canopy vs. tree-fall gap variation was only significant for germination; (3) species were significantly different for both seed predation ( [11%],  [16%],  [13%] Engl.,  sp. [79%],  [83%],  [84%],  sp. [85%],  [69%]) and for seed pathogens ( [43%],  [52%],  [68%] Engl.,  sp. [14%],  [7%],  [4%],  sp. [4%],  [7%]); and (4) there were significant interaction terms for elevation\u2009\u00d7\u2009species seed predation, where it was driven by the differences between study sites and the tree seed species , and for elevation\u2009\u00d7\u2009species seed pathogens, where it was driven by the differences between study sites and the tree seed species . Thus, cloud forest recruitment and regeneration is affected less by predation and more by pathogens, as elevation increases.In conclusion, (1) elevation affects both floristics and physical structure in these Andean cloud forests and I continue my work at other high-altitude sites in order to examine these differences in more detail and, (2) after 17\u00a0years while the old fields have species in common with the primary MR forest, they are not floristically similar and differences suggest that there may continue to be a lingering effect of the past crops of sugarcane vs. banana (a crop signature: Myster ). Structural differences are consistent with trends in successional vs. primary vegetation studies elsewhere (see chapters in Myster ). These permanent plot studies, and others like them, provide baseline data on forest dynamics (i.e., plant-plant replacements: ) and fluctuations of forest structure."},
{"url": "https://nzjforestryscience.springeropen.com/articles/10.1186/s40490-016-0075-x", "title": "Elastic constants of green ", "authors": ["Nicholas T. Davies", "Clemens M. Altaner", "Luis A. Apiolaza"], "publication": "New Zealand Journal of Forestry Science", "publication_date": "16 October 2016", "abstract": "Mathematical modelling is often used to investigate phenomena difficult or impossible to measure experimentally.", "full_text": ""},
{"url": "https://nzjforestryscience.springeropen.com/articles/10.1186/s40490-016-0084-9", "title": "Microcomputer tomography (microCT) as a tool in ", "authors": ["Hann\u00e9l Ham", "Anton du Plessis", "Stephan Gerhard le Roux"], "publication": "New Zealand Journal of Forestry Science", "publication_date": "20 January 2017", "abstract": "\n                           ", "full_text": "The development of female reproductive structures of  species can be divided into three general phases: first seasonal growth during which pollination occurs, second seasonal growth during which fertilisation occurs and a period of cone maturation (Sweet and Bollmann ; Williams ; Fernando ). The female strobilus contains ovules that become seeds after being fertilised by pollen during the second seasonal growth phase. Seed development and cone maturation are rapid after fertilisation (Bramlett et al. ; Williams ). During seed and cone maturation, density differences between different tissues are more evident, for example, the formation of the hard seedcoat (Bramlett et al. ). Microcomputed tomography (microCT) scans can be used as a non-destructive method to determine differences based on these density differences between tissues, as indicated in previous studies (du Plessis et al. ; Guelpa et al. ). The number of seed and viability depends on the type of crosses performed. Interspecific crosses, for example, yield far fewer viable seeds than do intraspecific crosses (Williams ).Although pollination and fertilisation in  species may yield a large number of mature cones filled with seed, some might be non-viable due to pollination failure or post-zygotic degeneration (Owens et al. ; El-Kassaby et al. ). Seeds need to be extracted, and viable seeds separated from non-viable ones (Bramlett et al. ) before sowing. Generally, this is done using one of the following methods: a water-soaking test; shaking in a cone tumbler (Fry and Stephens ); use of an optical sorting machine (Delwiche et al. ) and pressure vacuum (Lestander and Bergsten ); X-radiography (International Seed Testing Association ) or the Incubation Drying & Separation technique (IDS) (Simak , ; Bergsten ; Bergsten and Sundberg ; Downie and Bergsten ; Downie and Wang ; Falleri and Pacella ; Demelash et al. ). Viability of seeds is determined according to parameters such as colour, weight and size, often using lasers and/or other sensors as opposed to strictly manual sorting, but success may vary among species (Tigabu and Od\u00e9n ).The number of mature seed is used as an indicator for interspecific  hybridisation success although a high number of these seeds could be non-viable (Critchfield and Kinloch ; Critchfield ). There is a need to identify a reliable, repeatable and non-destructive method that could quantify successful seed set (viable seed) and to confirm how many weeks after pollination fertilisation takes place. This can provide valuable information for tissue-culture programmes (such as somatic embryogenesis or organogenesis), which are applied in agriculture with increasing success (Balla and Mansvelt ; Mansvelt et al. ), and might improve interspecific hybridisation success with  species (Reed ; Mehetre and Aher ; Lelu-Walter and P\u00e2ques ; Montalb\u00e1n et al. ; Montalb\u00e1n et al. ).The reproductive cycle of  is up to 28\u00a0months long, and fertilisation occurs around 16\u00a0months after pollination (Sweet and Bollmann ). After fertilisation, seeds are already full size but will still develop until the cones are harvested at around 28\u00a0months (Bramlett et al. ). Previous studies indicated that mature seed is visible in X-ray radiograph pictures (Bramlett et al. ). These images are two-dimensional (2D) and can be used for diagnosis but are limited in contrast and cannot be used for quantitative analysis (Tigabu and Od\u00e9n ). In contrast, microCT scans provide a three-dimensional (3D) model, which generates more accurate and reliable data.Computer tomography (CT) and microCT scanning methods were originally developed for use by the medical profession as a diagnostic tool to obtain a 3D image in a non-destructive manner (Stuppy et al. ; Staedler et al. ). It is not yet widely applied in plant sciences (Kalathingal et al. ; van der Niet et al. ) even though it is a relatively simple non-destructive technique for gaining insight into smaller objects or plant tissues and for helping to understand phenotypic information (total sum of the observable characteristics of an organism) (Stuppy et al. ; Staedler et al. ; Dhondt et al. ). During microCT scanning, thousands of 2D X-ray images are combined to construct an accurate 3D model. The data, based on differential X-ray attenuation, are analogous to those otherwise obtainable only by serial sectioning of plant tissues (Stuppy et al. ; van der Niet et al. ; Staedler et al. ). It requires no staining, sectioning or fixing but produces a 3D digital map (viewed from an arbitrary angle) of the specimen that allows measurements and visualisations (Stuppy et al. ; van der Niet et al. ; Staedler et al. ), for example, of metabolite content, pollination status or crop yield (Staedler et al. ; van der Niet et al. ). Previous experiments have been able to distinguish between pith, xylem, cortex, vascular bundles, leaf bases, seeds and ovuliferous scales (Pika-Biolzi et al. ). This is due to contrasting plant tissues (soft versus hard) absorbing X-rays differently due to different thicknesses of cell walls and cell contents (Stuppy et al. ). Previous studies have classified  seeds as viable or not using near-infrared transmittance (energy in the region of the electromagnetic radiation spectrum at wavelengths longer than those of visible light but shorter than those of radio waves) and reflectance spectroscopy (study of light as a function of wavelength) (Tigabu and Od\u00e9n ). However, qualitative and quantitative investigation of the internal morphology and histology of plants is a potential application of microCT scans (Stuppy et al. ).MicroCT has progressed to the point where service facilities are available worldwide and especially in university departments, making the method easily accessible and more cost-effective. Its use in non-destructive testing of various types of materials has made it a valuable tool in materials sciences (Maire and Withers ), geosciences (Cnudde and Boone ) and agricultural and food sciences (Schoeman et al. ). The method is very diverse and can provide quantitative information on density of biological materials such as maize kernels (Guelpa et al. ) to determine, for example, milling quality. It has also proved possible to scan large numbers of samples in a single operation, making high-throughput analysis feasible for this type of application (Guelpa et al. ).This study investigated microCT scans as an alternative non-destructive method in  breeding programmes. Three experiments were conducted to determine whether microCT scans can be used to confirm at what time (weeks after pollination) fertilisation occurred, whether seed viability can be assessed by scanning multiple seeds together and whether viable seed can be counted in a closed  cone.Mature seed cones from interspecific hybrid crosses are the first \u2018product\u2019 in the sequential process of tree breeding as it contains the \n                         progeny (seed). The number of seed harvested is generally considered an indication of breeding success (Johnsson ). However, pre-zygotic barriers (before fertilisation) can reduce the numbers of mature cones or seed harvested, while post-zygotic barriers (after fertilisation) can increase the number of hollow seeds without affecting the number of seed extracted from the cones (Critchfield and Kinloch ; Owens et al. ). Therefore, estimating the time (in weeks) after pollination that fertilisation might occur can help to estimate viable seed set during interspecific hybridisation and make subsequent tissue-culture (embryo rescue and micropropagation) projects more effective\u00a0(Cisneros and Tel-Zur ). Previous studies used radiography to determine whether mature seed can be viable (Bramlett et al. ; Tigabu and Od\u00e9n ). However, they could only determine whether seeds are full, partially full or empty and could not predict with certainty whether these seed might be viable. Full seeds were thus classified as potentially sound seed but damaged full seeds (insects, aborted ovules, etc.) could not be distinguished from healthy full seeds.This study investigated microCT scans as a prospective tool in tree breeding using three different types of experiments to refine the technique of estimating fullness and viability of seeds. The advantages of microCT scans are the non-destructive imaging capability and quantitative analysis capabilities, for example measuring porosity in seeds. Although it is a non-destructive imaging method, the sampling process is destructive in that conelets are \u2018destroyed\u2019 because they cannot be grafted back onto the trees to develop into mature seed cones. However, reconstructed images create a permanent, tangible record and life-like image for future analysis, comparison or prediction studies (Perrin et al. ; van der Niet et al. ; Staedler et al. ). Seed damage due to insects or aborted ovules will be clearly visible. Furthermore, X-ray absorption during CT scans depends on physical density and based on grey value differences (du Plessis et al. ; Lindgren et al. ).Fertilisation in  pure species and hybrids are estimated to take place between 10 (Fernando et al. ) to 16\u00a0months after pollination (Sweet and Bollmann ). The results from Experiment 1 indicated that fertilisation occurred between weeks 64 and 68 (15 to 16\u00a0months after pollination) in , narrowing down the known time of fertilisation Therefore, seed set and seed damage could be estimated as early as week 68 (15 to 16\u00a0months after pollination). To confirm precisely when fertilisation happens and perform reliable seed set counts in , different clones will need to be microCT scanned between weeks 63 and 69 after pollination to narrow down the time of fertilisation.Experiments 2 and 3 investigated whether microCT scans can be used as an alternative method to determine seed set and viability accurately and measurably (Kalathingal et al. ). Studies with microCT scans are very limited in plant sciences, due to plant tissues mostly consisting of light elements which display low X-ray absorptions. However, this can be compensated with very long scanning times but with decreasing quality due to the probability of motion artefacts. The results of the seed viability tests indicated clear differences between seed (viable versus non-viable) by quantitative porosity measurements indicated by the colour-coded 3D images. When mature seed cones are harvested in large quantities, this can be a useful tool to distinguish between cones with a high number of non-viable seed, lowering the cost of seed extraction.Previous studies indicated that X-rays are absorbed to varying degrees by different parts of the seed and helps to distinguish between different tissues and injuries, although physiological changes are not visible (Simak ; Kriebel ). Very mild X-ray doses (~100\u00a0kV) do not injure physiologically sound (freshly collected) seed, and the germination ability should remain high (Simak ; Gustafsson and Simak ; Ohba and Simak ) but might differ between species (Gustafsson and Simak ). As mature seed is also less affected by mild X-ray\u00a0doses (Gustafsson and Simak ; Johnsson ), this study was carried out only on mature seed with an X-ray dose of ~100\u00a0kV. Seed germination with moistened filter paper confirmed that mature seed scanned at ~100\u00a0kV X-ray doses were not damaged as germination was consistent with seed viability determined during experiment 2 (seeds were numbered). Seedling survival was, however, not determined the past 4\u00a0weeks after sowing. Broeckhoven et al. () concluded that small doses of X-rays have no harming effect on live reptiles of various sizes. Previous studies also indicated that samples of up to 150 maize kernels yielded good survival results and allowed for the scanning of more than 1000 maize kernels per day with semi-automated analysis (du Plessis et al. ; Guelpa et al. ).MicroCT was applied in a  tree breeding study with three experiments. In the first experiment, the fertilisation time of  was narrowed down by microCT imaging to probably occur between weeks 64 and 68 after pollination. Full-sized seeds were visible at week 68, indicating initial seed set although not fully mature seeds. In the second experiment, quantitative porosity analyses of seeds were used to identify non-viable seed. This method can be used for qualitative viewing for faster and lower-cost application or more time-consuming quantitative analysis. The clarity of seed viability assessment by microCT scans should allow its use for breeding assessments or calibration of indirect measures. An experiment with a full cone with viable and non-viable seed demonstrates the potential for in situ seed viability assessment. However, care should be taken to shake out all the seeds as some can still be trapped between scales at the bottom of the cones."},
{"url": "https://opengeospatialdata.springeropen.com/articles/10.1186/s40965-016-0013-0", "title": "Experiencing the built environment: strategies to measure objective and subjective qualities of places", "authors": ["Allen Sayegh", "Stefano Andreani", "Chrisoula Kapelonis", "Natasha Polozenko", "Stefan Stanojevic"], "publication": "Open Geospatial Data, Software and Standards", "publication_date": "12 December 2016", "abstract": "This article offers an alternative look at the experiential character of the built environment by combining objective analysis and subjective perception. The aim is to measure and elaborate on quantitative descriptions of \u2018hidden\u2019 urban characteristics, attempting to build correlations between different unseen but detectable qualities of cities.", "full_text": "The built environment is one of the most fascinating yet enigmatic artifacts of the human being. We perceive it as a complex entity resulting from the juxtaposition of spaces, flows, experiences, objects, and events. Each environment has certain qualities, and \u2013 even though shared characteristics do exist \u2013 those qualities vary from place to place. Although a variety of criteria, parameters, and indicators attempt to capture key figures of city life [], they are yet far from depicting the more subjective aspects that constitute the experiential character of built environments. And far less is known of the role of new media and digital tools in helping understand and improve the relationship between people and urban contexts.This article offers an alternative look at the experiential nature of the built environment by combining objective analysis and subjective perception. On the one hand, the physical reality is constructed out of an established configuration of elements, matter in space, that can be considered more or less static in a contained timeframe. These elements are tangible, with clear relationships between one another, and can be quantified through different means. On the other hand, the individual \u2013 the \u2018urban actor\u2019 \u2013 filters this environment with her personal, varying perceptions and relations to the surrounding context in specific moments. This article argues that, by building upon objective layers of data and affording them with the complexity and variation of subjective, personal feedback, it is possible to gain a more holistic, novel understanding of the city, and of our presence within it.The way in which we receive information and data about our interaction and relation to the built environment is largely fed to us through systematized, sensor based statistics. This information gets translated into discrete, numeric, tangible data, which has often been flattened in order to package it in a more comprehensible, digestible form []. Although data might be framed technically, economically, ethically, temporally, spatially and philosophically [], the common sensor-based static methods of data collection leave little room for interpretation, and fail to consider the complexities and variables that may influence the ways in which people perceive their surroundings. In fact, each individual experience, perception, view of places is colored by a personal, subjective interpretation. The presented study addresses this additional layer describing the subjective views and attitudes that could potentially impact on and influence how urban data is collected, interpreted, and used.Putting the human being at the center and forefront, this article draws from an ongoing research pursued by the Responsive Environments and Artifacts Lab (REAL) at the Harvard Graduate School of Design that investigates the role of new augmenting and responsive technologies in articulating, mapping and exploiting the specificities of places through a multi-sensory approach []. The objective is to measure and elaborate on quantitative descriptions of \u2018hidden\u2019 urban characteristics, attempting to build correlations between different unseen but detectable qualities of the built environment. This research has been elaborating alternative methods, hacking existing technologies, and devising new tools to: a. measure and quantify qualities of the built environment; b. visualize them and make comparisons; c. extrapolate meaning; d. create correlations between those qualities and typologies of built environments.Over the last few years there has been a growing interest in trying to scientifically analyze, quantity, and predict the dynamics and functioning mechanisms of cities. For instance, in his  Michael Batty employs complexity theory to create mathematical models of different aspects of urban structures and to develop decision-making tools that aim at predicting interactions and flows in future cities []. By the same token, Luis Bettencourt and Geoffrey West apply the principle of \u201cscaling\u201d to find correlations between urban metrics and socio-economic systems, and the size of cities, claiming that the understanding of cities\u2019 dynamics, growth and evolution in a scientifically predictable, quantitative way is crucial in city planning []. In a way, the promise of big data analytics \u2013 widely put forward by smart city models [] \u2013 tends towards the limits of an absolutely objective understanding of how cities work. Through the use of sensors, the aim is to be able to have control over urban systems with a systematic top-down approach [].In all these studies the subjective view of the individual is often overlooked, and for all the good reasons. Understanding the behavior of people in relation to the built environment is a challenging task, and going deeper into the experiential realm of each unique individual is basically impossible. Nevertheless, there are a number of investigations that attempt to better define the role that urban morphologies, spatial arrangements, and public places design play in conveying certain perceptions to the human being. Some of those first attempts can be traced back to the 1960s with the Psychogeography movement, defined by Guy Debord as \u201cthe study of the precise laws and specific effects of the geographical environment, consciously organized or not, on the emotions and behavior of individuals\u201d []. The research conducted by the related Situationist International was indeed looking at the arrangement of the elements of the urban setting in close relation with the sensations they provoke.This study introduces an applied research method to quantify objective features of the built environment and the related subjective experience. The assumption is that the combination of objective and subjective datasets can help reveal a more comprehensive understanding of spaces and places within the city, perhaps identifying their inner \u2018character.\u2019 There are a number of other projects that share some overlaps with the one presented here. For instance, the \u2018happy maps\u2019 developed by Daniele Quercia offer a tool to automatically suggest routes that are not only short but also emotionally pleasant []. Urban data is leveraged also in the work of Marco De Nadai, who developed a computational way using smartphones to test Jane Jacobs\u2019 conditions for making cities vibrant and how they relate to the vitality of urban contexts []. Christian Nold\u2019s Biomapping method then used the galvanic skin response of volunteers as an indicator of emotional arousal in conjunction with their geographical location. The resulting maps visualize points of high and low arousal [].This information is successful in allowing for a preliminary overview and understanding of individuals, and the ways in which they use their environment. However this data possesses shortcomings in that it is a simplified view of the human/city relationship. Data becomes more meaningful and substantial with an added layer of active, subjective inputs. One way of obtaining this active subjective data set is through a greater and incentivized engagement of the individual, by means of a smartphone application. Apps are particularly successful in gathering real time information, and allowing for an active engagement and participation, based on the pure fact that smartphones have become an extension of the human and allow for constant input and feedback loops [].In this way it then becomes possible to mediate between technology and built form, establishing a synchronicity between the two \u2013 \u201curbanizing technology,\u201d in the words of Saskia Sassen [] \u2013 and fully engaging and utilizing emotive perceptions of the urban actors in order to influence and inform the built environment, and vice versa. The implementation of data, both subjective and objective, is in fact crucial to the behavior of the individual whilst navigating the built environment via digital terms. To that end, the data needs not only to be collected, but also put back into the system to benefit and retrofit the behavior of the user through feedback loops []. Creating this understanding of the city enables applications in larger scale planning efforts, providing valuable information for the larger intervention decisions by design and planning disciplines. The presented tool might as well be used by citizens who can be exposed to \u2018hidden\u2019 characteristics of places.The presented mode of thinking about data allows for a range of interpretations and applications which may serve to enhance or manipulate human interaction and experience within the built environment. These applications are scalar, in that the data can manifest itself into a range of outputs. Real time data may provide the actor with a snapshot overview of the built environment, drawing from an accumulation of certain feelings or perceptions based on the data inputs of numerous other actors. These nodal points, or snapshot into particular moments within the urban form, have the potential to build upon each other as data inputs increase, creating a culmination of data which could then manifest itself into larger networks and have an impact throughout the city. By revealing the \u2018mood\u2019 of urban environments, this data could in fact be used by city planners to better inform programmatic strategies at different levels \u2013 from urban acupuncture interventions to larger infrastructural changes.A work-in-progress application of this research is focused on how people relate to different modes of urban mobility. Through the accumulation of subjective datasets, the objective is to draw a picture of the moods, emotions, and feelings at different points of the commute or trip. The assumption is that in the future machine intelligence will provide personalized routes catered to the desires and preferences of the actor [], for example, providing routes that are most safe, comfortable, or exciting to the individual. With the emerging of big-data driven connected vehicles and self-driving cars [], this level of route customization might very well disrupt even conventional infrastructure boundaries."},
{"url": "https://opengeospatialdata.springeropen.com/articles/10.1186/s40965-017-0025-4", "title": "Pilot implementation of the US EPA interoperable watershed network", "authors": ["Tad Slawecki", "Dwane Young", "Britt Dean", "Brandon Bergenroth", "Kimberly Sparks"], "publication": "Open Geospatial Data, Software and Standards", "publication_date": "24 May 2017", "abstract": "The mission of the United States Environmental Protection Agency (EPA) is to protect human health and the environment, including air, water and land. Understanding the extent of pollution in waters and identifying waters for protection has been based in part on water quality monitoring data collected and shared by parties (federal, state, tribal, and local) throughout the U.S. To date, this monitoring data has been largely represented by data collected as a water quality sample (data collected by a technician in the field or analyzed in a lab). EPA\u2019s \u201cSTORage and RETrieval\u201d (STORET) and the Water Quality Exchange (WQX) have served as the repository for all this sampling data. However, these tools and systems were not designed to handle today\u2019s continuous water quality sensors. EPA has therefore embarked on the Interoperable Watersheds Network (IWN) project, which is focused on identifying a common set of formats and standards for data, and on testing and validating these standards as well as new ways of sharing data and metadata. The completed IWN will greatly expand the sharing of data and its use, thereby streamlining the assessment, restoration, and protection of surface water quality at all levels of government.", "full_text": "The United States Environmental Protection Agency (EPA) mission is the protection of human health and the environment, including the waters of the United States. EPA\u2019s \u201cSTORage and RETrieval\u201d (STORET) data system [] has been used to collect and hold millions of water quality sample measurements and associated metadata collected since the 1960s. Additional systems like the Water Quality Exchange (WQX) [] and the Water Quality Portal (WQP) [] have facilitated the communications and exchange of water quality sampling data between data providers and promoted discoverability of and access to data across agencies. However, STORET, WQX and WQP emphasize the handling of discretely sampled \u201cgrab\u201d data and are not well-suited to manage high-frequency \u201ccontinuous\u201d data generated by modern, affordable water quality monitoring sensors. The use of these sensors is becoming ubiquitous with a proliferation of this telemetered \u2018real-time\u2019 data on the internet and development of new sensor technology for nutrients and other parameters of interest promises to expand and diversify applications.Stakeholders in the watersheds were engaged in site visits and on monthly calls to develop use cases, to define data workflows and attendant technology stacks, and to provide feedback throughout.A straightforward software development approach was used that first elicited requirements for the major projected components and then iteratively implemented the components with many opportunities for stakeholder input.Short, simple descriptions were solicited from representatives of the pilot watersheds to define user stories. These descriptions of desirable features presented from the stakeholder perspective were used as the launching point for an agile development process. Regular interactions with stakeholders served to inform the implementation towards its responsive endpoint.As implemented, IWN data is currently made available using WaterML 2 and SOS 2.0 through either 52\u00a0N or Kisters servers with the SOS 2.0 Hydrology Profile enabled, so data services are compliant with the requirements in the Best Practice document for the OGC SOS 2.0 hydrology profile for SOS 2.0 implementations serving OGC WaterML 2.0 []. In addition, the related catalog and Currents discovery tool fulfill the common cases requirements for data discovery and download established in the Scope section of the Best Practice document.The Source Water Protection (Hackensack-Passaic A) and Water Safety (Little Miami A) user stories share a need for discovery and visualization, while the Water Quality Assessment (Hackensack-Passic B) and TMDL Implementation (Little Miami B) user stories call for large multiple-site, multiple-parameter downloads.GetSensorParameters and GetOrganizationParameters results both include the organization\u2019s parameter IDs for use in querying data by parameter directly from the organization\u2019s service endpoint. The catalog harvests metadata from registered organizations\u2019 service endpoints daily.The successful implementation of the pilot IWN demonstrates the feasibility of the original strategy for sharing continuous data, although scalability of the approach will be a concern. In particular, bandwidth, storage, and CPU requirements for the catalog server will likely increase as data providers engage with the IWN and register more data appliances. Data providers are deemed unlikely to run into scalability issues as data appliances configured for this pilot ran successfully on with minimal resources (e.g. Amazon Web Services\u2019 most-lightweight hardware configuration \u2013 t2.micro).To align complementary efforts and promote interoperability, the next IWN phase will encompass coordination and cooperation with other Federal agencies (e.g. USGS, NOAA) and academia (e.g. Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI)). Additionally, EPA hopes to engage with the private sector to encourage sensor and data management vendors to provide SOS and WaterML 2 access to data.A description is provided here of the naming scheme for SOS objects on IWM data appliances. Basic recipes and other information for installing and configuring 52\u00b0North SOS and the pilot IWN ingestion script are provided at ."},
{"url": "https://progressinorthodontics.springeropen.com/articles/10.1186/s40510-016-0155-8", "title": "Evaluating the effects of consolidation on intrusion and retraction using temporary anchorage devices\u2014a FEM study", "authors": ["Monica Namburi", "Sleevaraju Nagothu", "Chetan S. Kumar", "N. Chakrapani", "C. H. Hanumantharao", "Supradeep K. Kumar"], "publication": "Progress in Orthodontics", "publication_date": "9 January 2017", "abstract": "Extraction of premolars and retracting the anterior teeth using mini-implants and anterior retraction hooks became advent now a day. In such treatments, consolidation of arches is not done in regular practice. So, the present study is concentrated on effects of consolidation in two implant and three implant combinations of retraction and intrusion.", "full_text": "Dental protrusion is common in many ethnic groups around the world. It is characterized by dento-alveolar flaring of only maxillary teeth or both the maxillary and the mandibular anterior teeth with resultant protrusion of the lips and the convexity of the face []. Extracting the first four premolars and retracting the anterior segments with maximum anchorage is one of the ways to reduce the protrusion and to straighten the patient\u2019s profile. The retraction of four incisors after canine retraction is accepted as a method to minimize the mesial movement of the posterior teeth segment, whereas en-masse retraction of six anterior teeth may create anchorage problems. Anchorage may be aided by the use of intra- and extra-oral appliances. However, intraoral anchorage devices may provide insufficient anchorage, whereas extra-oral appliances provide a sufficient anchorage but are dependent on patient compliance []. The goal of orthodontic treatment is to achieve the desired tooth movement with minimum unwanted side effects and to improve patient aesthetics [].In retraction mechanics, temporary anchorage devices (TADs) or orthodontic mini-implants (OMIs) are used for anchorage purposes [, ], which gained popularity and are used successfully in orthodontic treatment. Hooks are used on the archwire as force application points to achieve anterior retraction. These hooks or power arms can be crimped, screwed on, soldered to the archwire with silver solder, or welded []. The force is applied directly from the implant to the power arms with the help of e-chains or coil springs for effective space closure. This force vector can be controlled by changing mini-implant insertion height and/or anterior retraction hook height, thereby raising a number of different force action line alternatives. Orthodontists therefore, prior to mini-implant installation, should define which force action lines will be employed and determine the force vector which will exert upon the anterior teeth. In en-masse retraction consolidation of the arches which means making them as a single unit using ligature wire as the anterior (canine to canine) and posterior segments (premolar to molars on one side as one segment and on the other side as another segment) should be done. Consolidation of arches is advised to inhibit the unwanted tooth movement.Application of engineering knowledge in dentistry with the use of computational techniques has helped to understand oral biomechanics aspects. One of such software is finite element analysis (FEM) []. This method can simplify the physiologic responses of dento-alveolar complex to orthodontic forces by exhibiting quantitative data, and is recently preferred by the researchers of the field []. The main advantage of using finite element analysis is that many alternative designs can be tried out for their validity, safety, and integrity using the computer, even before the first prototype is built []. In spite of the significant advances that have been made in developing finite element models, the results obtained must be carefully examined before they can be used.Many studies evaluated the stress in the bone [, ], PDL [\u2013],  implants [\u2013] in mini-implant-assisted retraction [] with different height of implants and retraction hooks [, ] and materials [, ]. But there are no studies which evaluated the stresses on the bone and PDL in two implant and three implant combinations. There are no studies regarding the effects of consolidation during retraction. So, the present study was conducted to evaluate the analysis of stresses and displacement pattern in two implant and three implant combinations with and without consolidation.The goal of the present study is to compare and evaluate the effects of consolidation on intrusion and retraction in two implant and three implant scenarios.In this study, 12 finite element models were created with variations in number of mini-implants, height of placement of bilateral mini-implants and different force levels from the mid-implant and with and without consolidation.Comparing the consolidation and non-consolidation, the consolidated arches in group I showed more palatal tipping whereas non-consolidated models showed labial flaring of the teeth as crown moves labially and apex moves palatally, which is unfavorable. Whereas in group II, bodily movements are observed in consolidation and in non-consolidation system, the labial flaring of centrals and laterals are observed. The center of resistance for the present study model might be located at 10-mm height as the bodily movement is seen in 10-mm implant and 9-mm anterior retraction hook with consolidation in group II. The force levels passing away from the center of resistance will cause tipping and at the center of resistance causes bodily movement which is proved correct in the present study. Abhishek Parashar et al. [] conducted a study and inferred that the bodily movement with very minimal torque loss was observed with 8-mm implant and concluded the same.In vertical displacements, it is concluded that intrusion is seen in three implant system as expected. Intrusive movement was more in the high pull implant than in the low pull and medium pull implant in all cases. The force levels passing from the high pull implant to the retraction hook will cause intrusion and from the medium pull implants shows bodily movement and low pull implant showed tipping forces. A study conducted by Shrinivas S Ashekar et al. [] suggest that the force levels passing from the high pull implant to the retraction hook will cause intrusion and from the medium pull implant shows bodily movement and the low pull implant showed tipping forces which supports the present study. So, the high pull implants show high intrusive effect, and the canines showed extrusion in all cases as the retraction force is more towards the canine as the tipping of the canines is observed. In consolidation and non-consolidation systems, the intrusion effect is high in non-consolidated system than in the consolidated system as the force are distributed among the teeth in consolidated arches, so less intrusive effect is seen in consolidation.The present study showed highest stresses on lateral incisors, and this may be due to the short roots of the lateral incisors []. Burstone and Viecilli [] stated that it is a natural concept that larger teeth have more PDL and root support than smaller teeth, and hence, when the same load is applied, the stress magnitudes in the PDL for larger teeth are smaller and larger for smaller teeth. Consequently, the resistance to tooth movement of larger teeth is larger compared to smaller teeth. The maxillary molars suggested that widely divergent roots will require higher loads (causing rotation about the vertical axis of the root) to achieve similar levels of stress, even if the surface area of the root is similar to other less divergent tooth roots and this supports the present study [].Stresses on PDL are high in laterals and canines compared to other teeth in all cases, and in 7\u00a0mm with mid-implant model (group II), the stresses are more in the posterior teeth than in the anterior teeth and consolidated system showed less stresses when compared with the non-consolidated system. Unfavorable compressive stresses are seen in non-consolidated group, which concluded that consolidation is better than non-consolidation as the stresses are distributed in consolidation system.So, the present study concludes that consolidation is better than non-consolidation in retraction using implant with anterior retraction hook, and intrusion is effective in three implants than bilateral implants.This study suggests that with consolidation of the anterior teeth, desirable tooth movement can be achieved with less stresses on all surrounding structures."},
{"url": "https://progressinorthodontics.springeropen.com/articles/10.1186/s40510-017-0171-3", "title": "Comparison of surgical and non-surgical orthodontic treatment approaches on occlusal and cephalometric outcomes in patients with Class II Division I malocclusions", "authors": ["Sheila Daniels", "Patrick Brady", "Arya Daniels", "Stacey Howes", "Kyungsup Shin", "Satheesh Elangovan", "Veerasathpurush Allareddy"], "publication": "Progress in Orthodontics", "publication_date": "3 July 2017", "abstract": "This study aimed to examine end-of-treatment outcomes of severe Class II Division I malocclusion patients treated with surgical or non-surgical approaches. This study tests the hypotheses that occlusal outcomes (ABO-OGS) and cephalometric outcomes differ between these groups.", "full_text": "Class II Division I malocclusions typically manifest with increased overjet and retrognathic mandibles. Of Class II malocclusions, there are two subcategories: Division I (characterized by increased overjet and a retrognathic mandible) and Division II (in which maxillary lateral incisors or canines are proclined relative to the central incisors). Class II Division I malocclusions are the more common of the two in the European population []. National estimates in the USA indicate that 23% of children, 15% of youths, and 13% of adults have a discrepancy of 5\u00a0mm or more in overjet alone, thereby signifying a Class II Division I malocclusion []. Left untreated, Class II malocclusions can pose a variety of complications both present and future including those in the functional, psychological, and sociological realms [, ].Treatment options for Class II Division 1 malocclusions are three-pronged: orthopedic growth modification, masking with extractions of premolars, and orthognathic surgery. Each option has been proven to be an effective means of treatment [\u2013]. The decision as to which path to take depends on a variety of factors: time (as in, age of patient) and magnitude (amount of discrepancy: mild, moderate, or severe) []. A significant skeletal component is usually present in severe Class II Division 1 malocclusions. In these cases, the ideal method of treatment is orthodontic treatment in conjunction with orthognathic surgery as this is the only treatment which addresses the skeletal base discrepancy. However, due to finances or personal preference, patients are not always accepting of this option. In these situations, one of the other modes of treatment may be attempted in lieu of orthognathic surgery. In a younger patient, many orthopedic options achieve good facial harmony. However, while a masking treatment can address occlusal discrepancies, it will not improve skeletal position and therefore profile esthetics [, \u2013].Depending on the type of Class II corrector used (for example: Herbst, Twin Block, Headgear, or Forsus appliances), the end of treatment skeletal outcomes could vary [\u2013]. Despite the fact that, especially in the USA, Class II malocclusions are possibly the most common malocclusions encountered by practitioners in private practice and in residency programs, there is little agreement on the best practice modality. This could be because treatment is multi-factorial, depending on age, timing of treatment, and patient concerns and desires. There is a paucity of studies that have compared outcomes of surgical versus non-surgical treatment of adolescent patients []. This is an important age to assess treatment outcomes because it is one of the most common ages for orthodontic treatment and treatment options might be confined depending on completion of the pubertal growth spurt.The objective of the present study is to examine end-of-treatment cast-based and cepaholometric outcomes in patients with Class II Division I malocclusions treated orthodontically in conjunction with orthognathic surgery or without any orthognathic surgery. The study tests the hypothesis that end-of-treatment outcomes differ between the two treatment approaches.After the records were gathered, 60 patients were identified which fulfilled the inclusion criteria: 40 non-surgical and 20 surgical cases were included in the study. The study cohort was comprised of 28 female patients (21 in the non-surgical group and 7 in the surgical group) and 32 male patients (19 in the non-surgical group and 13 in the surgical group). Two patients were identified as Hispanic and two as multi-racial (Caucasian-African American). The remaining 56 patients were Caucasian. The mean age of the surgical group at the start of treatment was 14.8\u00a0years (compared to 12.9\u00a0years in the non-surgical group) [\u2009<\u20090.001]. The mean age of the surgical group at the end of treatment was 17.4\u00a0years (compared to 15.4\u00a0years in the non-surgical group) [\u2009<\u20090.001]. The duration of treatment for the surgical group was 2.6\u00a0years (compared to 2.5\u00a0years in the non-surgical group).Both groups utilized TADs or HG for anchorage purposes. The breakdown in each group was as follows: non-surgical group\u2014headgear (\u2009=\u200929), headgear and TADs (1), TADs with no headgear (\u2009=\u20093), and neither headgear nor TADs (\u2009=\u20097)\u2014and surgical group\u2014headgear (\u2009=\u20093), TADs in the lower arch only (\u2009=\u20091), neither headgear nor TADs (\u2009=\u200916).In the non-surgical group, the retention options delivered were as follows: fixed (bonded) retainers on the lingual aspect of maxillary central incisors along with Hawley retainers (\u2009=\u20092), Hawley retainers only (2), Hawley retainer and bonded lower retainer (2), Hawley retainers only (33), and tooth positioner followed by Hawley retainers (1). In the surgical group, the retention protocols included the following: Hawley retainers only (17), tooth positioner and Hawley retainers (2), and one patient was given a tooth positioner and never returned for the Hawley retainer.Consent deband, indicating premature treatment completion, was tracked in each group. Of the non-surgical patients, 32 did not have a consent deband. The remaining 8 patients opted for consent deband: 1 finished with a crossbite, 1 consent debanded due to patient burnout, 1 decided to stop treatment as correction could not be achieved and would consider surgery or extractions at a later time point, and 5 consent debanded with no reason indicated. In the surgical group, 15 did not consent deband and the remaining 5 did. Consent deband with no reason indicated was done in 4 patients and 1 with the reason being that they did not want to wear their elastics anymore.The present study is a retrospective analysis of consecutively treated patients with Class II Division I malocclusions. The goal of our study was to compare the end-of-treatment outcomes in patients with Class II Division I malocclusions who were treated surgically or non-surgically. Our findings showed that the cast-grading outcomes were similar between the surgical and the non-surgical treatment groups and certain end-of-treatment cephalometric values differed between the two groups. Our results showed that after adjustment for all available patient level covariates, the final mean ABO-COGS deband score in the surgical group was 0.854 points lower than that in the non-surgical group. The deband ANB angle in the surgical group was 2.24\u00b0 lower than that in the non-surgical group. This indicates that the maxilla/mandible relationship improved in the surgical group to a greater extent. This can be expected since skeletal positions change with a surgical treatment. Both FMIA and IMPA angle give information about lower incisor position. The deband FMIA angle and the IMPA angle indicate the position of the mandibular incisors. Our study results showed that the FMIA angle was not significantly different between the two treatment groups. The deband IMPA was significantly higher for the non-surgical group (100.4\u00b0) compared to the surgical group (92.3\u00b0). However, this difference became statistically non-significant once age, gender initial discrepancy index, and other cephalometric variables were adjusted in the regression models. Following adjusting for all confounders, those treated surgically had 3.3\u00b0 lower IMPA compared to those treated non-surgically. This indicates that the mandibular incisors were more upright at the end of treatment in the surgical group. We would expect these values to change according to which treatment plan, surgical or non-surgical, was chosen. For instance, if lower premolars were extracted before a mandibular advancement, we would expect some uprighting of the lower incisors during space closure. Also, initial crowding would have an effect on incisor position. If no extractions were done, the way to gain arch length to resolve lower anterior crowding is to procline the lower incisors. The deband upper incisors to SN plane angle, a measurement of upper incisor position, was shown to be 10.56\u00b0 higher in the surgical treatment group (\u2009=\u20090.001) after adjustment of all covariates in the regression model. We expected this finding because, if a surgical option cannot be entertained, a masking approach by extracting upper premolars is most likely considered instead. During space closure, there will be uprighting of the upper incisors, thereby leading to a decreased upper incisor to SN plane angle. This finding could also be explained in that for a surgical treatment option where teeth might not need to be extracted, if there is an existing upper anterior crowding, incisors will be proclined to gain space for alignment. Deband cephalometric overbite was found to be 0.606\u00a0mm lower in the surgical group while deband cephalometric overjet was shown to be 0.188\u00a0mm higher in the non-surgical treatment group after adjustment for all covariates in the regression models and these were not statistically significant.One of the first pieces of literature analyzing need for orthognathic surgery based on severity was put forth by Proffit et al. in 1992 []. When reviewing an adolescent population treated non-surgically (through camouflage treatment) or surgically, Proffit et al. identified certain parameters which might be useful when deciding treatment. They evaluated cephalometric and cast measurement before and after treatment to determine efficacy of treatment. Our study found end-of-treatment occlusion to be similar in both groups. This was supported by work of Proffit et al. as well. Our study showed that overjet was slightly higher in the surgical group, which was not seen in the study by Proffit et al. This could be attributed to differing practitioners\u2019 approach to treatment or variability in the success of the surgical treatment in respective surgical populations.Mihalik et al. performed a long-term follow-up of Class II adults treated with camouflage treatment or surgical treatment and analyzed post-deband results []. Patients in this population were recalled 12\u00a0years after treatment. This group found that both groups showed acceptable correction of the malocclusion. This was echoed by our study which found ABO cast grading outcomes at the end of treatment was not significantly different between the surgical and non-surgical groups. At recall, Mahalik et al. reported that in both populations, overbite increased to a small extent and overjet increased in the surgical group by 10\u201320% []. Our study found that deband cephalometric overbite was lower in the surgical group (compared to non-surgical) and overjet was increased compared to the non-surgical group, although neither value was statistically significant. This might be expected because with camouflage treatment, as the upper incisors are retracted, overbite increases.In an adolescent population (less than 20\u00a0years of age), Tulloch et al. discussed the difficulty in treatment planning as these patients might still be undergoing growth. This study emphasized that in severe cases, surgical treatment is most likely the best option []. They examined 500 patients in a study with similar inclusion criteria as ours. Patients were treated non-surgically or surgically and end-of-treatment outcomes were reviewed based on division into three categories: orthodontic success, orthodontic failure, and surgical success. This study assessed success of treatment through reduction in overjet to less than 4\u00a0mm []. Cephalometric radiographs were reviewed and patients were placed into two sub-groups based on gender. Initial ANB in this study was about 6\u00b0, similar to that in our study, but the initial overjet measurement is that both groups were significantly more (7.8 and 8.6\u00a0mm) when compared to our population (2.9 versus 3.1\u00a0mm). This study found that 98% of patients in their entire population did not meet their criteria for correction of overjet []. Since our study evaluated ANB change as a measure of AP correction, we were able to focus exclusively on skeletal position instead of tooth position. Tulloch concluded that neither gender nor age were associated with success of correction of overjet and concluded that more factors go into a \u201csuccessful\u201d or \u201cunsuccessful\u201d case than practitioners might think. Since these factors were held constant in our linear regression models, we were able to analyze differing variables without the risk of bias.Kinzinger et al. studied outcomes in patients with Class II Division I malocclusions where in 60 young adults were evaluated after a surgical or non-surgical treatment []. Their results showed changes in all skeletal categories, as can be presumed because with this method of treatment, the skeletal base is being influenced directly. Each group in this study achieved a reduction in overjet. The surgical group was found to have significant protrusion of upper incisors, as did our research. This might be attributed to the biomechanical differences in treating a surgical case versus a non-surgical case. One might imagine that not only will a camouflage treatment increase overbite as incisors are retracted, but they will also upright. If a surgical patient has minimal crowding, it might not be outside the realm of possibility that the practitioner might choose to procline the upper incisors to allow for the alignment of the teeth before the patient is sent for surgery.The findings of our study should be interpreted keeping the inherent limitations of retrospective studies in perspective. What we found is an association and not a true causal effect. Our analysis was limited to the variables we could gather from the treatment chart notes. The population in our study was relatively homogenous considering the location of the dental school and the population it serves. Consequently, our study results cannot be generalized to all Class II Division I malocclusions.We can conclude that amongst Class II Division I cases identified in this study, there were some differences in deband outcomes between non-surgical and surgical populations. Those treated surgically had a significantly larger reduction in ANB angle and increased maxillary incisor proclination compared to those treated non-surgically. Further information should be gathered at other institutions to compile a more diverse picture of successful treatment options in the Class II Division I population."}
]