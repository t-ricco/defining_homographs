{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kojak\n",
    "\n",
    "Classifying the meaning of homographs by their context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a custom lemmatizer/tokenizer with stopwords\n",
    "\n",
    "def get_wordnet_pos_aux(word):\n",
    "    \n",
    "    treebank_tag = pos_tag([word])[0][1]\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return word, wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return word, wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return word, wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return word, wordnet.ADV\n",
    "    else:\n",
    "        return word, 'n'\n",
    "    \n",
    "def get_wordnet_pos(words):\n",
    "    return [get_wordnet_pos_aux(x) for x in words]\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t1,t2) for t1,t2 in get_wordnet_pos(word_tokenize(doc))]\n",
    "    \n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“']\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'In the early day of physic , the impossibility to describe life and pleasure be not see a a shortcoming , because neither sens nor material property nor scale be thought to be related to motion . And pleasure be not consider a serious subject of investigation for a respectable researcher anyway . Today , the situation be different . In our adventure we have learn that our sens of time , hearing , touch , smell and sight be primarily detector of motion . Without motion , there would be no sens . Furthermore , all detector be make of matter . During the exploration on electromagnetism we begin to understand that all property of matter be due to motion of charge constituent . Density , stiffness , colour and all other material property result from the electromagnetic behaviour of the Lego brick of matter : namely , the molecule , the atom and the electron . Thus , the property of matter be also consequence of motion . Moreover , we saw that these tiny constituent be not correctly described by classical electrodynamics . We even found that light itself do not behave classically . Therefore the inability of classical physic to describe matter , light and the sens be indeed due to it intrinsic limitation . '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([x+' ' for x in lt(corpus[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare stopwords, preprocess the data from source file abstracts.json\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop += ['?','!','.',',',':',';','[',']','[]','“' ]\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“', '?', '!', '’', 'et', 'al.', 'study', \"\"]\n",
    "stop = set(stop)\n",
    "\n",
    "class MyPapers(object):\n",
    "    # a memory-friendly way to load a large corpora\n",
    "     def __init__(self, dirname):\n",
    "            self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "        with open(self.dirname) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "        # iterate through all file names in our directory\n",
    "        for paper in data:\n",
    "            sentences = sent_tokenize(paper['full_text'])\n",
    "            for sentence in sentences:\n",
    "                try:\n",
    "                    line = re.sub(r'[?\\.,!:;\\(\\)“\\[\\]]',' ',sentence)\n",
    "                    line = [word for word in line.lower().split() if word not in stop]\n",
    "                    yield line\n",
    "                except:\n",
    "                    print(\"Empty line found\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Instantiate iterable on the data\n",
    "\n",
    "#papers is an iterable of scholarly papers, tokenized for prcessing\n",
    "papers = MyPapers('data/train_data.json') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_sentence(json_file, word_list):\n",
    "    words = []\n",
    "    for w in word_list:\n",
    "        for _ in w.split('_'):\n",
    "            words.append(_)\n",
    "    for paper in json_file:\n",
    "        for sentence in tokenize.sent_tokenize(paper['full_text']):\n",
    "            if all(word in sentence.lower() for word in words):\n",
    "                return sentence\n",
    "            \n",
    "def MyPapers_plus(papers):\n",
    "    \n",
    "    phrases = gensim.models.phrases.Phrases(sentences = papers, min_count = 5, threshold = 150)\n",
    "    bigram = gensim.models.phrases.Phraser(phrases)\n",
    "    phrases2 = gensim.models.phrases.Phrases(sentences = bigram[papers], min_count = 5, threshold = 300)\n",
    "    trigram = gensim.models.phrases.Phraser(phrases2)\n",
    "    \n",
    "    return trigram[bigram[papers]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declaring the multiple definitions of \"charge\" and \"state\"\n",
    "\n",
    "charge_def = {1:\"(criminal law) a pleading describing some wrong or offense\",\n",
    "              2:\"a quantity of explosive to be set off at one time\",\n",
    "              3:\"the quantity of unbalanced electricity in a body (either positive or negative) and construed as an excess or deficiency of electrons\",\n",
    "              4:\"request for payment of a debt\"}\n",
    "\n",
    "state_def = {1:\"the condition of matter with respect to structure, form, constitution, phase, or the like\",\n",
    "            2:\"a politically unified people occupying a definite territory\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.word2vec.Word2Vec.load(\"data/journal.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.corpus_count\n",
    "vectors = model.wv\n",
    "vocab = vectors.vocab\n",
    "word_counts = generate_word_counts(MyPapers_plus(papers))\n",
    "len(vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_contexts(sequences, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    \n",
    "    # For each line (sentence)\n",
    "    for line in sequences:\n",
    "        L = len(line)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(line):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            context_words = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    #in_words.append([word])\n",
    "                    context_words.append(line[i])\n",
    "            x = word\n",
    "            y = context_words\n",
    "\n",
    "            #x = np.array(in_words,dtype=np.int32)\n",
    "            #y = np_utils.to_categorical(context_words, V)\n",
    "            yield(x,y)\n",
    "\n",
    "def read_glossary(glossary):\n",
    "    \n",
    "    vector_glossary = dict()\n",
    "    \n",
    "    for k, v in glossary.items():\n",
    "        vector_glossary[k] = {key:vector_average2(tokenize.word_tokenize(value)) for (key,value) in v.items()}\n",
    "    \n",
    "    return vector_glossary\n",
    "\n",
    "def get_target_sentences(documents, target):\n",
    "    \n",
    "    context_sentences = []\n",
    "\n",
    "    for document in documents:\n",
    "        #print(document[:15])\n",
    "        sentence = document\n",
    "        if target in sentence:\n",
    "            #str_sentence = streamlined_sentence(sentence)\n",
    "            #print[str_sentence]\n",
    "            sentence.remove(target)\n",
    "            context_sentences.append(sentence)\n",
    "            \n",
    "    return context_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "lt = LemmaTokenizer()\n",
    "tfidf = TfidfVectorizer(input=corpus, tokenizer = lt, stop_words = stop, min_df=1, max_df = .9)\n",
    "X = tfidf.fit_transform(corpus)\n",
    "vocab = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>+1</th>\n",
       "      <th>1.6</th>\n",
       "      <th>10.4.2</th>\n",
       "      <th>15</th>\n",
       "      <th>187</th>\n",
       "      <th>19</th>\n",
       "      <th>1910</th>\n",
       "      <th>1922</th>\n",
       "      <th>1970</th>\n",
       "      <th>1996</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.133663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.133663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.167738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083869</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.16859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.115239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.203767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.090591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        +1       1.6   10.4.2        15       187        19      1910  \\\n",
       "0      NaN       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "1      NaN       NaN      NaN  0.133663       NaN       NaN  0.133663   \n",
       "2      NaN       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "3      NaN       NaN      NaN       NaN  0.167738       NaN       NaN   \n",
       "4      NaN       NaN      NaN       NaN       NaN  0.111832       NaN   \n",
       "5      NaN       NaN  0.16859       NaN       NaN       NaN       NaN   \n",
       "6      NaN       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "7      NaN       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "8   0.0921       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "9      NaN  0.203767      NaN       NaN       NaN       NaN       NaN   \n",
       "10     NaN       NaN      NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "        1922      1970      1996  \n",
       "0        NaN       NaN       NaN  \n",
       "1        NaN       NaN       NaN  \n",
       "2        NaN       NaN       NaN  \n",
       "3        NaN  0.083869       NaN  \n",
       "4        NaN       NaN       NaN  \n",
       "5        NaN       NaN       NaN  \n",
       "6        NaN       NaN       NaN  \n",
       "7   0.115239       NaN       NaN  \n",
       "8        NaN       NaN       NaN  \n",
       "9        NaN       NaN       NaN  \n",
       "10       NaN       NaN  0.090591  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = pd.SparseDataFrame(data = X, columns = vocab)\n",
    "col = space.columns.values\n",
    "n=0\n",
    "space[col[n:n+10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.044432\n",
       "1     0.073683\n",
       "2     0.066484\n",
       "3     0.046234\n",
       "4          NaN\n",
       "5          NaN\n",
       "6     0.068601\n",
       "7     0.127054\n",
       "8          NaN\n",
       "9          NaN\n",
       "10         NaN\n",
       "Name: charge, dtype: float64\n",
       "BlockIndex\n",
       "Block locations: array([0, 6], dtype=int32)\n",
       "Block lengths: array([4, 2], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space['charge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          NaN\n",
       "1          NaN\n",
       "2          NaN\n",
       "3          NaN\n",
       "4          NaN\n",
       "5          NaN\n",
       "6          NaN\n",
       "7          NaN\n",
       "8     0.346166\n",
       "9     0.153175\n",
       "10    0.340492\n",
       "Name: state, dtype: float64\n",
       "BlockIndex\n",
       "Block locations: array([8], dtype=int32)\n",
       "Block lengths: array([3], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAFkCAYAAAANPR4aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAECj/w8MdPx9NTtKlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1172276d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_topics = 5\n",
    "svd = decomposition.TruncatedSVD(n_components=num_topics)\n",
    "doctopic = svd.fit_transform(X)\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.bar(range(num_topics),svd.explained_variance_ratio_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "atom             0.088992\n",
       "charge           0.070134\n",
       "classical        0.235246\n",
       "complete              NaN\n",
       "connection            NaN\n",
       "consequence      0.117623\n",
       "contrast              NaN\n",
       "court                 NaN\n",
       "crime                 NaN\n",
       "criminal              NaN\n",
       "detector         0.235246\n",
       "due              0.202998\n",
       "electron         0.117623\n",
       "everyday              NaN\n",
       "example               NaN\n",
       "expect                NaN\n",
       "experiment            NaN\n",
       "field                 NaN\n",
       "figure                NaN\n",
       "found            0.101499\n",
       "generally             NaN\n",
       "give                  NaN\n",
       "include               NaN\n",
       "individual            NaN\n",
       "investigation    0.117623\n",
       "light            0.202998\n",
       "matter           0.588115\n",
       "modern                NaN\n",
       "motion           0.507495\n",
       "physic           0.177984\n",
       "process               NaN\n",
       "related          0.117623\n",
       "see              0.117623\n",
       "serious          0.117623\n",
       "show                  NaN\n",
       "supreme               NaN\n",
       "therefore        0.117623\n",
       "time             0.117623\n",
       "tiny             0.101499\n",
       "without          0.117623\n",
       "would            0.117623\n",
       "year                  NaN\n",
       "“                     NaN\n",
       "”                     NaN\n",
       "Name: 0, dtype: float64\n",
       "BlockIndex\n",
       "Block locations: array([ 0,  5, 10, 19, 24, 28, 31, 36], dtype=int32)\n",
       "Block lengths: array([3, 1, 3, 1, 3, 2, 3, 5], dtype=int32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below I just copied and pasted code for an RNN (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# keras\n",
    "np.random.seed(13)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Activation, SimpleRNN, GRU, LSTM, Bidirectional, Convolution1D, MaxPooling1D, Merge, Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a custom lemmatizer/tokenizer with stopwords\n",
    "\n",
    "def get_wordnet_pos_aux(word):\n",
    "    \n",
    "    treebank_tag = pos_tag([word])[0][1]\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return word, wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return word, wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return word, wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return word, wordnet.ADV\n",
    "    else:\n",
    "        return word, 'n'\n",
    "    \n",
    "def get_wordnet_pos(words):\n",
    "    return [get_wordnet_pos_aux(x) for x in words]\n",
    "\n",
    "class Lemmatizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return \"\".join([ x + ' ' for x in \n",
    "                        [self.wnl.lemmatize(t1,t2) for t1,t2 in get_wordnet_pos(word_tokenize(doc))]])\n",
    "    \n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“']\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5,\n",
       "  1,\n",
       "  151,\n",
       "  152,\n",
       "  2,\n",
       "  20,\n",
       "  1,\n",
       "  153,\n",
       "  7,\n",
       "  82,\n",
       "  154,\n",
       "  6,\n",
       "  83,\n",
       "  4,\n",
       "  16,\n",
       "  84,\n",
       "  3,\n",
       "  3,\n",
       "  155,\n",
       "  85,\n",
       "  156,\n",
       "  32,\n",
       "  86,\n",
       "  87,\n",
       "  33,\n",
       "  86,\n",
       "  157,\n",
       "  4,\n",
       "  158,\n",
       "  7,\n",
       "  4,\n",
       "  88,\n",
       "  7,\n",
       "  10,\n",
       "  6,\n",
       "  83,\n",
       "  4,\n",
       "  16,\n",
       "  159,\n",
       "  3,\n",
       "  89,\n",
       "  160,\n",
       "  2,\n",
       "  90,\n",
       "  15,\n",
       "  3,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  1,\n",
       "  165,\n",
       "  4,\n",
       "  166,\n",
       "  5,\n",
       "  91,\n",
       "  167,\n",
       "  25,\n",
       "  34,\n",
       "  168,\n",
       "  8,\n",
       "  91,\n",
       "  32,\n",
       "  2,\n",
       "  92,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  6,\n",
       "  172,\n",
       "  4,\n",
       "  173,\n",
       "  35,\n",
       "  2,\n",
       "  10,\n",
       "  93,\n",
       "  10,\n",
       "  174,\n",
       "  94,\n",
       "  4,\n",
       "  54,\n",
       "  32,\n",
       "  175,\n",
       "  36,\n",
       "  35,\n",
       "  4,\n",
       "  95,\n",
       "  2,\n",
       "  21,\n",
       "  55,\n",
       "  1,\n",
       "  176,\n",
       "  56,\n",
       "  177,\n",
       "  25,\n",
       "  178,\n",
       "  7,\n",
       "  179,\n",
       "  8,\n",
       "  36,\n",
       "  33,\n",
       "  2,\n",
       "  21,\n",
       "  4,\n",
       "  37,\n",
       "  7,\n",
       "  10,\n",
       "  2,\n",
       "  19,\n",
       "  96,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  6,\n",
       "  36,\n",
       "  97,\n",
       "  87,\n",
       "  33,\n",
       "  183,\n",
       "  38,\n",
       "  1,\n",
       "  184,\n",
       "  185,\n",
       "  2,\n",
       "  1,\n",
       "  186,\n",
       "  187,\n",
       "  2,\n",
       "  21,\n",
       "  188,\n",
       "  1,\n",
       "  189,\n",
       "  1,\n",
       "  9,\n",
       "  6,\n",
       "  1,\n",
       "  98,\n",
       "  190,\n",
       "  1,\n",
       "  33,\n",
       "  2,\n",
       "  21,\n",
       "  4,\n",
       "  99,\n",
       "  100,\n",
       "  2,\n",
       "  10,\n",
       "  191,\n",
       "  25,\n",
       "  192,\n",
       "  8,\n",
       "  193,\n",
       "  57,\n",
       "  96,\n",
       "  4,\n",
       "  16,\n",
       "  194,\n",
       "  101,\n",
       "  11,\n",
       "  39,\n",
       "  195,\n",
       "  25,\n",
       "  196,\n",
       "  58,\n",
       "  8,\n",
       "  22,\n",
       "  197,\n",
       "  102,\n",
       "  16,\n",
       "  198,\n",
       "  199,\n",
       "  103,\n",
       "  1,\n",
       "  200,\n",
       "  2,\n",
       "  39,\n",
       "  20,\n",
       "  7,\n",
       "  82,\n",
       "  21,\n",
       "  22,\n",
       "  6,\n",
       "  1,\n",
       "  32,\n",
       "  4,\n",
       "  201,\n",
       "  37,\n",
       "  7,\n",
       "  26,\n",
       "  104,\n",
       "  202],\n",
       " [1,\n",
       "  40,\n",
       "  2,\n",
       "  203,\n",
       "  4,\n",
       "  88,\n",
       "  7,\n",
       "  1,\n",
       "  40,\n",
       "  2,\n",
       "  9,\n",
       "  5,\n",
       "  204,\n",
       "  1,\n",
       "  40,\n",
       "  2,\n",
       "  9,\n",
       "  4,\n",
       "  3,\n",
       "  205,\n",
       "  100,\n",
       "  2,\n",
       "  1,\n",
       "  41,\n",
       "  2,\n",
       "  105,\n",
       "  42,\n",
       "  206,\n",
       "  207,\n",
       "  17,\n",
       "  208,\n",
       "  15,\n",
       "  1,\n",
       "  40,\n",
       "  2,\n",
       "  9,\n",
       "  209,\n",
       "  8,\n",
       "  26,\n",
       "  4,\n",
       "  43,\n",
       "  11,\n",
       "  1,\n",
       "  10,\n",
       "  2,\n",
       "  98,\n",
       "  2,\n",
       "  210,\n",
       "  106,\n",
       "  6,\n",
       "  19,\n",
       "  211,\n",
       "  11,\n",
       "  1,\n",
       "  41,\n",
       "  2,\n",
       "  105,\n",
       "  59,\n",
       "  60,\n",
       "  3,\n",
       "  212,\n",
       "  213,\n",
       "  4,\n",
       "  214,\n",
       "  5,\n",
       "  215,\n",
       "  11,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  107,\n",
       "  220,\n",
       "  41,\n",
       "  221,\n",
       "  4,\n",
       "  222,\n",
       "  223],\n",
       " [18,\n",
       "  12,\n",
       "  61,\n",
       "  224,\n",
       "  1,\n",
       "  62,\n",
       "  6,\n",
       "  225,\n",
       "  2,\n",
       "  44,\n",
       "  5,\n",
       "  226,\n",
       "  63,\n",
       "  45,\n",
       "  108,\n",
       "  5,\n",
       "  18,\n",
       "  12,\n",
       "  4,\n",
       "  1,\n",
       "  109,\n",
       "  2,\n",
       "  227,\n",
       "  23,\n",
       "  6,\n",
       "  1,\n",
       "  109,\n",
       "  2,\n",
       "  228,\n",
       "  18,\n",
       "  229,\n",
       "  18,\n",
       "  230,\n",
       "  61,\n",
       "  231,\n",
       "  1,\n",
       "  110,\n",
       "  2,\n",
       "  44,\n",
       "  62,\n",
       "  55,\n",
       "  1,\n",
       "  18,\n",
       "  111,\n",
       "  232,\n",
       "  2,\n",
       "  233,\n",
       "  108,\n",
       "  4,\n",
       "  44,\n",
       "  62,\n",
       "  55,\n",
       "  12,\n",
       "  110,\n",
       "  90,\n",
       "  234,\n",
       "  235,\n",
       "  2,\n",
       "  19,\n",
       "  236,\n",
       "  6,\n",
       "  237],\n",
       " [238,\n",
       "  46,\n",
       "  239,\n",
       "  240,\n",
       "  27,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  5,\n",
       "  46,\n",
       "  1,\n",
       "  64,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  6,\n",
       "  251,\n",
       "  252,\n",
       "  4,\n",
       "  253,\n",
       "  254,\n",
       "  1,\n",
       "  14,\n",
       "  112,\n",
       "  27,\n",
       "  255,\n",
       "  3,\n",
       "  256,\n",
       "  19,\n",
       "  113,\n",
       "  46,\n",
       "  257,\n",
       "  14,\n",
       "  28,\n",
       "  29,\n",
       "  114,\n",
       "  85,\n",
       "  1,\n",
       "  30,\n",
       "  258,\n",
       "  47,\n",
       "  1,\n",
       "  259,\n",
       "  260,\n",
       "  2,\n",
       "  3,\n",
       "  115,\n",
       "  4,\n",
       "  1,\n",
       "  27,\n",
       "  261,\n",
       "  26,\n",
       "  116,\n",
       "  24,\n",
       "  262,\n",
       "  1,\n",
       "  45,\n",
       "  12,\n",
       "  117,\n",
       "  2,\n",
       "  115,\n",
       "  4,\n",
       "  6,\n",
       "  263,\n",
       "  8,\n",
       "  1,\n",
       "  117,\n",
       "  102,\n",
       "  16,\n",
       "  65,\n",
       "  3,\n",
       "  118,\n",
       "  1,\n",
       "  27,\n",
       "  264,\n",
       "  8,\n",
       "  26,\n",
       "  265,\n",
       "  16,\n",
       "  266,\n",
       "  3,\n",
       "  267,\n",
       "  23,\n",
       "  93,\n",
       "  268,\n",
       "  1,\n",
       "  37,\n",
       "  111,\n",
       "  269,\n",
       "  270,\n",
       "  2,\n",
       "  271,\n",
       "  6,\n",
       "  14,\n",
       "  28,\n",
       "  29,\n",
       "  119,\n",
       "  272,\n",
       "  273,\n",
       "  1,\n",
       "  274,\n",
       "  2,\n",
       "  275,\n",
       "  23,\n",
       "  24,\n",
       "  1,\n",
       "  46,\n",
       "  116,\n",
       "  1,\n",
       "  14,\n",
       "  66,\n",
       "  276,\n",
       "  28,\n",
       "  29,\n",
       "  114,\n",
       "  7,\n",
       "  65,\n",
       "  3,\n",
       "  118,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  28,\n",
       "  29,\n",
       "  67],\n",
       " [3,\n",
       "  280,\n",
       "  281,\n",
       "  120,\n",
       "  2,\n",
       "  3,\n",
       "  121,\n",
       "  282,\n",
       "  283,\n",
       "  2,\n",
       "  22,\n",
       "  4,\n",
       "  3,\n",
       "  68,\n",
       "  9,\n",
       "  284,\n",
       "  4,\n",
       "  57,\n",
       "  285,\n",
       "  286,\n",
       "  9,\n",
       "  122,\n",
       "  22,\n",
       "  48,\n",
       "  287,\n",
       "  288,\n",
       "  1,\n",
       "  289,\n",
       "  290,\n",
       "  4,\n",
       "  123,\n",
       "  291,\n",
       "  3,\n",
       "  3,\n",
       "  69,\n",
       "  124,\n",
       "  292,\n",
       "  5,\n",
       "  36,\n",
       "  31,\n",
       "  84,\n",
       "  125,\n",
       "  293,\n",
       "  15,\n",
       "  3,\n",
       "  294,\n",
       "  295,\n",
       "  70,\n",
       "  1,\n",
       "  22,\n",
       "  123,\n",
       "  11,\n",
       "  17,\n",
       "  9,\n",
       "  4,\n",
       "  296,\n",
       "  58,\n",
       "  7,\n",
       "  297,\n",
       "  3,\n",
       "  69,\n",
       "  124,\n",
       "  5,\n",
       "  71,\n",
       "  7,\n",
       "  126,\n",
       "  25,\n",
       "  298,\n",
       "  127,\n",
       "  38,\n",
       "  72,\n",
       "  20,\n",
       "  299,\n",
       "  3,\n",
       "  122,\n",
       "  9,\n",
       "  4,\n",
       "  300,\n",
       "  11,\n",
       "  128,\n",
       "  35,\n",
       "  47,\n",
       "  3,\n",
       "  68,\n",
       "  35,\n",
       "  4,\n",
       "  301,\n",
       "  47,\n",
       "  1,\n",
       "  302,\n",
       "  303,\n",
       "  128,\n",
       "  304,\n",
       "  6,\n",
       "  305,\n",
       "  306,\n",
       "  3,\n",
       "  69,\n",
       "  307,\n",
       "  1,\n",
       "  31,\n",
       "  49,\n",
       "  308,\n",
       "  8,\n",
       "  309,\n",
       "  310,\n",
       "  42,\n",
       "  16,\n",
       "  4,\n",
       "  311],\n",
       " [5,\n",
       "  129,\n",
       "  92,\n",
       "  130,\n",
       "  4,\n",
       "  312,\n",
       "  3,\n",
       "  89,\n",
       "  313,\n",
       "  131,\n",
       "  1,\n",
       "  314,\n",
       "  112,\n",
       "  27,\n",
       "  34,\n",
       "  315,\n",
       "  8,\n",
       "  316,\n",
       "  317,\n",
       "  15,\n",
       "  318,\n",
       "  23,\n",
       "  113,\n",
       "  17,\n",
       "  44,\n",
       "  97,\n",
       "  319,\n",
       "  18,\n",
       "  320,\n",
       "  4,\n",
       "  321,\n",
       "  132,\n",
       "  133,\n",
       "  4,\n",
       "  61,\n",
       "  3,\n",
       "  322,\n",
       "  65,\n",
       "  323,\n",
       "  2,\n",
       "  130,\n",
       "  6,\n",
       "  4,\n",
       "  324,\n",
       "  121,\n",
       "  3,\n",
       "  4,\n",
       "  134,\n",
       "  5,\n",
       "  135,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  132,\n",
       "  133],\n",
       " [3,\n",
       "  136,\n",
       "  328,\n",
       "  2,\n",
       "  20,\n",
       "  1,\n",
       "  329,\n",
       "  2,\n",
       "  10,\n",
       "  26,\n",
       "  330,\n",
       "  331,\n",
       "  1,\n",
       "  332,\n",
       "  333,\n",
       "  72,\n",
       "  10,\n",
       "  6,\n",
       "  49,\n",
       "  1,\n",
       "  60,\n",
       "  7,\n",
       "  1,\n",
       "  50,\n",
       "  2,\n",
       "  129,\n",
       "  20,\n",
       "  334,\n",
       "  60,\n",
       "  335,\n",
       "  1,\n",
       "  336,\n",
       "  2,\n",
       "  1,\n",
       "  337,\n",
       "  6,\n",
       "  4,\n",
       "  37,\n",
       "  7,\n",
       "  3,\n",
       "  137,\n",
       "  7,\n",
       "  10,\n",
       "  8,\n",
       "  4,\n",
       "  138,\n",
       "  51,\n",
       "  338,\n",
       "  1,\n",
       "  137,\n",
       "  4,\n",
       "  43,\n",
       "  15,\n",
       "  339,\n",
       "  10,\n",
       "  11,\n",
       "  1,\n",
       "  340,\n",
       "  73,\n",
       "  139,\n",
       "  15,\n",
       "  341,\n",
       "  10,\n",
       "  11,\n",
       "  1,\n",
       "  342,\n",
       "  2,\n",
       "  22,\n",
       "  343,\n",
       "  6,\n",
       "  15,\n",
       "  57,\n",
       "  10,\n",
       "  11,\n",
       "  1,\n",
       "  344,\n",
       "  73,\n",
       "  345,\n",
       "  1,\n",
       "  346,\n",
       "  19,\n",
       "  106,\n",
       "  6,\n",
       "  1,\n",
       "  347,\n",
       "  73,\n",
       "  348],\n",
       " [24,\n",
       "  3,\n",
       "  107,\n",
       "  2,\n",
       "  349,\n",
       "  350,\n",
       "  5,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  6,\n",
       "  354,\n",
       "  355,\n",
       "  136,\n",
       "  3,\n",
       "  356,\n",
       "  31,\n",
       "  7,\n",
       "  357,\n",
       "  1,\n",
       "  358,\n",
       "  2,\n",
       "  21,\n",
       "  359,\n",
       "  74,\n",
       "  360,\n",
       "  8,\n",
       "  140,\n",
       "  75,\n",
       "  50,\n",
       "  141,\n",
       "  3,\n",
       "  361,\n",
       "  15,\n",
       "  362,\n",
       "  19,\n",
       "  363,\n",
       "  19,\n",
       "  4,\n",
       "  364,\n",
       "  5,\n",
       "  365,\n",
       "  9,\n",
       "  103,\n",
       "  74,\n",
       "  366,\n",
       "  3,\n",
       "  76,\n",
       "  2,\n",
       "  367,\n",
       "  9,\n",
       "  368,\n",
       "  38,\n",
       "  17,\n",
       "  77,\n",
       "  11,\n",
       "  369,\n",
       "  370,\n",
       "  17,\n",
       "  140,\n",
       "  75,\n",
       "  50,\n",
       "  74,\n",
       "  58,\n",
       "  8,\n",
       "  1,\n",
       "  76,\n",
       "  371,\n",
       "  51,\n",
       "  142,\n",
       "  143,\n",
       "  76,\n",
       "  3,\n",
       "  49,\n",
       "  5,\n",
       "  125,\n",
       "  372,\n",
       "  54,\n",
       "  9,\n",
       "  373,\n",
       "  1,\n",
       "  75,\n",
       "  50,\n",
       "  374,\n",
       "  5,\n",
       "  375,\n",
       "  376,\n",
       "  59,\n",
       "  4,\n",
       "  5,\n",
       "  377,\n",
       "  71,\n",
       "  7,\n",
       "  126,\n",
       "  94,\n",
       "  4,\n",
       "  127,\n",
       "  38,\n",
       "  39,\n",
       "  20],\n",
       " [5,\n",
       "  71,\n",
       "  7,\n",
       "  1,\n",
       "  39,\n",
       "  72,\n",
       "  378,\n",
       "  1,\n",
       "  13,\n",
       "  2,\n",
       "  3,\n",
       "  41,\n",
       "  78,\n",
       "  1,\n",
       "  379,\n",
       "  380,\n",
       "  3,\n",
       "  78,\n",
       "  34,\n",
       "  4,\n",
       "  16,\n",
       "  101,\n",
       "  11,\n",
       "  1,\n",
       "  381,\n",
       "  2,\n",
       "  79,\n",
       "  1,\n",
       "  382,\n",
       "  383,\n",
       "  2,\n",
       "  59,\n",
       "  384,\n",
       "  4,\n",
       "  1,\n",
       "  78,\n",
       "  95,\n",
       "  2,\n",
       "  3,\n",
       "  68,\n",
       "  144,\n",
       "  5,\n",
       "  1,\n",
       "  385,\n",
       "  31,\n",
       "  1,\n",
       "  31,\n",
       "  49,\n",
       "  8,\n",
       "  3,\n",
       "  386,\n",
       "  79,\n",
       "  56,\n",
       "  3,\n",
       "  145,\n",
       "  77,\n",
       "  144,\n",
       "  13,\n",
       "  146,\n",
       "  43,\n",
       "  70,\n",
       "  147,\n",
       "  80,\n",
       "  6,\n",
       "  146,\n",
       "  43,\n",
       "  148,\n",
       "  147,\n",
       "  80,\n",
       "  387,\n",
       "  3,\n",
       "  145,\n",
       "  9,\n",
       "  5,\n",
       "  17,\n",
       "  77,\n",
       "  13,\n",
       "  34,\n",
       "  54,\n",
       "  104,\n",
       "  388,\n",
       "  47,\n",
       "  24,\n",
       "  1,\n",
       "  79,\n",
       "  17,\n",
       "  9,\n",
       "  4,\n",
       "  389,\n",
       "  5,\n",
       "  17,\n",
       "  70,\n",
       "  13,\n",
       "  48,\n",
       "  5,\n",
       "  3,\n",
       "  148,\n",
       "  13],\n",
       " [3,\n",
       "  18,\n",
       "  390,\n",
       "  138,\n",
       "  149,\n",
       "  24,\n",
       "  3,\n",
       "  64,\n",
       "  391,\n",
       "  3,\n",
       "  392,\n",
       "  48,\n",
       "  13,\n",
       "  18,\n",
       "  30,\n",
       "  48,\n",
       "  5,\n",
       "  63,\n",
       "  393,\n",
       "  24,\n",
       "  3,\n",
       "  64,\n",
       "  394,\n",
       "  3,\n",
       "  45,\n",
       "  12,\n",
       "  23,\n",
       "  395,\n",
       "  6,\n",
       "  45,\n",
       "  12,\n",
       "  23,\n",
       "  4,\n",
       "  134,\n",
       "  5,\n",
       "  135,\n",
       "  80,\n",
       "  119,\n",
       "  396,\n",
       "  2,\n",
       "  12],\n",
       " [13,\n",
       "  52,\n",
       "  42,\n",
       "  99,\n",
       "  81,\n",
       "  13,\n",
       "  30,\n",
       "  51,\n",
       "  12,\n",
       "  131,\n",
       "  3,\n",
       "  13,\n",
       "  66,\n",
       "  397,\n",
       "  398,\n",
       "  13,\n",
       "  30,\n",
       "  52,\n",
       "  399,\n",
       "  56,\n",
       "  3,\n",
       "  400,\n",
       "  42,\n",
       "  150,\n",
       "  63,\n",
       "  401,\n",
       "  402,\n",
       "  30,\n",
       "  15,\n",
       "  120,\n",
       "  3,\n",
       "  403,\n",
       "  2,\n",
       "  14,\n",
       "  53,\n",
       "  52,\n",
       "  81,\n",
       "  7,\n",
       "  150,\n",
       "  14,\n",
       "  53,\n",
       "  404,\n",
       "  405,\n",
       "  12,\n",
       "  14,\n",
       "  406,\n",
       "  407,\n",
       "  141,\n",
       "  2,\n",
       "  408,\n",
       "  67,\n",
       "  14,\n",
       "  53,\n",
       "  409,\n",
       "  410,\n",
       "  12,\n",
       "  4,\n",
       "  81,\n",
       "  51,\n",
       "  12,\n",
       "  11,\n",
       "  411,\n",
       "  1,\n",
       "  13,\n",
       "  66,\n",
       "  6,\n",
       "  14,\n",
       "  53,\n",
       "  52,\n",
       "  6,\n",
       "  412,\n",
       "  413,\n",
       "  5,\n",
       "  1,\n",
       "  14,\n",
       "  28,\n",
       "  29,\n",
       "  5,\n",
       "  142,\n",
       "  143,\n",
       "  149,\n",
       "  414,\n",
       "  415,\n",
       "  6,\n",
       "  416,\n",
       "  139,\n",
       "  67]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For simplicity, one \"sentence\" per line & ensuring a count of two words min\n",
    "\n",
    "ltzr = Lemmatizer()\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "corpus = [ltzr(c).encode('utf-8', 'ignore') for c in corpus]\n",
    "#print(corpus)\n",
    "\n",
    "# Tokenize using Keras\n",
    "my_filter='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "tokenizer = Tokenizer(filters=my_filter)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Convert tokenized sentences to sequence format\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "\n",
    "# Vocab size\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Dimension of our network\n",
    "dim = 100\n",
    "window_size = 6\n",
    "\n",
    "# What is this output? \n",
    "#sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(sequences, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    \n",
    "    # For each line (sentence)\n",
    "    for line in sequences:\n",
    "        L = len(line)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(line):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            context_words = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    #in_words.append([word])\n",
    "                    context_words.append(line[i])\n",
    "            x = word\n",
    "            y = context_words\n",
    "\n",
    "            #x = np.array(in_words,dtype=np.int32)\n",
    "            #y = np_utils.to_categorical(context_words, V)\n",
    "            yield(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_window(x,y):\n",
    "    index = tokenizer.word_index\n",
    "    print(index.keys()[index.values().index(x)])\n",
    "    print(map(lambda n: index.keys()[index.values().index(n)],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "charge\n",
      "['due', 'to', 'motion', 'of', 'constituent', 'density', 'stiffness', 'colour']\n",
      "1\n",
      "charge\n",
      "['of', 'mass', 'e', 'and', 'constrain', 'by', 'the', 'quantum']\n",
      "2\n",
      "charge\n",
      "['investigation', 'arrest', 'filing', 'of', 'trial', 'and', 'appeal']\n",
      "3\n",
      "charge\n",
      "['court', 'disallowed', 'a', 'murder', 'against', 'keeler', 'under', 'california']\n",
      "4\n",
      "charge\n",
      "['constant', 'h', 'the', 'elementary', 'e', 'and', 'the', 'boltzmann']\n",
      "5\n",
      "charge\n",
      "['a', 'polarizers', 'for', 'rotate', 'rotating', 'charge', 'be', 'present']\n",
      "6\n",
      "charge\n",
      "['for', 'rotate', 'charge', 'rotating', 'be', 'present', 'in', 'every']\n",
      "7\n",
      "state\n",
      "['classical', 'everyday', 'case', 'the', 'of', 'a', 'quantum', 'system']\n",
      "8\n",
      "state\n",
      "['a', 'general', 'oven', 'particle', 'sometimes', 'give', 'up', 'say']\n",
      "9\n",
      "state\n",
      "['atom', 'in', 'an', 'oven', 'have', 'no', 'intrinsic', 'orientation']\n",
      "10\n",
      "state\n",
      "['either', 'in', 'an', 'up', 'or', 'in', 'a', 'down']\n",
      "11\n",
      "state\n",
      "['or', 'in', 'a', 'down']\n",
      "12\n",
      "state\n",
      "['violates', 'a', 'federal', 'or', 'criminal', 'statute', 'or', 'in']\n",
      "13\n",
      "state\n",
      "['citizen', 'can', 'also', 'vote']\n",
      "14\n",
      "state\n",
      "['citizen', 'can', 'also', 'vote', 'statute', 'into', 'law', 'although']\n",
      "15\n",
      "state\n",
      "['into', 'law', 'although', 'a', 'legislature', 'adopts', 'most', 'state']\n",
      "16\n",
      "state\n",
      "['state', 'legislature', 'adopts', 'most', 'statute', 'citizen', 'voting', 'on']\n",
      "17\n",
      "state\n",
      "['law', 'by', 'both', 'the', 'legislature', 'and', 'california', 's']\n"
     ]
    }
   ],
   "source": [
    "win_size = 4\n",
    "count = 0\n",
    "for x,y in generate_data(sequences, win_size, V):\n",
    "    if [x] in tokenizer.texts_to_sequences([\"charge\", \"state\"]):\n",
    "        print(count)\n",
    "        print_window(x, y)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array(2*[3]+2*[1]+3*[3]+5*[1]+6*[2]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19  37   7  10   2  96 180 181 182]\n",
      " [ 19   2 210 106   6 211  11   1  41]\n",
      " [ 19   0  90 234 235   2 236   6 237]\n",
      " [ 19  27 255   3 256 113  46 257  14]\n",
      " [ 19  73 345   1 346 106   6   1 347]\n",
      " [ 19   3 361  15 362 363  19   4 364]\n",
      " [ 19  15 362  19 363   4 364   5 365]\n",
      " [ 13  39  72 378   1   2   3  41  78]\n",
      " [ 13   3 145  77 144 146  43  70 147]\n",
      " [ 13   9   5  17  77  34  54 104 388]\n",
      " [ 13 389   5  17  70  48   5   3 148]\n",
      " [ 13   0   0   0   0  48   5   3 148]\n",
      " [ 13 391   3 392  48  18  30  48   5]\n",
      " [ 13   0   0   0   0  52  42  99  81]\n",
      " [ 13  52  42  99  81  30  51  12 131]\n",
      " [ 13  51  12 131   3  66 397 398  13]\n",
      " [ 13  13  66 397 398  30  52 399  56]\n",
      " [ 13  12  11 411   1  66   6  14  53]]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 2*win_size\n",
    "Z = []\n",
    "X = []\n",
    "for x,y in generate_data(sequences, win_size, V):\n",
    "    if [x] in tokenizer.texts_to_sequences([\"charge\", \"state\"]):\n",
    "        X.append(x)\n",
    "        Z.append(y)\n",
    "        \n",
    "Z = sequence.pad_sequences(Z, maxlen=maxlen)\n",
    "X = np.array(X).reshape(-1,1)\n",
    "X = np.concatenate([X,Z], axis = 1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((18, 9), (18, 1))\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train, test, split\n",
    "X_train,X_test, y_train,  y_test = train_test_split(X,Y)\n",
    "\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 3, 2, 1, 1, 3, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen+1))\n",
    "# Bidirectional LSTM!!!\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 13 samples, validate on 5 samples\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 0s - loss: 0.6886 - acc: 0.3846 - val_loss: 0.6491 - val_acc: 0.2000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s - loss: 0.6386 - acc: 0.4615 - val_loss: 0.6198 - val_acc: 0.2000\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s - loss: 0.6152 - acc: 0.4615 - val_loss: 0.5893 - val_acc: 0.2000\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s - loss: 0.5720 - acc: 0.4615 - val_loss: 0.5570 - val_acc: 0.2000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s - loss: 0.5289 - acc: 0.4615 - val_loss: 0.5220 - val_acc: 0.2000\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s - loss: 0.4795 - acc: 0.4615 - val_loss: 0.4835 - val_acc: 0.2000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s - loss: 0.4458 - acc: 0.4615 - val_loss: 0.4410 - val_acc: 0.2000\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s - loss: 0.3799 - acc: 0.4615 - val_loss: 0.3937 - val_acc: 0.2000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s - loss: 0.3183 - acc: 0.4615 - val_loss: 0.3407 - val_acc: 0.2000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s - loss: 0.2715 - acc: 0.4615 - val_loss: 0.2808 - val_acc: 0.2000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s - loss: 0.2043 - acc: 0.4615 - val_loss: 0.2126 - val_acc: 0.2000\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s - loss: 0.0937 - acc: 0.4615 - val_loss: 0.1348 - val_acc: 0.2000\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s - loss: -0.0133 - acc: 0.4615 - val_loss: 0.0452 - val_acc: 0.2000\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s - loss: -0.1355 - acc: 0.4615 - val_loss: -0.0584 - val_acc: 0.2000\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s - loss: -0.2531 - acc: 0.4615 - val_loss: -0.1785 - val_acc: 0.2000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s - loss: -0.3837 - acc: 0.4615 - val_loss: -0.3184 - val_acc: 0.2000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s - loss: -0.5698 - acc: 0.4615 - val_loss: -0.4820 - val_acc: 0.2000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s - loss: -0.7156 - acc: 0.4615 - val_loss: -0.6742 - val_acc: 0.2000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s - loss: -0.9537 - acc: 0.4615 - val_loss: -0.9013 - val_acc: 0.2000\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s - loss: -1.3811 - acc: 0.4615 - val_loss: -1.1699 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c01b750>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 13 samples, validate on 5 samples\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 0s - loss: -1.4877 - acc: 0.4615 - val_loss: -1.4663 - val_acc: 0.2000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s - loss: -1.7595 - acc: 0.4615 - val_loss: -1.8069 - val_acc: 0.2000\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s - loss: -2.2424 - acc: 0.4615 - val_loss: -2.1999 - val_acc: 0.2000\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s - loss: -2.6448 - acc: 0.4615 - val_loss: -2.6534 - val_acc: 0.2000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s - loss: -2.8932 - acc: 0.4615 - val_loss: -3.1758 - val_acc: 0.2000\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s - loss: -3.7926 - acc: 0.4615 - val_loss: -3.7741 - val_acc: 0.2000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s - loss: -4.5135 - acc: 0.4615 - val_loss: -4.4530 - val_acc: 0.2000\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s - loss: -5.3410 - acc: 0.4615 - val_loss: -5.2080 - val_acc: 0.2000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s - loss: -5.7711 - acc: 0.4615 - val_loss: -6.0290 - val_acc: 0.2000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s - loss: -6.3530 - acc: 0.4615 - val_loss: -6.8926 - val_acc: 0.2000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s - loss: -7.4921 - acc: 0.4615 - val_loss: -7.7733 - val_acc: 0.2000\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s - loss: -7.7372 - acc: 0.4615 - val_loss: -8.6397 - val_acc: 0.2000\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s - loss: -8.5939 - acc: 0.4615 - val_loss: -9.4662 - val_acc: 0.2000\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s - loss: -8.5249 - acc: 0.4615 - val_loss: -10.2386 - val_acc: 0.2000\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s - loss: -9.6539 - acc: 0.4615 - val_loss: -10.9536 - val_acc: 0.2000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s - loss: -10.2184 - acc: 0.4615 - val_loss: -11.6147 - val_acc: 0.2000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s - loss: -10.1934 - acc: 0.4615 - val_loss: -12.2407 - val_acc: 0.2000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s - loss: -11.8110 - acc: 0.4615 - val_loss: -12.8204 - val_acc: 0.2000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s - loss: -11.6698 - acc: 0.4615 - val_loss: -13.3997 - val_acc: 0.2000\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s - loss: -12.0171 - acc: 0.4615 - val_loss: -13.9143 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11fc6a6d0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 13 samples, validate on 5 samples\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 0s - loss: -12.0416 - acc: 0.4615 - val_loss: -14.4971 - val_acc: 0.2000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s - loss: -12.5488 - acc: 0.4615 - val_loss: -15.2021 - val_acc: 0.2000\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s - loss: -12.9203 - acc: 0.4615 - val_loss: -15.7227 - val_acc: 0.2000\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s - loss: -13.1556 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s - loss: -13.2421 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s - loss: -13.2583 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s - loss: -13.2970 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s - loss: -13.1605 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s - loss: -13.3207 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s - loss: -13.0731 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s - loss: -13.3207 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s - loss: -12.9589 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c288f90>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
