{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kojak\n",
    "\n",
    "Can we identify different meanings of the same word by what topic that word lies in?\n",
    "\n",
    "We apply Latent Dirichlet Allocation to attempt to extract distinct topics in our corpus (taken from 4000 research papers) in order determine usage contexts contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import gensim\n",
    "import re\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a custom lemmatizer/tokenizer with stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“', '?', '!', '’', 'et', 'al']\n",
    "stop = set(stop)\n",
    "\n",
    "def get_wordnet_pos_aux(word):\n",
    "    \n",
    "    treebank_tag = pos_tag([word])[0][1]\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return word, wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return word, wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return word, wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return word, wordnet.ADV\n",
    "    else:\n",
    "        return word, 'n'\n",
    "    \n",
    "def get_wordnet_pos(words):\n",
    "    return [get_wordnet_pos_aux(x) for x in words]\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t1,t2) for t1,t2 in get_wordnet_pos(word_tokenize(doc)) if t1 not in stop]\n",
    "    \n",
    "lt = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare stopwords, preprocess the data from source file abstracts.json\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop += ['?','!','.',',',':',';','[',']','[]','“' ]\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“', '?', '!', '’', 'et', 'al.', 'study', \"\"]\n",
    "stop = set(stop)\n",
    "\n",
    "class MyPapers(object):\n",
    "    # a memory-friendly way to load a large corpora\n",
    "     def __init__(self, dirname):\n",
    "            self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "        with open(self.dirname) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "        # iterate through all file names in our directory\n",
    "        for paper in data:\n",
    "            sentences = sent_tokenize(paper['full_text'])\n",
    "            for sentence in sentences:\n",
    "                try:\n",
    "                    line = re.sub(r'[?\\.,!:;\\(\\)“\\[\\]]',' ',sentence)\n",
    "                    line = [word for word in line.lower().split() if word not in stop]\n",
    "                    yield line\n",
    "                except:\n",
    "                    print(\"Empty line found\")\n",
    "                    continue\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If we are using 'tiny_corpus.txt'\n",
    "#corpus = [lt(c) for c in corpus]\n",
    "\n",
    "# Declare what word we are searchig for\n",
    "target = u'state'\n",
    "\n",
    "#If we are using 'abstract_scraper/abstracts.json'\n",
    "corpus = MyPapers('../data/train_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.dictionary.Dictionary(corpus)\n",
    "text = [dictionary.doc2bow(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_lookup = {dictionary[id]:id for id in dictionary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lookup['progressive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA = gensim.models.ldamulticore.LdaMulticore(corpus = text, id2word=dictionary, num_topics = 150,workers = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_topics = LDA.get_term_topics(word_id = id_lookup[target], minimum_probability=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00369623206532 ['practices', 'achieved', 'countries', 'political', 'contribute', 'quite', 'policy', 'air', 'developing', 'unemployment', 'educational', 'isolated', 'development', 'economic', 'economy']\n",
      "0.00382743032762 ['us', 'variable', 'variables', 'definition', 'regression', 'mentioned', 'evaluate', 'steps', 'variations', 'let', 'next', 'interested', 'vehicle', 'binary', 'two']\n",
      "0.00370139472187 ['reference', 'enhanced', 'extracts', 'welfare', 'papers', 'inputs', 'producers', 'topics', 'robustness', 'covered', 'obtaining', 'outputs', 'also', 'hierarchical', 'emerge']\n",
      "0.00312431899667 ['estimate', 'decrease', 'complexity', 'equation', 'supply', 'decreased', 'unit', 'constraints', 'decisions', 'permanent', 'fruit', 'increase', 'actually', 'model', 'mathematical']\n",
      "0.00467741048122 ['states', 'drugs', 'united', 'icu', 'sensing', 'long-term', 'maintaining', 'principles', 'examining', 'continuously', 'interference', 'nations', 'deployed', 'politics', 'union']\n"
     ]
    }
   ],
   "source": [
    "for t, p in target_topics:\n",
    "    topic_words = LDA.get_topic_terms(topicid = t, topn = 15)\n",
    "    words = [dictionary[w[0]] for w in topic_words]\n",
    "    print(p, words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA.save('LDA_150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2052"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_corpus will be a list of ony those papers containing the target word\n",
    "target_corpus = []\n",
    "\n",
    "for paper in corpus:\n",
    "    if target in paper:\n",
    "        target_corpus.append(paper)\n",
    "        \n",
    "len(target_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_mini = [dictionary.doc2bow(c) for c in target_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_mini = gensim.models.ldamulticore.LdaMulticore(corpus = text_mini, id2word=dictionary, num_topics = 25,workers = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18,\n",
       "  '0.017*\"state\" + 0.002*\"states\" + 0.001*\"transition\" + 0.001*\"conditions\" + 0.001*\"also\" + 0.001*\"system\" + 0.001*\"one\" + 0.001*\"social\" + 0.001*\"different\" + 0.001*\"local\"'),\n",
       " (16,\n",
       "  '0.014*\"state\" + 0.001*\"process\" + 0.001*\"university\" + 0.001*\"context\" + 0.001*\"system\" + 0.001*\"power\" + 0.001*\"institutional\" + 0.001*\"changes\" + 0.001*\"information\" + 0.001*\"2012\"'),\n",
       " (1,\n",
       "  '0.015*\"state\" + 0.002*\"control\" + 0.001*\"one\" + 0.001*\"matrix\" + 0.001*\"first\" + 0.001*\"two\" + 0.001*\"new\" + 0.001*\"based\" + 0.001*\"time\" + 0.001*\"system\"'),\n",
       " (17,\n",
       "  '0.016*\"state\" + 0.002*\"also\" + 0.002*\"system\" + 0.002*\"process\" + 0.002*\"used\" + 0.002*\"time\" + 0.002*\"e\" + 0.001*\"may\" + 0.001*\"described\" + 0.001*\"local\"'),\n",
       " (19,\n",
       "  '0.011*\"state\" + 0.001*\"also\" + 0.001*\"results\" + 0.001*\"system\" + 0.001*\"one\" + 0.001*\"dialog\" + 0.001*\"law\" + 0.001*\"license\" + 0.001*\"material\" + 0.001*\"2\"'),\n",
       " (5,\n",
       "  '0.010*\"state\" + 0.001*\"members\" + 0.001*\"%\" + 0.001*\"run\" + 0.001*\"5\" + 0.001*\"us\" + 0.001*\"results\" + 0.001*\"welfare\" + 0.001*\"models\" + 0.001*\"rate\"'),\n",
       " (14,\n",
       "  '0.018*\"state\" + 0.002*\"system\" + 0.002*\"model\" + 0.001*\"e\" + 0.001*\"may\" + 0.001*\"two\" + 0.001*\"network\" + 0.001*\"machine\" + 0.001*\"0\" + 0.001*\"problem\"'),\n",
       " (3,\n",
       "  '0.011*\"state\" + 0.002*\"also\" + 0.001*\"using\" + 0.001*\"within\" + 0.001*\"process\" + 0.001*\"control\" + 0.001*\"systems\" + 0.001*\"policies\" + 0.001*\"national\" + 0.001*\"two\"'),\n",
       " (9,\n",
       "  '0.014*\"state\" + 0.001*\"role\" + 0.001*\"also\" + 0.001*\"energy\" + 0.001*\"development\" + 0.001*\"may\" + 0.001*\"paper\" + 0.001*\"number\" + 0.001*\"economic\" + 0.001*\"would\"'),\n",
       " (6,\n",
       "  '0.009*\"state\" + 0.001*\"order\" + 0.001*\"states\" + 0.001*\"mpec\" + 0.001*\"fault\" + 0.001*\"market\" + 0.001*\"may\" + 0.001*\"various\" + 0.001*\"remittances\" + 0.001*\"like\"'),\n",
       " (10,\n",
       "  '0.015*\"state\" + 0.001*\"data\" + 0.001*\"policy\" + 0.001*\"system\" + 0.001*\"e\" + 0.001*\"one\" + 0.001*\"information\" + 0.001*\"two\" + 0.001*\"channel\" + 0.001*\"national\"'),\n",
       " (2,\n",
       "  '0.011*\"state\" + 0.002*\"fractional\" + 0.001*\"social\" + 0.001*\"also\" + 0.001*\"section\" + 0.001*\"research\" + 0.001*\"properties\" + 0.001*\"present\" + 0.001*\"derivatives\" + 0.001*\"energy\"'),\n",
       " (13,\n",
       "  '0.010*\"state\" + 0.002*\"new\" + 0.001*\"analysis\" + 0.001*\"information\" + 0.001*\"time\" + 0.001*\"dynamics\" + 0.001*\"use\" + 0.001*\"system\" + 0.001*\"0\" + 0.001*\"case\"'),\n",
       " (15,\n",
       "  '0.016*\"state\" + 0.002*\"also\" + 0.002*\"forest\" + 0.001*\"new\" + 0.001*\"research\" + 0.001*\"model\" + 0.001*\"paper\" + 0.001*\"time\" + 0.001*\"information\" + 0.001*\"system\"'),\n",
       " (7,\n",
       "  '0.017*\"state\" + 0.002*\"security\" + 0.002*\"system\" + 0.002*\"also\" + 0.002*\"role\" + 0.002*\"time\" + 0.002*\"case\" + 0.001*\"may\" + 0.001*\"model\" + 0.001*\"policies\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_mini.show_topics(num_topics=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_topics_mini = LDA_mini.get_term_topics(word_id = id_lookup[target], minimum_probability=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0253577839995 ['state', 'also', 'one', 'network', 'system', 'steady', 'better', 'states', 'e', 'dynamics', 'changes', 'many', 'spectral', 'population', 'transitions']\n",
      "0.0235171939871 ['state', 'also', 'matrix', 'system', 'group', 'welfare', 'result', 'way', 'regulations', 'asymptotic', '=0', 'vector', 'global', 'national', 'hence']\n",
      "0.0200564424083 ['state', 'inhibitory', 'social', 'welfare', 'time', 'well', 'development', 'paper', 'literature', 'skills', 'economic', 'cost', 'data', 'important', 'divine']\n",
      "0.0240323341216 ['state', 'result', 'system', 'migration', 'human', 'health', 'welfare', 'analysis', 'security', 'information', 'three', 'would', 'however', 'power', 'may']\n",
      "0.0210780655487 ['state', 'time', 'point', 'however', 'data', 'also', 'one', 'research', 'may', 'system', 'step', 'example', 'states', 'risk', 'science']\n",
      "0.0278143472402 ['state', 'important', 'social', 'states', 'using', 'model', 'could', 'problem', 'integers', 'actual', 'human', 'production', 'national', 'first', 'current']\n",
      "0.0209875713714 ['state', 'also', 'one', 'surveyed', 'thus', 'industry', 'd-state', 'field', 'samples', 'estimated', 'may', 'teachers', 'probability', 'korea', 'north']\n",
      "0.0212418431147 ['state', 'fractional', 'derivatives', 'predicting', 'derivative', 'china', 'system', 'security', 'fault', 'policies', 'university', 'states', 'conformable', 'research', 'used']\n",
      "0.0249583054297 ['state', 'sector', 'problem', 'decrease', 'emergency', 'selection', 'market', 'finite', 'private', 'second', 'techniques', 'using', '1', 'proposed', 'opt']\n"
     ]
    }
   ],
   "source": [
    "for t, p in target_topics_mini:\n",
    "    topic_words = LDA_mini.get_topic_terms(topicid = t, topn = 15)\n",
    "    words = [dictionary[w[0]] for w in topic_words]\n",
    "    print(p, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function takes as arguments a list of tokenized documents and a window size\n",
    "# and returns each word in the document along with its window context as a tuple\n",
    "\n",
    "def generate_windows(documents, window_size):\n",
    "    maxlen = window_size*2\n",
    "    \n",
    "    for document in documents:\n",
    "        L = len(document)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(document):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            context_words = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    #in_words.append([word])\n",
    "                    context_words.append(document[i])\n",
    "            x = word\n",
    "            y = context_words\n",
    "\n",
    "            #x = np.array(in_words,dtype=np.int32)\n",
    "            #y = np_utils.to_categorical(context_words, V)\n",
    "            yield(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_contexts(documents, target, window_size = 6):\n",
    "    \n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        text = document\n",
    "        if target in text:\n",
    "            #print(target)\n",
    "            windows = generate_windows([text],window_size)\n",
    "            #print windows[:2]\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append((w[1]))\n",
    "                    \n",
    "    return context_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
