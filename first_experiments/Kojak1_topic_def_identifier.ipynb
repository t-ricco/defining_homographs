{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kojak\n",
    "\n",
    "Can we identify different meanings of the same word by what topic that word lies in?\n",
    "\n",
    "We apply Latent Dirichlet Allocation to attempt to extract distinct topics in our corpus (taken from 4000 research papers) in order determine usage contexts contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pdfminer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import gensim\n",
    "import re\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Opening our corpus\n",
    "\n",
    "#>>>f = codecs.open(\"test\", \"r\", \"utf-8\")\n",
    "with codecs.open(\"tiny_corpus.txt\",\"rb\",\"utf-8\") as f:\n",
    "    corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a custom lemmatizer/tokenizer with stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“', '?', '!', '’', 'et', 'al']\n",
    "stop = set(stop)\n",
    "\n",
    "def get_wordnet_pos_aux(word):\n",
    "    \n",
    "    treebank_tag = pos_tag([word])[0][1]\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return word, wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return word, wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return word, wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return word, wordnet.ADV\n",
    "    else:\n",
    "        return word, 'n'\n",
    "    \n",
    "def get_wordnet_pos(words):\n",
    "    return [get_wordnet_pos_aux(x) for x in words]\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t1,t2) for t1,t2 in get_wordnet_pos(word_tokenize(doc)) if t1 not in stop]\n",
    "    \n",
    "lt = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare stopwords, preprocess the data from source file abstracts.json\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop += ['?','!','.',',',':',';','[',']','[]','“' ]\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“', '?', '!', '’', 'et', 'al.', 'study', \"\"]\n",
    "stop = set(stop)\n",
    "\n",
    "class MyPapers(object):\n",
    "    # a memory-friendly way to load a large corpora\n",
    "     def __init__(self, dirname):\n",
    "            self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "        with open(self.dirname) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "        # iterate through all file names in our directory\n",
    "        for paper in data:\n",
    "            #print(paper)\n",
    "            #yield paper['full_text'].lower().split()\n",
    "            line = [word for word in paper['full_text'].lower().split() if word not in stop]\n",
    "            #print(line)\n",
    "            line = [re.sub(r'[?\\.,!:;\\(\\)“\\[\\]]','',l) for l in line]\n",
    "            #print(line)\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If we are using 'tiny_corpus.txt'\n",
    "#corpus = [lt(c) for c in corpus]\n",
    "\n",
    "# Declare what word we are searchig for\n",
    "target = u'state'\n",
    "\n",
    "#If we are using 'abstract_scraper/abstracts.json'\n",
    "corpus = MyPapers('abstract_scraper/abstracts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_corpus will be a list of ony those papers containing the target word\n",
    "target_corpus = []\n",
    "\n",
    "for paper in corpus:\n",
    "    if target in paper:\n",
    "        target_corpus.append(paper)\n",
    "        \n",
    "len(target_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function takes as arguments a list of tokenized documents and a window size\n",
    "# and returns each word in the document along with its window context as a tuple\n",
    "\n",
    "def generate_windows(documents, window_size):\n",
    "    maxlen = window_size*2\n",
    "    \n",
    "    for document in documents:\n",
    "        L = len(document)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(document):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            context_words = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    #in_words.append([word])\n",
    "                    context_words.append(document[i])\n",
    "            x = word\n",
    "            y = context_words\n",
    "\n",
    "            #x = np.array(in_words,dtype=np.int32)\n",
    "            #y = np_utils.to_categorical(context_words, V)\n",
    "            yield(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_contexts(documents, target, window_size = 6):\n",
    "    \n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        text = document\n",
    "        if target in text:\n",
    "            #print(target)\n",
    "            windows = generate_windows([text],window_size)\n",
    "            #print windows[:2]\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append((w[1]))\n",
    "                    \n",
    "    return context_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.dictionary.Dictionary(corpus)\n",
    "text = [dictionary.doc2bow(c) for c in extract_contexts(corpus, target,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA = gensim.models.ldamodel.LdaModel(corpus = text, id2word=dictionary, num_topics = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.002*\"\" + 0.001*\"pain\" + 0.001*\"appropriate\" + 0.001*\"cell\" + 0.001*\"highway\" + 0.001*\"syndrome\" + 0.001*\"primary\" + 0.001*\"cells\" + 0.001*\"health\" + 0.001*\"cancer\"'),\n",
       " (1,\n",
       "  u'0.001*\"\" + 0.001*\"l\" + 0.001*\"slopes\" + 0.001*\"less\" + 0.001*\"observed\" + 0.000*\"consequently\" + 0.000*\"exercise\" + 0.000*\"overall\" + 0.000*\"increase\" + 0.000*\"height\"'),\n",
       " (2,\n",
       "  u'0.001*\"\" + 0.001*\"bim\" + 0.001*\"patients\" + 0.001*\"ethics\" + 0.001*\"dmn\" + 0.001*\"\\xa7\" + 0.001*\"complained\" + 0.001*\"migraine\" + 0.000*\"increase\" + 0.000*\"consent\"'),\n",
       " (3,\n",
       "  u'0.001*\"\" + 0.001*\"network\" + 0.001*\"resting\" + 0.001*\"like\" + 0.001*\"networks\" + 0.001*\"failures\" + 0.001*\"important\" + 0.000*\"research\" + 0.000*\"university\" + 0.000*\"activity\"'),\n",
       " (4,\n",
       "  u'0.002*\"cognitive\" + 0.002*\"\" + 0.001*\"function\" + 0.001*\"intake\" + 0.001*\"fluid\" + 0.001*\"stress\" + 0.001*\"functional\" + 0.001*\"pain\" + 0.001*\"studies\" + 0.001*\"may\"'),\n",
       " (5,\n",
       "  u'0.002*\"\" + 0.001*\"network\" + 0.001*\"student\" + 0.001*\"mental\" + 0.001*\"young\" + 0.001*\"performance\" + 0.001*\"brain\" + 0.001*\"functional\" + 0.000*\"buildings\" + 0.000*\"function\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.print_topics(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below I just copied and pasted code for an RNN (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# keras\n",
    "np.random.seed(13)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Activation, SimpleRNN, GRU, LSTM, Bidirectional, Convolution1D, MaxPooling1D, Merge, Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5,\n",
       "  1,\n",
       "  151,\n",
       "  152,\n",
       "  2,\n",
       "  20,\n",
       "  1,\n",
       "  153,\n",
       "  7,\n",
       "  82,\n",
       "  154,\n",
       "  6,\n",
       "  83,\n",
       "  4,\n",
       "  16,\n",
       "  84,\n",
       "  3,\n",
       "  3,\n",
       "  155,\n",
       "  85,\n",
       "  156,\n",
       "  32,\n",
       "  86,\n",
       "  87,\n",
       "  33,\n",
       "  86,\n",
       "  157,\n",
       "  4,\n",
       "  158,\n",
       "  7,\n",
       "  4,\n",
       "  88,\n",
       "  7,\n",
       "  10,\n",
       "  6,\n",
       "  83,\n",
       "  4,\n",
       "  16,\n",
       "  159,\n",
       "  3,\n",
       "  89,\n",
       "  160,\n",
       "  2,\n",
       "  90,\n",
       "  15,\n",
       "  3,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  1,\n",
       "  165,\n",
       "  4,\n",
       "  166,\n",
       "  5,\n",
       "  91,\n",
       "  167,\n",
       "  25,\n",
       "  34,\n",
       "  168,\n",
       "  8,\n",
       "  91,\n",
       "  32,\n",
       "  2,\n",
       "  92,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  6,\n",
       "  172,\n",
       "  4,\n",
       "  173,\n",
       "  35,\n",
       "  2,\n",
       "  10,\n",
       "  93,\n",
       "  10,\n",
       "  174,\n",
       "  94,\n",
       "  4,\n",
       "  54,\n",
       "  32,\n",
       "  175,\n",
       "  36,\n",
       "  35,\n",
       "  4,\n",
       "  95,\n",
       "  2,\n",
       "  21,\n",
       "  55,\n",
       "  1,\n",
       "  176,\n",
       "  56,\n",
       "  177,\n",
       "  25,\n",
       "  178,\n",
       "  7,\n",
       "  179,\n",
       "  8,\n",
       "  36,\n",
       "  33,\n",
       "  2,\n",
       "  21,\n",
       "  4,\n",
       "  37,\n",
       "  7,\n",
       "  10,\n",
       "  2,\n",
       "  19,\n",
       "  96,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  6,\n",
       "  36,\n",
       "  97,\n",
       "  87,\n",
       "  33,\n",
       "  183,\n",
       "  38,\n",
       "  1,\n",
       "  184,\n",
       "  185,\n",
       "  2,\n",
       "  1,\n",
       "  186,\n",
       "  187,\n",
       "  2,\n",
       "  21,\n",
       "  188,\n",
       "  1,\n",
       "  189,\n",
       "  1,\n",
       "  9,\n",
       "  6,\n",
       "  1,\n",
       "  98,\n",
       "  190,\n",
       "  1,\n",
       "  33,\n",
       "  2,\n",
       "  21,\n",
       "  4,\n",
       "  99,\n",
       "  100,\n",
       "  2,\n",
       "  10,\n",
       "  191,\n",
       "  25,\n",
       "  192,\n",
       "  8,\n",
       "  193,\n",
       "  57,\n",
       "  96,\n",
       "  4,\n",
       "  16,\n",
       "  194,\n",
       "  101,\n",
       "  11,\n",
       "  39,\n",
       "  195,\n",
       "  25,\n",
       "  196,\n",
       "  58,\n",
       "  8,\n",
       "  22,\n",
       "  197,\n",
       "  102,\n",
       "  16,\n",
       "  198,\n",
       "  199,\n",
       "  103,\n",
       "  1,\n",
       "  200,\n",
       "  2,\n",
       "  39,\n",
       "  20,\n",
       "  7,\n",
       "  82,\n",
       "  21,\n",
       "  22,\n",
       "  6,\n",
       "  1,\n",
       "  32,\n",
       "  4,\n",
       "  201,\n",
       "  37,\n",
       "  7,\n",
       "  26,\n",
       "  104,\n",
       "  202],\n",
       " [1,\n",
       "  40,\n",
       "  2,\n",
       "  203,\n",
       "  4,\n",
       "  88,\n",
       "  7,\n",
       "  1,\n",
       "  40,\n",
       "  2,\n",
       "  9,\n",
       "  5,\n",
       "  204,\n",
       "  1,\n",
       "  40,\n",
       "  2,\n",
       "  9,\n",
       "  4,\n",
       "  3,\n",
       "  205,\n",
       "  100,\n",
       "  2,\n",
       "  1,\n",
       "  41,\n",
       "  2,\n",
       "  105,\n",
       "  42,\n",
       "  206,\n",
       "  207,\n",
       "  17,\n",
       "  208,\n",
       "  15,\n",
       "  1,\n",
       "  40,\n",
       "  2,\n",
       "  9,\n",
       "  209,\n",
       "  8,\n",
       "  26,\n",
       "  4,\n",
       "  43,\n",
       "  11,\n",
       "  1,\n",
       "  10,\n",
       "  2,\n",
       "  98,\n",
       "  2,\n",
       "  210,\n",
       "  106,\n",
       "  6,\n",
       "  19,\n",
       "  211,\n",
       "  11,\n",
       "  1,\n",
       "  41,\n",
       "  2,\n",
       "  105,\n",
       "  59,\n",
       "  60,\n",
       "  3,\n",
       "  212,\n",
       "  213,\n",
       "  4,\n",
       "  214,\n",
       "  5,\n",
       "  215,\n",
       "  11,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  107,\n",
       "  220,\n",
       "  41,\n",
       "  221,\n",
       "  4,\n",
       "  222,\n",
       "  223],\n",
       " [18,\n",
       "  12,\n",
       "  61,\n",
       "  224,\n",
       "  1,\n",
       "  62,\n",
       "  6,\n",
       "  225,\n",
       "  2,\n",
       "  44,\n",
       "  5,\n",
       "  226,\n",
       "  63,\n",
       "  45,\n",
       "  108,\n",
       "  5,\n",
       "  18,\n",
       "  12,\n",
       "  4,\n",
       "  1,\n",
       "  109,\n",
       "  2,\n",
       "  227,\n",
       "  23,\n",
       "  6,\n",
       "  1,\n",
       "  109,\n",
       "  2,\n",
       "  228,\n",
       "  18,\n",
       "  229,\n",
       "  18,\n",
       "  230,\n",
       "  61,\n",
       "  231,\n",
       "  1,\n",
       "  110,\n",
       "  2,\n",
       "  44,\n",
       "  62,\n",
       "  55,\n",
       "  1,\n",
       "  18,\n",
       "  111,\n",
       "  232,\n",
       "  2,\n",
       "  233,\n",
       "  108,\n",
       "  4,\n",
       "  44,\n",
       "  62,\n",
       "  55,\n",
       "  12,\n",
       "  110,\n",
       "  90,\n",
       "  234,\n",
       "  235,\n",
       "  2,\n",
       "  19,\n",
       "  236,\n",
       "  6,\n",
       "  237],\n",
       " [238,\n",
       "  46,\n",
       "  239,\n",
       "  240,\n",
       "  27,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  5,\n",
       "  46,\n",
       "  1,\n",
       "  64,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  6,\n",
       "  251,\n",
       "  252,\n",
       "  4,\n",
       "  253,\n",
       "  254,\n",
       "  1,\n",
       "  14,\n",
       "  112,\n",
       "  27,\n",
       "  255,\n",
       "  3,\n",
       "  256,\n",
       "  19,\n",
       "  113,\n",
       "  46,\n",
       "  257,\n",
       "  14,\n",
       "  28,\n",
       "  29,\n",
       "  114,\n",
       "  85,\n",
       "  1,\n",
       "  30,\n",
       "  258,\n",
       "  47,\n",
       "  1,\n",
       "  259,\n",
       "  260,\n",
       "  2,\n",
       "  3,\n",
       "  115,\n",
       "  4,\n",
       "  1,\n",
       "  27,\n",
       "  261,\n",
       "  26,\n",
       "  116,\n",
       "  24,\n",
       "  262,\n",
       "  1,\n",
       "  45,\n",
       "  12,\n",
       "  117,\n",
       "  2,\n",
       "  115,\n",
       "  4,\n",
       "  6,\n",
       "  263,\n",
       "  8,\n",
       "  1,\n",
       "  117,\n",
       "  102,\n",
       "  16,\n",
       "  65,\n",
       "  3,\n",
       "  118,\n",
       "  1,\n",
       "  27,\n",
       "  264,\n",
       "  8,\n",
       "  26,\n",
       "  265,\n",
       "  16,\n",
       "  266,\n",
       "  3,\n",
       "  267,\n",
       "  23,\n",
       "  93,\n",
       "  268,\n",
       "  1,\n",
       "  37,\n",
       "  111,\n",
       "  269,\n",
       "  270,\n",
       "  2,\n",
       "  271,\n",
       "  6,\n",
       "  14,\n",
       "  28,\n",
       "  29,\n",
       "  119,\n",
       "  272,\n",
       "  273,\n",
       "  1,\n",
       "  274,\n",
       "  2,\n",
       "  275,\n",
       "  23,\n",
       "  24,\n",
       "  1,\n",
       "  46,\n",
       "  116,\n",
       "  1,\n",
       "  14,\n",
       "  66,\n",
       "  276,\n",
       "  28,\n",
       "  29,\n",
       "  114,\n",
       "  7,\n",
       "  65,\n",
       "  3,\n",
       "  118,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  28,\n",
       "  29,\n",
       "  67],\n",
       " [3,\n",
       "  280,\n",
       "  281,\n",
       "  120,\n",
       "  2,\n",
       "  3,\n",
       "  121,\n",
       "  282,\n",
       "  283,\n",
       "  2,\n",
       "  22,\n",
       "  4,\n",
       "  3,\n",
       "  68,\n",
       "  9,\n",
       "  284,\n",
       "  4,\n",
       "  57,\n",
       "  285,\n",
       "  286,\n",
       "  9,\n",
       "  122,\n",
       "  22,\n",
       "  48,\n",
       "  287,\n",
       "  288,\n",
       "  1,\n",
       "  289,\n",
       "  290,\n",
       "  4,\n",
       "  123,\n",
       "  291,\n",
       "  3,\n",
       "  3,\n",
       "  69,\n",
       "  124,\n",
       "  292,\n",
       "  5,\n",
       "  36,\n",
       "  31,\n",
       "  84,\n",
       "  125,\n",
       "  293,\n",
       "  15,\n",
       "  3,\n",
       "  294,\n",
       "  295,\n",
       "  70,\n",
       "  1,\n",
       "  22,\n",
       "  123,\n",
       "  11,\n",
       "  17,\n",
       "  9,\n",
       "  4,\n",
       "  296,\n",
       "  58,\n",
       "  7,\n",
       "  297,\n",
       "  3,\n",
       "  69,\n",
       "  124,\n",
       "  5,\n",
       "  71,\n",
       "  7,\n",
       "  126,\n",
       "  25,\n",
       "  298,\n",
       "  127,\n",
       "  38,\n",
       "  72,\n",
       "  20,\n",
       "  299,\n",
       "  3,\n",
       "  122,\n",
       "  9,\n",
       "  4,\n",
       "  300,\n",
       "  11,\n",
       "  128,\n",
       "  35,\n",
       "  47,\n",
       "  3,\n",
       "  68,\n",
       "  35,\n",
       "  4,\n",
       "  301,\n",
       "  47,\n",
       "  1,\n",
       "  302,\n",
       "  303,\n",
       "  128,\n",
       "  304,\n",
       "  6,\n",
       "  305,\n",
       "  306,\n",
       "  3,\n",
       "  69,\n",
       "  307,\n",
       "  1,\n",
       "  31,\n",
       "  49,\n",
       "  308,\n",
       "  8,\n",
       "  309,\n",
       "  310,\n",
       "  42,\n",
       "  16,\n",
       "  4,\n",
       "  311],\n",
       " [5,\n",
       "  129,\n",
       "  92,\n",
       "  130,\n",
       "  4,\n",
       "  312,\n",
       "  3,\n",
       "  89,\n",
       "  313,\n",
       "  131,\n",
       "  1,\n",
       "  314,\n",
       "  112,\n",
       "  27,\n",
       "  34,\n",
       "  315,\n",
       "  8,\n",
       "  316,\n",
       "  317,\n",
       "  15,\n",
       "  318,\n",
       "  23,\n",
       "  113,\n",
       "  17,\n",
       "  44,\n",
       "  97,\n",
       "  319,\n",
       "  18,\n",
       "  320,\n",
       "  4,\n",
       "  321,\n",
       "  132,\n",
       "  133,\n",
       "  4,\n",
       "  61,\n",
       "  3,\n",
       "  322,\n",
       "  65,\n",
       "  323,\n",
       "  2,\n",
       "  130,\n",
       "  6,\n",
       "  4,\n",
       "  324,\n",
       "  121,\n",
       "  3,\n",
       "  4,\n",
       "  134,\n",
       "  5,\n",
       "  135,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  132,\n",
       "  133],\n",
       " [3,\n",
       "  136,\n",
       "  328,\n",
       "  2,\n",
       "  20,\n",
       "  1,\n",
       "  329,\n",
       "  2,\n",
       "  10,\n",
       "  26,\n",
       "  330,\n",
       "  331,\n",
       "  1,\n",
       "  332,\n",
       "  333,\n",
       "  72,\n",
       "  10,\n",
       "  6,\n",
       "  49,\n",
       "  1,\n",
       "  60,\n",
       "  7,\n",
       "  1,\n",
       "  50,\n",
       "  2,\n",
       "  129,\n",
       "  20,\n",
       "  334,\n",
       "  60,\n",
       "  335,\n",
       "  1,\n",
       "  336,\n",
       "  2,\n",
       "  1,\n",
       "  337,\n",
       "  6,\n",
       "  4,\n",
       "  37,\n",
       "  7,\n",
       "  3,\n",
       "  137,\n",
       "  7,\n",
       "  10,\n",
       "  8,\n",
       "  4,\n",
       "  138,\n",
       "  51,\n",
       "  338,\n",
       "  1,\n",
       "  137,\n",
       "  4,\n",
       "  43,\n",
       "  15,\n",
       "  339,\n",
       "  10,\n",
       "  11,\n",
       "  1,\n",
       "  340,\n",
       "  73,\n",
       "  139,\n",
       "  15,\n",
       "  341,\n",
       "  10,\n",
       "  11,\n",
       "  1,\n",
       "  342,\n",
       "  2,\n",
       "  22,\n",
       "  343,\n",
       "  6,\n",
       "  15,\n",
       "  57,\n",
       "  10,\n",
       "  11,\n",
       "  1,\n",
       "  344,\n",
       "  73,\n",
       "  345,\n",
       "  1,\n",
       "  346,\n",
       "  19,\n",
       "  106,\n",
       "  6,\n",
       "  1,\n",
       "  347,\n",
       "  73,\n",
       "  348],\n",
       " [24,\n",
       "  3,\n",
       "  107,\n",
       "  2,\n",
       "  349,\n",
       "  350,\n",
       "  5,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  6,\n",
       "  354,\n",
       "  355,\n",
       "  136,\n",
       "  3,\n",
       "  356,\n",
       "  31,\n",
       "  7,\n",
       "  357,\n",
       "  1,\n",
       "  358,\n",
       "  2,\n",
       "  21,\n",
       "  359,\n",
       "  74,\n",
       "  360,\n",
       "  8,\n",
       "  140,\n",
       "  75,\n",
       "  50,\n",
       "  141,\n",
       "  3,\n",
       "  361,\n",
       "  15,\n",
       "  362,\n",
       "  19,\n",
       "  363,\n",
       "  19,\n",
       "  4,\n",
       "  364,\n",
       "  5,\n",
       "  365,\n",
       "  9,\n",
       "  103,\n",
       "  74,\n",
       "  366,\n",
       "  3,\n",
       "  76,\n",
       "  2,\n",
       "  367,\n",
       "  9,\n",
       "  368,\n",
       "  38,\n",
       "  17,\n",
       "  77,\n",
       "  11,\n",
       "  369,\n",
       "  370,\n",
       "  17,\n",
       "  140,\n",
       "  75,\n",
       "  50,\n",
       "  74,\n",
       "  58,\n",
       "  8,\n",
       "  1,\n",
       "  76,\n",
       "  371,\n",
       "  51,\n",
       "  142,\n",
       "  143,\n",
       "  76,\n",
       "  3,\n",
       "  49,\n",
       "  5,\n",
       "  125,\n",
       "  372,\n",
       "  54,\n",
       "  9,\n",
       "  373,\n",
       "  1,\n",
       "  75,\n",
       "  50,\n",
       "  374,\n",
       "  5,\n",
       "  375,\n",
       "  376,\n",
       "  59,\n",
       "  4,\n",
       "  5,\n",
       "  377,\n",
       "  71,\n",
       "  7,\n",
       "  126,\n",
       "  94,\n",
       "  4,\n",
       "  127,\n",
       "  38,\n",
       "  39,\n",
       "  20],\n",
       " [5,\n",
       "  71,\n",
       "  7,\n",
       "  1,\n",
       "  39,\n",
       "  72,\n",
       "  378,\n",
       "  1,\n",
       "  13,\n",
       "  2,\n",
       "  3,\n",
       "  41,\n",
       "  78,\n",
       "  1,\n",
       "  379,\n",
       "  380,\n",
       "  3,\n",
       "  78,\n",
       "  34,\n",
       "  4,\n",
       "  16,\n",
       "  101,\n",
       "  11,\n",
       "  1,\n",
       "  381,\n",
       "  2,\n",
       "  79,\n",
       "  1,\n",
       "  382,\n",
       "  383,\n",
       "  2,\n",
       "  59,\n",
       "  384,\n",
       "  4,\n",
       "  1,\n",
       "  78,\n",
       "  95,\n",
       "  2,\n",
       "  3,\n",
       "  68,\n",
       "  144,\n",
       "  5,\n",
       "  1,\n",
       "  385,\n",
       "  31,\n",
       "  1,\n",
       "  31,\n",
       "  49,\n",
       "  8,\n",
       "  3,\n",
       "  386,\n",
       "  79,\n",
       "  56,\n",
       "  3,\n",
       "  145,\n",
       "  77,\n",
       "  144,\n",
       "  13,\n",
       "  146,\n",
       "  43,\n",
       "  70,\n",
       "  147,\n",
       "  80,\n",
       "  6,\n",
       "  146,\n",
       "  43,\n",
       "  148,\n",
       "  147,\n",
       "  80,\n",
       "  387,\n",
       "  3,\n",
       "  145,\n",
       "  9,\n",
       "  5,\n",
       "  17,\n",
       "  77,\n",
       "  13,\n",
       "  34,\n",
       "  54,\n",
       "  104,\n",
       "  388,\n",
       "  47,\n",
       "  24,\n",
       "  1,\n",
       "  79,\n",
       "  17,\n",
       "  9,\n",
       "  4,\n",
       "  389,\n",
       "  5,\n",
       "  17,\n",
       "  70,\n",
       "  13,\n",
       "  48,\n",
       "  5,\n",
       "  3,\n",
       "  148,\n",
       "  13],\n",
       " [3,\n",
       "  18,\n",
       "  390,\n",
       "  138,\n",
       "  149,\n",
       "  24,\n",
       "  3,\n",
       "  64,\n",
       "  391,\n",
       "  3,\n",
       "  392,\n",
       "  48,\n",
       "  13,\n",
       "  18,\n",
       "  30,\n",
       "  48,\n",
       "  5,\n",
       "  63,\n",
       "  393,\n",
       "  24,\n",
       "  3,\n",
       "  64,\n",
       "  394,\n",
       "  3,\n",
       "  45,\n",
       "  12,\n",
       "  23,\n",
       "  395,\n",
       "  6,\n",
       "  45,\n",
       "  12,\n",
       "  23,\n",
       "  4,\n",
       "  134,\n",
       "  5,\n",
       "  135,\n",
       "  80,\n",
       "  119,\n",
       "  396,\n",
       "  2,\n",
       "  12],\n",
       " [13,\n",
       "  52,\n",
       "  42,\n",
       "  99,\n",
       "  81,\n",
       "  13,\n",
       "  30,\n",
       "  51,\n",
       "  12,\n",
       "  131,\n",
       "  3,\n",
       "  13,\n",
       "  66,\n",
       "  397,\n",
       "  398,\n",
       "  13,\n",
       "  30,\n",
       "  52,\n",
       "  399,\n",
       "  56,\n",
       "  3,\n",
       "  400,\n",
       "  42,\n",
       "  150,\n",
       "  63,\n",
       "  401,\n",
       "  402,\n",
       "  30,\n",
       "  15,\n",
       "  120,\n",
       "  3,\n",
       "  403,\n",
       "  2,\n",
       "  14,\n",
       "  53,\n",
       "  52,\n",
       "  81,\n",
       "  7,\n",
       "  150,\n",
       "  14,\n",
       "  53,\n",
       "  404,\n",
       "  405,\n",
       "  12,\n",
       "  14,\n",
       "  406,\n",
       "  407,\n",
       "  141,\n",
       "  2,\n",
       "  408,\n",
       "  67,\n",
       "  14,\n",
       "  53,\n",
       "  409,\n",
       "  410,\n",
       "  12,\n",
       "  4,\n",
       "  81,\n",
       "  51,\n",
       "  12,\n",
       "  11,\n",
       "  411,\n",
       "  1,\n",
       "  13,\n",
       "  66,\n",
       "  6,\n",
       "  14,\n",
       "  53,\n",
       "  52,\n",
       "  6,\n",
       "  412,\n",
       "  413,\n",
       "  5,\n",
       "  1,\n",
       "  14,\n",
       "  28,\n",
       "  29,\n",
       "  5,\n",
       "  142,\n",
       "  143,\n",
       "  149,\n",
       "  414,\n",
       "  415,\n",
       "  6,\n",
       "  416,\n",
       "  139,\n",
       "  67]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For simplicity, one \"sentence\" per line & ensuring a count of two words min\n",
    "\n",
    "ltzr = Lemmatizer()\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "corpus = [ltzr(c).encode('ascii', 'ignore') for c in corpus]\n",
    "#print(corpus)\n",
    "\n",
    "# Tokenize using Keras\n",
    "my_filter='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "tokenizer = Tokenizer(filters=my_filter)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Convert tokenized sentences to sequence format\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "\n",
    "# Vocab size\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Dimension of our network\n",
    "dim = 100\n",
    "window_size = 2\n",
    "\n",
    "# What is this output? \n",
    "#sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(sequences, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    \n",
    "    # For each line (sentence)\n",
    "    for line in sequences:\n",
    "        L = len(line)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(line):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            context_words = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    #in_words.append([word])\n",
    "                    context_words.append(line[i])\n",
    "            x = word\n",
    "            y = context_words\n",
    "\n",
    "            #x = np.array(in_words,dtype=np.int32)\n",
    "            #y = np_utils.to_categorical(context_words, V)\n",
    "            yield(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_window(x,y):\n",
    "    index = tokenizer.word_index\n",
    "    print(index.keys()[index.values().index(x)])\n",
    "    print(map(lambda n: index.keys()[index.values().index(n)],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "charge\n",
      "['due', 'to', 'motion', 'of', 'constituent', 'density', 'stiffness', 'colour']\n",
      "1\n",
      "charge\n",
      "['of', 'mass', 'e', 'and', 'constrain', 'by', 'the', 'quantum']\n",
      "2\n",
      "charge\n",
      "['investigation', 'arrest', 'filing', 'of', 'trial', 'and', 'appeal']\n",
      "3\n",
      "charge\n",
      "['court', 'disallowed', 'a', 'murder', 'against', 'keeler', 'under', 'california']\n",
      "4\n",
      "charge\n",
      "['constant', 'h', 'the', 'elementary', 'e', 'and', 'the', 'boltzmann']\n",
      "5\n",
      "charge\n",
      "['a', 'polarizers', 'for', 'rotate', 'rotating', 'charge', 'be', 'present']\n",
      "6\n",
      "charge\n",
      "['for', 'rotate', 'charge', 'rotating', 'be', 'present', 'in', 'every']\n",
      "7\n",
      "state\n",
      "['classical', 'everyday', 'case', 'the', 'of', 'a', 'quantum', 'system']\n",
      "8\n",
      "state\n",
      "['a', 'general', 'oven', 'particle', 'sometimes', 'give', 'up', 'say']\n",
      "9\n",
      "state\n",
      "['atom', 'in', 'an', 'oven', 'have', 'no', 'intrinsic', 'orientation']\n",
      "10\n",
      "state\n",
      "['either', 'in', 'an', 'up', 'or', 'in', 'a', 'down']\n",
      "11\n",
      "state\n",
      "['or', 'in', 'a', 'down']\n",
      "12\n",
      "state\n",
      "['violates', 'a', 'federal', 'or', 'criminal', 'statute', 'or', 'in']\n",
      "13\n",
      "state\n",
      "['citizen', 'can', 'also', 'vote']\n",
      "14\n",
      "state\n",
      "['citizen', 'can', 'also', 'vote', 'statute', 'into', 'law', 'although']\n",
      "15\n",
      "state\n",
      "['into', 'law', 'although', 'a', 'legislature', 'adopts', 'most', 'state']\n",
      "16\n",
      "state\n",
      "['state', 'legislature', 'adopts', 'most', 'statute', 'citizen', 'voting', 'on']\n",
      "17\n",
      "state\n",
      "['law', 'by', 'both', 'the', 'legislature', 'and', 'california', 's']\n"
     ]
    }
   ],
   "source": [
    "win_size = 4\n",
    "count = 0\n",
    "for x,y in generate_data(sequences, win_size, V):\n",
    "    if [x] in tokenizer.texts_to_sequences([\"charge\", \"state\"]):\n",
    "        print(count)\n",
    "        print_window(x, y)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array(2*[3]+2*[1]+3*[3]+5*[1]+6*[2]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19  37   7  10   2  96 180 181 182]\n",
      " [ 19   2 210 106   6 211  11   1  41]\n",
      " [ 19   0  90 234 235   2 236   6 237]\n",
      " [ 19  27 255   3 256 113  46 257  14]\n",
      " [ 19  73 345   1 346 106   6   1 347]\n",
      " [ 19   3 361  15 362 363  19   4 364]\n",
      " [ 19  15 362  19 363   4 364   5 365]\n",
      " [ 13  39  72 378   1   2   3  41  78]\n",
      " [ 13   3 145  77 144 146  43  70 147]\n",
      " [ 13   9   5  17  77  34  54 104 388]\n",
      " [ 13 389   5  17  70  48   5   3 148]\n",
      " [ 13   0   0   0   0  48   5   3 148]\n",
      " [ 13 391   3 392  48  18  30  48   5]\n",
      " [ 13   0   0   0   0  52  42  99  81]\n",
      " [ 13  52  42  99  81  30  51  12 131]\n",
      " [ 13  51  12 131   3  66 397 398  13]\n",
      " [ 13  13  66 397 398  30  52 399  56]\n",
      " [ 13  12  11 411   1  66   6  14  53]]\n"
     ]
    }
   ],
   "source": [
    "maxlen = 2*win_size\n",
    "Z = []\n",
    "X = []\n",
    "for x,y in generate_data(sequences, win_size, V):\n",
    "    if [x] in tokenizer.texts_to_sequences([\"charge\", \"state\"]):\n",
    "        X.append(x)\n",
    "        Z.append(y)\n",
    "        \n",
    "Z = sequence.pad_sequences(Z, maxlen=maxlen)\n",
    "X = np.array(X).reshape(-1,1)\n",
    "X = np.concatenate([X,Z], axis = 1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((18, 9), (18, 1))\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train, test, split\n",
    "X_train,X_test, y_train,  y_test = train_test_split(X,Y)\n",
    "\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 3, 2, 1, 1, 3, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen+1))\n",
    "# Bidirectional LSTM!!!\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 13 samples, validate on 5 samples\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 0s - loss: 0.6886 - acc: 0.3846 - val_loss: 0.6491 - val_acc: 0.2000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s - loss: 0.6386 - acc: 0.4615 - val_loss: 0.6198 - val_acc: 0.2000\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s - loss: 0.6152 - acc: 0.4615 - val_loss: 0.5893 - val_acc: 0.2000\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s - loss: 0.5720 - acc: 0.4615 - val_loss: 0.5570 - val_acc: 0.2000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s - loss: 0.5289 - acc: 0.4615 - val_loss: 0.5220 - val_acc: 0.2000\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s - loss: 0.4795 - acc: 0.4615 - val_loss: 0.4835 - val_acc: 0.2000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s - loss: 0.4458 - acc: 0.4615 - val_loss: 0.4410 - val_acc: 0.2000\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s - loss: 0.3799 - acc: 0.4615 - val_loss: 0.3937 - val_acc: 0.2000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s - loss: 0.3183 - acc: 0.4615 - val_loss: 0.3407 - val_acc: 0.2000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s - loss: 0.2715 - acc: 0.4615 - val_loss: 0.2808 - val_acc: 0.2000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s - loss: 0.2043 - acc: 0.4615 - val_loss: 0.2126 - val_acc: 0.2000\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s - loss: 0.0937 - acc: 0.4615 - val_loss: 0.1348 - val_acc: 0.2000\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s - loss: -0.0133 - acc: 0.4615 - val_loss: 0.0452 - val_acc: 0.2000\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s - loss: -0.1355 - acc: 0.4615 - val_loss: -0.0584 - val_acc: 0.2000\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s - loss: -0.2531 - acc: 0.4615 - val_loss: -0.1785 - val_acc: 0.2000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s - loss: -0.3837 - acc: 0.4615 - val_loss: -0.3184 - val_acc: 0.2000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s - loss: -0.5698 - acc: 0.4615 - val_loss: -0.4820 - val_acc: 0.2000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s - loss: -0.7156 - acc: 0.4615 - val_loss: -0.6742 - val_acc: 0.2000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s - loss: -0.9537 - acc: 0.4615 - val_loss: -0.9013 - val_acc: 0.2000\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s - loss: -1.3811 - acc: 0.4615 - val_loss: -1.1699 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c01b750>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 13 samples, validate on 5 samples\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 0s - loss: -1.4877 - acc: 0.4615 - val_loss: -1.4663 - val_acc: 0.2000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s - loss: -1.7595 - acc: 0.4615 - val_loss: -1.8069 - val_acc: 0.2000\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s - loss: -2.2424 - acc: 0.4615 - val_loss: -2.1999 - val_acc: 0.2000\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s - loss: -2.6448 - acc: 0.4615 - val_loss: -2.6534 - val_acc: 0.2000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s - loss: -2.8932 - acc: 0.4615 - val_loss: -3.1758 - val_acc: 0.2000\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s - loss: -3.7926 - acc: 0.4615 - val_loss: -3.7741 - val_acc: 0.2000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s - loss: -4.5135 - acc: 0.4615 - val_loss: -4.4530 - val_acc: 0.2000\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s - loss: -5.3410 - acc: 0.4615 - val_loss: -5.2080 - val_acc: 0.2000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s - loss: -5.7711 - acc: 0.4615 - val_loss: -6.0290 - val_acc: 0.2000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s - loss: -6.3530 - acc: 0.4615 - val_loss: -6.8926 - val_acc: 0.2000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s - loss: -7.4921 - acc: 0.4615 - val_loss: -7.7733 - val_acc: 0.2000\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s - loss: -7.7372 - acc: 0.4615 - val_loss: -8.6397 - val_acc: 0.2000\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s - loss: -8.5939 - acc: 0.4615 - val_loss: -9.4662 - val_acc: 0.2000\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s - loss: -8.5249 - acc: 0.4615 - val_loss: -10.2386 - val_acc: 0.2000\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s - loss: -9.6539 - acc: 0.4615 - val_loss: -10.9536 - val_acc: 0.2000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s - loss: -10.2184 - acc: 0.4615 - val_loss: -11.6147 - val_acc: 0.2000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s - loss: -10.1934 - acc: 0.4615 - val_loss: -12.2407 - val_acc: 0.2000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s - loss: -11.8110 - acc: 0.4615 - val_loss: -12.8204 - val_acc: 0.2000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s - loss: -11.6698 - acc: 0.4615 - val_loss: -13.3997 - val_acc: 0.2000\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s - loss: -12.0171 - acc: 0.4615 - val_loss: -13.9143 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11fc6a6d0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 13 samples, validate on 5 samples\n",
      "Epoch 1/20\n",
      "13/13 [==============================] - 0s - loss: -12.0416 - acc: 0.4615 - val_loss: -14.4971 - val_acc: 0.2000\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s - loss: -12.5488 - acc: 0.4615 - val_loss: -15.2021 - val_acc: 0.2000\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s - loss: -12.9203 - acc: 0.4615 - val_loss: -15.7227 - val_acc: 0.2000\n",
      "Epoch 4/20\n",
      "13/13 [==============================] - 0s - loss: -13.1556 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 5/20\n",
      "13/13 [==============================] - 0s - loss: -13.2421 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 6/20\n",
      "13/13 [==============================] - 0s - loss: -13.2583 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 7/20\n",
      "13/13 [==============================] - 0s - loss: -13.2970 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 8/20\n",
      "13/13 [==============================] - 0s - loss: -13.1605 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 9/20\n",
      "13/13 [==============================] - 0s - loss: -13.3207 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 10/20\n",
      "13/13 [==============================] - 0s - loss: -13.0731 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 11/20\n",
      "13/13 [==============================] - 0s - loss: -13.3207 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 12/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 13/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 14/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 15/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 16/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 17/20\n",
      "13/13 [==============================] - 0s - loss: -12.9589 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 18/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 19/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n",
      "Epoch 20/20\n",
      "13/13 [==============================] - 0s - loss: -13.4897 - acc: 0.4615 - val_loss: -15.9424 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c288f90>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
