{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Kojak\n",
    "\n",
    "** The problem  **\n",
    "\n",
    "We will attempt to identify amboguously defined words - words that are homographs (spelled the same, but with multiple meanings) and determine the exact meaning of the word from a context window.\n",
    "\n",
    "Here we attempt to do this in a few stages\n",
    "1. train a word embedding on some training corpus using skip-gram (Here we use 1000 sholarly research papers) \n",
    "2. identify common homographs and extract the various context windows\n",
    "3. interpret the context windows as vectors in the embedding space and appy a clustering algorith (DBSCAN). Each cluster is interpreted as a distinct definition of the homograph. Each cluster then is representative vector.\n",
    "4. apply to a test corpus - match context of given homograph to most similar group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook\n",
    "\n",
    "Loads a pre-trained word embedding model, uses DBSCAN clustering to identify several 'definitions' of a set of homographs, and saves those definitions for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare stopwords, preprocess the data from source file\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop += ['?','!',':',';','[',']','[]','“' ]\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"',\"''\",'\"\"',\"``\",'”', '“', '?', '!', '’', 'et', 'al', 'al.']\n",
    "stop = set(stop)\n",
    "\n",
    "class MyPapers(object):\n",
    "    # a memory-friendly way to load a large corpora\n",
    "     def __init__(self, dirname):\n",
    "            self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "        with open(self.dirname) as data_file:    \n",
    "            data = json.load(data_file)\n",
    "        # iterate through all file names in our directory\n",
    "        for paper in data:\n",
    "            sentences = tokenize.sent_tokenize(paper)\n",
    "            for line in sentences:\n",
    "                try:\n",
    "                    line = [word for word in paper['full_text'].lower().split() if word not in stop]\n",
    "                    line = [re.sub(r'[?\\.,!:;\\(\\)“\\[\\]]',' ',l) for l in line]\n",
    "                    yield line\n",
    "                except:\n",
    "                    print(\"Empty line found\")\n",
    "                    continue\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "Import word2vec word embeddings trained on 2848 scholarly journal articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.word2vec.Word2Vec.load(\"data/journal.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2848"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207432"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** contexts to vectors **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "# The function takes as arguments a list of tokenized documents and a window size\n",
    "# and returns each word in the document along with its window context as a tuple\n",
    "\n",
    "def generate_word_counts(documents, window_size = 6):\n",
    "    maxlen = window_size*2\n",
    "    counts = Counter()\n",
    "    \n",
    "    for document in documents:\n",
    "        L = len(document)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(document):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            context_words = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    #in_words.append([word])\n",
    "                    context_words.append(document[i])\n",
    "            x = word\n",
    "            y = context_words\n",
    "            counts[x] += 1\n",
    "            for _ in y:\n",
    "                counts[_] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "# Takes list of word tokens as arguments\n",
    "# Returns a list of vectors whose components are the arithmetic mean of the \n",
    "# corresponding component of all of the input vectors\n",
    "\n",
    "def get_vectors(word_list):\n",
    "    vecs = []\n",
    "    for word in word_list:\n",
    "        vecs.append(vectors[word])        \n",
    "    return vecs\n",
    "\n",
    "# Takes list of vectors as arguments\n",
    "# Returns a single vector whose components are the arithmetic mean of the \n",
    "# corresponding component of all of the input vectors\n",
    "\n",
    "def vector_average(vector_list):\n",
    "    A = np.array(vector_list)\n",
    "    dim = A.shape[0]\n",
    "    ones = np.ones(dim)\n",
    "    return ones.dot(A)/dim\n",
    "\n",
    "# Takes list of tokenized documents, target word and window size as arguments\n",
    "# Returns list of vectors where each vector represents the context window \n",
    "# of the target word in the word embedding space\n",
    "\n",
    "def context2vectors(documents,target,window_size = 6):\n",
    "\n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        if target in document:\n",
    "            windows = generate_windows([document],window_size)\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append(vector_average2(get_vectors(w[1])))\n",
    "                    \n",
    "    return context_vectors\n",
    "# Takes list of vectors as arguments\n",
    "# Returns a single vector whose components are the arithmetic mean of the \n",
    "# corresponding component of all of the input vectors weighted by Inverse Document Frecuency\n",
    "\n",
    "def vector_average2(words): #, word_counts, vectors):\n",
    "    total = sum(list(word_counts.values()))\n",
    "    words = [x for x in words if x in list(vectors.vocab.keys())]\n",
    "    vector_list = list(map((lambda x: vectors[x]*np.log((1 + total)/(1 + word_counts[x]))),words))\n",
    "    \n",
    "    if len(vector_list) == 0:\n",
    "        return 0\n",
    "    elif len(vector_list) == 1:\n",
    "        vector_sum = vector_list[0]\n",
    "    else:\n",
    "        vector_sum = reduce((lambda x,y: np.add(x,y)),vector_list)\n",
    "        \n",
    "    weighted_average = (1.0/len(words))*vector_sum\n",
    "    \n",
    "    return weighted_average\n",
    "\n",
    "# Takes list of tokenized documents, target word and window size as arguments\n",
    "# Returns list of vectors where each vector represents the context window \n",
    "# of the target word in the word embedding space\n",
    "\n",
    "def context2vectors2(documents,target,window_size = 6):\n",
    "\n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        if target in document:\n",
    "            windows = generate_windows([document],window_size)\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append(vector_average2(w[1]))\n",
    "                    \n",
    "    return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Instantiate iterable on the data\n",
    "\n",
    "#papers is an iterable of scholarly papers, tokenized for prcessing\n",
    "papers = MyPapers('data/train_data.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MyPapers_plus(papers):\n",
    "    \n",
    "    phrases = gensim.models.phrases.Phrases(sentences = papers, min_count = 5, threshold = 150)\n",
    "    bigram = gensim.models.phrases.Phraser(phrases)\n",
    "    phrases2 = gensim.models.phrases.Phrases(sentences = bigram[papers], min_count = 5, threshold = 300)\n",
    "    trigram = gensim.models.phrases.Phraser(phrases2)\n",
    "    \n",
    "    return trigram[bigram[papers]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = generate_word_counts(MyPapers_plus(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts['new_york_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.dictionary.Dictionary(MyPapers_plus(papers))\n",
    "text = [dictionary.doc2bow(c) for c in MyPapers_plus(papers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Clustering with DBSAN **\n",
    "\n",
    "Use DBSCAN to determine similar usages of the target homographs. Each of these similar usages will be combined to a representative vector in the embedding space and constitute a \"definition\" of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function takes as arguments a list of tokenized documents and a window size\n",
    "# and returns each word in the document along with its window context as a tuple\n",
    "\n",
    "def generate_windows(documents, window_size):\n",
    "    maxlen = window_size*2\n",
    "    \n",
    "    for document in documents:\n",
    "        L = len(document)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(document):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            context_words = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    #in_words.append([word])\n",
    "                    context_words.append(document[i])\n",
    "            x = word\n",
    "            y = context_words\n",
    "\n",
    "            yield(x,y)\n",
    "            \n",
    "\n",
    "#Arguments: The desired cluster number, a list of documents making up the corpus, the target homograph\n",
    "#           a list of labels for the conext sentences indicating the homographs usage, and window size\n",
    "# The function prints the representative context windows for the target word within the desired cluster   \n",
    "\n",
    "def print_cluster_context(cluster_number, documents, target, labels, window_size = 6):\n",
    "    \n",
    "    context_vectors = []\n",
    "\n",
    "    for document in documents:\n",
    "        text = document\n",
    "        if target in text:\n",
    "            #print(target)\n",
    "            windows = generate_windows([text],window_size)\n",
    "            #print windows[:2]\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_vectors.append((w[1]))\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == cluster_number:\n",
    "            print(context_vectors[i])\n",
    "            \n",
    "#Arguments: The desired cluster number, a list of documents making up the corpus, the target homograph\n",
    "#           a list of labels for the conext sentences indicating the homographs usage, and window size\n",
    "# The function prints the representative context windows for the target word within the desired cluster   \n",
    "\n",
    "def cluster_context(documents, target, labels, window_size = 6):\n",
    "    \n",
    "    context_windows = []\n",
    "\n",
    "    for document in documents:\n",
    "        text = document\n",
    "        if target in text:\n",
    "            #print(target)\n",
    "            windows = generate_windows([text],window_size)\n",
    "            #print windows[:2]\n",
    "            for w in windows:\n",
    "                if w[0] == target:\n",
    "                    context_windows.append((w[1]))\n",
    "                    \n",
    "    cluster_windows = defaultdict(list)                \n",
    "    for i, label in enumerate(labels):\n",
    "        cluster_windows[label].append(context_windows[i])\n",
    "    \n",
    "    return cluster_windows\n",
    "\n",
    "# Arguments: List of vectors, each representing a context window and a list of labels \n",
    "#            corresponding to the context vectors\n",
    "# Returns:   A dictionary where keys are the identified labels from clustering and the value is a single \n",
    "#            representing the cluster\n",
    "\n",
    "def identify_definition(context_vectors, labels):\n",
    "    \n",
    "    cluster_numbers = set(labels)\n",
    "    definitions = dict()\n",
    "    cluster_vectors = defaultdict(list)\n",
    "    if len(set(labels)) == 1:\n",
    "        print(\"No consistent definition found\")\n",
    "        definitions[0] = np.zeros(len(context_vectors[0]))\n",
    "        return definitions\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        cluster_vectors[label].append(context_vectors[i])\n",
    "    \n",
    "    for key in cluster_vectors.keys():\n",
    "        if key < 0:\n",
    "            continue\n",
    "        else:\n",
    "            v = vector_average(cluster_vectors[key])\n",
    "            definitions[key] = v/np.linalg.norm(v)\n",
    "                    \n",
    "    return definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = u'train'\n",
    "context_vectors = context2vectors2(MyPapers_plus(papers), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = .12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='brute', eps=0.12, leaf_size=30, metric='cosine',\n",
       "    min_samples=5, n_jobs=1, p=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps = epsilon, metric = 'cosine', algorithm = 'brute', min_samples = 5)\n",
    "dbscan.fit(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "labels = dbscan.labels_\n",
    "n_clusters = len(set(labels)) # - (1 if -1 in labels else 0)\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1,\n",
       "        1, -1, -1, -1,  1,  0, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1,\n",
       "       -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1,  0,  0,  0,\n",
       "        0, -1, -1,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1,  1, -1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_definitions = identify_definition(context_vectors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contexts = cluster_context(MyPapers_plus(papers), target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording Definitions\n",
    "\n",
    "Use the DBSCAN clusters to determine the various definitions of a word, then create a dictionary for the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_window(window, dictionary):\n",
    "    cosine_dists = []\n",
    "    wv = vector_average2(window)\n",
    "    window_vector = wv/np.linalg.norm(wv)\n",
    "    for k,v in dictionary.items():\n",
    "        cosine_dists.append((1 - np.dot(window_vector, v),k))\n",
    "    cosine_dists.sort()\n",
    "    print(cosine_dists)\n",
    "    \n",
    "    return cosine_dists[0][1]\n",
    "    \n",
    "def extract_dictionary(papers, homographs):\n",
    "    dictionary = dict()\n",
    "    for word in homographs:\n",
    "        print(\"Calculating context vectors for {}\".format(word))\n",
    "        context_vectors = context2vectors2(papers, word)\n",
    "        print(\"Clustering...\")\n",
    "        dbscan = DBSCAN(eps = epsilon, metric = 'cosine', algorithm = 'brute', min_samples = 5)\n",
    "        dbscan.fit(context_vectors)\n",
    "        labels = dbscan.labels_\n",
    "        print(\"found {} distinct definitions\".format(len(set(labels))))\n",
    "        print(\"Building definition for {}\".format(word))\n",
    "        dictionary[word] = identify_definition(context_vectors, labels)\n",
    "        \n",
    "    print(\"Dictionary complete\")    \n",
    "    return dictionary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating context vectors for attribute\n",
      "Clustering...\n",
      "Calculating context vectors for bank\n",
      "Clustering...\n",
      "Calculating context vectors for charge\n",
      "Clustering...\n",
      "Calculating context vectors for feet\n",
      "Clustering...\n",
      "Calculating context vectors for second\n",
      "Clustering...\n",
      "Calculating context vectors for train\n",
      "Clustering...\n"
     ]
    }
   ],
   "source": [
    "homographs = ['attribute', 'bank', 'charge', 'second', 'train']\n",
    "\n",
    "dictionary = extract_dictionary(papers, homographs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['bank'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.07683784553169537, 1), (0.22122995662929146, 0)]\n",
      "[(0.064747383698259942, 1), (0.31271402436150497, 0)]\n",
      "[(0.070376832753871299, 1), (0.28017731437017335, 0)]\n",
      "[(0.05249510644251687, 1), (0.21839005681020573, 0)]\n",
      "[(0.060763873207947849, 1), (0.26094942894628526, 0)]\n",
      "[(0.032630030628247209, 1), (0.21364742175876938, 0)]\n",
      "[(0.14818864957332134, 0), (0.3066886185737927, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['obtained', 'means', 'local', 'field', 'potential', 'spike', 'analysis ', 'wide_range', 'topological', 'network', 'measures', ' 14'] \n",
      "\n",
      "[(0.13866049494623311, 0), (0.29864189469882385, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['putting', 'probes', 'local', 'field', 'potential', 'spike', 'analysis ', 'aim', 'reconstruct', 'functional', 'time-varying', 'brain'] \n",
      "\n",
      "[(0.18461941947731031, 1), (0.27436335460479133, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['improving', 'access', 'psychological', 'therapies', ' iapt ', 'developed', 'low', 'intensity', 'psychological', 'wellbeing', 'practitioners', ' pwps '] \n",
      "\n",
      "[(0.16149103702921819, 0), (0.29188197546836037, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['endpoint', 'detection', 'detect', 'learned', ' in', 'examples', 'high', 'voltage', 'sockets  ', 'hypothesis', 'cannot', 'reproduce'] \n",
      "\n",
      "[(0.13095840723514629, 0), (0.20831646862447939, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['purposes carozza', '  ', 'employs', 'use', 'virtual_reality', 'goggles', 'construction', 'site', 'workers', 'forming', 'ability', 'identify'] \n",
      "\n",
      "[(0.1647418025740377, 1), (0.21284007892888912, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['construction', 'plant', 'equipment', 'represented', 'virtual', 'environment', 'students', 'increase', 'learning', 'experience ', 'showed', 'improvement'] \n",
      "\n",
      "[(0.10231111353994982, 0), (0.23017283841161007, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['drawings', 'used', 'create', '3d', 'model', 'simulation', 'construction', 'workers', 'hazard', 'recognition ', 'results', 'indicated'] \n",
      "\n",
      "[(0.19108702921290155, 0), (0.22339260984369114, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['  ', 'examined', 'use', 'multi-user', 'serious', 'game', 'workers', 'risk', 'identification ', 'game', 'provided', 'multiple'] \n",
      "\n",
      "[(0.12116132227240795, 0), (0.1736330689531298, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['simulator', 'using', 'immersive', 'virtual_reality', 'technology', 'used', 'participants', 'risk', 'identification', 'prevention ', 'results', 'indicated'] \n",
      "\n",
      "[(0.12811289877719767, 1), (0.22478598725331189, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['  ', 'argue', 'traditional', 'teaching', 'methods', 'used', 'educate', 'workers', 'main', 'source', 'education', 'engineering'] \n",
      "\n",
      "[(0.10098846288433239, 0), (0.13119810579304803, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['training ', 'outlined', 'approach', 'developing', 'ar', 'system', 'novice', 'operators', 'real', 'worksite', 'environment', 'populated'] \n",
      "\n",
      "[(0.11366347055994408, 0), (0.18855462524121636, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['similar', 'images', 'current', 'viewpoint ', 'also ', 'plan', 'cnns', 'specifically', 'task', 'lcd', 'dynamic', 'environment'] \n",
      "\n",
      "[(0.18705177184781818, 0), (0.28630609109274197, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['later', 'improved', 'achieve', 'f1', 'score', '0 77', '0 54', 'test', 'data', 'based', 'approach paedophilia ', 'whilst'] \n",
      "\n",
      "[(0.14410757347873537, 0), (0.23036309481270689, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['approach', 'requires', 'one', 'image', 'sample', 'character', 'recognition', 'engine our', 'current', 'system', 'allows', 'people'] \n",
      "\n",
      "[(0.11538668650900008, 1), (0.24560859681898206, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['applications ', 'intel', 'supports', 'programs', 'initiatives', 'designed', 'classroom', 'teachers', 'integrate', 'technology', 'lessons', 'promote'] \n",
      "\n",
      "[(0.11255588285777707, 0), (0.27089262092977229, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['embedded', 'feature', 'space', 'favor', 'task', 'hand ', 'proposed', 'method', 'using', 'labeled_unlabeled', 'data', 'evaluate'] \n",
      "\n",
      "[(0.12739714389891832, 0), (0.1856933250005206, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['pass', 'small', 'sample', 'expert', 'hand-labeling ', 'then ', 'semi-supervised', 'learner', 'labeled_unlabeled', 'data', 'turn', 'lets'] \n",
      "\n",
      "[(0.17969553793928039, 1), (0.19346175636006901, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['measuring', 'glucose', 'levels ', 'children', 'help', 'jerry', 'star', 'games', 'sequence', 'animated', 'storybooks', 'play'] \n",
      "\n",
      "[(0.20790029053875014, 0), (0.23697466595880068, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['body', 'composition', 'measurement ', 'since', 'endurance_athletes', 'often', 'several', 'times', 'per_day ', 'might', 'difficult', 'assure'] \n",
      "\n",
      "[(0.1232824581205193, 1), (0.33909585336354353, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " [' zeid', 'al  ', '  another', 'challenge', 'stem', 'education', 'teachers', 'teach', 'integrated_stem', 'schools ', 'schools ', 'common'] \n",
      "\n",
      "[(0.1090826257456563, 0), (0.26790601308910045, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['promising', 'results ', 'however', 'main', 'drawback', 'necessity', 'algorithm', 'first', 'using', 'training_set ', 'depending', 'various'] \n",
      "\n",
      "[(0.12302189001657471, 0), (0.31181303984166631, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['wind_farm', 'scada', 'used ', 'respectively ', 'input', 'output', 'anfis', 'stage ', 'then ', 'forecasted', 'wind_speed', 'anfis'] \n",
      "\n",
      "[(0.14325417910460181, 0), (0.35303820203758784, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['  ', 'results', 'in different', 'types', 'kernels', 'used', 'svms ', 'commonly', 'used', 'kernel', 'functions', 'power'] \n",
      "\n",
      "[(0.21252445011897203, 0), (0.23329081633167914, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['significantly', 'reduced', 'changes', 'border', 'procedures', 'set', '9/11 ', 'findings', 'add', 'literature', 'importance', 'border'] \n",
      "\n",
      "[(0.13799621817215557, 1), (0.23703563917988268, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['different', 'perspectives ', 'concurrence', 'also', 'reached', 'need', 'fishermen', 'fishery', 'management ', 'fishermen', 'would', 'share'] \n",
      "\n",
      "[(0.1678567350791853, 0), (0.19322221356086278, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['parties', ' supplier', 'manufacture ', 'repair', 'maintain', 'machineries', 'worker ', 'also ', 'much', 'idle', 'time', 'means'] \n",
      "\n",
      "[(0.20907117627360283, 1), (0.21873765063698325, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['competition ', 'instance ', 'roro', 'services', 'also', 'compete', 'links', ' mostly', 'relevant', 'channel', 'tunnel ', 'roro'] \n",
      "\n",
      "[(0.14182448854660357, 1), (0.21814800276531932, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['paradox', 'responsible', 'future', 'absence', 'positions', 'futile', 'anyone', ' streeck ', '  ', 'shall', 'return', 'conundrum'] \n",
      "\n",
      "[(0.17120352054798871, 0), (0.20147033393878011, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['yelp’s', 'filtered', 'reviews', 'proxy', 'review', 'fraud', 'neural_network', 'predicting', 'social', 'activeness ', 'social', 'activeness'] \n",
      "\n",
      "[(0.090189657812858925, 0), (0.21368945098853398, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['identify', 'fraudulent', 'users', 'platform ', 'case', 'yelp ', 'supervised', 'model', 'using', 'neural_network', 'backpropagation', 'algorithm'] \n",
      "\n",
      "[(0.15247322092940763, 0), (0.25233644283418288, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['extend', 'user', 'behavioral', 'feature', 'set ', 'however ', 'model', 'social', 'interaction', 'features ', 'results', 'thus'] \n",
      "\n",
      "[(0.16794683959533752, 0), (0.18915389561202067, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['examine', 'measurement', 'properties', 'instrument ', 'found', 'easy', 'observers', 'use', 'fcom', 'easy', 'use', 'classroom '] \n",
      "\n",
      "[(0.15263727488572942, 0), (0.23473013465342052, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['patterns', 'massive', 'amounts', 'data ', 'critical', 'able', 'large', 'models', 'fulfill', 'needs', 'recent_advances', 'machine_learning'] \n",
      "\n",
      "[(0.11684529139633515, 0), (0.323368095823379, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['optimization', 'method', 'effectively', 'used', 'parameter', 'estimation', 'various', 'machine_learning', 'models ', 'number', 'parameters', 'increase '] \n",
      "\n",
      "[(0.12039300042682011, 0), (0.33613556771446462, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['wide_range', 'machine_learning', 'algorithms', 'use', 'optimization', 'methods', 'model', 'parameters', '   ', 'algorithms ', 'training', 'phase'] \n",
      "\n",
      "[(0.11856869009815174, 0), (0.32976169328304517, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['function ', 'created', 'based', 'parameters ', 'needs', 'optimized', 'model ', 'optimization', 'method', 'finds', 'parameter', 'values'] \n",
      "\n",
      "[(0.13027158897075464, 0), (0.23119206620911781, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['science ', 'optimization', 'methods', 'proving', 'vital', 'order', 'models', 'able', 'extract', 'information', 'patterns', 'huge'] \n",
      "\n",
      "[(0.12216747083294965, 0), (0.24653468179676929, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['analytics ', 'critical', 'able', 'scale', 'machine_learning', 'techniques', 'large-scale', 'models', '   ', 'addition ', 'recent', 'breakthroughs'] \n",
      "\n",
      "[(0.16146583311439433, 0), (0.3135248405301535, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['learning', 'rate', 'convergence', 'criteria ', 'standard', 'approach', 'model', 'different', 'parameters', 'test', 'validation', 'dataset '] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[(0.13322873440308158, 0), (0.30959955519254356, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['similarities', 'uses', 'kernel', 'drug–target_pairs', 'known', 'labels', 'svm-classifier ', 'approaches', 'presented', ' – ', 'represent', 'drug–target'] \n",
      "\n",
      "[(0.1161721657550332, 0), (0.24728606287478516, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['simplicity ', 'general', 'recommendation', 'efficiently', 'defining_ad', 'would', 'powerful', 'classifier', 'use', 'built-in', 'class', 'probability'] \n",
      "\n",
      "[(0.12059867808953206, 1), (0.29381687699642811, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['started', 'change', '2005 ', 'government', 'launched', 'program', 'professors', 'identifying', 'gifted', 'students ', '2010 ', 'example '] \n",
      "\n",
      "[(0.12192031457530117, 0), (0.24772056067143344, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['recognition ', 'authors', 'divide', 'input', 'image', 'patches', 'convolutional_neural', 'network', ' cnn ', 'patch', 'fashion', 'find'] \n",
      "\n",
      "[(0.11181050739496978, 0), (0.28072215698793523, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['predictions', 'create', 'image', 'level', 'histogram', 'used', 'logistic_regression', 'classifier ', 'es', 'method we', 'presented', 'neural_network'] \n",
      "\n",
      "[(0.12222683794592015, 1), (0.2142033795870335, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['far', 'afield', 'los_angeles', 'calcutta ', 'united', 'goal', 'conscientiously', 'better', 'serve', 'dharma ', 'receiving', 'full'] \n",
      "\n",
      "[(0.13508348103410683, 1), (0.34240551771758421, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['university', 'vocational', 'programs', 'require', 'communication', 'skills ', 'students', 'communication', 'skills ', 'doctor ', 'psychologist', 'veterinarian'] \n",
      "\n",
      "[(0.11524349628979969, 1), (0.15105732054110677, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['transfer', ' hannon ', '  ', 'simulate', 'experiential', 'situations', 'medical', 'students', ' amer ', 'mur ', 'amer ', '&'] \n",
      "\n",
      "[(0.15391084141768618, 1), (0.19614297562503091, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['acquire', 'skills', 'unnatural', 'situations ', 'ability', 'transfer', 'skills', 'situations', 'difficult ', 'alternatives', 'discussed', 'concept'] \n",
      "\n",
      "[(0.1858619698744558, 0), (0.18884798162882277, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['innovative', 'design', 'future', 'transport', 'aero-train', 'partly', 'partly', 'aircraft ', 'vehicle', 'designed', 'wings', 'flies'] \n",
      "\n",
      "[(0.22350272552017236, 1), (0.25040598638569089, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['pool ', 'one', 'freezing', 'cold', 'salt', 'water', 'fishermen ', 'and ', 'course ', 'forum', 'athletes’', 'tournaments '] \n",
      "\n",
      "[(0.2299455325484584, 1), (0.25236604394576112, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['demonstrated', 'significantly', 'higher', 'percentfast ', 'since', 'g3', 'hec', 'all ', 'influence', 'repetition', 'cannot', 'neglected '] \n",
      "\n",
      "[(0.12283963637045014, 1), (0.26316773724385478, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['covering', 'evolution', 'high', 'school ', 'states', 'begin', 'teachers', 'proper', 'implementation', 'ngss ', 'students', 'younger'] \n",
      "\n",
      "[(0.16229259554964082, 1), (0.23398449994287018, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['fact', 'latin-american', 'countries', 'present', 'ideal', 'environments', 'young', 'scientists ', 'countries', 'lack', 'local', 'scientific'] \n",
      "\n",
      "[(0.11366387490713059, 1), (0.22714292533146463, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['climate_change ', 'result ', 'new', 'educational', 'strategies', 'required', 'future_generations', 'scientists', 'well', 'increase', 'public', 'science'] \n",
      "\n",
      "[(0.15136095553844886, 0), (0.25665466483322352, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['methods', 'usually', 'take', 'number', 'labeled', 'samples', 'classifiers ', 'classification', 'accuracy', 'depends', 'selected', 'training'] \n",
      "\n",
      "[(0.17105491429082775, 0), (0.34171402648451621, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['similarity', 'functions', 'concatenated', 'feature', 'vector ', 'used', 'classifier', 'determining', 'whether', 'corresponding', 'two', 'tracks'] \n",
      "\n",
      "[(0.14243217383518247, 0), (0.27353145943108981, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['first ', 'similarities', 'based', 'different', 'descriptors', 'adopted', 'classifier ', 'then ', 'decision', 'made', 'classifier', 'integrated'] \n",
      "\n",
      "[(0.12535552056943122, 0), (0.2213657180092411, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['inconvenient', 'collect', 'large', 'number', 'ecg_heartbeats', 'order', 'reliable', 'biometric', 'system ', 'hence ', 'issue', 'might'] \n",
      "\n",
      "[(0.19455523349082171, 0), (0.30208000649072053, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['sample_size', 'environment', 'allocating', 'small', 'number', 'observations', 'model ', 'chose', 'arbitrary', 'number', '20', 'observations'] \n",
      "\n",
      "[(0.15270361877654171, 0), (0.30377728795167369, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['embedded', 'systems ', 'multiple', 'objects', 'environment', 'used', 'evolved', 'neural_network', 'time '] \n",
      "\n",
      "[(0.14719068709589167, 0), (0.22425185261766556, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['sequence', 'file', 'parsing', 'routine ', 'strand', 'able', 'classify', 'malware', 'data', 'changes while', 'produce', 'winning'] \n",
      "\n",
      "[(0.099297807529133086, 0), (0.22485731018601807, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['krause', 'use', 'publicly_available ', 'noisy', 'data', 'sources', 'generic', 'models', 'vastly', 'improve', 'upon', 'state-of-the-art'] \n",
      "\n",
      "[(0.15293552040256031, 0), (0.31331597948990608, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['monocular', 'camera', 'images', 'independently', 'camera', 'calibration', 'cnn', 'predict', 'probability', 'task-space', 'motion', 'gripper'] \n",
      "\n",
      "[(0.17535952349303341, 0), (0.21031531541220727, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " [' i e  ', 'audio ', 'video ', 'text ', 'offer', 'possibility', 'evaluate', 'different', 'task ', 'i e  ', 'stream', 'independently '] \n",
      "\n",
      "[(0.23324608311959572, 1), (0.25392904269001515, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['sources', 'many', 'developed', 'countries', 'airline', 'transportation ', 'trips ', 'commuting ', 'resource', 'poor', 'countries', 'information'] \n",
      "\n",
      "[(0.26095857124419186, 0), (0.28041907725017823, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['shown', 'bci', 'potential', 'alternative', 'treatment', 'option', 'adhd', 'subjects', ' –  ', 'neurofeedback', 'adhd', 'treatment'] \n",
      "\n",
      "[(0.17354134597081994, 0), (0.2214085889228119, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['proportional', 'relative', 'frequency', 'training_set ', 'creating', 'balanced', 'set', 'undersampling', 'majority', 'class', 'retaining', 'original'] \n",
      "\n",
      "[(0.21706128992853779, 0), (0.24869080742646299, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['pre-diagnosis', 'model', 'considerably', 'fewer', 'data', 'points', 'on ', 'result ', 'likely', 'pre-diagnosis', 'accuracy', 'scores'] \n",
      "\n",
      "[(0.13639896465179691, 0), (0.24814148980403949, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['  ', 'truccolo', '   ', 'modir', '   ', 'spike', 'analysis', 'active', 'neurobiological', 'research', 'area', 'calling'] \n",
      "\n",
      "[(0.11452588823800491, 0), (0.29895268578299028, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['matrix_factorization', 'used', 'machine_learning ', 'adjacency_matrix', 'given', 'goal', 'model', 'tuning', 'matrix', 'latent', 'features', 'way'] \n",
      "\n",
      "[(0.12671857945392151, 0), (0.29003195278188143, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['  ', 'gives', 'methodology', 'extract', 'relevant', 'data', 'station', 'making', 'relationship', 'tables', 'interlocking', 'tables '] \n",
      "\n",
      "[(0.17331999937807785, 0), (0.22634908938815668, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['safety', 'control', 'system', 'supervises', 'correct', 'movement', 'yard ', 'since', 'real', 'time', 'safety', 'critical'] \n",
      "\n",
      "[(0.14516161313115927, 0), (0.29805392573240774, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['triangular', 'maps ', 'intervals', 'referred', 'build', 'terminology each', 'headed', 'interval', 'call', 'engine ', 'good', 'estimates'] \n",
      "\n",
      "[(0.23529267923974495, 0), (0.30170986100385178, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " [' opposing', 'brexit  ', ' uncommitted  ', 'labeled', 'tweets', 'used', 'classifier', 'automatically', 'labeled', 'remaining', 'tweets ', 'tweet'] \n",
      "\n",
      "[(0.17366838955503749, 0), (0.20534145447800178, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['consistency', 'tag', 'application', 'removes', 'requirement', 'retailers', 'resource', 'staff', 'tag', 'items', 'store', ' beck'] \n",
      "\n",
      "[(0.14931409094080594, 1), (0.1561190525629037, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['represent', 'best', 'way', 'teach', 'properly', 'pocus', 'new', 'operators ', 'here ', 'report', 'revision', 'common'] \n",
      "\n",
      "[(0.21420140510528651, 1), (0.23629829293610738, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['doctors', 'trained', 'ultrasound', 'all ', 'actually ', 'decided', 'nurse', 'ultrasound', 'low', 'doctor/population', 'ratio', 'sierra_leone'] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.17383358732182752, 1), (0.20810594065382038, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['learners', 'prepare', 'conventional', 'e-learning', 'material ', 'also', 'aspects', 'image', 'acquisition ', 'unique', 'haptic', 'experience '] \n",
      "\n",
      "[(0.14561275115441918, 0), (0.23537551605605733, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['light', 'railway ', 'connections ', 'bus', 'station ', 'high-speed', 'station also ', 'one', 'aims', 'also', 'create', 'mixed-use'] \n",
      "\n",
      "[(0.11936434968356113, 0), (0.20921706416948826, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['uses', 'embodied', 'tasks ', 'folding', 'cutting', 'paper ', 'two-dimensional', 'three-dimensional', 'spatial_thinking ', 'analyses', 'explored', 'spatial_thinking'] \n",
      "\n",
      "[(0.18542400659298941, 1), (0.21243092638100436, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['embodied', 'spatial', 'training', 'may', 'provide', 'means', 'fundamental', 'cognitive', 'skill ', 'spatial_thinking ', 'turn', 'important'] \n",
      "\n",
      "[(0.19395178355139042, 1), (0.21690514870690492, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['whether', 'authentic ', 'remains', 'determined', 'whether', 'possible', 'people', 'make', 'use', 'physically', 'implausible', 'inconsistencies '] \n",
      "\n",
      "[(0.11271207396995275, 0), (0.26202516704832401, 1)]\n",
      "Cluster -1, defined as 0 \n",
      " ['extending', '70 263', 'm these', 'training', 'data', 'used', 'obia', 'software', ' fa  ', 'identify', 'extract', 'peatland'] \n",
      "\n",
      "[(0.1137951359722732, 1), (0.23695129188290087, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['courses ', 'fosters', 'ability', 'implement', 'assessment', 'types', 'learners reviewing', 'challenges', 'probable', 'solutions', 'implementing_self-assessment_peer-assessment_writing', 'courses '] \n",
      "\n",
      "[(0.1512523605077627, 1), (0.24414162338508105, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['definitions', 'education', 'sustainability', ' efs  ', 'yet', 'common', 'thought', 'appears', 'notion', 'enabling', 'transformative', 'change'] \n",
      "\n",
      "[(0.12760464748931755, 1), (0.31893676325021114, 0)]\n",
      "Cluster -1, defined as 1 \n",
      " ['groups ', 'training', 'means', 'experience ', 'members', 'confidence', 'others', 'likely', 'others', 'come', 'advice', ' davis'] \n",
      "\n",
      "[(0.08801696085699906, 0), (0.23958272648450452, 1)]\n",
      "[(0.082612799581231799, 0), (0.2826635528132766, 1)]\n",
      "[(0.06280800111967455, 0), (0.25913471012337119, 1)]\n",
      "[(0.081599266619847066, 0), (0.2912400400203532, 1)]\n",
      "[(0.10679400181780041, 0), (0.25018497590367939, 1)]\n",
      "[(0.072870306273542074, 0), (0.26632135256993283, 1)]\n",
      "[(0.063907092751236294, 0), (0.30274956476766435, 1)]\n",
      "[(0.062857566960994982, 0), (0.24485694026177152, 1)]\n",
      "[(0.056686818511701986, 0), (0.22163469825588156, 1)]\n",
      "[(0.042822485673962074, 0), (0.25713452928042668, 1)]\n",
      "[(0.04399161986430733, 0), (0.23553173533002347, 1)]\n",
      "[(0.045817020827544797, 0), (0.25871662509245219, 1)]\n",
      "[(0.056833493080000297, 0), (0.23733410280097478, 1)]\n",
      "Number correct: 19\n",
      "Number wrong: 0\n"
     ]
    }
   ],
   "source": [
    "# TEST WINDOW #\n",
    "\n",
    "target = u'train'\n",
    "context_vectors = context2vectors2(MyPapers_plus(papers), target)\n",
    "dbscan = DBSCAN(eps = epsilon, metric = 'cosine', algorithm = 'brute', min_samples = 4)\n",
    "dbscan.fit(context_vectors)\n",
    "labels = dbscan.labels_\n",
    "target_definitions = identify_definition(context_vectors, labels)\n",
    "#target_definitions = dictionary[target]\n",
    "\n",
    "\n",
    "contexts = cluster_context(MyPapers_plus(papers), target, labels)\n",
    "correct = 0\n",
    "wrong = 0\n",
    "for c in contexts:\n",
    "    for window in contexts[c]:\n",
    "        d = define_window(window, target_definitions)\n",
    "        if c == d:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(\"Cluster {}, defined as {} \\n\".format(c, d), window, '\\n')\n",
    "            if c != -1:\n",
    "                wrong += 1\n",
    "                \n",
    "print(\"Number correct: {}\\nNumber wrong: {}\".format(correct, wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, jerks!', \"How's it going?\", 'By, the way.', 'You smell.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "p = \"Hello, jerks! How's it going? By, the way. You smell.\"\n",
    "tokenize.sent_tokenize(p)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
